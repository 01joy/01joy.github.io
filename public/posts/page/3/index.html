<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Posts | bitJoy</title>
<meta name="keywords" content="">
<meta name="description" content="Posts - bitJoy">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.36819bea596090d8b48cf10d9831382996197aa7e4fc86f792f7c08c9ca4d23b.css" integrity="sha256-NoGb6llgkNi0jPENmDE4KZYZeqfk/Ib3kvfAjJyk0js=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/posts/index.xml">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
    
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
    
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="bitJoy (Alt + H)">bitJoy</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    Posts
  </h1>
</header>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">和我一起构建搜索引擎（二）网络爬虫
    </h2>
  </header>
  <div class="entry-content">
    <p>网络爬虫又称网络蜘蛛、Web采集器等，它是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。
我们在设计网络爬虫的时候需要注意两点：
鲁棒性。Web中有些服务器会制造采集器陷阱（spider traps），这些陷阱服务器实际上是Web页面的生成器，它能在某个域下生成无数网页，从而使采集器陷入到一个无限的采集循环中去。采集器必须能从这些陷阱中跳出来。当然，这些陷阱倒不一定都是恶意的，有时可能是网站设计疏忽所导致的结果。
礼貌性。Web服务器具有一些隐式或显式的政策来控制采集器访问它们的频率。设计采集器时必须要遵守这些代表礼貌性的访问政策。
采集器的基本架构如下图所示。
基本上是“抓取→分析→得到新的URL→再抓取→再分析”这样一个死循环过程。
以上内容摘自王斌老师翻译的《信息检索导论》课本。
由于我们要做的是一个新闻搜索引擎，所以抓取的是新闻数据，对于爬虫，网上也有很多的开源程序，如nutch等，Github上还有人专门开发了抓取新闻的组件newspaper，可以很方便的提取新闻标题、正文、时间等信息。不过用python写爬虫也是分分钟的事情，下面我们一起来试一试。
首先找一个新闻网站，为简单起见，要找那种结构清晰、html代码便于解析的门户网站，比如搜狐新闻、参考消息等。
搜狐新闻的国内要闻列表如下：
结构非常清楚，左边是带URL的标题，右边括号里有新闻时间。这一页列表就有200条新闻，如果我们要获取1000条，只要不断模拟点击下一页即可。下一页的URL也只是在首页的基础上加上_xxx.shtml，xxx就是不同的页码。
查看列表的html源码，得知列表都在类名为newsblue1的td中，所以只需要解析html源码就可以得到新闻标题、URL和时间，python解析html可以用BeautifulSoup包，非常方便。
进入到新闻详细页面，正文部分如下：
查看html源码，正文位于类名为text clear的div中，据此可以很方便的提取新闻正文。
得到一条新闻的所有数据之后，我们需要将之结构化成xml文件，借助相应的xml包可以很方便的完成这项工作。xml格式定义如下：
注意爬虫需要访问网络，难免会出现一些异常，所以捕获异常是非常有必要的。另外，搜狐每篇新闻正文后面都会有一段’//’开始的注释，这个需要过滤掉，短于140个字的新闻我也过滤掉了。整个搜索系统的配置参数都存储在config.ini文件中。
下面是完整的python 3.4&#43;代码。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # -*- coding: utf-8 -*- &#34;&#34;&#34; Created on Sat Dec 19 11:57:01 2015 @author: bitjoy.net &#34;&#34;&#34; from bs4 import BeautifulSoup import urllib.request import xml.etree.ElementTree as ET import configparser def get_news_pool(root, start, end): news_pool = [] for i in range(start,end,-1): page_url = &#39;&#39; if i != start: page_url = root &#43;&#39;_%d.shtml&#39;%(i) else: page_url = root &#43; &#39;.shtml&#39; try: response = urllib.request.urlopen(page_url) except Exception as e: print(&#34;—–%s: %s—–&#34;%(type(e), page_url)) continue html = response.read() soup = BeautifulSoup(html) td = soup.find(&#39;td&#39;, class_ = &#34;newsblue1&#34;) a = td.find_all(&#39;a&#39;) span = td.find_all(&#39;span&#39;) for i in range(len(a)): date_time = span[i].string url = a[i].get(&#39;href&#39;) title = a[i].string news_info = [&#39;2016-&#39;&#43;date_time[1:3]&#43;&#39;-&#39;&#43;date_time[4:-1]&#43;&#39;:00&#39;,url,title] news_pool.append(news_info) return(news_pool) def crawl_news(news_pool, min_body_len, doc_dir_path, doc_encoding): i = 1 for news in news_pool: try: response = urllib.request.urlopen(news[1]) except Exception as e: print(&#34;—–%s: %s—–&#34;%(type(e), news[1])) continue html = response.read() soup = BeautifulSoup(html) try: body = soup.find(&#39;div&#39;, class_ = &#34;text clear&#34;).find(&#39;div&#39;).get_text() except Exception as e: print(&#34;—–%s: %s—–&#34;%(type(e), news[1])) continue if &#39;//&#39; in body: body = body[:body.index(&#39;//&#39;)] body = body.replace(&#34; &#34;, &#34;&#34;) if len(body) &lt;= min_body_len: continue doc = ET.Element(&#34;doc&#34;) ET.SubElement(doc, &#34;id&#34;).text = &#34;%d&#34;%(i) ET.SubElement(doc, &#34;url&#34;).text = news[1] ET.SubElement(doc, &#34;title&#34;).text = news[2] ET.SubElement(doc, &#34;datetime&#34;).text = news[0] ET.SubElement(doc, &#34;body&#34;).text = body tree = ET.ElementTree(doc) tree.write(doc_dir_path &#43; &#34;%d.xml&#34;%(i), encoding = doc_encoding, xml_declaration = True) i &#43;= 1 if __name__ == &#39;__main__&#39;: config = configparser.ConfigParser() config.read(&#39;../config.ini&#39;, &#39;utf-8&#39;) root = &#39;http://news.sohu.com/1/0903/61/subject212846158&#39; news_pool = get_news_pool(root, 854, 849) crawl_news(news_pool, 140, config[&#39;DEFAULT&#39;][&#39;doc_dir_path&#39;], config[&#39;DEFAULT&#39;][&#39;doc_encoding&#39;]) print(&#39;done!&#39;) 完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。
...</p>
  </div>
  <footer class="entry-footer"><span title='2016-01-04 16:23:19 +0800 CST'>January 4, 2016</span>&nbsp;·&nbsp;2 min</footer>
  <a class="entry-link" aria-label="post link to 和我一起构建搜索引擎（二）网络爬虫" href="http://localhost:1313/posts/2016-01-04-introduction-to-building-a-search-engine-2/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">和我一起构建搜索引擎（一）简介
    </h2>
  </header>
  <div class="entry-content">
    <p>我们上网用得最多的一项服务应该是搜索，不管大事小情，都喜欢百度一下或谷歌一下，那么百度和谷歌是怎样从浩瀚的网络世界中快速找到你想要的信息呢，这就是搜索引擎的艺术，属于信息检索的范畴。
这学期学习了《现代信息检索》课程，使用的是Stanford的教材Introduction to Information Retrieval，网上有电子版，大家可以参考。
本课程的大作业是完成一个新闻搜索引擎，要求是这样的：定向采集3-4个新闻网站，实现这些网站信息的抽取、索引和检索。网页数目不少于10万条。能按相关度、时间和热度（需要自己定义）进行排序，能实现相似新闻的自动聚类。
截止日期12月31日，我们已经在规定时间完成了该系统，自认为检索效果不错，所以在此把过程记录如下，欢迎讨论。
网上有很多开源的全文搜索引擎，比如Lucene、Sphinx、Whoosh等，都提供了很好的API，开发者只需要调用相关接口就可以实现一个全功能的搜索引擎。不过既然学习了IR这门课，自然要把相关技术实践一下，所以我们打算自己实现一个搜索引擎。
这是简介部分，主要介绍整个搜索引擎的思路和框架。
上图为本搜索引擎的框架图。首先爬虫程序从特定的几个新闻网站抓取新闻数据，然后过滤网页中的图片、视频、广告等无关元素，抽取新闻的主体内容，得到结构化的xml数据。然后一方面使用内存式单遍扫描索引构建方法（SPIMI）构建倒排索引，供检索模型使用；另一方面根据向量空间模型计算两两新闻之间的余弦相似度，供推荐模块使用。最后利用概率检索模型中的BM25公式计算给定关键词下的文档相关性评分，BM25打分结合时间因素得到热度评分，根据评分给出排序结果。
在后续博文中，我会详细介绍每个部分的实现。
完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。
以下是系列博客：
和我一起构建搜索引擎（一）简介
和我一起构建搜索引擎（二）网络爬虫
和我一起构建搜索引擎（三）构建索引
和我一起构建搜索引擎（四）检索模型
和我一起构建搜索引擎（五）推荐阅读
和我一起构建搜索引擎（六）系统展示
和我一起构建搜索引擎（七）总结展望
</p>
  </div>
  <footer class="entry-footer"><span title='2016-01-04 13:56:23 +0800 CST'>January 4, 2016</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 和我一起构建搜索引擎（一）简介" href="http://localhost:1313/posts/2016-01-04-introduction-to-building-a-search-engine-1/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">认真你就赢了
    </h2>
  </header>
  <div class="entry-content">
    <p>突然发现，从小到大，自己做事都做得很慢，别人一会做完的作业，我可能要花好几个小时。但拿作业一对比，明显能看出差距，自己精雕细琢的作品不是别人随随便便就能比的。
最近几次和同学合作完成大作业也遇到了类似的情况，数据抓取的同学给我的数据，不是格式不对就是内容缺胳膊少腿，质量极其差，还不愿修改，曰：只是做一个演示系统，有数据就行了。他不知道他这样的数据给我，我们后面做得再好，最终的演示效果也不会好，他这样的随意，后面的人不知要多花多少时间来弥补。我也无意跟他多费口舌，自己挽起袖子重做了他的工作。
类似的事情，我遇到的不在少数，和别人沟通的时间远远超过了自己完成任务的时间。所以往往一个很简单的工作，我要花比别人多两到三倍的时间。这个过程就像工匠在雕琢自己的作品，是不计时间的，直到自己认为完美为止。这大概就是老罗所说的工匠精神吧。
在一个完美主义者的眼里，这是一个千疮百孔的世界。
糟糕的文档排版，错别字和错误标点一堆，一群人并排走挡了后面或对面的人，开水房离宿舍十万八千里，蚊香的设计，电脑接口位置的设计，U盘接口的设计，凸出的摄像头，插队，说脏话。。。
当然也有同学劝我，这些东西差不多就行了，何必花这么多时间做这么好干什么，还不如去看个电影打个球。也经常听人说Take it easy，别太认真，认真你就输了。
但是我始终相信，态度决定一切。你一天认真做了，别人不一定看得到，但坚持一个月甚至一年，总会有志同道合的人发现你，而你的坚持也将一点点的改变这个行业这个世界。就像老罗做手机，虽然销量不怎么样，但他的工匠精神、他的情怀，值得每一个人尊敬。T2统一听筒和各种传感器的位置、消失的电源键、消失的SIM卡插槽、消失的金属中框断点完全是超出iPhone的美好设计。希望老罗的情怀之路能够坚持下去、越走越远。
</p>
  </div>
  <footer class="entry-footer"><span title='2016-01-04 12:42:21 +0800 CST'>January 4, 2016</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 认真你就赢了" href="http://localhost:1313/posts/2016-01-04-attitude-is-everything/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">2016新年快乐
    </h2>
  </header>
  <div class="entry-content">
    <p>2015年过得好快，梳理一下，2015年的时间线大概是这样的：
3月来北京计算所做毕设→5月返回武大修改论文→5月30公开答辩→6月毕业季→7月回北京计算所→8月回家陪父母→9月国科大开学→持续高强度的学习→2016元旦还在图书馆研究NPC问题。
2015年给我的总体感受是很忙，但忙的事情都很琐碎，并没有什么大的里程碑事件，不过以下三件事情我认为值得一提。
本科四年修成正果，研究生三年新的起航 买了一辆属于自己的山地车，1k2，虽然是二手的，但足够我骑着它去看世界了:-) 也许是在城市里待久了，我特别享受这种亲近大自然的感觉，蓝天、白云、草原、大海这些美景永远也看不够。
在国科大认识了两个好基友，虽然都是单身汪，但至少想看电影吃火锅的时候还可以有个伴。（此处居然少了三人合照） 2015年共写了14篇博客，包含3篇技术博客，bitjoy.net 历史累计PV1039，UV520，IP502。
展望2016年，大的方向基本都确定了，目标如下：
完成国科大下学期的课程任务 接手pLink软件 刷完LeetCode所有题目 读10本书 去电影院看10场电影（2015下半年在怀柔村里没看一部电影/(ㄒoㄒ)/~~） 改正坐姿 大家一起见证！
</p>
  </div>
  <footer class="entry-footer"><span title='2016-01-03 12:25:50 +0800 CST'>January 3, 2016</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 2016新年快乐" href="http://localhost:1313/posts/2016-01-03-2016-happy-new-year/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">北国的雪
    </h2>
  </header>
  <div class="entry-content">
    <p>
第一次在北方过冬，今年北京11月6日就下雪了，然而我在广州的小伙伴还穿着短袖吃着冰棍呢。。。
北京2015年的第一场雪，比以往时候来的更早一些
今天又下起了第二场雪，下了整整两天的大雪，然而我房间的暖气却不暖了，大叔来修了两次，无功而返，说是一楼的宿舍暖气都有问题，当初设计有缺陷(╯‵□′)╯︵┻━┻
这样也好，给了自己去图书馆的理由❉
</p>
  </div>
  <footer class="entry-footer"><span title='2015-11-22 12:16:00 +0800 CST'>November 22, 2015</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 北国的雪" href="http://localhost:1313/posts/2015-11-22-snow-in-beijing/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">解决Win10间歇性断网的问题
    </h2>
  </header>
  <div class="entry-content">
    <p>安装WIN10一个月以来，校园有线网经常间歇性断网，通常是20分钟不到就断了，需要重启或者把有线连接关闭再打开才可以。在微博上问过微软客服也无果，后来Google到某国外的解决办法，现记录如下。
说到底WIN10断网的问题还是和驱动有关，先看一下我的有线网卡Broadcom NetLink (TM) Gigabit Ethernet，驱动信息是这样的：
还是13年的驱动，版本号是15.6.0.14，于是第一想到的是更新驱动。点击驱动右键选更新-&gt;自动搜索更新的驱动程序软件-&gt;提示“已安装适合设备的最佳驱动程序软件”，但这明明不是最新的驱动啊！
于是在Broadcom的官网上找到了最新驱动win_b57_x64-17.2.0.2，版本号是17.2.0.2，更新日期2015-10-27，原来这才是最新的驱动。
（2018.1.25更新：上面的地址已失效，最新地址请点击此处，并选择DOWNLOADS→Software→NetLink®/NetXtreme® I Desktop/Mobile/Server (x64)，也可以从本站下载。）
在安装最新驱动之前，我们需要关闭WIN10的自动更新驱动功能，因为WIN10会认为它的15.6.0.14版本是最新的，在windows update时把实际最新的17.2.0.2版本替换掉。具体做法是在Cortana中搜索“更改设备安装设置”并打开，选择否，从不安装来自Windows更新的驱动程序软件，如下。
然后重启进入安全模式，再次在设备管理器中右键点击网卡驱动，选择更新-&gt;浏览计算机以查找驱动程序软件-&gt;从计算机的设备驱动程序列表中选取-&gt;点击从磁盘安装按钮-&gt;浏览找到你之前在网上下载的最新驱动（*.inf格式）-&gt;选中-&gt;依次确定。刷新之后再次查看驱动信息如下：
可以看到驱动已经更新到最新的版本了。再次重启进入正常模式，目前用了两天了也没有再断过网。
其他WIN10驱动问题应该也可以用类似的方法解决。
（话说我的WIN10偶尔会死机，就是用着用着突然鼠标和键盘完全动不了了，只能强制重启，有谁知道这是怎么回事吗？）
</p>
  </div>
  <footer class="entry-footer"><span title='2015-11-13 10:55:51 +0800 CST'>November 13, 2015</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 解决Win10间歇性断网的问题" href="http://localhost:1313/posts/2015-11-13-solution-to-win10s-network-problem/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">读施一公博客有感
    </h2>
  </header>
  <div class="entry-content">
    <p>大家好，施一公老师的26篇博客大概可以分为四类：1）讲述个人生活经历2）评论社会问题3）讨论国家科技和人才引进政策4）介绍学习方法。这些博客比较全面地反应了施一公的求学经历、由学生到教授的转变过程以及回国之后为中国人才引进所做出的努力。
给我感触最深的有3点：1）环境对人的影响很大2）坚持总会有所收获3）做一个有担当、有社会责任感的科研人。
1）环境对人的影响很大 《从&lt;高考1977&gt;说起》这篇博客详细介绍了施一公高考前的家庭情况，施一公的父亲是哈工大毕业，母亲是北京矿业学院（今中国矿业大学）毕业，在上世纪五六十年代，父母都是名校大学毕业，可谓是少有的知识分子家庭。施一公还有两个姐姐、一个哥哥、一个表哥和一个表姐，哥哥姐姐们刻苦的学习、父亲悉心的辅导以及不错的高考成绩对施一公产生了很大的影响，争强好胜的施一公自然不甘示弱，以84年全国数学联赛省第一名的成绩保送清华。诚然，施一公的成绩和他自己的刻苦努力分不开，但是从小良好的家庭氛围也功不可没。
2）坚持总会有所收获 这可以从施一公的两个例子中看出。
《今天3000米》讲到施一公从82年跑步的“倒数第一“、“颜面尽失”之后开始坚持长跑，直到89年从未间断，在85年的清华新生运动会3000米竞走中，以16分10秒轻松获得第一名。
跑步这件事我也深有体会，我高中几乎没有体育锻炼，大一刚入学的体能测试中，1000米项目跑了4’15’’，小组倒数第一，跑完全程脸都发白，当时真的担心大学因体育挂科毕不了业。后来大三下的时候，开始坚持跑步，一开始每晚跑3圈，一个月后加一圈，最后稳定在每晚跑5圈，一直坚持到毕业。在毕业体能测试中，还是1000米项目，我居然跑了3’42’’，小组顺数第一名，跑完之后虽然有点累，但并不感觉难受，连我自己都不太相信。
《如何做一名优秀的博士生：（一）时间的付出》中，施一公讲到他在留学期间的时间付出。“留学的第一年，我情绪波动很大，内心浮躁而迷茫，根本无心念书。”“第二年，每周五天、每天从上午9点做实验到晚上7、8点，周末也会去两个半天。””到了第三年，晚上常常干到11点多，赶最后一班校车从霍普金斯医学院回Homewood campus（我住在附近）。””研究生阶段后期，我的刻苦在实验室是出了名的。每天晚上做实验到半夜三点左右，回到住处躺下来睡觉时常常已是四点以后；但每天早晨八点都会被窗外纽约第一大道(First Avenue)上的汽车喧闹声吵醒，九点左右又回到实验室开始了新的一天…”
施一公几年如一日的坚持没有白费，他顺利毕业并获得名校终生教职席位。
其实正如H老师所说“以大多数人努力的程度，根本还没到拼智商的时候。” 坚持做一件事，点滴积累，做到极致。无论什么事情，坚持做下去，一定能有所收获，对于体力活更是如此。
3）做一个有担当、有社会责任感的科研人 在读施一公博客的时候，心潮澎湃，热血沸腾，无论是施一公自己排除万难坚持回国的行动，还是施一公回国之后号召海龟回国的倡议，亦或是施一公为人才引进，千人计划建言献策的付出，都真真切切的体现了他的强烈的爱国热情。
施一公回国后的去私心、敢担当、有作为，坚持职业操守，“我申请基金的时候一定不和评委在评审前或评审后做任何形式的私下沟通；我当评委的时候一定不和申请人在评审前或评审后做任何形式的私下沟通”等都在用切身行动一点点改善国内的科研环境。
在pFind组，H老师也时常教导我们要对学术保留一点敬畏之心，做好科研，尽自己一份力改善国际社会对中国学术界的看法。
总的来说，施一公老师的博客内容丰富，让我受益匪浅，也给了我很多启发，关于如何做一名合格的研究生，我还完全是门外汉，前面的师兄师姐都给出了很多方法论的解读，我也把施一公关于如何做一名优秀的博士生的几个要点罗列如下，希望用此标准来要求自己。
如何一名优秀的博士生：
时间的付出 方法论的转变 正确分析负面结果 耗费时间的完美主义阻碍创新进取 科研文献与学术讲座的取与舍 挑战传统思维 祝大家工作顺利！
-bitJoy
</p>
  </div>
  <footer class="entry-footer"><span title='2015-10-31 10:23:57 +0800 CST'>October 31, 2015</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 读施一公博客有感" href="http://localhost:1313/posts/2015-10-31-review-about-shiyigongs-blogs/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">浮点数知识及Grisu算法介绍
    </h2>
  </header>
  <div class="entry-content">
    <p>进入研究生生涯完成的第一个新生培训作业是“2.5亿个浮点数的外部排序算法”，前后折腾了将近一个月，结果是在i7处理器上，限制512MB内存，排序用时250秒左右。
这个作业的常规思路大部分人都能想到，按块读取文件-&gt;atof转换为double-&gt;内部快速排序或基数排序-&gt;dtoa转换为char*-&gt;按块写入文件。这里面中间的三个过程都很耗时，特别是atof和dtoa，因为精度只要求保留9位小数，所以可以自己实现atof和dtoa来加速，也可以使用多线程加速。
整个作业都是基于对IEEE754浮点数的深刻理解展开的，所以下面详细讲解浮点数的一些知识。
IEEE754双精度浮点数 目前大多数CPU内浮点数的表示都遵循IEEE754标准，IEEE754双精度浮点数（double）表示如下图所示。
IEEE754 double在内存中的形式[1]
sign bit：符号位，1位，用来表示正负号，0表示非负；1表示负 exponent：指数位，11位，用来表示次方数，是一个无符号数 fraction：尾数位，52位，用来表示精确度，也是一个无符号数，有些资料也叫做mantissa或significand 这种表示形式有两点需要注意。
第一，既然exponent是无符号的，那么怎样表示负指数呢？
IEEE754规定，二进制串中算得的e需要减去一个偏移量bias，对于double，bias=1023，即e’=e-bias。因为\(e\in[0,2^{11}-1]\)，所以最终\(e’\in[-2^{10}&#43;1,2^{10}]\)。但是如果把e本身看作有符号数e”，则\(e”\in[-2^{10},2^{10}-1]\)，既然e”和e’相差微小，为什么不直接把e看成有符号数，而非要把它看成无符号数，再减去一个偏移量bias呢？
这是因为如果把e看成无符号数再减偏移量，浮点数大小比较速度更快。引用维基百科的一段话：
By arranging the fields so that the sign bit is in the most significant bit position, the biased exponent in the middle, then the mantissa in the least significant bits, the resulting value will be ordered properly, whether it’s interpreted as a floating point or integer value. This allows high speed comparisons of floating point numbers using fixed point hardware.
...</p>
  </div>
  <footer class="entry-footer"><span title='2015-08-30 20:45:36 +0800 CST'>August 30, 2015</span>&nbsp;·&nbsp;4 min</footer>
  <a class="entry-link" aria-label="post link to 浮点数知识及Grisu算法介绍" href="http://localhost:1313/posts/2015-08-30-introduction-to-floating-point-numbers-and-grisu-algorithm/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">百度图片批量下载器（python3 &#43; pyqt5 &#43; eric6 &#43; cx_Freeze4）
    </h2>
  </header>
  <div class="entry-content">
    <p>去年暑假在北大计算所实习的时候，任务之一就是批量下载百度图片。当时没学python，用c#实现了一个简易版本的批量下载器，如下图。
C#版本百度图片批量下载器（抓的是百度的wap站点，现在好像不能用了）
当时“时间紧，任务重“，既没仔细研究百度图片API，也没处理好界面线程阻塞的问题。这个问题其实很有意思，趁着暑假在家，实现了一个比较完美的python版本，先上效果图。
python3版本百度图片批量下载器
新版使用了python-3.4.3.amd64.msi &#43; PyQt5-5.5-gpl-Py3.4-Qt5.5.0-x64.exe &#43; eric6-6.0.8.zip &#43; cx_Freeze-4.3.4-cp34-none-win_amd64.whl，完整项目在我的GitHub上。大致有如下几点工作：
研究百度图片API，获取原始图片URL列表 使用python3进行多线程下载 利用pyqt5实现界面 利用cx_Freeze4打包整个程序 下面记录每个步骤的要点，供后人参考。
百度图片API 正常使用百度图片搜索的时候，URL是这样的：
http://image.baidu.com/search/index?ct=201326592&amp;z=0&amp;tn=baiduimage&amp;ipn=r&amp;word=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6&amp;pn=0&amp;istype=2&amp;ie=utf-8&amp;oe=utf-8&amp;cl=2&amp;lm=-1&amp;st=-1&amp;fr=&amp;fmq=1439374041843_R&amp;ic=0&amp;se=&amp;sme=&amp;width=0&amp;height=0&amp;face=0
里面有很多参数，有些我们并不需要，精简之后大概是这样的：
http://image.baidu.com/i?tn=baiduimage&amp;ie=utf-8&amp;word=%E7%BE%8E%E5%A5%B3&amp;pn=&amp;rn=&amp;z=
word为搜索关键词；pn为page number当前是第几页，实际含义是image id，表示第几张图片，从0开始；rn为每一页的图片数量，最大为60；z表示图片尺寸，z=9特大尺寸，z=3大尺寸，z=2中等尺寸，z=1小尺寸，z=0所有尺寸。
但是这个URL是给”人“看的，下一页的图片是动态加载的，其html代码的图片URL数量固定。一番查询之后发现，将tn=baiduimage换成tn=resultjson_com能获取到所有图片URL的json，json当然是给”猴“看的，这样就能轻松获取到所有图片的URL。
慢着，仔细看看json中的objURL，是一串连”猴“都看不懂的字符串，原来百度把图片真实URL加密了，好在加密方法是简单的字符映射，参考这篇博客成功解密。
更新：tn=resultjson_com的objURL是加密了，但是tn=resultjson的objURL并没有加密，所以采用tn=resultjson最佳。
通过控制pn和rn就能获取指定数量的图片URL，但是我发现rn最大只能为60，并且不同的pn可能会有相同的图片url（比如pn=0和pn=1都有ippr_z2C$qAzdH3FAzdH3Fooo_z&amp;e3Bd8vs7k_z&amp;e3Bv54_z&amp;e3BvgAzdH3F7rs5w1utsjAzdH3Fda8nAzdH3Fa080AzdH3Fda8na080aldm9bb8m_z&amp;e3B3r2这个objURL），所以使用python的集合（set）去重。
更新：pn实际上指图片的id，pn=0、rn=60能获取到从0~59这60个URL列表，pn=1、rn=60能获取到从1~60这60个URL列表，所以pn=0和pn=1的列表中当然有59个是重复的。正确的做法是pn=0、rn=60获取0~59这60个URL列表，然后pn=60、rn=60获取60~119这60个列表，以此类推，这样获取到的URL就不会有重复的了。
获取图片URL列表的简要代码如下：
1 2 3 4 5 6 7 8 9 10 11 def ParseJSON(self, pn, rn, st): url = &#39;http://image.baidu.com/i?tn=resultjson_com&amp;amp;amp;ie=utf-8&amp;amp;amp;word=%s&amp;amp;amp;pn=%d&amp;amp;amp;rn=%d&amp;amp;amp;z=%d&#39;%(self.word, pn, rn, self.size) #print(url) request = urllib.request.Request(url = url, headers = my_header) html = urllib.request.urlopen(request).read() hjson = json.loads(html.decode(&#39;gbk&#39;)) for i in range(0, len(hjson[&#39;data&#39;])-1):#最后一个数据为空 img_url = self.DecodeURL(hjson[&#39;data&#39;][i][&#39;objURL&#39;]) if img_url not in st: st.add(img_url)#去重 self.progressBar_updated_signal.emit()#更新进度条 DecodeURL是解密函数。很奇怪，json最后一个数据是空的。
...</p>
  </div>
  <footer class="entry-footer"><span title='2015-08-13 17:27:15 +0800 CST'>August 13, 2015</span>&nbsp;·&nbsp;4 min</footer>
  <a class="entry-link" aria-label="post link to 百度图片批量下载器（python3 &#43; pyqt5 &#43; eric6 &#43; cx_Freeze4）" href="http://localhost:1313/posts/2015-08-13-baidu-image-downloader-python3-pyqt5-eric6-cx_freeze4/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">再见武大
    </h2>
  </header>
  <div class="entry-content">
    <p>2015年6月27日，武汉大学在梅园操场举办了2015年毕业典礼。
武汉大学2015年毕业典礼，正在发言的是雷军学长[1]
武汉大学2015年毕业典礼[2]
坐在台下，顿感恍惚，四年前同样在梅操，同样是这些人，我们举办开学典礼。
武汉大学2011年开学典礼[3]
原来梅操到梅操的距离只有四年。
毕业典礼结束，下午集体收拾行李，大家都默默无语。晚上去红牛—-大一第一次聚餐的地方—-吃最后的晚餐，这次聚餐喝了两箱啤酒，6瓶白酒！白酒下肚，前一口酒落地又向上翻滚，和后一口酒相互撞击，四年的往事喷涌而出。离别之际，每个人都把自己的心声说出来了，说出了自己的家境、对某某的感情、一个宿舍的兄弟情，说出了自己的抱负、未来的理想，再互相拥抱、道一声珍重。
山水一程，三生有幸[4]
回到宿舍，所有人吐得一塌糊涂，昏睡过去。也许这就是离别的滋味，折磨着你，让你难受，只有把它吐出来，离开了，平静了，一切就好了。
第二天醒来，发现隔壁宿舍的几个哥们已经走了，宿舍冷清了许多。去小卖部买了一些非必需品，只为把卡里的几十块钱用掉。打包行李，准备出发。
毕业了，离开了，那些大一的迷茫、兼职，大二的信息安全竞赛，大三繁重的课业、为保研奋斗的数学建模竞赛，大四悠闲的生活也将躲藏在记忆的某个角落，不再被轻易的发现。武大的樱花、牌坊、樱顶、新图、青楼、梅操电影、珞珈之声、每天晚上在奥场穿着17号球衣跑步的女生、一起练笛的同学、在梅园食堂吃饭的一对情侣、幽默装逼的室友，这一幅幅画面，也将随着时间的车轮，慢慢消散。
天下没有不散的筵席，我们来到这个世上，就注定要历经悲欢离合。在中国最美丽的大学，度过了我人生中最美好的年华，山水一程，三生有幸，感谢有你。
别怕，梦的方向叫做闯，青春还没散场！
参考：
[1]. 武汉大学官方微博
[2]. 武大新闻网：http://news.whu.edu.cn/info/1002/43788.htm
[3]. 守望珞珈的新浪博客：http://blog.sina.com.cn/s/blog_4da93d1f0100t6v9.html
[4]. 珞珈山水bbs毕业封面：http://bbs.whu.edu.cn/
</p>
  </div>
  <footer class="entry-footer"><span title='2015-06-28 11:32:18 +0800 CST'>June 28, 2015</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 再见武大" href="http://localhost:1313/posts/2015-06-28-farewell-to-whu/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="http://localhost:1313/posts/page/2/">
      «&nbsp;Prev&nbsp;2/4
    </a>
    <a class="next" href="http://localhost:1313/posts/page/4/">Next&nbsp;4/4&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">bitJoy</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
