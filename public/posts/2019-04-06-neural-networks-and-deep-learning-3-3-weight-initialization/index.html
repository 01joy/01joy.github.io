<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Neural Networks and Deep Learning（三·三）权重初始化及其他 | bitJoy</title>
<meta name="keywords" content="ReLU, tanh">
<meta name="description" content="权重初始化
在之前的章节中，我们都是用一个标准正态分布\(N(0,1^2)\)初始化所有的参数\(w\)和\(b\)，但是当神经元数量比较多时，会出现意想不到的问题。
假设一个神经网络的输入层有1000个神经元，且某个样本的1000维输入中，恰好有500维是0，另500维是1。我们目前考察隐藏层的第一个神经元，则该神经元为激活的输出为\(z = \sum_j w_j x_j&#43;b\)，因为输入中的500维是0，所以\(z\)相当于有501个来自\(N(0,1^2)\)的随机变量相加。因为\(w_j\)和\(b\)的初始化都是独立同分布的，所以\(z\)也是一个正态分布，均值为0，但方差变成了\(\sqrt{501} \approx 22.4\)，即\(z\sim N(0,\sqrt{501}^2)\)。我们知道对于正态分布，如果方差越小，则分布的形状是高廋型的；如果方差越大，则分布的形状是矮胖型的。所以\(z\)有很大的概率取值会远大于1或远小于-1。又因为激活函数是sigmoid，当\(z\)远大于1或远小于-1时，\(\sigma (z)\)趋近于1或者0，且导数趋于0，变化缓慢，导致梯度消失。

  
      
          
          
      
  
  
  

请注意，这里的梯度消失和之前介绍得梯度消失稍有不同，之前是说在误差反向传播过程中，损失函数对权重的导数中包含梯度消失项，所以可以通过更换损失函数来解决。但是这里的梯度消失并不是在误差反向传播过程中产生的，而是在正向传播产生的，跟损失函数没关系。
解决这个问题的方法很简单，根据上面的分析，如果输入\(x_j\)全为1，\(w\)和\(b\)都来自\(N(0,1^2)\)，则\(z\sim N(0, \sqrt{n&#43;1}^2)\)，其中\(n\)为输入样本的维度。要减小\(z\)的方差，减小\(w\)和\(b\)的方差就可以了。因为\(b\)只有一个，对整体的影响不大，可以不修改\(b\)的分布，\(b\)依然来自\(N(0,1^2)\)。把\(w_j\)的分布修改为\(N(0, (\frac{1}{\sqrt{n}})^2)\)，此时\(z\sim N(0, \sqrt{2}^2)\)，\(\sqrt{2}=1.414\)就非常接近1了，\(z\)的分布也变成了一个高廋型的，梯度消失问题也就不存在了。
如果是开头的例子，输入维度为1000，其中500为0，500为1，\(w_j\sim N(0, (\frac{1}{\sqrt{1000}})^2)\)，\(b\sim N(0,1^2)\)，则\(z\sim N(0, \sqrt{\frac{3}{2}}^2)\)，\(\sqrt{3/2} = 1.22\ldots\)也是高廋型的，不会有梯度消失的问题。
由下图可知，在新的权重初始化策略下，网络很快就收敛了，比之前的方法快很多。

怎样选择超参数
大原则：在网络优化的前期，尽量使网络结构、问题简单，以便快速得实验结果，不断尝试超参数取值，当找到正确的优化方向后，再慢慢把网络和问题变复杂，精细调整超参数。比如MNIST问题，开始可以减少训练数据，只取0和1的图片，做二分类；同时可以减少网络层数，验证集大小等，以便快速得到网络输出，判断网络性能变化。这样可以快速尝试新的超参数。
学习率\(\eta\)
在误差反向传播中，学习率太大，虽然可以加速学习，但在后期可能导致网络震荡，无法收敛；学习率太小，导致学习速度太慢，训练时间过长。

确定学习率的方法是：首先随便选定一个值，比如0.01，然后不断增大10倍：0.1, 1, 10, 100…如果发现cost曲线在震荡，说明选大了，要降低，直到找到一个比较合适的值。这个过程只要找到合适的数量级就可以了，不一定要非常精确。比如发现0.1是比较合适的，那么可以再尝试0.2,0.3…，如果发现0.5不错，可以设学习率为0.5的一半0.25，这样可以使得在后续epoch中，不容易发生震荡。最好的方法是可变学习率，即前期学习率稍大（0.5），后期学习率稍小（0.1）之类的。
epoch
no-improvement-in-ten rule，就是说如果模型在最近的10个epoch中，验证集的accuracy都没有提高，则可以stop了。在早期实验中可以这么做，后续精细优化时可以改变ten，比如no-improvement-in-20/30等。
正则化参数\(\lambda\)
首先不要正则（\(\lambda=0\)），使用上面提到的方法确定学习率\(\eta\)，在确定的学习率情况下，正则\(\lambda=1\)开始进行优化，比如每次乘以10或者除以10，观察验证集上的accuracy指标，找到正则化所在的合适的数量级，然后再fine-tune。
Mini-batch size
太小了，无法利用现有软件包的矩阵操作的优势，速度会很慢。极端情况下，如果mini-batch size=1，就是说每次只用一个sample做BP，则100次mini-batch=1会比一次mini-batch=100操作慢很多，因为很多软件包对矩阵操作有优化，而没有对for循环优化。太大了，则一次BP要很久，参数更新的次数也比较少。
其他技术
随机梯度下降SGD的变种
海森矩阵法
SGD优化的目标就是最小化损失函数\(C\)，\(C\)是所有参数\(w = w_1, w_2, \ldots\)的函数，即\(C=C(w)\)。希望能够通过改变\(w\)，不断最小化\(C\)，即找一个\(\Delta w\)，使得\(C(w&#43;\Delta w)\)最小化。把\(C(w&#43;\Delta w)\)泰勒展开得到：
$$\begin{eqnarray}C(w&#43;\Delta w) & = & C(w) &#43; \sum_j \frac{\partial C}{\partial w_j} \Delta w_j\nonumber \\ & & &#43; \frac{1}{2} \sum_{jk} \Delta w_j \frac{\partial^2 C}{\partial w_j\partial w_k} \Delta w_k &#43; \ldots\tag{1}\end{eqnarray}$$写成矩阵形式就是：">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.36819bea596090d8b48cf10d9831382996197aa7e4fc86f792f7c08c9ca4d23b.css" integrity="sha256-NoGb6llgkNi0jPENmDE4KZYZeqfk/Ib3kvfAjJyk0js=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
    
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
    
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="bitJoy (Alt + H)">bitJoy</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Neural Networks and Deep Learning（三·三）权重初始化及其他
    </h1>
    <div class="post-meta"><span title='2019-04-06 17:52:56 +0800 CST'>April 6, 2019</span>&nbsp;·&nbsp;1 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e6%9d%83%e9%87%8d%e5%88%9d%e5%a7%8b%e5%8c%96" aria-label="权重初始化">权重初始化</a></li>
                <li>
                    <a href="#%e6%80%8e%e6%a0%b7%e9%80%89%e6%8b%a9%e8%b6%85%e5%8f%82%e6%95%b0" aria-label="怎样选择超参数">怎样选择超参数</a></li>
                <li>
                    <a href="#%e5%ad%a6%e4%b9%a0%e7%8e%87" aria-label="学习率\(\eta\)">学习率\(\eta\)</a></li>
                <li>
                    <a href="#epoch" aria-label="epoch">epoch</a></li>
                <li>
                    <a href="#%e6%ad%a3%e5%88%99%e5%8c%96%e5%8f%82%e6%95%b0" aria-label="正则化参数\(\lambda\)">正则化参数\(\lambda\)</a></li>
                <li>
                    <a href="#mini-batch-size" aria-label="Mini-batch size">Mini-batch size</a></li>
                <li>
                    <a href="#%e5%85%b6%e4%bb%96%e6%8a%80%e6%9c%af" aria-label="其他技术">其他技术</a><ul>
                        
                <li>
                    <a href="#%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dsgd%e7%9a%84%e5%8f%98%e7%a7%8d" aria-label="随机梯度下降SGD的变种">随机梯度下降SGD的变种</a><ul>
                        
                <li>
                    <a href="#%e6%b5%b7%e6%a3%ae%e7%9f%a9%e9%98%b5%e6%b3%95" aria-label="海森矩阵法">海森矩阵法</a></li>
                <li>
                    <a href="#%e5%9f%ba%e4%ba%8e%e5%8a%a8%e9%87%8f%e7%9a%84%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d" aria-label="基于动量的梯度下降">基于动量的梯度下降</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%b8%8d%e5%90%8c%e7%9a%84%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0" aria-label="不同的激活函数">不同的激活函数</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="权重初始化">权重初始化<a hidden class="anchor" aria-hidden="true" href="#权重初始化">#</a></h1>
<p>在之前的章节中，我们都是用一个标准正态分布\(N(0,1^2)\)初始化所有的参数\(w\)和\(b\)，但是当神经元数量比较多时，会出现意想不到的问题。</p>
<p>假设一个神经网络的输入层有1000个神经元，且某个样本的1000维输入中，恰好有500维是0，另500维是1。我们目前考察隐藏层的第一个神经元，则该神经元为激活的输出为\(z = \sum_j w_j x_j+b\)，因为输入中的500维是0，所以\(z\)相当于有501个来自\(N(0,1^2)\)的随机变量相加。因为\(w_j\)和\(b\)的初始化都是独立同分布的，所以\(z\)也是一个正态分布，均值为0，但方差变成了\(\sqrt{501} \approx 22.4\)，即\(z\sim N(0,\sqrt{501}^2)\)。我们知道对于正态分布，如果方差越小，则分布的形状是高廋型的；如果方差越大，则分布的形状是矮胖型的。所以\(z\)有很大的概率取值会远大于1或远小于-1。又因为激活函数是sigmoid，当\(z\)远大于1或远小于-1时，\(\sigma (z)\)趋近于1或者0，且导数趋于0，变化缓慢，导致梯度消失。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center"><img loading="lazy" src="https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz32.png"></th>
          <th style="text-align: center"><img loading="lazy" src="/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/ch3.8.png"></th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<p>请注意，这里的梯度消失和<a href="https://bitjoy.net/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/">之前介绍得梯度消失</a>稍有不同，之前是说在误差反向传播过程中，损失函数对权重的导数中包含梯度消失项，所以可以通过更换损失函数来解决。但是这里的梯度消失并不是在误差反向传播过程中产生的，而是在正向传播产生的，跟损失函数没关系。</p>
<p>解决这个问题的方法很简单，根据上面的分析，如果输入\(x_j\)全为1，\(w\)和\(b\)都来自\(N(0,1^2)\)，则\(z\sim N(0, \sqrt{n+1}^2)\)，其中\(n\)为输入样本的维度。要减小\(z\)的方差，减小\(w\)和\(b\)的方差就可以了。因为\(b\)只有一个，对整体的影响不大，可以不修改\(b\)的分布，\(b\)依然来自\(N(0,1^2)\)。把\(w_j\)的分布修改为\(N(0, (\frac{1}{\sqrt{n}})^2)\)，此时\(z\sim N(0, \sqrt{2}^2)\)，\(\sqrt{2}=1.414\)就非常接近1了，\(z\)的分布也变成了一个高廋型的，梯度消失问题也就不存在了。</p>
<p>如果是开头的例子，输入维度为1000，其中500为0，500为1，\(w_j\sim N(0, (\frac{1}{\sqrt{1000}})^2)\)，\(b\sim N(0,1^2)\)，则\(z\sim N(0, \sqrt{\frac{3}{2}}^2)\)，\(\sqrt{3/2} = 1.22\ldots\)也是高廋型的，不会有梯度消失的问题。</p>
<p>由下图可知，在新的权重初始化策略下，网络很快就收敛了，比之前的方法快很多。</p>
<p><img loading="lazy" src="https://i0.wp.com/neuralnetworksanddeeplearning.com/images/weight_initialization_30.png"></p>
<h1 id="怎样选择超参数">怎样选择超参数<a hidden class="anchor" aria-hidden="true" href="#怎样选择超参数">#</a></h1>
<p>大原则：在网络优化的前期，尽量使网络结构、问题简单，以便快速得实验结果，不断尝试超参数取值，当找到正确的优化方向后，再慢慢把网络和问题变复杂，精细调整超参数。比如MNIST问题，开始可以减少训练数据，只取0和1的图片，做二分类；同时可以减少网络层数，验证集大小等，以便快速得到网络输出，判断网络性能变化。这样可以快速尝试新的超参数。</p>
<h1 id="学习率">学习率\(\eta\)<a hidden class="anchor" aria-hidden="true" href="#学习率">#</a></h1>
<p>在误差反向传播中，学习率太大，虽然可以加速学习，但在后期可能导致网络震荡，无法收敛；学习率太小，导致学习速度太慢，训练时间过长。</p>
<p><img loading="lazy" src="https://i0.wp.com/neuralnetworksanddeeplearning.com/images/multiple_eta.png"></p>
<p>确定学习率的方法是：首先随便选定一个值，比如0.01，然后不断增大10倍：0.1, 1, 10, 100…如果发现cost曲线在震荡，说明选大了，要降低，直到找到一个比较合适的值。这个过程只要找到合适的数量级就可以了，不一定要非常精确。比如发现0.1是比较合适的，那么可以再尝试0.2,0.3…，如果发现0.5不错，可以设学习率为0.5的一半0.25，这样可以使得在后续epoch中，不容易发生震荡。最好的方法是可变学习率，即前期学习率稍大（0.5），后期学习率稍小（0.1）之类的。</p>
<h1 id="epoch">epoch<a hidden class="anchor" aria-hidden="true" href="#epoch">#</a></h1>
<p>no-improvement-in-ten rule，就是说如果模型在最近的10个epoch中，验证集的accuracy都没有提高，则可以stop了。在早期实验中可以这么做，后续精细优化时可以改变ten，比如no-improvement-in-20/30等。</p>
<h1 id="正则化参数">正则化参数\(\lambda\)<a hidden class="anchor" aria-hidden="true" href="#正则化参数">#</a></h1>
<p>首先不要正则（\(\lambda=0\)），使用上面提到的方法确定学习率\(\eta\)，在确定的学习率情况下，正则\(\lambda=1\)开始进行优化，比如每次乘以10或者除以10，观察验证集上的accuracy指标，找到正则化所在的合适的数量级，然后再fine-tune。</p>
<h1 id="mini-batch-size">Mini-batch size<a hidden class="anchor" aria-hidden="true" href="#mini-batch-size">#</a></h1>
<p>太小了，无法利用现有软件包的矩阵操作的优势，速度会很慢。极端情况下，如果mini-batch size=1，就是说每次只用一个sample做BP，则100次mini-batch=1会比一次mini-batch=100操作慢很多，因为很多软件包对矩阵操作有优化，而没有对for循环优化。太大了，则一次BP要很久，参数更新的次数也比较少。</p>
<h1 id="其他技术">其他技术<a hidden class="anchor" aria-hidden="true" href="#其他技术">#</a></h1>
<h2 id="随机梯度下降sgd的变种">随机梯度下降SGD的变种<a hidden class="anchor" aria-hidden="true" href="#随机梯度下降sgd的变种">#</a></h2>
<h3 id="海森矩阵法">海森矩阵法<a hidden class="anchor" aria-hidden="true" href="#海森矩阵法">#</a></h3>
<p>SGD优化的目标就是最小化损失函数\(C\)，\(C\)是所有参数\(w = w_1, w_2, \ldots\)的函数，即\(C=C(w)\)。希望能够通过改变\(w\)，不断最小化\(C\)，即找一个\(\Delta w\)，使得\(C(w+\Delta w)\)最小化。把\(C(w+\Delta w)\)泰勒展开得到：</p>
$$\begin{eqnarray}C(w+\Delta w) & = & C(w) + \sum_j \frac{\partial C}{\partial w_j} \Delta w_j\nonumber \\ & & + \frac{1}{2} \sum_{jk} \Delta w_j \frac{\partial^2 C}{\partial w_j\partial w_k} \Delta w_k + \ldots\tag{1}\end{eqnarray}$$<p>写成矩阵形式就是：</p>
$$\begin{eqnarray}C(w+\Delta w) = C(w) + \nabla C \cdot \Delta w +\frac{1}{2} \Delta w^T H \Delta w + \ldots,\tag{2}\end{eqnarray}$$<p>其中的\(\nabla C\)就是常规的梯度向量，\(H\)就是著名的海森矩阵，其中\(H_{jk}=\partial^2 C / \partial w_j \partial w_k\)。如果把(2)中的高阶项扔掉，得到如下的近似等式：</p>
$$\begin{eqnarray} C(w+\Delta w) \approx C(w) + \nabla C \cdot \Delta w +\frac{1}{2} \Delta w^T H \Delta w.\tag{3}\end{eqnarray}$$<p>最小化(3)的右边，得：</p>
$$\begin{eqnarray}\Delta w = -H^{-1} \nabla C.\tag{4}\end{eqnarray}$$<p>所以我们可以通过如下方式更新\(w\)已达到最小化\(C\)的目的，当然也可以给\(\Delta w\)乘上学习率\(\eta\)。</p>
<p><img loading="lazy" src="/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/ch3.9.png"></p>
<p>海森矩阵法有点像通过解析的方式最小化\(C\)，感觉上比SGD方法更精确靠谱。事实上，海森矩阵法确实比SGD方法收敛速度更快，但是因为在公式(4)中需要求解海森矩阵\(H\)的逆矩阵\(H^{-1}\)，当网络的参数量很大时，求解过程会非常慢，导致海森矩阵法不实用。</p>
<h3 id="基于动量的梯度下降">基于动量的梯度下降<a hidden class="anchor" aria-hidden="true" href="#基于动量的梯度下降">#</a></h3>
<p>增加速度这个变量，个人不是太理解：</p>
$$\begin{eqnarray} v & \rightarrow & v’ = \mu v – \eta \nabla C \tag{5}\\w & \rightarrow & w’ = w+v’.\tag{6}\end{eqnarray}$$<h2 id="不同的激活函数">不同的激活函数<a hidden class="anchor" aria-hidden="true" href="#不同的激活函数">#</a></h2>
<p>tanh是和sigmoid很像的一个激活函数，其函数形式为：</p>
$$\begin{eqnarray}\tanh(z) \equiv \frac{e^z-e^{-z}}{e^z+e^{-z}}.\tag{7}\end{eqnarray}$$<p>事实上，sigmoid函数\(\sigma(z)\)和\(\tanh(z)\)有线性关系：</p>
$$\begin{eqnarray} \sigma(z) = \frac{1+\tanh(z/2)}{2},\tag{8}\end{eqnarray}$$<p>\(\tanh(z)\)的函数图像如下，和sigmoid非常类似：</p>
<p><img loading="lazy" src="/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/ch3.10.png"></p>
<p>\(\tanh(z)\)和sigmoid的主要区别就是值域不一样，前者值域为[-1,1]，后者值域为[0,1]。这会导致什么差异呢？观察(BP4)这个公式，对于第\(l\)层的第\(j\)个神经元和第\(l-1\)层的所有神经元的连接权重\(w_{jk}^l\)，如果使用sigmoid激活，则\(a_k^{l-1}\)都是非负的，而这些梯度共用一个\(\delta_j^l\)，所以对于固定的\(j\)，不同的\(k\)，所有的梯度\(\frac{\partial C}{\partial w^l_{jk}}\)方向是一样的！这在无形中就减小了搜索空间。而如果用\(\tanh(z)\)激活的话，不同的\(k\)的\(a^{l-1}_k\)正负号可能就不一样，搜索空间更大，更容易收敛。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center"><img loading="lazy" src="https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz21.png"></th>
          <th style="text-align: center"><img loading="lazy" src="/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/ch3.11.png"></th>
      </tr>
  </thead>
  <tbody>
  </tbody>
</table>
<p>另一个比较常见的激活函数是ReLU激活函数，其函数形式如下：</p>
$$ReLU(z)=max(0, z)\tag{9}$$<p>函数图像如下：</p>
<p><img loading="lazy" src="/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/ch3.12.png"></p>
<p>ReLU和Sigmoid、tanh很不一样，ReLU在\(z>0\)的方向上不会有梯度消失的问题。</p>
<p>好了，第三章的内容就全部介绍完毕了，这一章介绍了很多调试神经网络的经验法则，没有太多的理论基础，相信随着这个领域的发展，神经网络的黑盒子会被慢慢打开。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/relu/">ReLU</a></li>
      <li><a href="http://localhost:1313/tags/tanh/">Tanh</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/2025-08-15-announcement/">
    <span class="title">« Prev</span>
    <br>
    <span>公告</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/2019-03-24-neural-networks-and-deep-learning-3-2-overfitting-and-regularization/">
    <span class="title">Next »</span>
    <br>
    <span>Neural Networks and Deep Learning（三·二）过拟合与正则化</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="01joy/01joy.github.io"
        data-repo-id="R_kgDOPefF-w"
        data-category="Announcements"
        data-category-id="DIC_kwDOPefF-84CuPVG"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">bitJoy</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
