<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Posts | bitJoy</title>
<meta name="keywords" content="">
<meta name="description" content="Posts - bitJoy">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.36819bea596090d8b48cf10d9831382996197aa7e4fc86f792f7c08c9ca4d23b.css" integrity="sha256-NoGb6llgkNi0jPENmDE4KZYZeqfk/Ib3kvfAjJyk0js=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/posts/index.xml">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
    
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
    
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="bitJoy (Alt + H)">bitJoy</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    Posts
  </h1>
</header>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID 作者单位：Meta 论文链接：https://arxiv.org/pdf/2504.02137 来源：RecSys 2025 Motivation：论文要解决的问题是什么 搜推广的模型严重依赖于item id embedding的表征质量，但在工业场景下，搜推广的id表征存在如下挑战：
id量级非常大，常常是数十亿甚至是百亿的规模。因此，通常不可能给每个id一个单独的embedding（即文中的individual embedding, IE），IE的成本太高 id分布非常不均匀，马太效应严重。文中统计：0.1%的头部item占据了25%的曝光量；5.5%的腰部item占据了50%的曝光量；94.4%的尾部item只占据了25%的曝光量 id分布漂移严重：搜推广场景中item的变化非常频繁，无时无刻不在发生着新id的产生和旧id的退出，而且不同id存活的时间周期也不尽相同，所以id的准入准出策略很难完美适配所有item 针对上述问题，常见的做法是对item id采用hash然后查emb的方式（即文中的random hash，RH），将所有id hash到一个固定大小的空间，然后查emb。但是RH方式有如下缺点：
存在hash冲突，把不相关的id hash到一个桶里，导致语义混乱，学习效果不佳 无法解决id分布漂移的问题，比如hash到同一个桶的A、B两个id，如果B出现频率变高，则会带偏A的分布，影响了A的效果 无法进行知识共享，例如新出了商品iphone15，iphone15无法共享到老的iphone14的emb知识，iphone15的id emb必须完全重新学习。针对这种情况，作者做了一个更加极端的AA实验，就是copy一个完全相同的商品，只换item id，如果是IE或者RH策略，则新商品由于id emb是随机初始化的，效果不佳，这是id-based的通病 基于前缀n-gram的semantic id表征方法 针对上述问题，作者沿用了semantic id的思路，首先使用内容理解团队产出的文本、图片等多模态emb，然后基于过去3个月的item多模态emb，训练RQ-VAE模型，并产出所有item的semantic id。
上述过程都是常规操作，重点在于如何基于semantic id得到item emb表征。假设semantic id是L层，每层的codebook size是K：
最常规的做法：每层都初始化一个K*d的emb table，每层sid查各自的emb table，然后把L层的sid emb加起来。但是本文完全没有提这种方法，也没有和这种方法比较，非常奇怪。 为了比较，我个人再详细描述下这种常规做法。比如老item A的sid是(c1,c2,c3)；新来一个item B，它的sid是(c1,c2,c4)。用常规方法，A的emb是c1&#43;c2&#43;c3，B的emb是c1&#43;c2&#43;c4。两者c1、c2是可以共享的，所以常规方法也能起到一定的知识共享的效果，共享项有2项：c1、c2。但是因为RQ-VAE的沙漏问题，c2很有可能是沙漏瓶颈，信息量不足。 作者对比了Table 1中的几种方法： Trigram和Fourgram差不多，如果L=3用Trigram、L=4用Fourgram的话，本质上是把L个sid映射成了一个无冲突的int。但是这种方法映射出来的int数量太多了，是\(K^L\)。如果K=1024、L=3，则\(K^L\)就已经超过10亿了，这和直接无冲突的IE方法一样了，而且存在新id无法共享老id学到的知识的问题 All bigrams，就是所有的sid的2-gram。还是上面的例子，A的emb相当于\(c_1c_2&#43;c_2c_3\)，B的emb相当于\(c_1c_2&#43;c_2c_4\)，两者可共享\(c_1c_2\)项，相比于常规方法，虽然共享项数变少了，但粒度更精细了，孰好孰坏未可知。由Table 2可知，All bigrams的效果至少比Trigram和Fourgram好很多了，而且如果层数L越大，可共享项越多 Prefix-ngram（简称Prefix-SID方法），本文提出的新方法，把所有前缀组合成新id查emb，然后所有emb再求和。还是上面的例子，A的emb相当于\(c_1&#43;c_1c_2&#43;c_1c_2c_3&#43;c_2&#43;c_2c_3&#43;c_3\)，B的emb相当于\(c_1&#43;c_1c_2&#43;c_1c_2c_4&#43;c_2&#43;c_2c_4&#43;c_4\)，两者可共享\(c_1, c_1c_2, c_2\)三项，比之前的所有方法可共享的信息都多，而且如果层数L越大，可共享项越多，因此这种方法的效果最好，训练也最稳定 实验结果很丰富，做了很多分析，Prefix-SID方法有如下优势：
相比于IE和RH方法，Prefix-SID方法对中长尾item的提升尤其显著，因为新id和老id的表征有了知识共享 对id分布漂移问题更不敏感：由于电商模型训练时消费数据的顺序是和数据的时间一致的，比如一个月的数据，按照1号、2号、…31号这样的时间先后顺序依次训练，理论上4号的模型在4号的测试集上的效果是最好的。作者做了一个实验，分别用20号和4号的模型都在4号的测试集上进行评测，看看20号的模型指标相比4号降低了多少。作者发现，使用Prefix-SID方法和IE方法，两者的指标降低幅度都差不多，都比较小。首先IE方法由于不存在hash冲突，所以20号的模型仍然能比较好地预测4号的数据；其次，Prefix-SID方法虽然有hash冲突，但是因为冲突的item都是语义相似的，可以进行新老item的知识共享，所以这个冲突反而是好事，对模型效果无影响。但是作者发现RH方法的20号的模型在4号数据上评测指标下降比较多，因为有hash冲突，而且冲突是随机的，20号的分布已经变化很大了，导致在4号数据上效果不佳。Table 4的指标越小越好。 基于Prefix-SID方法虽然也有hash冲突，但是冲突到同一个semantic id的item表征更相似，而RH冲突到同一个桶里的item是完全随机的，相似度差。作者以IE为base，把Prefix-SID和RH都各自都冲突到同一个桶的IE emb提取出来，计算类内相似度和类间相似度，发现基于Prefix-SID的类内相似度方差小，类间距离大，说明Prefix-SID确实能把相似item聚到一起。 评论 可借鉴 基于Prefix-SID方法确实能提高新item和老item的信息共享数量，方法值得借鉴 论文实验分析很丰富 可改进 基于Prefix-SID方法居然没有和最常规的加和方法比较，是本文最大的不足 </p>
  </div>
  <footer class="entry-footer"><span title='2025-10-09 18:17:16 +0800 CST'>October 9, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID" href="http://localhost:1313/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings 作者单位：沃尔玛 论文链接：https://arxiv.org/pdf/2507.17080 来源：RecSys 2025 Motivation：论文要解决的问题是什么 多模态q2i召回通常使用CLIP的对比学习方式进行训练，在电商场景下存在2个问题：
CLIP这种方式通常是对图片整体的表征，缺乏细粒度的目标检测能力，尤其在电商场景，比如fig1，卖衣服场景，传统CLIP只能识别整张图片是一件T恤，难以关注T恤上的图案等细节特征；另外，电商图片往往存在很多附加背景、道具、模特等元素，会影响主体物体的表征 电商标题、属性等文本描述通常参差不齐，存在错误、堆砌、图文不符等问题，导致CLIP训练时图文对齐效果不佳 VL-CLIP解决方案 针对图片的处理：
将图片和商品类型（product type）输入到开源模型Grounding DINO中，让模型进行目标检测，将可信度超过某个阈值且可信度最高的区域抠出来，输入到CLIP的图像encoder中。通过这步预处理，相当于对电商图片进行了关键主体识别和提取，只提取和商品最相关的主体进行图像表征。文中使用的图像编码器是ViT-B/32。 针对文本的处理：
将商品的类型、标题、描述、性别、年龄等文本描述以及图片本身输入到Summarizer多模态大模型，让大模型产出精简、准确的文本描述\(q_0\) 将\(q_0\)和商品图文信息输入到Evaluator多模态大模型，让大模型对\(q_0\)的质量进行评判，如果\(q_0\)质量很好，则直接输出&lt;STOP&gt;；否则指出\(q_0\)的问题所在，并说明改进方法 如果第2步输出不是&lt;STOP&gt;，则将第2步的输出再输入到Refiner大模型，让大模型根据第2步的结果继续调整并输出更优的文本描述\(q_i\) 不断重复第2、3步，直到输出&lt;STOP&gt;，或者最多重复5遍 将产出的精准的文本描述q输入到CLIP的文本encoder中，文中使用的是BERT系列。产出的emb维度是512 上述Summarizer、Evaluator、Refiner都是VLM，文中使用的是GPT-4o，三个任务的prompt设计参考论文附录Table 9 上述对图片和文本的处理本质上是去噪，提取图片的主体物品、让文本描述更加精准。
产出多模态emb之后，后续的操作就是常规的召回流程了，使用HNSW进行ANN召回。
评论 可借鉴 使用Grounding DINO对图片进行主体识别，值得借鉴 使用VLM对商品标题、描述等文本信息进行去噪，值得借鉴 但如果商品量级很大的话，这两个步骤估计会很耗时 可改进 如果是q2i场景，直接用query文本是不是更真实，更接近搜索日子的真实数据分布？ </p>
  </div>
  <footer class="entry-footer"><span title='2025-10-08 23:31:54 +0800 CST'>October 8, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings" href="http://localhost:1313/posts/2025-10-08-vl-clip-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook 作者单位：Snap 论文链接：https://arxiv.org/pdf/2507.22224 来源：CIKM 2025 这是CIKM 2025的一篇resource文章，比较简单。核心内容是开源了一个基于semantic id的生成式推荐框架GRID，可以很方便地做各种消融对比实验。
主要内容 主要结论如下：
对于semantic id生成算法，简单的RQ-KMeans效果反而是最好的，好于R-VQ和RQ-VAE 生产pretrain emb的LLM模型参数量越大，效果越好，但是提升幅度有限 生产semantic id的codebook size和网络层数并不是越大越好，常规的3层，每层256个id效果反而最好 生成式推荐时，是否需要在用户行为序列基础上增加一个user id，实验发现增加user id效果反而变差，不增加user id效果最好 生成式网络结构encoder-decoder对比decoder-only，发现前者效果更好，因为前者能充分学习到行为序列完整的信息 对行为流进行滑动窗口数据增强能提升模型的泛化能力 当semantic id到item存在映射冲突时，随机选一个item的效果和对冲突item追加一个区分标识（digit），两者效果差不多 在生成式beam search的时候，限制只输出合法semantic id和不增加限制，两者效果差不多 评论 看这篇文章主要是想看看不同semantic id生产方法的对比，发现RQ-KMeans居然比RQ-VAE更好。个人感觉这两个方法效果应该差不多，后者应该更好点才对。首先，RQ-VAE的量化loss本质上和KMeans聚类是一个意思；其次，RQ-VAE还增加了一个重构loss，感觉产出来的semantic id和原始emb的信息损失应该更少。
此外，本文的所有实验都是基于亚马逊的公开数据集，数据量肯定不能和真正的工业数据集相提并论，所以文中很多结论有可能只适用于本文的设定，换一个场景估计结论就变了，所以看看就好。
最后，文中很多结论只写了现象，要是能增加原因分析就好了。
</p>
  </div>
  <footer class="entry-footer"><span title='2025-10-07 12:09:43 +0800 CST'>October 7, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook" href="http://localhost:1313/posts/2025-10-07-grid-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation 作者单位：网易云音乐 论文链接：https://arxiv.org/pdf/2508.20359 来源：CIKM 2025 Motivation：论文要解决的问题是什么 多模态emb在搜推的应用方式，通常是先将多模态emb转换成semantic id，然后把semantic id用到搜推模型中，这种方式有如下两个问题：
模态内语义退化：多模态emb转换成semantic id通常使用RQ-VAE或者RQ-KMeans的方法，这种方法在不断残差的过程中，后续残差聚类结果已经不能反映初始emb的聚类效果了。其实就是semantic id的沙漏问题，具体可以看这篇文章，后续有空再分享这个问题。 简单来说，如下图所示，初始有DJ、Rock、Lullaby、Choir四个类，但是对残差emb（即RQ-VAE的第二层）聚类的话，初始的四个类的item就打散了，会聚到不同的簇中，也就是RQ-VAE的后续层的聚类效果已经和初始emb的聚类效果很不一样了，这就是文中说的语义退化问题 模态间建模差异：搜推场景的item通常有多种模态特征，比如文本、图像、音频等，传统方法在多模态融合方面比较简单，不能很好地捕捉多模态之间的关系。 PSRQ生产semantic id 本文是音乐推荐场景，主要用到两种模态：text和audio，分别用百川和MERT提取text和audio的模态emb。
生产semantic id的方法如下图所示：
fig2a是传统的RQ-KMeans的方法，每一层都用上一层的残差进行聚类。如上文所述，由于沙漏问题，会导致后续层次的semantic id存在语义退化问题 fig2b是本文新提出的PSRQ量化方法，在RQ-KMeans基础上，每一层除了有上一层的残差向量，还会concat上初始emb减去残差emb后的向量。这样就能区分出残差相似，但初始emb不同的item了，也就避免了RQ方法的沙漏问题，后续semantic id也能保留初始emb的语义信息。fig1d能看出来第二层semantic id仍然能够反映初始emb的分类效果。 Semantic id在下游的应用方法 如下图所示：
每个item有两套多模态emb：text和audio，但是有三套semantic id，除了text和audio各自产一套semantic id之外，还会把text和audio的emb concat起来，再产一套semantic id，相当于多模态融合的semantic id semantic id的emb在排序模型中随机初始化，然后端到端训练 semantic id在用户建模时，使用DIN模型，query用的是多模态融合的semantic id emb，行为流分别用text和audio的semantic id emb。作者说这种方法既能捕捉到单模态细粒度的信息，又能建模跨模态的交互信息 评论 可借鉴 PSRQ的semantic id生产方法确实很有意思，在每一层都用上原始emb，这样不同簇的item在每一层都能分开，不会出现沙漏问题，使得每一层的semantic id都能保留原始emb的语义聚类信息 产了多套semantic id，单模态semantic id是常规操作；多模态emb concat后也产一套semantic id，是个创新点 用户建模时query用多模态semantic id，行为流用单模态semantic id，也是个创新点，虽然论文说这种方法效果最好，但是有点存疑 论文有个实验结果对比了不同semantic id量化方法的效果，结论是：PSRQ &gt; RQ-KMeans = RQ-VAE &gt; VQ &gt; PQ 可改进 pretrain emb和semantic id的生产都没有对齐协同信号 semantic id在下游应用时直接端到端训练，而没有使用codebook初始化，会不会丢失信息比较多？ 产semantic id的过程中，模态内语义退化的问题，描述了现象，但是没有用定量的指标来说明问题，感觉可以借鉴【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】的方法，定量说明后续层的semantic id的聚类效果或者说区分能力相比初始emb已经相差甚远了 fig2b中，第一层的codebook的dim=d，后续层的codebook的dim=2d，那么后续层的残差dim也是2d，那么初始emb怎么和后续的残差emb相减呢，维度对不上啊？我理解可能是这样的，后续层聚类的时候用的是concat的dim=2d的emb，但是算聚类中心的时候只用了残差本身的emb，这样就能解释得通了，但是文中对这部分的细节没有解释。 </p>
  </div>
  <footer class="entry-footer"><span title='2025-10-06 21:01:25 +0800 CST'>October 6, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation" href="http://localhost:1313/posts/2025-10-06-psrq-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System 作者单位：快手 论文链接：https://arxiv.org/pdf/2508.10584 来源：CIKM 2025 Motivation：论文要解决的问题是什么 Semantic id生产时，要么没有和协同信号对齐（fig2(1)），要么是两阶段对齐方式（fig2(2)）：
例如LETTER先生成协同emb，然后和semantic id对齐 或者例如QARM，先协同对齐emb，再生产semantic id 把协同对齐和生产semantic id分成两个阶段，天然有信息损失，不是最优的。本文的目的就是把生产协同emb，以及semantic id的协同对齐放到一个模型中联合训练完成，尽量减少信息损失（fig2(3)）。
主模型 主模型如上图所示，中间的ICDM是user和item的双塔模型，用于学习user和item的协同id-based emb；两边分别是生产user和item的semantic id的量化模型。
中间的ICDM就是经典的召回双塔模型，使用点击样本进行训练，唯一不同的是，在user和item塔都有流行度去偏模块，用于学习user和item的无偏emb，后续user和item的semantic id协同对齐用的也是无偏的emb。
两边分别是user和item的semantic id量化模型，两者比较类似，以item为例：
先把item的各种信息，如title、desc、ocr等信息用文本构造成prompt，输入到LLM，借助LLM的summary和reasoning能力，产出item的详细描述 然后把LLM产出的描述再输入到一个预训练的embedding模型PLM，文中用的是bge m3模型，得到item emb 后续就是标准的RQ-VAE过程了 需要注意的是，上述前两步，分别用到了LLM和PLM两个大模型，而且看图上这两个模型都是freeze的，也就是说并不微调这两个大模型。后续协同对齐用的emb是RQ-VAE重构emb的中间层结果，即图中的item quantized emb。
semantic id的协同对齐方面，有三大类对齐任务：
U2I对齐：量化user emb和协同item emb对齐、量化item emb和协同user emb对齐 U2U和I2I对齐：量化user emb和协同user emb对齐、量化item emb和协同item emb对齐 U2U和I2I的共现对齐：点击相同item的两个量化user emb对齐、同一个user点击的两个item的量化item emb对齐 由于fig3中的协同模型和semantic id模型是联合训练的，总共有3大类loss：
中间的ICDM的双塔召回模型的loss 两边的产semantic id的loss 三个模块的对齐loss 评论 可借鉴 把semantic id的生产和协同信号对齐统一成一阶段的模式，信息损失更少 中间的ICDM模型生产协同emb时进行了去偏，协同对齐的时候用的是去偏的emb，这是其他论文很少提到的 可改进 太复杂了！3个模块，3大类loss，每类loss又有很多个小loss，总loss数量加起来有十多个。。。 任务太多，各种去偏、对齐loss，真的不会互相影响吗？ 中间的ICDM模块有必要吗？我理解ICDM本质是为了训练产出协同emb，但是因为训练样本本身是点击样本，样本本身已经包含了搜推场景的协同信号，也就是ICDM本身没必要存在了，直接用相同的样本训练两边的semantic id量化模型就行了，也能实现在训练semantic id的过程中，完成协同信号的对齐 生产semantic id的emb来自LLM和PLM，但是这两个大模型都是freeze的，如果把这两个模型也sft，效果会不会更好？其实我原本以为的一阶段就是这样的，这也是我在【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】中提到的一阶段方法。 </p>
  </div>
  <footer class="entry-footer"><span title='2025-10-05 20:26:43 +0800 CST'>October 5, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System" href="http://localhost:1313/posts/2025-10-05-das-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou 作者单位：快手 论文链接：https://arxiv.org/pdf/2411.11739 来源：CIKM 2025 Motivation：论文要解决的问题是什么 多模态emb在搜推场景应用时通常采用如下图的两阶段方式，先预训练多模态emb，然后作为一个冻结特征放到搜推模型中。这种方式存在2个问题：
表征不对齐：多模态emb预训练的任务通常是图片分类或者文本的MLM，和下游搜推任务不对齐 表征不更新：多模态emb在搜推任务中作为冻结特征，没有更新 本文的方法就是想要解决上述2个问题。 对齐搜推任务的多模态emb预训练 为了解决多模态emb表征不对齐的问题，本文提出的多模态emb预训练任务直接对齐搜推场景，使用U2I和I2I召回模型，挖掘出相似item pair，然后通过对比学习微调多模态大模型。
具体来说，通过U2I和I2I模型，能够拿到item emb；然后用每一个target item emb去行为流中检索出最相似的商品，作为trigger item emb。&lt;trigger, target&gt;构成一对正样本，然后进行对比学习训练。
通过召回模型构造的训练样本，和搜推场景的协同信号对齐了，解决了开头提到的第一个问题，即表征不对齐的问题。
Semantic id生产方法 Semantic id的生产方法如上图右半部分所示，有两种方式：
VQ：直接圈定一定数量（如N）的item emb作为底池，编号1~N，然后任意来一个item emb，通过对底池emb进行KNN搜索，找出top-k相似商品，假设是(a,b,…,k)，则VQ编码的semantic id就是(a,b,…,k)。文中取k=25，感觉挺大的。。。 RQ-Kmeans：对圈定的N个item emb不断进行Kmeans聚类、求残差、残差继续Kmeans聚类的过程。文中取迭代次数为L=6，但是没说每次聚到多少个类。 注意：文中的RQ-Kmeans方法和RQ-VAE还不一样，RQ-Kmeans没有训练过程，也没有重构loss，纯粹是每次进行聚类，然后选聚类中心作为码本的过程。文中也没有对比过为啥不用RQ-VAE。
产出两套semantic id之后，直接在下游排序任务中进行端到端更新，解决开头提到的表征不更新的问题。具体建模方法比较常规，不是本文的重点，略讲。
评论 可借鉴 多模态emb预训练任务是i2i的，直接和下游搜推任务对齐 semantic id有两种产出方式，VQ和RQ-Kmeans，尽可能多地保留原始多模态emb的信息 可改进 多模态emb预训练和下游任务对齐，在2025年不算新鲜事了，常规操作。而且文中i2i的构造过程依赖U2I和I2I召回模型，有外部依赖，不够漂亮 VQ的方法，k=25这也太长了吧，相当于一个小型行为流了，会导致下游任务的特征处理更复杂 为什么用RQ-Kmeans而不是RQ-VAE，没有任何说明与对比 从pretrain emb量化成semantic id的过程中，存在严重的信息丢失，这在Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs论文中有讨论 </p>
  </div>
  <footer class="entry-footer"><span title='2025-10-04 18:24:40 +0800 CST'>October 4, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou" href="http://localhost:1313/posts/2025-10-04-qarm-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs 作者单位：香港城市大学&amp;腾讯 论文链接：https://arxiv.org/pdf/2509.02017 来源：CIKM 2025 Motivation：论文要解决的问题是什么 LLM4SR的基本范式如下，即用LLM直接来做搜推的范式（这种方式在学术界常见，但在工业界不常见）。由于LLM的输入词表范围是有限的（通常比较小），因此其token emb dim通常比较大，比如2048或者4096；而搜推场景的item量级很大，而且在不断更新，因此工业界经典的id-based的搜推模型的item emb dim通常比较小，比如64或128。经典的id-based的搜推模型能比较好地学习到搜推场景的协同信号，为了让LLM模型也能感知这种信息，LLM4SR范式通常会先预训练一个id-based的经典搜推模型，然后将其中的item id emb通过下图的Linear Projection的映射层，映射到LLM token emb的空间，让LLM也能感知搜推的协同信号。
上述LLM4SR范式存在两个问题：
维度坍缩：id-based训出来的id emb dim比较小（如64），LLM token emb dim比较大（如4096），在由id emb通过Linear Projection映射到toen emb的过程中，虽然64映射到4096空间了，但扩维后的矩阵存在低秩问题，即还是只利用了4096中的64维的空间。
论文中，作者分两种情况进行了分析，如果Linear Projection只是一个线性层的话，通过公式推导能得出上述结论；如果Linear Projection包含非线性变换，作者通过实验分析也发现了维度坍缩的现象。 灾难遗忘：除了使用id-based模型产出的id emb，LLM4SR也常用多模态模型产出item emb表征，然后转换成semantic id输入到LLM4SR中。在这种情况下，产出的semantic id通过会遗忘多模态item emb的信息，导致下游LLM4SR的效果不佳。
论文中，作者用公式9来衡量semantic id保留pretrain多模态emb的信息量。具体来说，如果行为流中的商品序列是{A,B,C,D}，target item是E。使用pretrain多模态emb能计算出E和A~D的相似度，例如相似度&lt;E,A&gt; &gt; &lt;E,B&gt;。如果将pretrain多模态emb转换成semantic id，然后由semantic id恢复出新的A~E的emb之后，再计算E和A~D的相似度，如果仍然有&lt;E,A&gt; &gt; &lt;E,B&gt;，则认为一致（concordant），否则不一致（disconcordant）。这个分析方法挺好的，通过这个指标能估算出转换成semantic id之后，仍然保留原有pretrain多模态emb对搜推场景的序关系的保留程度。 作者发现，转换成semantic id之后，信息只保留了37.14%；进一步，如果semantic id是在下游任务中端到端训练的，则信息只保留了5.5%，也就是说94.5%的pretrain emb的序的信息都丢掉了，也就是灾难遗忘。 Semantic id构建方法 3套emb来源，一套id-based经典搜推模型产出的包含协同信号的emb，另外两套是LLM2CLIP产出的多模态文本和图片emb。作者提到传统CLIP对长文本处理能力较弱，所以升级到LLM2CLIP，能更好地处理长文本。 Semantic id构建方法是经典的RQ-VAE的方法，但有如下两个改进点： 将emb的重构loss由MSE升级成MMD (maximum mean discrepancy)，MSE是计算原始emb和重构emb的欧式距离的误差，而MMD是计算两个分布的diff，实验表明能MMD比MSE能保留更多的pretrain多模态emb信息（即上述公式9），保留44.36% 对量化后的emb做了对齐，因为LLM2CLIP本身进行了图文模态的对齐，所以文中只新增了id emb分别和文本、图片模态的对齐 此外，还有一点论文没提但可能和常规RQ-VAE不同之处，就是原始emb在进行RQ-VAE之前，有一个Encoder升维的操作，在重构loss前对应有一个Decoder降维的操作，而semantic id量化恢复emb是Decoder之前的那个。这一升一降，估计也有助于缓解维度坍缩。 ...</p>
  </div>
  <footer class="entry-footer"><span title='2025-10-04 11:10:11 +0800 CST'>October 4, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs" href="http://localhost:1313/posts/2025-10-04-mme-sid-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">公告
    </h2>
  </header>
  <div class="entry-content">
    <p>博客数据恢复中，敬请期待！
测试图片： 测试代码：
1 2 3 4 5 6 7 8 9 10 11 12 13 // Necessary header files for input output functions #include &lt;iostream&gt; using namespace std; // main() function: where the execution of // C&#43;&#43; program begins int main() { // This statement prints &#34;Hello World&#34; cout &lt;&lt; &#34;Hello World&#34;; return 0; } 测试数学公式： This is an inline \(a^*=x-b^*\) equation.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-08-15 22:03:06 +0800 CST'>August 15, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 公告" href="http://localhost:1313/posts/2025-08-15-announcement/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">一念成博
    </h2>
  </header>
  <div class="entry-content">
    <p>证明你能做一件事的最好方法就是做成这件事！
从保研开始，我根本就没打算读博士，心里想的是，好好学习，认真刷题，顺利毕业，高薪就业。
2016年底，也就是进实验室半年之后，贺老师开始“怂恿”我读博：“贫寒人家子弟，有个高学历，在这个拼爹的时代，更容易出人头地。年轻时多读点儿难读的书，也会更好地在未来人工智能时代生存。”我不为所动。
2017年春节在家，疯狂刷题看书，为开学后的实习面试以及半年之后的校招面试准备着。
2017年2月份，开年工作计划会，老师说只要我愿意读博，博士的三个课题都帮我规划好了，要知道我们实验室没有哪个博士生是在三年级之前就确定方向的，大家都是摸着石头过河。
我还是不为所动，疯狂刷题看书，攒实习面经。
到了8月，老师最后来信希望我能认真考虑一下读博的事情：“pFind&#43;NIBS是难得的良性成长环境，换一个新环境未见得能成长像现在这么快”。甚至把我的博士女朋友都搬出来了。我跟欣欣聊了聊，思想开始有点动摇了。但是那段时间忙于找工作，没空想太多。
9月10日教师节，我和欣欣分手，与工作无关，与硕士博士无关。
后来有一天，当我在对比百度凤巢和微软的Offer时，偶然看到凤巢前辈李沐博士写的一篇博客：《博士这五年》。李沐是上海交大ACM班的，毕业之后去了百度凤巢，但是后来毅然辞职去CMU攻读博士学位。博士五年期间，他不但发表了多篇很牛的paper，而且亲手写了一个类似TensorFlow的深度学习平台MXNet，MXNet现已加入Apache家族，并被Amazon选为官方深度学习框架。他博士答辩的评委有来自Google, Amazon, Apple的AI负责人，阵容非常强大。最后，李沐光荣毕业，加入Amazon。
这篇博客的结尾在谈到如何选择工作和读博时有一段话，令我印象深刻：“不过我觉得还是会选择读博。赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次……更重要的是理想和情怀。人一生要工作五十年，为什么不花五年来追求下理想和情怀呢？”。
看完这篇博客之后，那一整天，我都没心思上班。校招至今，拿了一堆Offer，不能说轻而易举，但是我现在知道拿Offer就跟考试一样，只要准备好，不会差到哪里去。甚至可以说，校招面试比回答贺老师的问题要简单多了。未来可以工作的时间还很长，何不花几年时间来挑战自己呢。当你在面对两难选择时，选择更难的那个，日后你会感谢当初自己的选择。
人拼了命的工作，到底是为了什么，除了更早的成为工厂里的螺丝钉，成为房奴、孩奴，你还能得到什么。是的，提前三年工作，你也许能赚到100万，能积累更丰富的工作经验和更广阔的人脉。但是，这又怎样，这些东西该来的肯定会来，博士毕业之后，我同样能得到，只是比大多数人晚了一点。
与其早早毕业，成为一个nobody，还不如再潜心修炼几年，成为somebody。博士这几年，我能得到更加系统全面的训练，pFind&#43;NIBS的良性科研环境，也不可多得。曾经在知乎上看到有个人回答为什么选择读博：“在一个很安全的环境里，父母健在，自己不用操心赚钱养家；有老板给你提供指导、资金；你可以安心研究自己感兴趣的问题；发表的文章也将署上自己的名字，流传后世；习得的技能也将转化为自己的能力；获得的博士学位也将是自己的荣誉…”，这么好的事情，为什么不去做呢？
那一天之后，我内心几乎就决定要转博了。
然而，话虽如此，想到要放弃到手的大Offer，继续在实验室里待至少三年；想到免不了要经历大多数博士师兄师姐们经历过的痛苦日子；想到彼时同学们都已经年入x万，有房有车，说不定我日后的面试官就是现在的同学；想到我最亲密的女朋友对我的不信任；想到自己的苦衷无处倾诉…
那段日子过得很艰难，也许是我到目前为止最低谷的时期。左手是各大互联网公司的Offer，立即可以实现我很多的愿望；右手是若干年未知的博士磨砺。虽然内心知道往右走是正确的，但还是下不了这个狠心。脑海中的两个小人，吵个不停。
期间和很多在读的、已毕业的博士师兄师姐们聊过，也和很多公司的面试官聊过，当然也和老师父母聊过。得到的回答无外乎三种：读、不读，根据自己的情况决定。这些谈话更像是换了种方式的倾述，我已经记不清具体的内容了，只知道，我做选择的决心越来越坚定了。
之后恰逢十一长假，给自己放了一个长长的假。规划去了郑州、登封、杭州。了却夙愿，重新开始。
10月9日，长假结束。我给老师发了一封邮件：“贺老师，您好。非常感谢您的信任和等待，我决定读博了！这将是我人生二十多年来所作的第一个重大决定，我接受挑战！”
人这一生，说长也长，说短也短，去做你认为对的事情吧，去追求你想要的生活。愿我们都能绽放美丽，不负芳华！
</p>
  </div>
  <footer class="entry-footer"><span title='2018-02-14 20:28:36 +0800 CST'>February 14, 2018</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 一念成博" href="http://localhost:1313/posts/2018-02-14-why-phd/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">伪·2018届校招面经
    </h2>
  </header>
  <div class="entry-content">
    <p>作为一名曾经的2018届硕士毕业生，为找工作忙活了大半年，最终收获了微软、百度、头条、Face&#43;&#43;等十多个Offer。校招季对我来说，在9月份就差不多结束了。本来很早就酝酿了这篇博客，但是由于之后一系列事情，耽搁至今，趁着提交完年终技术报告，回家之前，把这段经历记录一下。
首先介绍一些计算机专业校招的基本情况。由于移动互联网、人工智能等浪潮的兴起，计算机专业的毕业生就业前景可谓一片大好，尤其是对于名校毕业基础扎实的同学，应届生薪资倒挂老员工的事情几乎每年都在上演。所以首先祝贺所有CSer，这是属于你们的时代，各行各业都有属于你的舞台，尽情去施展才华吧。
本专业的毕业生就业去向主要有这么几类：国内互联网公司、国外互联网公司（外企）、国企。其中国内互联网公司又分大厂和新兴创业公司，大厂如BAT、网易、360、京东、华为等，创业公司主要集中在人工智能这块，如商汤科技、Face&#43;&#43;、头条、滴滴等。外企大概也分为两类，一类是来自日本的企业，比如Indeed、WAP；另一类是来自美国的企业，比如Microsoft、Google、Hulu、FreeWheel、Amazon等。国企是指传统的国有企业里面的IT部门，比如各大银行、证监会等。这几类公司的校招时间刚好都错开了，一般来说，日企来华校招是最早的，大概每年5月份就来了；接着是国内互联网公司的内推季，大概在7~8月份；进入9月份之后，就是国内互联网公司的正式校招了；美国的企业大概会在9~10月份启动校招，有可能一直持续到11月份；国企就比较晚了，听说最晚能持续到第二年3、4月份的。这种安排，对我们来说，既是好事，也是坏事，好处就是对于纠结的同学，可以每种类型的公司都试一试，多拿几个offer，最后根据自己的情况决定去哪里；坏处就是持续时间真的很长，面到最后，身心俱疲，需要做好心理准备。
我经历过的面试主要是国内互联网和部分外企的研发岗，下面也将主要介绍这两类企业，按时间先后顺序。
Indeed（FAILED） Indeed是全球最大的招聘信息搜索引擎公司，总部位于美国德州的首府奥斯汀，2012年被日本的Recruit收购，然后成立了Indeed Tokyo办公室。本文提到的Indeed都是指Indeed Tokyo，即拿到offer的话，要求去东京工作，不过可以轮转去奥斯汀总部。
Indeed是最早开始校招的，当国内公司还在实习招聘的时候，它就跑来进行校招了。我参加了2017年4月17日在北大举办的校园宣讲会，介绍了Indeed的基本情况和招聘流程，以及抽奖机械硬盘等。Indeed的办公室很有科技范，其工位设置尤为吸引人，是六边形的环形设计，每个人既可以专注于自己的工作，又便于和组内同事讨论。宣讲的人包括HR和从该校毕业的学长，这个HR是中国人，后面有一轮HR面也是他，大家可以多多留意。
Indeed最大的吸引力是，700万~800万日元的年薪，折合人民币大概四五十万吧，这样诱人的薪资，让每个路过其宣传海报的同学都驻足观看。当然其面试难度也不小，首先有一轮在线笔试，这个在线笔试有三次机会，只要有一次全部AC，就算通过。在线笔试题一共4道，难度比LeetCode稍大，但是一定提醒大家，他们家的题都有数据范围，而且范围很小，前3题用暴力解法几乎都可以过，所以一定要先试试暴力求解，不行再想DP。
通过在线笔试之后，会有一个大约30分钟的HR面，就是上面提到的来华宣讲的中国人。这个面试严格来说是Case interview，通过Skype进行，主要考察逻辑逻辑思维能力和英文口语能力。由于是中国人，所以刚开始会用中文介绍下题意，然后让你思考一下，最后用英文给出解答。我当时的题目是，如何把微信支付的流水从xxx提高到yyy。由于提前非常认真的看了http://www.caseinterview.com/的视频教学，学到很多，这次HR面顺利通过。
通过HR面之后，还有一轮Skype技术面，是从Indeed Tokyo那边打过来的，需要解算法题，通常是一题&#43;好几个follow up。不过很多是往年的原题，在一亩三分地上都有，大家可以仔细在上面看看。我当时被问到的题是之前准备过的，但是没答好，比较突兀的给出了最优解，面试官可能觉得我是背答案了吧。。。
如果这轮Skype技术面也通过的话，就可以免费飞到东京参加on-site面了，听说on-site面是3轮面试，一整个上午或一整个下午，几乎也是原题，可以在一亩三分地上找到。
说来也奇怪，Indeed每年的面试题都差不多，但通过面试的人总是寥寥无几，这才是高级的面试官，考察的是应聘者的解题思路，而不是答案。
Indeed Tokyo很不错，如果能拿到Offer，说明你很优秀，离人生巅峰也不远了。
Works Applications（FAILED） Works Applications简称WAP，是一家日本的ERP软件开发公司，ERP全称是Enterprise Resource Planning，简单理解就是面向企业用户的各种管理系统。WAP是正宗的日本企业，其风格和Indeed Tokyo截然不同，上班要求穿正装，估计各种行为规范也不少，但是钱也不少，折合人民币估计也有四十多万吧。WAP虽然总部在东京，但它在上海有办公室，国内校招生基本上都在上海办公。
WAP的招聘流程和Indeed很像，首先会有一个宣讲会，建议大家都参加，类似于报名考试。宣讲会之后会收到一个在线笔试的链接，要求3天之内做完2道编程题，题目比较简单。在线笔试通过之后，有一轮在线技术面试，使用的是牛客网平台，要求视频面时不能离开面试页面查资料。视频面也比较简单，大概Leetcode的easy~medium题。
对于WAP，前期的在线面试只是开胃小菜，好戏还在后头。通过两轮在线面试之后，会邀请去某个酒店现场面试。现场面试有三轮，全程英文，一般是先来段英文自我介绍，然后开始做题。比较搞笑的是，见到一面面试官时，被问到感觉如何，我说good，然后面试官说别人都是很nervous，我居然说good，感觉要被自己坑了，还好出的题都会做。前两面都不难，大概LeetCode中等题，第三面感觉是一个boss，已经不考LeetCode算法题了，考类似智力题的东西，比如有人被考到囚犯和帽子颜色的问题，我被问到的是怎样实现求两数的平均值，常规的(a&#43;b)/2有可能导致a&#43;b溢出，我想了很多方法，面试官都不满意，后来发现《程序员面试笔试宝典》上有。求平均值的问题可以先转换为求和，用位运算是a&#43;b=((a&amp;b)«1)&#43;(a^b)，a&#43;b就是按位加，对应二进制也是按位加，要进位的情况就是对应位都为1，所以先用a&amp;b找出需要进位的位，然后左移1位表示进位；还有些位可能只有一个1或者没有1，这部分加和的结果可以用异或表示，即a^b，所以a&#43;b=((a&amp;b)«1)&#43;(a^b)。那么，求平均值就是(a&#43;b)/2=(a&amp;b)&#43;((a^b)»1)。要是早点看了《程序员面试笔试宝典》，我估计也能拿到WAP的Offer了。
三轮技术面之后，会有一个HR面，听说如果前面的技术面过关的话，HR面会遇到日本boss，直接发放Offer；否则是一个中国人，寒暄几句之后，被告知技术面没有通过，但是可以参加暑期为期一周的实习活动，实习通过的话，也可以获得Offer。每年的实习主题都差不多，比如做一个酒店管理系统、电影院管理系统之类的，由于我觉得时间代价太高了，没有参加暑期实习。
虽然WAP的工资很好，但是要想拿Offer，比Indeed简单，LeetCode中等题足够，好好准备一下现场第三面。另外，即使拿到Offer，也要考虑一下工作内容是否符合自己的兴趣，毕竟ERP和当前火热的AI相比还是太古老了，而且穿正装上班估计也只此一家了。
深信服（OFFER） 深信服公司是面向企业的安全与云计算解决方案供应商，可以理解为企业版360。听说创始人是从华为跳出来的，公司整体风格和华为很像，从宣讲会上还听说这家薪资不错，尤其是博士，宣称比BAT华为都高。
深信服的提前批招聘也很早，7月初就来所里宣讲了。首先有两轮电话技术面试，面试官都会提前短信约时间，给人感觉不错。电话面试的内容比较广，网络、操作系统、C&#43;&#43;、算法等都会问到。面试官手里应该有一个问题清单，挨个问下去，不会的跳过，节奏比较快。所以面试深信服之前，要好好复习计算机基础，尤其是网络相关的，因为其主营业务和网络密切相关。
能通过前两轮电话面试的，基础都很扎实，接下来会邀请去参加他们的星云计划暑期夏令营。原本夏令营是要去深圳总部的，但是北京的很多同学都没时间，于是临时把夏令营分成了南北两波，北京的同学被安排在九华山庄度假村。在这里会听好多深信服的介绍和讲座，其中有一个清华的博士，在校期间发过很牛的Paper，自称是那一届的全国博士Top5，谈了很多选择去深信服的理由，核心思想是博士在深信服有很大的自主权，可以试错，主导一些项目，而且薪资估计真的很高。最后会有一个Boss面，主要是问项目经历，Boss是连夜赶来北京的，面试的时候哈欠连天，也没问什么实质性的问题。去的人应该都过了。其实这个夏令营主要是去体验生活的:)
最后的Offer，中规中矩，薪资并没有想象的高，也不是自己喜欢做的事情，拒。
华为（OFFER） 华为就不用介绍了，早年凭借电信网络产品赚得盆满钵满，近几年的智能手机业务更是冲出国门走向世界，真的是我国民族企业的榜样。华为严格来说是一个制造商企业，不算互联网企业，而且其招聘比较看重学历，给人感觉有点像国企。但是毕竟其产品都是计算机相关设备，对计算机专业人才的需求还是很旺盛的。华为的另一大特点是有钱，并且舍得给员工砸钱，我上一届的硕士师兄去了华为，工资碾压BAT，成功倒挂一大批老员工。仔细看看近几年各大重点高校的毕业生去向，去华为的占了很大比例，如果你想快速积累财富，又能吃苦，去华为能很好的满足你的要求。
因为师兄去了华为，3月份收到内部通知说可以提前批内推了，于是把简历给了师兄进行内推。7月初的时候要求做一个性格测试，华为特色，其他公司都没有这一环节，据说是在筛选符合华为价值观的同学。7月22日参加华为提前批优招，真的是优招，去的大部分是清北中科院的，猜测还要求本科是985高校。 优招面试很简单，因为是业务面试，主要问问项目，面试官是那种成功人士风格的Boss。二面就不问技术了，会问周围同学老师是怎样评价自己的，科研压力大吗，想去哪工作之类的，类似的问题也是在衡量应聘者和华为公司的match程度。我应该是非常match的，面试结束的时候，Boss还跟我握手了！
优招面试结束后没几天，会有一个在线笔试，编程题，三道题，最好全AC，我是前两题AC，第三题过了80%。至此，华为所有的笔试面试都结束了。但是直到9月初，才被再次邀请去华为北研参加Offer沟通会，这个会和大一刚入学参加各大社团的招新差不多，华为的各大部门开始抢人，我去了2012实验室中央软件院。
四维图新（OFFER） 华为虽然是最早面完的，但是Offer迟迟没有下来，国内其他互联网公司又还没开始面试，心急之下，看到四维图新在招聘C&#43;&#43;研发工程师，做地图搜索的，和自己有点关系。网上查了一下，发现还是腾讯地图的数据供应商，而且还是母校武大测绘学院有很紧密的合作，应该是个靠谱的公司。
跑去面试，可能是公司比较小，面试流程还很原始，直接在接待室问了我几个问题，有些题目有一定难度，连红黑树都被问到了。然后被直接拉去工位，打开VS，开始编程，所幸全部AC。等了一会，直接HR面，拿到普通OFFER。我说想申请SP，HR说下周再来一轮Boss面吧。于是下周又跑去Boss面，Boss果然是Boss，气场就不一样，问题也很灵活，都是他们地图搜索开发过程中的实际问题，比如给定中国地图和一个GPS坐标，怎样快速定位这个坐标。类似的题目很有意思，虽然有一个题目回答得不是很好，但总体上聊得还比较开心。Boss面完之后，又一轮HR面，被告知拿到SP，而且如果能来实习，实习表现好，且能申请到户口指标，则有可能有户口。
这个Offer是我校招季拿到的最早的Offer，薪资还不错，也算是稳住了阵脚。但是公司规模和名气都不算大，暂时拿来保底吧。
百度（OFFER） 百度公司和我的专业是最匹配的了，国内做搜索技术最强的，非百度莫属。百度很人性化的一点是，公司不同部门的招聘分开进行，互不冲突，所以可以同时向不同部门投递简历。我就一口气投递了网页搜索部、商务搜索部和基础架构部。很幸运，同时拿到了这三个部门的提前批Offer。
百度各部门的面试流程都很像，前两轮技术面，第三轮是Boss面或者HR面，越往后面试官的级别越高，第三面的面试官很可能就是你未来的Leader。第一轮面试比较基础，问一些网络、操作系统、C&#43;&#43;的基础知识，然后写两道算法题。第二面先写两道算法题，然后问项目，项目问得很细，我的几个搜索引擎的项目，不但问了项目的实现细节，还问了很多follow up，比如，在实战场景中，千亿级别的数据量，怎样建索引使得查询更高效，如何实现怎个搜索过程等。因为面的是搜索部门，他们对相关的技术非常了解，不要抱任何侥幸心理，不会就说不会，切莫班门弄虎。第三面Boss面比较宏观，问问职业规划，如果面试官对你比较感兴趣，会主动介绍本部门的工作，凤巢的三面面试官甚至直接加了我的微信，受宠若惊啊。
提前批面试完毕之后，9月初会有一个在线笔试，这个笔试也会刷人，所以不要掉以轻心，一定要认真准备。我当时是因为宿舍网络问题，被坑死了，那个在线笔试的系统也很变态，是个国外的系统，动不动就掉线，还只能登陆3次，超过自动退出。于是，很悲剧的3题只AC了2题。之后的几天，一直寝食难安，担心会栽在最后的笔试上。
所幸，没过多久，收到了电话通知，笔试通过，需要确定部门，让我从三个部门中选一个。我当时那个纠结啊，网页搜索部、商务搜索部和基础架构部都是百度非常核心的部门，基架的低层技术很强，网搜是典型的文本检索，商搜是广告检索，网搜的三面面试官对我很好，时不时在微信上联系我；我和商搜的三面面试官也聊得很开心，商搜是百度最赚钱的部门，各种大牛非常之多。几番权衡之后，选择了商搜（凤巢），同时也拿到了SP。
Microsoft（OFFER） 微软是我面的唯一一个美国外企，面试流程数它最多了，前后经历了：1轮在线笔试&#43;2轮skype面试&#43;3轮on-site面试。
首先，要拿到微软的skype面试机会就很难，需要通过Hihocoder的在线笔试。Hihocoder的题型和难度都相比于LeetCode复杂得多，我有一次很幸运的做到了前100名好像，拿到了skype面试机会。两轮skype面试难度也不小，比如search range，不但要求bug free，还要求你写测试用例；还比如对快排进行优化；手写堆排序；概率题等。微软的在线编程和skype面试和国内互联网不太一样，建议大家看看一亩三分地上的面经。
过了两轮skype面之后，会被邀请去参加他们的探星夏令营，大概是在8月中旬，地点就在丹棱街的微软大厦。探星夏令营第一天是参观，我因为实验室忙就没去，第二天是三轮面试。我因为研究的方向是搜索引擎，所以被安排到bing组面试了。微软的现场面试难度也不小，不是像LeetCode那样直接叫你写个DP、排序什么的，而是给出一个实际问题，需要将其抽象成一个计算机问题，然后才是代码实现。前两面顺利通过。此时已经是下午4点多了，HR说三面安排不过来，让回去等。这一等直接从8月中旬等到9月初，期间还以为是二面挂了，“让回去等”是委婉的拒绝 ，看来微软还是说话算话的。三面是Boss面，和国内互联网比较像，面项目，问了很多细节，然后根据项目衍生出一个字符串压缩的题目，让写压缩和解压缩的代码。虽然写完了，但是没保证bug free，和面试官聊了聊可能的bug以及解决方案。
过了大概一周，面试结果出来了，没有直接说给Offer，但是说面试反馈非常Positive，让加一个微信群。国庆节之前，收到微软HR电话，让我们稍安勿躁，国庆后会给正式Offer。后来直到10月31日，才收到HR的电话，正式通知Offer详情。接起电话，HR就说准备好纸笔，因为Offer内容比较多，然后就说了Package里面的各种福利，各种美金。总的来说，Package加起来在硕士里面应该是Top级别的，外企各种Balance，不加班，做的是自己喜欢的方向，而且还有可能拿户口，甚至人肉翻墙，可以说这个Offer是非常诱人的。
京东（OFFER） 京东和百度类似，也是部门自己招聘，所以可以面多个部门。我面了AI和大数据部门以及商业推荐部门。印象比较深的是，原本面了一个做分布式的组，一面发现我更适合做搜索和架构，然后就被推荐到一个做京东智能音箱的组，这个组的三面面试官是从雅虎北研过来的，听口音感觉是广东人。因为我是做搜索，智能音箱里面也需要搜索，两个人聊得很不错，面试官当场就说帮我争取SP。
面完技术面之后，过了大概一周，还要进行HR面。面试通知邮件也没说是哪个部门的。其中有个部门的HR面居然是群面，太奇葩了，也是我经历过的唯一一个群面。一屋子3个面试官，6个学生，就菜鸟网络和京东物流的对比展开讨论。首先自我介绍，有清华北大的，也有中科院各所的，还有北邮的。每次讨论我都是倒数几个发言的，对于这种压力测试，真是不适用。不过还好，HR后来跟我说我的表现不错。
HR跟我谈薪资的时候，我客套说差不多就行，后来这两个部门都拿到了Offer，薪资还真的就是差不多，白菜价。因为已经有其他选择，也没有再争取SP。听别人说争取一下能有28左右？感觉京东的定价真是因人而异啊。后来有一天还收到三面面试官的电话，问我去向定哪了，真觉得有点愧对他。
360（OFFER） 本来不打算面360，但是该公司在8月8号组织了一场中科院专场招聘会，在所有OFFER都还没有最终确定的情况下，去360逛一逛也没坏处。360的办公楼在酒仙桥，和MTK在一起，周围在施工，几乎没有吃饭的地方，给人的第一印象不是很好。10点钟到现场之后，已经人山人海了，和菜市场没什么区别，中间等待的时间都超过了面试时间。
面试分为三轮，前两轮是技术面，第三轮是HR面。一面问了一些基础知识，写了一两个算法题。二面遇到了负责360地图开发的程序员，因为地图中也涉及POI搜索，聊得很欢。HR面被问到知道360的哪些产品，虽然我现在一个360的产品都不用了，但是知道的还是不少。
面完之后，觉得Offer稳了，然后开心的回所里。第二天收到邮件通知，面试通过，还需参加一个在线笔试，类似于行测。做完之后，查看状态，被告知所有面试笔试都通过了，个人信息已经在Offer池中，但是没有正式Offer。Offer池是什么鬼，也就是没人要被扔到池子里等人捞呗。问了下其他人，大部分也是被扔到池子里了，只听说有一个人收到书面Offer。从此对360无感，无论是你们组织面试，还是我们参加面试，费了一天劲，硬是不发OFFER，坑爹。后来在10月16日，收到一封360的邮件，正式书面Offer，难道是被人相中捞起来了，真是无语。拒。
阿里巴巴（FAILED） 阿里内推只能选一个部门，内推失败之后也只有一次校招机会，所以大家选部门一定要慎重，根据自己的实力和兴趣进行选择。当时群里给出了蚂蚁金服的内推消息之后，我第一时间就选择内推蚂蚁金服了。结果面了两轮之后查状态已经挂了，也没感觉面得差。可能是因为内推蚂蚁金服的人太多了，实力要求也很高，而且自己做搜索引擎的，和蚂蚁金服不太match。
因为内推挂了之后，无法再面其他部门了。只能参加校招流程，校招在线笔试之后一直就没消息，状态也没更新，难度笔试挂了？
...</p>
  </div>
  <footer class="entry-footer"><span title='2018-02-04 19:03:36 +0800 CST'>February 4, 2018</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 伪·2018届校招面经" href="http://localhost:1313/posts/2018-02-04-2018-campus-recruiting/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="http://localhost:1313/posts/page/2/">Next&nbsp;2/7&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">bitJoy</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
