<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Posts | bitJoy</title>
<meta name="keywords" content="">
<meta name="description" content="Posts - bitJoy">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.36819bea596090d8b48cf10d9831382996197aa7e4fc86f792f7c08c9ca4d23b.css" integrity="sha256-NoGb6llgkNi0jPENmDE4KZYZeqfk/Ib3kvfAjJyk0js=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/posts/index.xml">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
    
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
    
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="bitJoy (Alt + H)">bitJoy</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    Posts
  </h1>
</header>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID 作者单位：Meta 论文链接：https://arxiv.org/pdf/2504.02137 来源：RecSys 2025 Motivation：论文要解决的问题是什么 搜推广的模型严重依赖于item id embedding的表征质量，但在工业场景下，搜推广的id表征存在如下挑战：
id量级非常大，常常是数十亿甚至是百亿的规模。因此，通常不可能给每个id一个单独的embedding（即文中的individual embedding, IE），IE的成本太高 id分布非常不均匀，马太效应严重。文中统计：0.1%的头部item占据了25%的曝光量；5.5%的腰部item占据了50%的曝光量；94.4%的尾部item只占据了25%的曝光量 id分布漂移严重：搜推广场景中item的变化非常频繁，无时无刻不在发生着新id的产生和旧id的退出，而且不同id存活的时间周期也不尽相同，所以id的准入准出策略很难完美适配所有item 针对上述问题，常见的做法是对item id采用hash然后查emb的方式（即文中的random hash，RH），将所有id hash到一个固定大小的空间，然后查emb。但是RH方式有如下缺点：
存在hash冲突，把不相关的id hash到一个桶里，导致语义混乱，学习效果不佳 无法解决id分布漂移的问题，比如hash到同一个桶的A、B两个id，如果B出现频率变高，则会带偏A的分布，影响了A的效果 无法进行知识共享，例如新出了商品iphone15，iphone15无法共享到老的iphone14的emb知识，iphone15的id emb必须完全重新学习。针对这种情况，作者做了一个更加极端的AA实验，就是copy一个完全相同的商品，只换item id，如果是IE或者RH策略，则新商品由于id emb是随机初始化的，效果不佳，这是id-based的通病 基于前缀n-gram的semantic id表征方法 针对上述问题，作者沿用了semantic id的思路，首先使用内容理解团队产出的文本、图片等多模态emb，然后基于过去3个月的item多模态emb，训练RQ-VAE模型，并产出所有item的semantic id。
上述过程都是常规操作，重点在于如何基于semantic id得到item emb表征。假设semantic id是L层，每层的codebook size是K：
最常规的做法：每层都初始化一个K*d的emb table，每层sid查各自的emb table，然后把L层的sid emb加起来。但是本文完全没有提这种方法，也没有和这种方法比较，非常奇怪。 为了比较，我个人再详细描述下这种常规做法。比如老item A的sid是(c1,c2,c3)；新来一个item B，它的sid是(c1,c2,c4)。用常规方法，A的emb是c1&#43;c2&#43;c3，B的emb是c1&#43;c2&#43;c4。两者c1、c2是可以共享的，所以常规方法也能起到一定的知识共享的效果，共享项有2项：c1、c2。但是因为RQ-VAE的沙漏问题，c2很有可能是沙漏瓶颈，信息量不足。 作者对比了Table 1中的几种方法： Trigram和Fourgram差不多，如果L=3用Trigram、L=4用Fourgram的话，本质上是把L个sid映射成了一个无冲突的int。但是这种方法映射出来的int数量太多了，是\(K^L\)。如果K=1024、L=3，则\(K^L\)就已经超过10亿了，这和直接无冲突的IE方法一样了，而且存在新id无法共享老id学到的知识的问题 All bigrams，就是所有的sid的2-gram。还是上面的例子，A的emb相当于\(c_1c_2&#43;c_2c_3\)，B的emb相当于\(c_1c_2&#43;c_2c_4\)，两者可共享\(c_1c_2\)项，相比于常规方法，虽然共享项数变少了，但粒度更精细了，孰好孰坏未可知。由Table 2可知，All bigrams的效果至少比Trigram和Fourgram好很多了，而且如果层数L越大，可共享项越多 Prefix-ngram（简称Prefix-SID方法），本文提出的新方法，把所有前缀组合成新id查emb，然后所有emb再求和。还是上面的例子，A的emb相当于\(c_1&#43;c_1c_2&#43;c_1c_2c_3&#43;c_2&#43;c_2c_3&#43;c_3\)，B的emb相当于\(c_1&#43;c_1c_2&#43;c_1c_2c_4&#43;c_2&#43;c_2c_4&#43;c_4\)，两者可共享\(c_1, c_1c_2, c_2\)三项，比之前的所有方法可共享的信息都多，而且如果层数L越大，可共享项越多，因此这种方法的效果最好，训练也最稳定 实验结果很丰富，做了很多分析，Prefix-SID方法有如下优势：
相比于IE和RH方法，Prefix-SID方法对中长尾item的提升尤其显著，因为新id和老id的表征有了知识共享 对id分布漂移问题更不敏感：由于电商模型训练时消费数据的顺序是和数据的时间一致的，比如一个月的数据，按照1号、2号、…31号这样的时间先后顺序依次训练，理论上4号的模型在4号的测试集上的效果是最好的。作者做了一个实验，分别用20号和4号的模型都在4号的测试集上进行评测，看看20号的模型指标相比4号降低了多少。作者发现，使用Prefix-SID方法和IE方法，两者的指标降低幅度都差不多，都比较小。首先IE方法由于不存在hash冲突，所以20号的模型仍然能比较好地预测4号的数据；其次，Prefix-SID方法虽然有hash冲突，但是因为冲突的item都是语义相似的，可以进行新老item的知识共享，所以这个冲突反而是好事，对模型效果无影响。但是作者发现RH方法的20号的模型在4号数据上评测指标下降比较多，因为有hash冲突，而且冲突是随机的，20号的分布已经变化很大了，导致在4号数据上效果不佳。Table 4的指标越小越好。 基于Prefix-SID方法虽然也有hash冲突，但是冲突到同一个semantic id的item表征更相似，而RH冲突到同一个桶里的item是完全随机的，相似度差。作者以IE为base，把Prefix-SID和RH都各自都冲突到同一个桶的IE emb提取出来，计算类内相似度和类间相似度，发现基于Prefix-SID的类内相似度方差小，类间距离大，说明Prefix-SID确实能把相似item聚到一起。 评论 可借鉴 基于Prefix-SID方法确实能提高新item和老item的信息共享数量，方法值得借鉴 论文实验分析很丰富 可改进 基于Prefix-SID方法居然没有和最常规的加和方法比较，是本文最大的不足 </p>
  </div>
  <footer class="entry-footer"><span title='2025-10-09 18:17:16 +0800 CST'>October 9, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID" href="http://localhost:1313/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings 作者单位：沃尔玛 论文链接：https://arxiv.org/pdf/2507.17080 来源：RecSys 2025 Motivation：论文要解决的问题是什么 多模态q2i召回通常使用CLIP的对比学习方式进行训练，在电商场景下存在2个问题：
CLIP这种方式通常是对图片整体的表征，缺乏细粒度的目标检测能力，尤其在电商场景，比如fig1，卖衣服场景，传统CLIP只能识别整张图片是一件T恤，难以关注T恤上的图案等细节特征；另外，电商图片往往存在很多附加背景、道具、模特等元素，会影响主体物体的表征 电商标题、属性等文本描述通常参差不齐，存在错误、堆砌、图文不符等问题，导致CLIP训练时图文对齐效果不佳 VL-CLIP解决方案 针对图片的处理：
将图片和商品类型（product type）输入到开源模型Grounding DINO中，让模型进行目标检测，将可信度超过某个阈值且可信度最高的区域抠出来，输入到CLIP的图像encoder中。通过这步预处理，相当于对电商图片进行了关键主体识别和提取，只提取和商品最相关的主体进行图像表征。文中使用的图像编码器是ViT-B/32。 针对文本的处理：
将商品的类型、标题、描述、性别、年龄等文本描述以及图片本身输入到Summarizer多模态大模型，让大模型产出精简、准确的文本描述\(q_0\) 将\(q_0\)和商品图文信息输入到Evaluator多模态大模型，让大模型对\(q_0\)的质量进行评判，如果\(q_0\)质量很好，则直接输出&lt;STOP&gt;；否则指出\(q_0\)的问题所在，并说明改进方法 如果第2步输出不是&lt;STOP&gt;，则将第2步的输出再输入到Refiner大模型，让大模型根据第2步的结果继续调整并输出更优的文本描述\(q_i\) 不断重复第2、3步，直到输出&lt;STOP&gt;，或者最多重复5遍 将产出的精准的文本描述q输入到CLIP的文本encoder中，文中使用的是BERT系列。产出的emb维度是512 上述Summarizer、Evaluator、Refiner都是VLM，文中使用的是GPT-4o，三个任务的prompt设计参考论文附录Table 9 上述对图片和文本的处理本质上是去噪，提取图片的主体物品、让文本描述更加精准。
产出多模态emb之后，后续的操作就是常规的召回流程了，使用HNSW进行ANN召回。
评论 可借鉴 使用Grounding DINO对图片进行主体识别，值得借鉴 使用VLM对商品标题、描述等文本信息进行去噪，值得借鉴 但如果商品量级很大的话，这两个步骤估计会很耗时 可改进 如果是q2i场景，直接用query文本是不是更真实，更接近搜索日子的真实数据分布？ </p>
  </div>
  <footer class="entry-footer"><span title='2025-10-08 23:31:54 +0800 CST'>October 8, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings" href="http://localhost:1313/posts/2025-10-08-vl-clip-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook 作者单位：Snap 论文链接：https://arxiv.org/pdf/2507.22224 来源：CIKM 2025 这是CIKM 2025的一篇resource文章，比较简单。核心内容是开源了一个基于semantic id的生成式推荐框架GRID，可以很方便地做各种消融对比实验。
主要内容 主要结论如下：
对于semantic id生成算法，简单的RQ-KMeans效果反而是最好的，好于R-VQ和RQ-VAE 生产pretrain emb的LLM模型参数量越大，效果越好，但是提升幅度有限 生产semantic id的codebook size和网络层数并不是越大越好，常规的3层，每层256个id效果反而最好 生成式推荐时，是否需要在用户行为序列基础上增加一个user id，实验发现增加user id效果反而变差，不增加user id效果最好 生成式网络结构encoder-decoder对比decoder-only，发现前者效果更好，因为前者能充分学习到行为序列完整的信息 对行为流进行滑动窗口数据增强能提升模型的泛化能力 当semantic id到item存在映射冲突时，随机选一个item的效果和对冲突item追加一个区分标识（digit），两者效果差不多 在生成式beam search的时候，限制只输出合法semantic id和不增加限制，两者效果差不多 评论 看这篇文章主要是想看看不同semantic id生产方法的对比，发现RQ-KMeans居然比RQ-VAE更好。个人感觉这两个方法效果应该差不多，后者应该更好点才对。首先，RQ-VAE的量化loss本质上和KMeans聚类是一个意思；其次，RQ-VAE还增加了一个重构loss，感觉产出来的semantic id和原始emb的信息损失应该更少。
此外，本文的所有实验都是基于亚马逊的公开数据集，数据量肯定不能和真正的工业数据集相提并论，所以文中很多结论有可能只适用于本文的设定，换一个场景估计结论就变了，所以看看就好。
最后，文中很多结论只写了现象，要是能增加原因分析就好了。
</p>
  </div>
  <footer class="entry-footer"><span title='2025-10-07 12:09:43 +0800 CST'>October 7, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook" href="http://localhost:1313/posts/2025-10-07-grid-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation 作者单位：网易云音乐 论文链接：https://arxiv.org/pdf/2508.20359 来源：CIKM 2025 Motivation：论文要解决的问题是什么 多模态emb在搜推的应用方式，通常是先将多模态emb转换成semantic id，然后把semantic id用到搜推模型中，这种方式有如下两个问题：
模态内语义退化：多模态emb转换成semantic id通常使用RQ-VAE或者RQ-KMeans的方法，这种方法在不断残差的过程中，后续残差聚类结果已经不能反映初始emb的聚类效果了。其实就是semantic id的沙漏问题，具体可以看这篇文章，后续有空再分享这个问题。 简单来说，如下图所示，初始有DJ、Rock、Lullaby、Choir四个类，但是对残差emb（即RQ-VAE的第二层）聚类的话，初始的四个类的item就打散了，会聚到不同的簇中，也就是RQ-VAE的后续层的聚类效果已经和初始emb的聚类效果很不一样了，这就是文中说的语义退化问题 模态间建模差异：搜推场景的item通常有多种模态特征，比如文本、图像、音频等，传统方法在多模态融合方面比较简单，不能很好地捕捉多模态之间的关系。 PSRQ生产semantic id 本文是音乐推荐场景，主要用到两种模态：text和audio，分别用百川和MERT提取text和audio的模态emb。
生产semantic id的方法如下图所示：
fig2a是传统的RQ-KMeans的方法，每一层都用上一层的残差进行聚类。如上文所述，由于沙漏问题，会导致后续层次的semantic id存在语义退化问题 fig2b是本文新提出的PSRQ量化方法，在RQ-KMeans基础上，每一层除了有上一层的残差向量，还会concat上初始emb减去残差emb后的向量。这样就能区分出残差相似，但初始emb不同的item了，也就避免了RQ方法的沙漏问题，后续semantic id也能保留初始emb的语义信息。fig1d能看出来第二层semantic id仍然能够反映初始emb的分类效果。 Semantic id在下游的应用方法 如下图所示：
每个item有两套多模态emb：text和audio，但是有三套semantic id，除了text和audio各自产一套semantic id之外，还会把text和audio的emb concat起来，再产一套semantic id，相当于多模态融合的semantic id semantic id的emb在排序模型中随机初始化，然后端到端训练 semantic id在用户建模时，使用DIN模型，query用的是多模态融合的semantic id emb，行为流分别用text和audio的semantic id emb。作者说这种方法既能捕捉到单模态细粒度的信息，又能建模跨模态的交互信息 评论 可借鉴 PSRQ的semantic id生产方法确实很有意思，在每一层都用上原始emb，这样不同簇的item在每一层都能分开，不会出现沙漏问题，使得每一层的semantic id都能保留原始emb的语义聚类信息 产了多套semantic id，单模态semantic id是常规操作；多模态emb concat后也产一套semantic id，是个创新点 用户建模时query用多模态semantic id，行为流用单模态semantic id，也是个创新点，虽然论文说这种方法效果最好，但是有点存疑 论文有个实验结果对比了不同semantic id量化方法的效果，结论是：PSRQ &gt; RQ-KMeans = RQ-VAE &gt; VQ &gt; PQ 可改进 pretrain emb和semantic id的生产都没有对齐协同信号 semantic id在下游应用时直接端到端训练，而没有使用codebook初始化，会不会丢失信息比较多？ 产semantic id的过程中，模态内语义退化的问题，描述了现象，但是没有用定量的指标来说明问题，感觉可以借鉴【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】的方法，定量说明后续层的semantic id的聚类效果或者说区分能力相比初始emb已经相差甚远了 fig2b中，第一层的codebook的dim=d，后续层的codebook的dim=2d，那么后续层的残差dim也是2d，那么初始emb怎么和后续的残差emb相减呢，维度对不上啊？我理解可能是这样的，后续层聚类的时候用的是concat的dim=2d的emb，但是算聚类中心的时候只用了残差本身的emb，这样就能解释得通了，但是文中对这部分的细节没有解释。 </p>
  </div>
  <footer class="entry-footer"><span title='2025-10-06 21:01:25 +0800 CST'>October 6, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation" href="http://localhost:1313/posts/2025-10-06-psrq-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System 作者单位：快手 论文链接：https://arxiv.org/pdf/2508.10584 来源：CIKM 2025 Motivation：论文要解决的问题是什么 Semantic id生产时，要么没有和协同信号对齐（fig2(1)），要么是两阶段对齐方式（fig2(2)）：
例如LETTER先生成协同emb，然后和semantic id对齐 或者例如QARM，先协同对齐emb，再生产semantic id 把协同对齐和生产semantic id分成两个阶段，天然有信息损失，不是最优的。本文的目的就是把生产协同emb，以及semantic id的协同对齐放到一个模型中联合训练完成，尽量减少信息损失（fig2(3)）。
主模型 主模型如上图所示，中间的ICDM是user和item的双塔模型，用于学习user和item的协同id-based emb；两边分别是生产user和item的semantic id的量化模型。
中间的ICDM就是经典的召回双塔模型，使用点击样本进行训练，唯一不同的是，在user和item塔都有流行度去偏模块，用于学习user和item的无偏emb，后续user和item的semantic id协同对齐用的也是无偏的emb。
两边分别是user和item的semantic id量化模型，两者比较类似，以item为例：
先把item的各种信息，如title、desc、ocr等信息用文本构造成prompt，输入到LLM，借助LLM的summary和reasoning能力，产出item的详细描述 然后把LLM产出的描述再输入到一个预训练的embedding模型PLM，文中用的是bge m3模型，得到item emb 后续就是标准的RQ-VAE过程了 需要注意的是，上述前两步，分别用到了LLM和PLM两个大模型，而且看图上这两个模型都是freeze的，也就是说并不微调这两个大模型。后续协同对齐用的emb是RQ-VAE重构emb的中间层结果，即图中的item quantized emb。
semantic id的协同对齐方面，有三大类对齐任务：
U2I对齐：量化user emb和协同item emb对齐、量化item emb和协同user emb对齐 U2U和I2I对齐：量化user emb和协同user emb对齐、量化item emb和协同item emb对齐 U2U和I2I的共现对齐：点击相同item的两个量化user emb对齐、同一个user点击的两个item的量化item emb对齐 由于fig3中的协同模型和semantic id模型是联合训练的，总共有3大类loss：
中间的ICDM的双塔召回模型的loss 两边的产semantic id的loss 三个模块的对齐loss 评论 可借鉴 把semantic id的生产和协同信号对齐统一成一阶段的模式，信息损失更少 中间的ICDM模型生产协同emb时进行了去偏，协同对齐的时候用的是去偏的emb，这是其他论文很少提到的 可改进 太复杂了！3个模块，3大类loss，每类loss又有很多个小loss，总loss数量加起来有十多个。。。 任务太多，各种去偏、对齐loss，真的不会互相影响吗？ 中间的ICDM模块有必要吗？我理解ICDM本质是为了训练产出协同emb，但是因为训练样本本身是点击样本，样本本身已经包含了搜推场景的协同信号，也就是ICDM本身没必要存在了，直接用相同的样本训练两边的semantic id量化模型就行了，也能实现在训练semantic id的过程中，完成协同信号的对齐 生产semantic id的emb来自LLM和PLM，但是这两个大模型都是freeze的，如果把这两个模型也sft，效果会不会更好？其实我原本以为的一阶段就是这样的，这也是我在【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】中提到的一阶段方法。 </p>
  </div>
  <footer class="entry-footer"><span title='2025-10-05 20:26:43 +0800 CST'>October 5, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System" href="http://localhost:1313/posts/2025-10-05-das-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou 作者单位：快手 论文链接：https://arxiv.org/pdf/2411.11739 来源：CIKM 2025 Motivation：论文要解决的问题是什么 多模态emb在搜推场景应用时通常采用如下图的两阶段方式，先预训练多模态emb，然后作为一个冻结特征放到搜推模型中。这种方式存在2个问题：
表征不对齐：多模态emb预训练的任务通常是图片分类或者文本的MLM，和下游搜推任务不对齐 表征不更新：多模态emb在搜推任务中作为冻结特征，没有更新 本文的方法就是想要解决上述2个问题。 对齐搜推任务的多模态emb预训练 为了解决多模态emb表征不对齐的问题，本文提出的多模态emb预训练任务直接对齐搜推场景，使用U2I和I2I召回模型，挖掘出相似item pair，然后通过对比学习微调多模态大模型。
具体来说，通过U2I和I2I模型，能够拿到item emb；然后用每一个target item emb去行为流中检索出最相似的商品，作为trigger item emb。&lt;trigger, target&gt;构成一对正样本，然后进行对比学习训练。
通过召回模型构造的训练样本，和搜推场景的协同信号对齐了，解决了开头提到的第一个问题，即表征不对齐的问题。
Semantic id生产方法 Semantic id的生产方法如上图右半部分所示，有两种方式：
VQ：直接圈定一定数量（如N）的item emb作为底池，编号1~N，然后任意来一个item emb，通过对底池emb进行KNN搜索，找出top-k相似商品，假设是(a,b,…,k)，则VQ编码的semantic id就是(a,b,…,k)。文中取k=25，感觉挺大的。。。 RQ-Kmeans：对圈定的N个item emb不断进行Kmeans聚类、求残差、残差继续Kmeans聚类的过程。文中取迭代次数为L=6，但是没说每次聚到多少个类。 注意：文中的RQ-Kmeans方法和RQ-VAE还不一样，RQ-Kmeans没有训练过程，也没有重构loss，纯粹是每次进行聚类，然后选聚类中心作为码本的过程。文中也没有对比过为啥不用RQ-VAE。
产出两套semantic id之后，直接在下游排序任务中进行端到端更新，解决开头提到的表征不更新的问题。具体建模方法比较常规，不是本文的重点，略讲。
评论 可借鉴 多模态emb预训练任务是i2i的，直接和下游搜推任务对齐 semantic id有两种产出方式，VQ和RQ-Kmeans，尽可能多地保留原始多模态emb的信息 可改进 多模态emb预训练和下游任务对齐，在2025年不算新鲜事了，常规操作。而且文中i2i的构造过程依赖U2I和I2I召回模型，有外部依赖，不够漂亮 VQ的方法，k=25这也太长了吧，相当于一个小型行为流了，会导致下游任务的特征处理更复杂 为什么用RQ-Kmeans而不是RQ-VAE，没有任何说明与对比 从pretrain emb量化成semantic id的过程中，存在严重的信息丢失，这在Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs论文中有讨论 </p>
  </div>
  <footer class="entry-footer"><span title='2025-10-04 18:24:40 +0800 CST'>October 4, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou" href="http://localhost:1313/posts/2025-10-04-qarm-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs
    </h2>
  </header>
  <div class="entry-content">
    <p>
基本信息 论文标题：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs 作者单位：香港城市大学&amp;腾讯 论文链接：https://arxiv.org/pdf/2509.02017 来源：CIKM 2025 Motivation：论文要解决的问题是什么 LLM4SR的基本范式如下，即用LLM直接来做搜推的范式（这种方式在学术界常见，但在工业界不常见）。由于LLM的输入词表范围是有限的（通常比较小），因此其token emb dim通常比较大，比如2048或者4096；而搜推场景的item量级很大，而且在不断更新，因此工业界经典的id-based的搜推模型的item emb dim通常比较小，比如64或128。经典的id-based的搜推模型能比较好地学习到搜推场景的协同信号，为了让LLM模型也能感知这种信息，LLM4SR范式通常会先预训练一个id-based的经典搜推模型，然后将其中的item id emb通过下图的Linear Projection的映射层，映射到LLM token emb的空间，让LLM也能感知搜推的协同信号。
上述LLM4SR范式存在两个问题：
维度坍缩：id-based训出来的id emb dim比较小（如64），LLM token emb dim比较大（如4096），在由id emb通过Linear Projection映射到toen emb的过程中，虽然64映射到4096空间了，但扩维后的矩阵存在低秩问题，即还是只利用了4096中的64维的空间。
论文中，作者分两种情况进行了分析，如果Linear Projection只是一个线性层的话，通过公式推导能得出上述结论；如果Linear Projection包含非线性变换，作者通过实验分析也发现了维度坍缩的现象。 灾难遗忘：除了使用id-based模型产出的id emb，LLM4SR也常用多模态模型产出item emb表征，然后转换成semantic id输入到LLM4SR中。在这种情况下，产出的semantic id通过会遗忘多模态item emb的信息，导致下游LLM4SR的效果不佳。
论文中，作者用公式9来衡量semantic id保留pretrain多模态emb的信息量。具体来说，如果行为流中的商品序列是{A,B,C,D}，target item是E。使用pretrain多模态emb能计算出E和A~D的相似度，例如相似度&lt;E,A&gt; &gt; &lt;E,B&gt;。如果将pretrain多模态emb转换成semantic id，然后由semantic id恢复出新的A~E的emb之后，再计算E和A~D的相似度，如果仍然有&lt;E,A&gt; &gt; &lt;E,B&gt;，则认为一致（concordant），否则不一致（disconcordant）。这个分析方法挺好的，通过这个指标能估算出转换成semantic id之后，仍然保留原有pretrain多模态emb对搜推场景的序关系的保留程度。 作者发现，转换成semantic id之后，信息只保留了37.14%；进一步，如果semantic id是在下游任务中端到端训练的，则信息只保留了5.5%，也就是说94.5%的pretrain emb的序的信息都丢掉了，也就是灾难遗忘。 Semantic id构建方法 3套emb来源，一套id-based经典搜推模型产出的包含协同信号的emb，另外两套是LLM2CLIP产出的多模态文本和图片emb。作者提到传统CLIP对长文本处理能力较弱，所以升级到LLM2CLIP，能更好地处理长文本。 Semantic id构建方法是经典的RQ-VAE的方法，但有如下两个改进点： 将emb的重构loss由MSE升级成MMD (maximum mean discrepancy)，MSE是计算原始emb和重构emb的欧式距离的误差，而MMD是计算两个分布的diff，实验表明能MMD比MSE能保留更多的pretrain多模态emb信息（即上述公式9），保留44.36% 对量化后的emb做了对齐，因为LLM2CLIP本身进行了图文模态的对齐，所以文中只新增了id emb分别和文本、图片模态的对齐 此外，还有一点论文没提但可能和常规RQ-VAE不同之处，就是原始emb在进行RQ-VAE之前，有一个Encoder升维的操作，在重构loss前对应有一个Decoder降维的操作，而semantic id量化恢复emb是Decoder之前的那个。这一升一降，估计也有助于缓解维度坍缩。 ...</p>
  </div>
  <footer class="entry-footer"><span title='2025-10-04 11:10:11 +0800 CST'>October 4, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs" href="http://localhost:1313/posts/2025-10-04-mme-sid-paper-reading/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">公告
    </h2>
  </header>
  <div class="entry-content">
    <p>博客数据恢复中，敬请期待！
测试图片： 测试代码：
1 2 3 4 5 6 7 8 9 10 11 12 13 // Necessary header files for input output functions #include &lt;iostream&gt; using namespace std; // main() function: where the execution of // C&#43;&#43; program begins int main() { // This statement prints &#34;Hello World&#34; cout &lt;&lt; &#34;Hello World&#34;; return 0; } 测试数学公式： This is an inline \(a^*=x-b^*\) equation.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-08-15 22:03:06 +0800 CST'>August 15, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 公告" href="http://localhost:1313/posts/2025-08-15-announcement/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Linux性能分析工具简介
    </h2>
  </header>
  <div class="entry-content">
    <p>这半年一直在研究pLink 2的加速策略，了解了相关的性能分析工具，现记录如下。
对软件进行加速的技术路线一般为：首先对代码进行性能分析（ performance analysis，也称为 profiling），然后针对性能瓶颈提出优化方案，最后在大数据集上评测加速效果。所以进行性能优化的前提就是准确地测量代码中各个模块的时间消耗。听起来很简单，不就是测量每行代码的运行时间吗，直接用time_t t=clock();不就好了，但是事情并没有那么简单。如果只进行粗粒度的性能分析，比如测量几个大的模块的运行时间，clock()还比较准确，但是如果测量的是运算量比较小的函数调用，而且有大量的小函数调用，clock()就不太准确了。
比如下面的一段代码，我最开始的性能分析方法是在fun1()~fun3()前后添加time_t t=clock()，然后作差求和的。但是3个fun()加起来的时间居然不等于整个while循环的时间，有将近50%的时间不翼而飞了！
[cpp] while(true) { if(fun1()) { for(int i=0;i&lt;k;&#43;&#43;k) { if(flag1) { fun2(); } } } else { fun3(); } } [/cpp]
一种可能是while循环以及内部的for循环本身占用了较多的时间，但是好像不太可能呀。还有一种可能是clock()测量有误，time_t只能精确到秒，clock_t只能精确到毫秒，如果一次fun*()的时间太短，导致一次测量几乎为0，那么多次的while和for循环调用累加的时间也几乎为0，导致实际测量到的fun*()时间远小于真实时间。所以自己用代码进行性能分析可能会有较大的误差，最好借助已有的性能分析工具。
性能分析和操作系统有较大的关系。因为C&#43;&#43;11以前的多线程在不同操作系统中有不同的实现，比如在Windows中使用的是Win32 threads，需要包含windows.h头文件，在Linux中使用的是POSIX Threads，需要包含pthread.h头文件，所以选择性能分析工具首先需要看代码使用的多线程是哪个版本。如果使用的是Win32 threads，则需要在Windows平台选择热点分析工具；如果使用的是POSIX Threads，则需要在Linux平台选择热点分析工具；当然，如果代码中没有多线程或者采用的多线程是C&#43;&#43;11标准统一的多线程，原则上可以忽略操作系统的限制。
Windows平台上，之前用过微软的Visual Studio工具进行Profiling，效果很不错，网上的介绍也很多，这里就不详细介绍了。
通过网络搜索我发现了三款Linux平台下主流的热点分析工具，分别是GNU gprof、Valgrind和Google perftools，三款工具的主要特点如下表：
工具	使用命令	是否需要重新编译	Profiling速度	是否支持多线程热点分析	是否支持链接库热点分析 GNU gprof	./test; gprof ./test ./gmon.out	是	慢	否	否 Valgrind	Valgrind –tool=callgrind ./test	否	非常慢	是	是 Google perftools	LD_PRELOAD=/usr/lib/libprofiler.so CPUPROFILE=./test.prof ./test	否	快	是	是 GNU gprof是GNU G&#43;&#43;自带的热点分析工具，使用方法是：1. 使用-pg选项重新编译代码；2. 执行程序./test，生成热点分析结果gmont.out；3.使用gprof查看结果gprof ./test ./gmon.out。因为gprof要求用-pg重新编译代码，需要在Debug模式下进行Profiling，所以速度较慢。另外gprof不支持多线程的热点分析。这个工具另一个大问题是，不支持链接库的热点分析。很多大型项目为了模块化管理会生成很多动态链接库供其他程序调用，如果要分析每个模块的热点，这个工具就不适用了。
...</p>
  </div>
  <footer class="entry-footer"><span title='2017-02-07 19:15:56 +0800 CST'>February 7, 2017</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to Linux性能分析工具简介" href="http://localhost:1313/posts/2017-02-07-introduction-to-performance-analysis-tools-in-linux/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">2016年终总结
    </h2>
  </header>
  <div class="entry-content">
    <p>2017农历新年的钟声都已经敲响了，我这2016的年终总结才开始动笔。
2016年，经历了很多，也成长了很多，遇到了很多曾经以为只会出现在电视剧中的场景，令人开始怀疑这个世界。前几天在朋友圈看到一个同学发的状态，觉得很适合作为这篇年终总结的开端。（同学你要是觉得被侵权了，告诉我，我立马删掉:-)）
每个家庭的故事都会是一部长篇史诗。曾经总以为很多情节只会出现在电视剧中，现实的生活很是平淡无味，没有任何波澜，偶尔甚至还会抱怨一下自己不是故事的女主角，其不知现实的生活相比于电视剧，往往是有过之而无不及。 今天哥哥来电话了，从“天津”，一个美丽的谎言，一直在继续，还会坚持很久… 奶奶很开心… ——by angel
关于学习和科研。上半年在雁栖湖完成了研一的下半个学期，完完全全的结束了自己的学生时代。下半年开始进入实验室，直面惨淡的科研。原本以为会由组里的大师兄超哥指导我，没想到中秋前3天，直接接到H Boss的指令，要在中秋前完成一项我从来没做过的评测。不知道怎样设计实验，不知道怎样计算评价指标，甚至连需要评测的软件都不熟。不过好在加班加点完成了。
凌乱的工位
从9月到10月中旬，一直在各种数据集上做各种对比评测，基本上一周做完一个评测数据集，完成一份报告，直接提交给H Boss，下一周在做另一个评测的同时，要根据导师的反馈建议修改完上一个评测报告。这一个多月的时间，共完成了5份报告共计14个版本，真的是要吐了。
后来发现pLink要比对手慢，于是就尝试各种加速策略。开始从外围查找原因，尝试了各种策略，虽然多多少少能加速，但是效果都不完美，有可能对精度有影响。直到12月份，借助谷歌的开源性能分析工具gperftools才找到了软件的性能瓶颈，开始优化并取得了好几倍的加速效果。12月份对软件本身的性能优化大概是我这半年做的唯一一个和计算机本身有点关系的工作吧，可能也是为数不多的我比较享受的一件事。
整个这半年的工作，都是H Boss御驾亲征，亲自指导，当然这样有利有弊。好处是能直接和H Boss对话，机会难得呀，H那种严于律己、追求完美的品质，H的为人处世、口才都是非常值得我学习的。坦白说，虽然我只是无名小卒一个，但从小到大，真正让我从心里佩服的人没几个，H Boss绝对算是其中之一。当然不便之处也非常明显，首先会感觉特别累和压抑，除了每周一次的面谈，每天的邮件，还经常在晚上11点多收到老师的工作微信。所以这半年确实不轻松，工作日晚上基本都在加班，而且加班到晚上11点也是常事，周末也至少工作一天。几乎没有时间运动，身体素质应该下降了不少。与人的沟通也非常少，好几个师兄师姐都问我为啥看我整天都在工位上坐着，好像从来没有动过。夸张点说就是每天晚上下班要走的时候，才发现自己这一天还没有说话。
当然半年高强度的工作，也有不少的收获，基本摸清了pLink代码的来龙去脉，也加速了两三倍，自己的表现也稍微得到了老师和同学的肯定。不过我自己还是不太满意的，加速并没有达到理想的效果。
关于亲情。随着我们两兄弟的大学毕业，家里的情况也在稍稍好转，但是只能算是曲折前进吧。以前小的时候，都是爸妈两个人闹，现在哥哥出场了，真是可笑。谈了快两年的女朋友，女方父母又是要查户口本，又是催着要付定金，说什么不给定金就要拉回老家相亲。这TM比电视剧还荒唐，真把自己当商品了，是不是给的钱多就跟谁呀，混蛋。哥哥也不是个省油的灯，分手之后没过多久说什么被公司派去新加坡学习了，去了之后，连个固定的联系方式都没有，三天两头失联，都老大不小的人了，还让父母担心。工作了两年，一分钱都没攒到，连大学的助学贷款都要我这个还在上学的弟弟替他还。女朋友分手也就算了，没赚到钱也不要紧，关键是你不能让家人这样担心你呀，你定期给家人打个电话，说说你到底在哪里干什么，既然到了新加坡，发几张国外的照片回来分享一下，不可以吗？已经两年过年没回家了，而且两年除夕居然连个电话都没有，这不是不孝是什么，混蛋。
今年妈妈也终于愿意外出挣钱了，虽然不多，但是起码在和爸爸一起努力。家里装修好了一层房子，但是也就是把墙什么的弄好了，家具还没制备。本来想着过年回家给家里买个小米电视，但是爸妈死活说不要买，现在买了也就过年看几天，不划算。后来只好作罢。放假给老爸买的红米手机，终于在除夕这一天拿到手了。给爸妈包的红包，也在按计划逐年的递增着。
自己有时候也埋怨家里，为什么家境这么的不堪，为什么父母没有达到我理想的高度，为什么哥哥这么不争气，为什么没有人关心我。但是埋怨有用吗，肯定是没用的，还是要看各自的造化。
关于爱情。我还是太幼稚，看看我的家境吧，有哪个女生愿意摊上我家这些破事呢，我就不应该奢望有什么爱情。不过今年上半年，爱情确实来过，抛开所有的一切，纯粹的校园爱情。可是10月份的一件事，彻底打醒了我，爱情没有那么简单，需要考虑的问题太多了。关于那段时间的记忆，写过很多文字，也流过很多泪。回顾整个下半年，欣欣和我的状态都不太好，除了那件事的原因，和工作变化也有很大的关系。我们都从雁栖湖到中关村，需要经历一个由学生到科研工作者的角色转变，面对科研的未知，都显得有些手足无措。科研的不顺，生活的压力以及家里的一些烦心事，一股脑的涌向了我们，矛盾也时有发生。经历过不少的磕磕绊绊，总算顺利度过了2016年，没有了热恋时期的疯狂，生活终要归于平淡。正所谓陪伴是最长情的告白，爱情的意义是否就在于两个人一起经历，一起成长呢。让我们共同守护。
关于友情。是的，我还欠很多人一顿饭。很多同学，如果长时间不见面，恐怕真的要忘掉了。
关于个人提高。上半年忙于课程学习，下半年忙于科研，花在个人提高上的时间真的是太少了。不过感谢有欣欣一直做我的榜样，我现在的小目标就是希望比欣欣看更多的书、刷更多的题。
另外下半年回到市区之后，也去了一些之前没去过的地方，比如：清华艺术博物馆、繁星戏剧村、中国美术馆、三联书店、香山等地。其中前两个地方都是和欣欣一起去的，感觉超棒~第一次看达芬奇的特展，开始了解这样一个天才，后来还看过他的传记；第一次去剧院看话剧，感觉和看电影完全不一样，小剧场的效果也是棒棒的。2016年12月31日也是一个值得纪念的日子：
2017跨年活动~
今年借着CNCP2016会议的机会，去了一次大连，见识了一下海滨城市的风貌，后来还跑去渤海学游泳，海水很脏，而且咸得发苦。希望今后每年都去一个除了上班地点和家里之外的第三个城市。
大连滨海国家地质公园
大连滨海国家地质公园
最后看看年初计划的完成情况：
完成国科大下学期的课程任务：完成 接手pLink软件：完成 刷完LeetCode所有题目：进度147/461，没有完成 读10本书：目前读了《数学之美》、《大话设计模式》、《我不知道该说什么，关于死亡还是爱情》、《男人来自火星、女人来自金星，卷I》、《只有医生知道，卷I》、《文学的种子》、《讲理》、《暗时间》、《达·芬奇传：放飞的心灵》、《人间失格》，刚好10本，圆满完成任务:-) 去电影院看10场电影：目前看了《美人鱼》、《北京遇上西雅图之不二情书》、《忍者神龟2：破影而出》、《七月与安生》、《湄公河行动》、《比利·林恩的中场战事》、《你的名字》、《血战钢锯岭》，只有8场，其中7场是和欣欣一起看的~ 改正坐姿：有一段时间刻意改正了，但是这东西貌似改不过来？ 除了LeetCode完成度太差之外，其他计划完成度还是蛮高的。下面定一下2017年的年度计划：
发表pLink 2文章 至少完成毕业工作的80% 刷完LeetCode所有简单题和中等题，找工作之前最好刷完两遍 找到一个满意的工作 读10本书 去电影院看10场电影 看一场话剧（音乐会、歌剧等都可以） 学会游泳 去第三个城市 简短总结一下我的2016：完成了由上课到科研的转变；开始有能力感恩家人；遇到了欣欣，由一个人变成了两个人；第一次去剧场看话剧。展望2017，找工作和准备毕业迫在眉睫，注定又是繁忙的一年！
最后用汪老师的年终总结PPT封面的一句话来结束吧：
</p>
  </div>
  <footer class="entry-footer"><span title='2017-01-28 21:26:42 +0800 CST'>January 28, 2017</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to 2016年终总结" href="http://localhost:1313/posts/2017-01-28-2016-summary/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="http://localhost:1313/posts/page/2/">Next&nbsp;2/6&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">bitJoy</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
