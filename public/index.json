[{"content":"\n基本信息 论文标题：Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm 作者单位：阿里巴巴 论文链接：https://arxiv.org/abs/2510.14321 来源：arxiv 一、问题 电商emb召回场景，目前的方法都是直接字面语义上的对比学习训练（direct-embedding methods），即q2i的对比学习训练。对于复杂、困难的query，语义理解能力不足，比如下图Fig1中的query=\u0026ldquo;比茶更提神的饮料\u0026rdquo;，仍然会召回很多茶，因为字面理解没有理解query背后的深层含义。\n二、方法 使用LLM强大的推理能力（reasoning），先推理出CoT，然后基于CoT再产emb。比如上面的例子中，经过LLM推理之后，推理出咖啡、红牛等关键词，通过这些关键词再去产emb然后召回，效果就好很多。\n2.1 训练样本构造方法 如下图Fig2中的Data Construction部分：\n收集线上query，尤其是那种困难query，就是在现有direct-embedding表现不好的query 把这些query喂给现有召回模型，得到召回商品集合① 然后使用强大的Qwen3-30B-A3B-Instruct生产CoT扩展信息 Unconstrained Reasoning：首先不加任何限制地生产CoT，尽可能利用大模型的世界知识和推理能力，生产充分完全的CoT信息 Information Extraction：由于上一步产出的CoT信息太长了，不利于线上推理，因此把上一步产出的CoT和原始query再次输入给大模型，让大模型抽取其中的关键信息，以keyword list形式输出 Post Processing：最后对上一步抽取的关键词进行后处理，去除重复词，去除query中已有的词等，得到精简、干净的关键词列表，列表最大长度是16 接着把query和CoT喂给已有的向量召回模型，得到扩展的召回商品集合② 由于要训练模型的Reasoning能力，所以只取出集合②-①的差集部分，这部分是CoT带来的增益商品集合 最后使用相关性模型对商品集合②-①进行过滤，过滤出相关的商品 通过上述步骤，产出约7.5kw的\u0026lt;query, CoT, item\u0026gt;三元组 把上述样本划分成两部分，7.1kw的\u0026lt;query, CoT, item\u0026gt;三元组用于Cold start预训练；剩余400w的\u0026lt;query, item\u0026gt;用于RL微调 2.2 Cold Start预训练 对应图Fig2左下角部分，该模块通过大规模的\u0026lt;query, CoT,item\u0026gt;三元组数据预训练，想要达到两个目的：一是让基础模型具备think能力；二是让基础模型产出的emb和下游q2i任务对齐。\n这里使用的基础模型是Qwen2.5-3B-Instruct，比生产CoT的模型（Qwen3-30B-A3B-Instruct）小，其实也有点蒸馏的感觉，把大模型的CoT能力蒸馏到小模型中。\n训练任务包括两个，一个是CoT的NTP loss（对应图中的SFT loss），另一个是q2i的对比学习InfoNCE loss。query塔和item塔共享参数，他们的emb都是最后一个特殊token \u0026lt;emb\u0026gt; 的emb。\nLoss组合： 2.3 RL微调 上一步的SFT主要进行模仿学习，模仿更大的大模型的think能力，小模型本身的reasoning能力受限，接下来需要用GRPO对小模型进行RL微调。RL微调同时对生产CoT和生产emb两个任务都有作用，具体看下面的reward：\nRL微调设计了3个reward：\nFormat Reward：产出的CoT格式符合“\u0026lt;think\u0026gt; Specific CoT \u0026lt;/think\u0026gt;\u0026lt;emb\u0026gt;”就得1分，否则得0分 Length Reward：产出的CoT格式符合长度限制（\u0026lt;=16）就得1分，否则得0分 Retrieval Accuracy Reward：联合原始query和产出的CoT产出的增强query emb，与batch内所有的item emb求相似度，正确item所在的排名为\\(rank(d_i)\\)，再根据公式12计算一个排名的reward。核心思想是：正确的item与query的相似度排名越高则reward越大（即rank值越小则reward越大）。 最后，上述3个reward通过三个β系数组合起来： RL的训练目标，GRPO loss如下： 在RL阶段，除了有GRPO loss，原有的InfoNCE对比学习loss也还在，两个loss通过系数γ组合起来，如公式16所示。\n2.4 训练细节 产CoT的推理模型：Qwen3-30B-A3B-Instruct emb基座模型：Qwen2.5-3B-Instruct 训练资源：128 GPUs 预训练阶段： CoT最大长度：16 loss系数：\\(\\lambda_1=0.1, \\lambda_2=1\\)，也就是InfoNCE loss是主导的，NTP的loss权重很小 batchsize=128 lr=1e-5，cosine scheduler with a warmup ratio of 0.03 训练1个epoch RL阶段： GRPO每次采样8个CoT length reward长度阈值16 三个reward系数：\\(\\beta_1=0.5, \\beta_2=0.2, \\beta_3=1\\)，\\(\\beta_3\\)最大，即准度的reward最重要 loss系数\\(\\gamma_1=1,\\gamma_2=0.1\\)，即GRPO的loss权重最大 batchsize=256，RL阶段的batchsize是预训练阶段的2倍 lr=1e-6，cosine scheduler with a warmup ratio of 0.03，RL阶段的lr比预训练小 训练1个epoch 三、结果 如下图所示，最后一行LREM(Cold Start+RL)效果最好，但是LREM(Cold Start)效果很差啊，比Qwen2.5的好几个base都差。。。这就很奇怪了，理论上LREM(Cold Start)去掉CoT和SFT loss的结构和Qwen2.5 (Uni-Attn. Last)的结构是完全一样的，但是前者的指标比后者差很多，难道是加了CoT和SFT loss有负向影响？ LREM(Cold Start+RL)比LREM(Cold Start)提升非常大，也能说明LREM(Cold Start)效果很差。但是不应该呀，RL的几个reward，理论上在预训练阶段都有训练到，即使对于准度的reward3，其实也相当于positive在in-batch内的相似度要大于其他negative，本质上和InfoNCE loss的目的是一致的，为啥换成RL的形式后提升这么大？ 作者还分析了CoT的作用，把CoT换成空的、随机字符串、或者单纯重复query，效果都下降很多，说明CoT很重要。 四、评价 问题切入点很好，特别是在相关性、召回场景，query更加多样，困难的query也更多，而且本文主要就是解决困难query的场景，但是这种情况在线上的占比应该很小？作者没讲 CoT数据的生产经过了很多步骤，看得出来经过了多轮迭代优化，也说明这个环节有很多坑，直接用更大模型产出的无约束的CoT可能效果不行。。。 本文只针对query进行了CoT扩张，能不能对item也扩展一下呢？ 本文最终效果提升很大，但是这个提升真的来自RL吗，感觉有点怀疑呀。个人感觉CoT的信息可能更重要一点，需要补充一个对比实验，即用emb基座模型直接加入本文产出的CoT进行对比学习预训练，不加后面的RL，看看效果怎么样。就是CoT只作为附加特征，相当于LREM(Cold Start)基础上去掉SFT loss，感觉这样就能取得不错的效果吧。 ","permalink":"http://localhost:1313/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm\u003c/li\u003e\n\u003cli\u003e作者单位：阿里巴巴\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/abs/2510.14321\"\u003ehttps://arxiv.org/abs/2510.14321\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：arxiv\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"一问题\"\u003e一、问题\u003c/h1\u003e\n\u003cp\u003e电商emb召回场景，目前的方法都是直接字面语义上的对比学习训练（direct-embedding methods），即q2i的对比学习训练。对于复杂、困难的query，语义理解能力不足，比如下图Fig1中的query=\u0026ldquo;比茶更提神的饮料\u0026rdquo;，仍然会召回很多茶，因为字面理解没有理解query背后的深层含义。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-Fig1.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"二方法\"\u003e二、方法\u003c/h1\u003e\n\u003cp\u003e使用LLM强大的推理能力（reasoning），先推理出CoT，然后基于CoT再产emb。比如上面的例子中，经过LLM推理之后，推理出咖啡、红牛等关键词，通过这些关键词再去产emb然后召回，效果就好很多。\u003c/p\u003e\n\u003ch2 id=\"21-训练样本构造方法\"\u003e2.1 训练样本构造方法\u003c/h2\u003e\n\u003cp\u003e如下图Fig2中的Data Construction部分：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e收集线上query，尤其是那种困难query，就是在现有direct-embedding表现不好的query\u003c/li\u003e\n\u003cli\u003e把这些query喂给现有召回模型，得到召回商品集合①\u003c/li\u003e\n\u003cli\u003e然后使用强大的Qwen3-30B-A3B-Instruct生产CoT扩展信息\n\u003cul\u003e\n\u003cli\u003eUnconstrained Reasoning：首先不加任何限制地生产CoT，尽可能利用大模型的世界知识和推理能力，生产充分完全的CoT信息\u003c/li\u003e\n\u003cli\u003eInformation Extraction：由于上一步产出的CoT信息太长了，不利于线上推理，因此把上一步产出的CoT和原始query再次输入给大模型，让大模型抽取其中的关键信息，以keyword list形式输出\u003c/li\u003e\n\u003cli\u003ePost Processing：最后对上一步抽取的关键词进行后处理，去除重复词，去除query中已有的词等，得到精简、干净的关键词列表，列表最大长度是16\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e接着把query和CoT喂给已有的向量召回模型，得到扩展的召回商品集合②\n\u003cul\u003e\n\u003cli\u003e由于要训练模型的Reasoning能力，所以只取出集合②-①的差集部分，这部分是CoT带来的增益商品集合\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e最后使用相关性模型对商品集合②-①进行过滤，过滤出相关的商品\u003c/li\u003e\n\u003cli\u003e通过上述步骤，产出约7.5kw的\u0026lt;query, CoT, item\u0026gt;三元组\u003c/li\u003e\n\u003cli\u003e把上述样本划分成两部分，7.1kw的\u0026lt;query, CoT, item\u0026gt;三元组用于Cold start预训练；剩余400w的\u0026lt;query, item\u0026gt;用于RL微调\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-Fig2.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"22-cold-start预训练\"\u003e2.2 Cold Start预训练\u003c/h2\u003e\n\u003cp\u003e对应图Fig2左下角部分，该模块通过大规模的\u0026lt;query, CoT,item\u0026gt;三元组数据预训练，想要达到两个目的：一是让基础模型具备think能力；二是让基础模型产出的emb和下游q2i任务对齐。\u003c/p\u003e\n\u003cp\u003e这里使用的基础模型是Qwen2.5-3B-Instruct，比生产CoT的模型（Qwen3-30B-A3B-Instruct）小，其实也有点蒸馏的感觉，把大模型的CoT能力蒸馏到小模型中。\u003c/p\u003e\n\u003cp\u003e训练任务包括两个，一个是CoT的NTP loss（对应图中的SFT loss），另一个是q2i的对比学习InfoNCE loss。query塔和item塔共享参数，他们的emb都是最后一个特殊token \u0026lt;emb\u0026gt; 的emb。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-formula4.png\"\u003e\n\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-formula5-7.png\"\u003e\u003c/p\u003e\n\u003cp\u003eLoss组合：\n\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-formula8.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"23-rl微调\"\u003e2.3 RL微调\u003c/h2\u003e\n\u003cp\u003e上一步的SFT主要进行模仿学习，模仿更大的大模型的think能力，小模型本身的reasoning能力受限，接下来需要用GRPO对小模型进行RL微调。RL微调同时对生产CoT和生产emb两个任务都有作用，具体看下面的reward：\u003c/p\u003e\n\u003cp\u003eRL微调设计了3个reward：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFormat Reward：产出的CoT格式符合“\u0026lt;think\u0026gt; Specific CoT \u0026lt;/think\u0026gt;\u0026lt;emb\u0026gt;”就得1分，否则得0分\u003c/li\u003e\n\u003cli\u003eLength Reward：产出的CoT格式符合长度限制（\u0026lt;=16）就得1分，否则得0分\u003c/li\u003e\n\u003cli\u003eRetrieval Accuracy Reward：联合原始query和产出的CoT产出的增强query emb，与batch内所有的item emb求相似度，正确item所在的排名为\\(rank(d_i)\\)，再根据公式12计算一个排名的reward。核心思想是：正确的item与query的相似度排名越高则reward越大（即rank值越小则reward越大）。\n\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-formula11-12.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e最后，上述3个reward通过三个β系数组合起来：\n\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-formula13.png\"\u003e\u003c/p\u003e","title":"论文阅读：Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm"},{"content":"本博客使用的评论系统是Giscus，日常使用没啥问题，但是当博客内容很长的时候，就会出现URI_TOO_LONG的问题：\nAn error occurred URI_TOO_LONG hkg1:: 例如下面两篇博客：\n2017年国庆旅行——郑州、杭州 伪·2018届校招面经 正常Giscus评论 异常Giscus评论 针对这个问题，网上搜URI_TOO_LONG说的都是网页请求的URI太长导致的，但都没找到和Giscus相关的内容。\n后来在Giscus的Github Issue里找到一个相同的问题：https://github.com/giscus/giscus/issues/1340，里面一个人提到和Hugo所用的主题有关，另一个人提到和博客的meta name=\u0026quot;description\u0026quot;内容太长有关。但都蜻蜓点水，说的不是很详细，也没有给出一个通用的解决办法。\n后来怀疑这个报错可能是在加载Giscus评论系统的时候，发起的URI请求太长有关。因此针对出问题的博客，通过Chrome右键检查，找到Giscus模块出现的URI链接，如下图所示。\n光这么看看不出来这个URI的长度，可以把这个URI拷贝出来，粘贴到Word中，你会发现这个URI真的非常非常长，而且包含了完整的博客正文！进一步分析发现是URI中的description字段非常长，包含完整的博客正文。\n定位到问题之后，解决办法就很简单了，目标就是如何缩短博客html代码中的description字段长度。这里有很多种方法，最简单的方法就是，在每篇博客的头信息区域，增加自定义配置的description内容，简短一点就行，我就直接复用了博客标题。通过这种方法就缩短了Giscus发起URI中的description字段长度了。\n此外，还可以在主题中搜一下meta name=\u0026quot;description\u0026quot;出现的位置，把里面的代码改掉或者注释掉，都能解决这个问题。\n","permalink":"http://localhost:1313/posts/2025-11-09-resolve-the-uri-too-long-error-of-giscus-in-hugo/","summary":"\u003cp\u003e本博客使用的评论系统是\u003ca href=\"https://github.com/giscus/giscus\"\u003eGiscus\u003c/a\u003e，日常使用没啥问题，但是当博客内容很长的时候，就会出现\u003ccode\u003eURI_TOO_LONG\u003c/code\u003e的问题：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAn error occurred\n\nURI_TOO_LONG\n\nhkg1::\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e例如下面两篇博客：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://bitjoy.net/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/\"\u003e2017年国庆旅行——郑州、杭州\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitjoy.net/posts/2018-02-04-2018-campus-recruiting/\"\u003e伪·2018届校招面经\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e正常Giscus评论\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e异常Giscus评论\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-11-09-resolve-the-uri-too-long-error-of-giscus-in-hugo/normal-giscus.png\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-11-09-resolve-the-uri-too-long-error-of-giscus-in-hugo/abnormal-giscus.png\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e针对这个问题，网上搜\u003ccode\u003eURI_TOO_LONG\u003c/code\u003e说的都是网页请求的URI太长导致的，但都没找到和Giscus相关的内容。\u003c/p\u003e\n\u003cp\u003e后来在Giscus的Github Issue里找到一个相同的问题：\u003ca href=\"https://github.com/giscus/giscus/issues/1340\"\u003ehttps://github.com/giscus/giscus/issues/1340\u003c/a\u003e，里面一个人提到和Hugo所用的主题有关，另一个人提到和博客的\u003ccode\u003emeta name=\u0026quot;description\u0026quot;\u003c/code\u003e内容太长有关。但都蜻蜓点水，说的不是很详细，也没有给出一个通用的解决办法。\u003c/p\u003e\n\u003cp\u003e后来怀疑这个报错可能是在加载Giscus评论系统的时候，发起的URI请求太长有关。因此针对出问题的博客，通过Chrome右键\u003ccode\u003e检查\u003c/code\u003e，找到Giscus模块出现的URI链接，如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-11-09-resolve-the-uri-too-long-error-of-giscus-in-hugo/giscus-analysis.png\"\u003e\u003c/p\u003e\n\u003cp\u003e光这么看看不出来这个URI的长度，可以把这个URI拷贝出来，粘贴到Word中，你会发现这个URI真的非常非常长，而且包含了完整的博客正文！进一步分析发现是URI中的\u003ccode\u003edescription\u003c/code\u003e字段非常长，包含完整的博客正文。\u003c/p\u003e\n\u003cp\u003e定位到问题之后，解决办法就很简单了，目标就是如何缩短博客html代码中的\u003ccode\u003edescription\u003c/code\u003e字段长度。这里有很多种方法，最简单的方法就是，在每篇博客的头信息区域，增加自定义配置的\u003ccode\u003edescription\u003c/code\u003e内容，简短一点就行，我就直接复用了博客标题。通过这种方法就缩短了Giscus发起URI中的\u003ccode\u003edescription\u003c/code\u003e字段长度了。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-11-09-resolve-the-uri-too-long-error-of-giscus-in-hugo/giscus-solution.png\"\u003e\u003c/p\u003e\n\u003cp\u003e此外，还可以在主题中搜一下\u003ccode\u003emeta name=\u0026quot;description\u0026quot;\u003c/code\u003e出现的位置，把里面的代码改掉或者注释掉，都能解决这个问题。\u003c/p\u003e","title":"解决Hugo的Giscus评论系统出现URI_TOO_LONG的问题"},{"content":"\n基本信息 论文标题：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID 作者单位：Meta 论文链接：https://arxiv.org/pdf/2504.02137 来源：RecSys 2025 Motivation：论文要解决的问题是什么 搜推广的模型严重依赖于item id embedding的表征质量，但在工业场景下，搜推广的id表征存在如下挑战：\nid量级非常大，常常是数十亿甚至是百亿的规模。因此，通常不可能给每个id一个单独的embedding（即文中的individual embedding, IE），IE的成本太高 id分布非常不均匀，马太效应严重。文中统计：0.1%的头部item占据了25%的曝光量；5.5%的腰部item占据了50%的曝光量；94.4%的尾部item只占据了25%的曝光量 id分布漂移严重：搜推广场景中item的变化非常频繁，无时无刻不在发生着新id的产生和旧id的退出，而且不同id存活的时间周期也不尽相同，所以id的准入准出策略很难完美适配所有item 针对上述问题，常见的做法是对item id采用hash然后查emb的方式（即文中的random hash，RH），将所有id hash到一个固定大小的空间，然后查emb。但是RH方式有如下缺点：\n存在hash冲突，把不相关的id hash到一个桶里，导致语义混乱，学习效果不佳 无法解决id分布漂移的问题，比如hash到同一个桶的A、B两个id，如果B出现频率变高，则会带偏A的分布，影响了A的效果 无法进行知识共享，例如新出了商品iphone15，iphone15无法共享到老的iphone14的emb知识，iphone15的id emb必须完全重新学习。针对这种情况，作者做了一个更加极端的AA实验，就是copy一个完全相同的商品，只换item id，如果是IE或者RH策略，则新商品由于id emb是随机初始化的，效果不佳，这是id-based的通病 基于前缀n-gram的semantic id表征方法 针对上述问题，作者沿用了semantic id的思路，首先使用内容理解团队产出的文本、图片等多模态emb，然后基于过去3个月的item多模态emb，训练RQ-VAE模型，并产出所有item的semantic id。\n上述过程都是常规操作，重点在于如何基于semantic id得到item emb表征。假设semantic id是L层，每层的codebook size是K：\n最常规的做法：每层都初始化一个K*d的emb table，每层sid查各自的emb table，然后把L层的sid emb加起来。但是本文完全没有提这种方法，也没有和这种方法比较，非常奇怪。 为了比较，我个人再详细描述下这种常规做法。比如老item A的sid是(c1,c2,c3)；新来一个item B，它的sid是(c1,c2,c4)。用常规方法，A的emb是c1+c2+c3，B的emb是c1+c2+c4。两者c1、c2是可以共享的，所以常规方法也能起到一定的知识共享的效果，共享项有2项：c1、c2。但是因为RQ-VAE的沙漏问题，c2很有可能是沙漏瓶颈，信息量不足。 作者对比了Table 1中的几种方法： Trigram和Fourgram差不多，如果L=3用Trigram、L=4用Fourgram的话，本质上是把L个sid映射成了一个无冲突的int。但是这种方法映射出来的int数量太多了，是\\(K^L\\)。如果K=1024、L=3，则\\(K^L\\)就已经超过10亿了，这和直接无冲突的IE方法一样了，而且存在新id无法共享老id学到的知识的问题 All bigrams，就是所有的sid的2-gram。还是上面的例子，A的emb相当于\\(c_1c_2+c_2c_3\\)，B的emb相当于\\(c_1c_2+c_2c_4\\)，两者可共享\\(c_1c_2\\)项，相比于常规方法，虽然共享项数变少了，但粒度更精细了，孰好孰坏未可知。由Table 2可知，All bigrams的效果至少比Trigram和Fourgram好很多了，而且如果层数L越大，可共享项越多 Prefix-ngram（简称Prefix-SID方法），本文提出的新方法，把所有前缀组合成新id查emb，然后所有emb再求和。还是上面的例子，A的emb相当于\\(c_1+c_1c_2+c_1c_2c_3+c_2+c_2c_3+c_3\\)，B的emb相当于\\(c_1+c_1c_2+c_1c_2c_4+c_2+c_2c_4+c_4\\)，两者可共享\\(c_1, c_1c_2, c_2\\)三项，比之前的所有方法可共享的信息都多，而且如果层数L越大，可共享项越多，因此这种方法的效果最好，训练也最稳定 实验结果很丰富，做了很多分析，Prefix-SID方法有如下优势：\n相比于IE和RH方法，Prefix-SID方法对中长尾item的提升尤其显著，因为新id和老id的表征有了知识共享 对id分布漂移问题更不敏感：由于电商模型训练时消费数据的顺序是和数据的时间一致的，比如一个月的数据，按照1号、2号、\u0026hellip;31号这样的时间先后顺序依次训练，理论上4号的模型在4号的测试集上的效果是最好的。作者做了一个实验，分别用20号和4号的模型都在4号的测试集上进行评测，看看20号的模型指标相比4号降低了多少。作者发现，使用Prefix-SID方法和IE方法，两者的指标降低幅度都差不多，都比较小。首先IE方法由于不存在hash冲突，所以20号的模型仍然能比较好地预测4号的数据；其次，Prefix-SID方法虽然有hash冲突，但是因为冲突的item都是语义相似的，可以进行新老item的知识共享，所以这个冲突反而是好事，对模型效果无影响。但是作者发现RH方法的20号的模型在4号数据上评测指标下降比较多，因为有hash冲突，而且冲突是随机的，20号的分布已经变化很大了，导致在4号数据上效果不佳。Table 4的指标越小越好。 基于Prefix-SID方法虽然也有hash冲突，但是冲突到同一个semantic id的item表征更相似，而RH冲突到同一个桶里的item是完全随机的，相似度差。作者以IE为base，把Prefix-SID和RH都各自都冲突到同一个桶的IE emb提取出来，计算类内相似度和类间相似度，发现基于Prefix-SID的类内相似度方差小，类间距离大，说明Prefix-SID确实能把相似item聚到一起。 评论 可借鉴 基于Prefix-SID方法确实能提高新item和老item的信息共享数量，方法值得借鉴 论文实验分析很丰富 可改进 基于Prefix-SID方法居然没有和最常规的加和方法比较，是本文最大的不足 ","permalink":"http://localhost:1313/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID\u003c/li\u003e\n\u003cli\u003e作者单位：Meta\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2504.02137\"\u003ehttps://arxiv.org/pdf/2504.02137\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：RecSys 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003e搜推广的模型严重依赖于item id embedding的表征质量，但在工业场景下，搜推广的id表征存在如下挑战：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eid量级非常大，常常是数十亿甚至是百亿的规模。因此，通常不可能给每个id一个单独的embedding（即文中的individual embedding, IE），IE的成本太高\u003c/li\u003e\n\u003cli\u003eid分布非常不均匀，马太效应严重。文中统计：0.1%的头部item占据了25%的曝光量；5.5%的腰部item占据了50%的曝光量；94.4%的尾部item只占据了25%的曝光量\u003c/li\u003e\n\u003cli\u003eid分布漂移严重：搜推广场景中item的变化非常频繁，无时无刻不在发生着新id的产生和旧id的退出，而且不同id存活的时间周期也不尽相同，所以id的准入准出策略很难完美适配所有item\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-fig2.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-fig3.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e针对上述问题，常见的做法是对item id采用hash然后查emb的方式（即文中的random hash，RH），将所有id hash到一个固定大小的空间，然后查emb。但是RH方式有如下缺点：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e存在hash冲突，把不相关的id hash到一个桶里，导致语义混乱，学习效果不佳\u003c/li\u003e\n\u003cli\u003e无法解决id分布漂移的问题，比如hash到同一个桶的A、B两个id，如果B出现频率变高，则会带偏A的分布，影响了A的效果\u003c/li\u003e\n\u003cli\u003e无法进行知识共享，例如新出了商品iphone15，iphone15无法共享到老的iphone14的emb知识，iphone15的id emb必须完全重新学习。针对这种情况，作者做了一个更加极端的AA实验，就是copy一个完全相同的商品，只换item id，如果是IE或者RH策略，则新商品由于id emb是随机初始化的，效果不佳，这是id-based的通病\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"基于前缀n-gram的semantic-id表征方法\"\u003e基于前缀n-gram的semantic id表征方法\u003c/h1\u003e\n\u003cp\u003e针对上述问题，作者沿用了semantic id的思路，首先使用内容理解团队产出的文本、图片等多模态emb，然后基于过去3个月的item多模态emb，训练RQ-VAE模型，并产出所有item的semantic id。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-fig1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上述过程都是常规操作，重点在于如何基于semantic id得到item emb表征。假设semantic id是L层，每层的codebook size是K：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e最常规的做法：每层都初始化一个K*d的emb table，每层sid查各自的emb table，然后把L层的sid emb加起来。但是本文完全没有提这种方法，也没有和这种方法比较，非常奇怪。\n\u003cul\u003e\n\u003cli\u003e为了比较，我个人再详细描述下这种常规做法。比如老item A的sid是(c1,c2,c3)；新来一个item B，它的sid是(c1,c2,c4)。用常规方法，A的emb是c1+c2+c3，B的emb是c1+c2+c4。两者c1、c2是可以共享的，所以常规方法也能起到一定的知识共享的效果，共享项有2项：c1、c2。但是因为\u003ca href=\"https://arxiv.org/abs/2407.21488\"\u003eRQ-VAE的沙漏问题\u003c/a\u003e，c2很有可能是沙漏瓶颈，信息量不足。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e作者对比了Table 1中的几种方法：\u003c/li\u003e\n\u003cli\u003eTrigram和Fourgram差不多，如果L=3用Trigram、L=4用Fourgram的话，本质上是把L个sid映射成了一个无冲突的int。但是这种方法映射出来的int数量太多了，是\\(K^L\\)。如果K=1024、L=3，则\\(K^L\\)就已经超过10亿了，这和直接无冲突的IE方法一样了，而且存在新id无法共享老id学到的知识的问题\u003c/li\u003e\n\u003cli\u003eAll bigrams，就是所有的sid的2-gram。还是上面的例子，A的emb相当于\\(c_1c_2+c_2c_3\\)，B的emb相当于\\(c_1c_2+c_2c_4\\)，两者可共享\\(c_1c_2\\)项，相比于常规方法，虽然共享项数变少了，但粒度更精细了，孰好孰坏未可知。由Table 2可知，All bigrams的效果至少比Trigram和Fourgram好很多了，而且如果层数L越大，可共享项越多\u003c/li\u003e\n\u003cli\u003ePrefix-ngram（简称Prefix-SID方法），本文提出的新方法，把所有前缀组合成新id查emb，然后所有emb再求和。还是上面的例子，A的emb相当于\\(c_1+c_1c_2+c_1c_2c_3+c_2+c_2c_3+c_3\\)，B的emb相当于\\(c_1+c_1c_2+c_1c_2c_4+c_2+c_2c_4+c_4\\)，两者可共享\\(c_1, c_1c_2, c_2\\)三项，比之前的所有方法可共享的信息都多，而且如果层数L越大，可共享项越多，因此这种方法的效果最好，训练也最稳定\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-tab1.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-tab2.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e实验结果很丰富，做了很多分析，Prefix-SID方法有如下优势：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e相比于IE和RH方法，Prefix-SID方法对中长尾item的提升尤其显著，因为新id和老id的表征有了知识共享\u003c/li\u003e\n\u003cli\u003e对id分布漂移问题更不敏感：由于电商模型训练时消费数据的顺序是和数据的时间一致的，比如一个月的数据，按照1号、2号、\u0026hellip;31号这样的时间先后顺序依次训练，理论上4号的模型在4号的测试集上的效果是最好的。作者做了一个实验，分别用20号和4号的模型都在4号的测试集上进行评测，看看20号的模型指标相比4号降低了多少。作者发现，使用Prefix-SID方法和IE方法，两者的指标降低幅度都差不多，都比较小。首先IE方法由于不存在hash冲突，所以20号的模型仍然能比较好地预测4号的数据；其次，Prefix-SID方法虽然有hash冲突，但是因为冲突的item都是语义相似的，可以进行新老item的知识共享，所以这个冲突反而是好事，对模型效果无影响。但是作者发现RH方法的20号的模型在4号数据上评测指标下降比较多，因为有hash冲突，而且冲突是随机的，20号的分布已经变化很大了，导致在4号数据上效果不佳。Table 4的指标越小越好。\n\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-tab4.png\"\u003e\u003c/li\u003e\n\u003cli\u003e基于Prefix-SID方法虽然也有hash冲突，但是冲突到同一个semantic id的item表征更相似，而RH冲突到同一个桶里的item是完全随机的，相似度差。作者以IE为base，把Prefix-SID和RH都各自都冲突到同一个桶的IE emb提取出来，计算类内相似度和类间相似度，发现基于Prefix-SID的类内相似度方差小，类间距离大，说明Prefix-SID确实能把相似item聚到一起。\n\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-tab5.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e可借鉴\n\u003cul\u003e\n\u003cli\u003e基于Prefix-SID方法确实能提高新item和老item的信息共享数量，方法值得借鉴\u003c/li\u003e\n\u003cli\u003e论文实验分析很丰富\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e可改进\n\u003cul\u003e\n\u003cli\u003e基于Prefix-SID方法居然没有和最常规的加和方法比较，是本文最大的不足\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"论文阅读：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID"},{"content":"\n基本信息 论文标题：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings 作者单位：沃尔玛 论文链接：https://arxiv.org/pdf/2507.17080 来源：RecSys 2025 Motivation：论文要解决的问题是什么 多模态q2i召回通常使用CLIP的对比学习方式进行训练，在电商场景下存在2个问题：\nCLIP这种方式通常是对图片整体的表征，缺乏细粒度的目标检测能力，尤其在电商场景，比如fig1，卖衣服场景，传统CLIP只能识别整张图片是一件T恤，难以关注T恤上的图案等细节特征；另外，电商图片往往存在很多附加背景、道具、模特等元素，会影响主体物体的表征 电商标题、属性等文本描述通常参差不齐，存在错误、堆砌、图文不符等问题，导致CLIP训练时图文对齐效果不佳 VL-CLIP解决方案 针对图片的处理：\n将图片和商品类型（product type）输入到开源模型Grounding DINO中，让模型进行目标检测，将可信度超过某个阈值且可信度最高的区域抠出来，输入到CLIP的图像encoder中。通过这步预处理，相当于对电商图片进行了关键主体识别和提取，只提取和商品最相关的主体进行图像表征。文中使用的图像编码器是ViT-B/32。 针对文本的处理：\n将商品的类型、标题、描述、性别、年龄等文本描述以及图片本身输入到Summarizer多模态大模型，让大模型产出精简、准确的文本描述\\(q_0\\) 将\\(q_0\\)和商品图文信息输入到Evaluator多模态大模型，让大模型对\\(q_0\\)的质量进行评判，如果\\(q_0\\)质量很好，则直接输出\u0026lt;STOP\u0026gt;；否则指出\\(q_0\\)的问题所在，并说明改进方法 如果第2步输出不是\u0026lt;STOP\u0026gt;，则将第2步的输出再输入到Refiner大模型，让大模型根据第2步的结果继续调整并输出更优的文本描述\\(q_i\\) 不断重复第2、3步，直到输出\u0026lt;STOP\u0026gt;，或者最多重复5遍 将产出的精准的文本描述q输入到CLIP的文本encoder中，文中使用的是BERT系列。产出的emb维度是512 上述Summarizer、Evaluator、Refiner都是VLM，文中使用的是GPT-4o，三个任务的prompt设计参考论文附录Table 9 上述对图片和文本的处理本质上是去噪，提取图片的主体物品、让文本描述更加精准。\n产出多模态emb之后，后续的操作就是常规的召回流程了，使用HNSW进行ANN召回。\n评论 可借鉴 使用Grounding DINO对图片进行主体识别，值得借鉴 使用VLM对商品标题、描述等文本信息进行去噪，值得借鉴 但如果商品量级很大的话，这两个步骤估计会很耗时 可改进 如果是q2i场景，直接用query文本是不是更真实，更接近搜索日子的真实数据分布？ ","permalink":"http://localhost:1313/posts/2025-10-08-vl-clip-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-08-vl-clip-paper-reading/VL-CLIP-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings\u003c/li\u003e\n\u003cli\u003e作者单位：沃尔玛\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2507.17080\"\u003ehttps://arxiv.org/pdf/2507.17080\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：RecSys 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003e多模态q2i召回通常使用CLIP的对比学习方式进行训练，在电商场景下存在2个问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCLIP这种方式通常是对图片整体的表征，缺乏细粒度的目标检测能力，尤其在电商场景，比如fig1，卖衣服场景，传统CLIP只能识别整张图片是一件T恤，难以关注T恤上的图案等细节特征；另外，电商图片往往存在很多附加背景、道具、模特等元素，会影响主体物体的表征\u003c/li\u003e\n\u003cli\u003e电商标题、属性等文本描述通常参差不齐，存在错误、堆砌、图文不符等问题，导致CLIP训练时图文对齐效果不佳\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-08-vl-clip-paper-reading/VL-CLIP-fig1.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"vl-clip解决方案\"\u003eVL-CLIP解决方案\u003c/h1\u003e\n\u003cp\u003e针对图片的处理：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将图片和商品类型（product type）输入到开源模型\u003ca href=\"https://github.com/IDEA-Research/GroundingDINO\"\u003eGrounding DINO\u003c/a\u003e中，让模型进行目标检测，将可信度超过某个阈值且可信度最高的区域抠出来，输入到CLIP的图像encoder中。通过这步预处理，相当于对电商图片进行了关键主体识别和提取，只提取和商品最相关的主体进行图像表征。文中使用的图像编码器是ViT-B/32。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-08-vl-clip-paper-reading/VL-CLIP-fig2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e针对文本的处理：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e将商品的类型、标题、描述、性别、年龄等文本描述以及图片本身输入到Summarizer多模态大模型，让大模型产出精简、准确的文本描述\\(q_0\\)\u003c/li\u003e\n\u003cli\u003e将\\(q_0\\)和商品图文信息输入到Evaluator多模态大模型，让大模型对\\(q_0\\)的质量进行评判，如果\\(q_0\\)质量很好，则直接输出\u0026lt;STOP\u0026gt;；否则指出\\(q_0\\)的问题所在，并说明改进方法\u003c/li\u003e\n\u003cli\u003e如果第2步输出不是\u0026lt;STOP\u0026gt;，则将第2步的输出再输入到Refiner大模型，让大模型根据第2步的结果继续调整并输出更优的文本描述\\(q_i\\)\u003c/li\u003e\n\u003cli\u003e不断重复第2、3步，直到输出\u0026lt;STOP\u0026gt;，或者最多重复5遍\u003c/li\u003e\n\u003cli\u003e将产出的精准的文本描述q输入到CLIP的文本encoder中，文中使用的是BERT系列。产出的emb维度是512\u003c/li\u003e\n\u003cli\u003e上述Summarizer、Evaluator、Refiner都是VLM，文中使用的是GPT-4o，三个任务的prompt设计参考论文附录Table 9\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-08-vl-clip-paper-reading/VL-CLIP-fig3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上述对图片和文本的处理本质上是去噪，提取图片的主体物品、让文本描述更加精准。\u003c/p\u003e\n\u003cp\u003e产出多模态emb之后，后续的操作就是常规的召回流程了，使用HNSW进行ANN召回。\u003c/p\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e可借鉴\n\u003cul\u003e\n\u003cli\u003e使用Grounding DINO对图片进行主体识别，值得借鉴\u003c/li\u003e\n\u003cli\u003e使用VLM对商品标题、描述等文本信息进行去噪，值得借鉴\u003c/li\u003e\n\u003cli\u003e但如果商品量级很大的话，这两个步骤估计会很耗时\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e可改进\n\u003cul\u003e\n\u003cli\u003e如果是q2i场景，直接用query文本是不是更真实，更接近搜索日子的真实数据分布？\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"论文阅读：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings"},{"content":"\n基本信息 论文标题：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook 作者单位：Snap 论文链接：https://arxiv.org/pdf/2507.22224 来源：CIKM 2025 这是CIKM 2025的一篇resource文章，比较简单。核心内容是开源了一个基于semantic id的生成式推荐框架GRID，可以很方便地做各种消融对比实验。\n主要内容 主要结论如下：\n对于semantic id生成算法，简单的RQ-KMeans效果反而是最好的，好于R-VQ和RQ-VAE 生产pretrain emb的LLM模型参数量越大，效果越好，但是提升幅度有限 生产semantic id的codebook size和网络层数并不是越大越好，常规的3层，每层256个id效果反而最好 生成式推荐时，是否需要在用户行为序列基础上增加一个user id，实验发现增加user id效果反而变差，不增加user id效果最好 生成式网络结构encoder-decoder对比decoder-only，发现前者效果更好，因为前者能充分学习到行为序列完整的信息 对行为流进行滑动窗口数据增强能提升模型的泛化能力 当semantic id到item存在映射冲突时，随机选一个item的效果和对冲突item追加一个区分标识（digit），两者效果差不多 在生成式beam search的时候，限制只输出合法semantic id和不增加限制，两者效果差不多 评论 看这篇文章主要是想看看不同semantic id生产方法的对比，发现RQ-KMeans居然比RQ-VAE更好。个人感觉这两个方法效果应该差不多，后者应该更好点才对。首先，RQ-VAE的量化loss本质上和KMeans聚类是一个意思；其次，RQ-VAE还增加了一个重构loss，感觉产出来的semantic id和原始emb的信息损失应该更少。\n此外，本文的所有实验都是基于亚马逊的公开数据集，数据量肯定不能和真正的工业数据集相提并论，所以文中很多结论有可能只适用于本文的设定，换一个场景估计结论就变了，所以看看就好。\n最后，文中很多结论只写了现象，要是能增加原因分析就好了。\n","permalink":"http://localhost:1313/posts/2025-10-07-grid-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-07-grid-paper-reading/GRID-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook\u003c/li\u003e\n\u003cli\u003e作者单位：Snap\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2507.22224\"\u003ehttps://arxiv.org/pdf/2507.22224\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：CIKM 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这是CIKM 2025的一篇resource文章，比较简单。核心内容是开源了一个基于semantic id的生成式推荐框架GRID，可以很方便地做各种消融对比实验。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-07-grid-paper-reading/GRID-fig1.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"主要内容\"\u003e主要内容\u003c/h1\u003e\n\u003cp\u003e主要结论如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e对于semantic id生成算法，简单的RQ-KMeans效果反而是最好的，好于R-VQ和RQ-VAE\u003c/li\u003e\n\u003cli\u003e生产pretrain emb的LLM模型参数量越大，效果越好，但是提升幅度有限\u003c/li\u003e\n\u003cli\u003e生产semantic id的codebook size和网络层数并不是越大越好，常规的3层，每层256个id效果反而最好\u003c/li\u003e\n\u003cli\u003e生成式推荐时，是否需要在用户行为序列基础上增加一个user id，实验发现增加user id效果反而变差，不增加user id效果最好\u003c/li\u003e\n\u003cli\u003e生成式网络结构encoder-decoder对比decoder-only，发现前者效果更好，因为前者能充分学习到行为序列完整的信息\u003c/li\u003e\n\u003cli\u003e对行为流进行滑动窗口数据增强能提升模型的泛化能力\u003c/li\u003e\n\u003cli\u003e当semantic id到item存在映射冲突时，随机选一个item的效果和对冲突item追加一个区分标识（digit），两者效果差不多\u003c/li\u003e\n\u003cli\u003e在生成式beam search的时候，限制只输出合法semantic id和不增加限制，两者效果差不多\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cp\u003e看这篇文章主要是想看看不同semantic id生产方法的对比，发现RQ-KMeans居然比RQ-VAE更好。个人感觉这两个方法效果应该差不多，后者应该更好点才对。首先，RQ-VAE的量化loss本质上和KMeans聚类是一个意思；其次，RQ-VAE还增加了一个重构loss，感觉产出来的semantic id和原始emb的信息损失应该更少。\u003c/p\u003e\n\u003cp\u003e此外，本文的所有实验都是基于亚马逊的公开数据集，数据量肯定不能和真正的工业数据集相提并论，所以文中很多结论有可能只适用于本文的设定，换一个场景估计结论就变了，所以看看就好。\u003c/p\u003e\n\u003cp\u003e最后，文中很多结论只写了现象，要是能增加原因分析就好了。\u003c/p\u003e","title":"论文阅读：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook"},{"content":"\n基本信息 论文标题：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation 作者单位：网易云音乐 论文链接：https://arxiv.org/pdf/2508.20359 来源：CIKM 2025 Motivation：论文要解决的问题是什么 多模态emb在搜推的应用方式，通常是先将多模态emb转换成semantic id，然后把semantic id用到搜推模型中，这种方式有如下两个问题：\n模态内语义退化：多模态emb转换成semantic id通常使用RQ-VAE或者RQ-KMeans的方法，这种方法在不断残差的过程中，后续残差聚类结果已经不能反映初始emb的聚类效果了。其实就是semantic id的沙漏问题，具体可以看这篇文章，后续有空再分享这个问题。 简单来说，如下图所示，初始有DJ、Rock、Lullaby、Choir四个类，但是对残差emb（即RQ-VAE的第二层）聚类的话，初始的四个类的item就打散了，会聚到不同的簇中，也就是RQ-VAE的后续层的聚类效果已经和初始emb的聚类效果很不一样了，这就是文中说的语义退化问题 模态间建模差异：搜推场景的item通常有多种模态特征，比如文本、图像、音频等，传统方法在多模态融合方面比较简单，不能很好地捕捉多模态之间的关系。 PSRQ生产semantic id 本文是音乐推荐场景，主要用到两种模态：text和audio，分别用百川和MERT提取text和audio的模态emb。\n生产semantic id的方法如下图所示：\nfig2a是传统的RQ-KMeans的方法，每一层都用上一层的残差进行聚类。如上文所述，由于沙漏问题，会导致后续层次的semantic id存在语义退化问题 fig2b是本文新提出的PSRQ量化方法，在RQ-KMeans基础上，每一层除了有上一层的残差向量，还会concat上初始emb减去残差emb后的向量。这样就能区分出残差相似，但初始emb不同的item了，也就避免了RQ方法的沙漏问题，后续semantic id也能保留初始emb的语义信息。fig1d能看出来第二层semantic id仍然能够反映初始emb的分类效果。 Semantic id在下游的应用方法 如下图所示：\n每个item有两套多模态emb：text和audio，但是有三套semantic id，除了text和audio各自产一套semantic id之外，还会把text和audio的emb concat起来，再产一套semantic id，相当于多模态融合的semantic id semantic id的emb在排序模型中随机初始化，然后端到端训练 semantic id在用户建模时，使用DIN模型，query用的是多模态融合的semantic id emb，行为流分别用text和audio的semantic id emb。作者说这种方法既能捕捉到单模态细粒度的信息，又能建模跨模态的交互信息 评论 可借鉴 PSRQ的semantic id生产方法确实很有意思，在每一层都用上原始emb，这样不同簇的item在每一层都能分开，不会出现沙漏问题，使得每一层的semantic id都能保留原始emb的语义聚类信息 产了多套semantic id，单模态semantic id是常规操作；多模态emb concat后也产一套semantic id，是个创新点 用户建模时query用多模态semantic id，行为流用单模态semantic id，也是个创新点，虽然论文说这种方法效果最好，但是有点存疑 论文有个实验结果对比了不同semantic id量化方法的效果，结论是：PSRQ \u0026gt; RQ-KMeans = RQ-VAE \u0026gt; VQ \u0026gt; PQ 可改进 pretrain emb和semantic id的生产都没有对齐协同信号 semantic id在下游应用时直接端到端训练，而没有使用codebook初始化，会不会丢失信息比较多？ 产semantic id的过程中，模态内语义退化的问题，描述了现象，但是没有用定量的指标来说明问题，感觉可以借鉴【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】的方法，定量说明后续层的semantic id的聚类效果或者说区分能力相比初始emb已经相差甚远了 fig2b中，第一层的codebook的dim=d，后续层的codebook的dim=2d，那么后续层的残差dim也是2d，那么初始emb怎么和后续的残差emb相减呢，维度对不上啊？我理解可能是这样的，后续层聚类的时候用的是concat的dim=2d的emb，但是算聚类中心的时候只用了残差本身的emb，这样就能解释得通了，但是文中对这部分的细节没有解释。 ","permalink":"http://localhost:1313/posts/2025-10-06-psrq-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-06-psrq-paper-reading/PSRQ-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation\u003c/li\u003e\n\u003cli\u003e作者单位：网易云音乐\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2508.20359\"\u003ehttps://arxiv.org/pdf/2508.20359\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：CIKM 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003e多模态emb在搜推的应用方式，通常是先将多模态emb转换成semantic id，然后把semantic id用到搜推模型中，这种方式有如下两个问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e模态内语义退化\u003c/strong\u003e：多模态emb转换成semantic id通常使用RQ-VAE或者RQ-KMeans的方法，这种方法在不断残差的过程中，后续残差聚类结果已经不能反映初始emb的聚类效果了。其实就是semantic id的沙漏问题，具体可以看\u003ca href=\"https://arxiv.org/abs/2407.21488\"\u003e这篇文章\u003c/a\u003e，后续有空再分享这个问题。\n\u003cul\u003e\n\u003cli\u003e简单来说，如下图所示，初始有DJ、Rock、Lullaby、Choir四个类，但是对残差emb（即RQ-VAE的第二层）聚类的话，初始的四个类的item就打散了，会聚到不同的簇中，也就是RQ-VAE的后续层的聚类效果已经和初始emb的聚类效果很不一样了，这就是文中说的语义退化问题\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e模态间建模差异\u003c/strong\u003e：搜推场景的item通常有多种模态特征，比如文本、图像、音频等，传统方法在多模态融合方面比较简单，不能很好地捕捉多模态之间的关系。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-06-psrq-paper-reading/PSRQ-fig1.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"psrq生产semantic-id\"\u003ePSRQ生产semantic id\u003c/h1\u003e\n\u003cp\u003e本文是音乐推荐场景，主要用到两种模态：text和audio，分别用百川和MERT提取text和audio的模态emb。\u003c/p\u003e\n\u003cp\u003e生产semantic id的方法如下图所示：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efig2a是传统的RQ-KMeans的方法，每一层都用上一层的残差进行聚类。如上文所述，由于沙漏问题，会导致后续层次的semantic id存在语义退化问题\u003c/li\u003e\n\u003cli\u003efig2b是本文新提出的PSRQ量化方法，在RQ-KMeans基础上，每一层除了有上一层的残差向量，还会concat上初始emb减去残差emb后的向量。\u003cstrong\u003e这样就能区分出残差相似，但初始emb不同的item了\u003c/strong\u003e，也就避免了RQ方法的沙漏问题，后续semantic id也能保留初始emb的语义信息。fig1d能看出来第二层semantic id仍然能够反映初始emb的分类效果。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-06-psrq-paper-reading/PSRQ-fig2.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"semantic-id在下游的应用方法\"\u003eSemantic id在下游的应用方法\u003c/h1\u003e\n\u003cp\u003e如下图所示：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e每个item有两套多模态emb：text和audio，但是有三套semantic id，除了text和audio各自产一套semantic id之外，还会把text和audio的emb concat起来，再产一套semantic id，相当于多模态融合的semantic id\u003c/li\u003e\n\u003cli\u003esemantic id的emb在排序模型中随机初始化，然后端到端训练\u003c/li\u003e\n\u003cli\u003esemantic id在用户建模时，使用DIN模型，query用的是多模态融合的semantic id emb，行为流分别用text和audio的semantic id emb。作者说这种方法既能捕捉到单模态细粒度的信息，又能建模跨模态的交互信息\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-06-psrq-paper-reading/PSRQ-fig3.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e可借鉴\n\u003cul\u003e\n\u003cli\u003ePSRQ的semantic id生产方法确实很有意思，在每一层都用上原始emb，这样不同簇的item在每一层都能分开，不会出现沙漏问题，使得每一层的semantic id都能保留原始emb的语义聚类信息\u003c/li\u003e\n\u003cli\u003e产了多套semantic id，单模态semantic id是常规操作；多模态emb concat后也产一套semantic id，是个创新点\u003c/li\u003e\n\u003cli\u003e用户建模时query用多模态semantic id，行为流用单模态semantic id，也是个创新点，虽然论文说这种方法效果最好，但是有点存疑\u003c/li\u003e\n\u003cli\u003e论文有个实验结果对比了不同semantic id量化方法的效果，结论是：PSRQ \u0026gt; RQ-KMeans = RQ-VAE \u0026gt; VQ \u0026gt; PQ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e可改进\n\u003cul\u003e\n\u003cli\u003epretrain emb和semantic id的生产都没有对齐协同信号\u003c/li\u003e\n\u003cli\u003esemantic id在下游应用时直接端到端训练，而没有使用codebook初始化，会不会丢失信息比较多？\u003c/li\u003e\n\u003cli\u003e产semantic id的过程中，模态内语义退化的问题，描述了现象，但是没有用定量的指标来说明问题，感觉可以借鉴\u003ca href=\"https://bitjoy.net/posts/2025-10-04-mme-sid-paper-reading/\"\u003e【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】\u003c/a\u003e的方法，定量说明后续层的semantic id的聚类效果或者说区分能力相比初始emb已经相差甚远了\u003c/li\u003e\n\u003cli\u003efig2b中，第一层的codebook的dim=d，后续层的codebook的dim=2d，那么后续层的残差dim也是2d，那么初始emb怎么和后续的残差emb相减呢，维度对不上啊？我理解可能是这样的，后续层聚类的时候用的是concat的dim=2d的emb，但是算聚类中心的时候只用了残差本身的emb，这样就能解释得通了，但是文中对这部分的细节没有解释。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"论文阅读：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation"},{"content":"\n基本信息 论文标题：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System 作者单位：快手 论文链接：https://arxiv.org/pdf/2508.10584 来源：CIKM 2025 Motivation：论文要解决的问题是什么 Semantic id生产时，要么没有和协同信号对齐（fig2(1)），要么是两阶段对齐方式（fig2(2)）：\n例如LETTER先生成协同emb，然后和semantic id对齐 或者例如QARM，先协同对齐emb，再生产semantic id 把协同对齐和生产semantic id分成两个阶段，天然有信息损失，不是最优的。本文的目的就是把生产协同emb，以及semantic id的协同对齐放到一个模型中联合训练完成，尽量减少信息损失（fig2(3)）。\n主模型 主模型如上图所示，中间的ICDM是user和item的双塔模型，用于学习user和item的协同id-based emb；两边分别是生产user和item的semantic id的量化模型。\n中间的ICDM就是经典的召回双塔模型，使用点击样本进行训练，唯一不同的是，在user和item塔都有流行度去偏模块，用于学习user和item的无偏emb，后续user和item的semantic id协同对齐用的也是无偏的emb。\n两边分别是user和item的semantic id量化模型，两者比较类似，以item为例：\n先把item的各种信息，如title、desc、ocr等信息用文本构造成prompt，输入到LLM，借助LLM的summary和reasoning能力，产出item的详细描述 然后把LLM产出的描述再输入到一个预训练的embedding模型PLM，文中用的是bge m3模型，得到item emb 后续就是标准的RQ-VAE过程了 需要注意的是，上述前两步，分别用到了LLM和PLM两个大模型，而且看图上这两个模型都是freeze的，也就是说并不微调这两个大模型。后续协同对齐用的emb是RQ-VAE重构emb的中间层结果，即图中的item quantized emb。\nsemantic id的协同对齐方面，有三大类对齐任务：\nU2I对齐：量化user emb和协同item emb对齐、量化item emb和协同user emb对齐 U2U和I2I对齐：量化user emb和协同user emb对齐、量化item emb和协同item emb对齐 U2U和I2I的共现对齐：点击相同item的两个量化user emb对齐、同一个user点击的两个item的量化item emb对齐 由于fig3中的协同模型和semantic id模型是联合训练的，总共有3大类loss：\n中间的ICDM的双塔召回模型的loss 两边的产semantic id的loss 三个模块的对齐loss 评论 可借鉴 把semantic id的生产和协同信号对齐统一成一阶段的模式，信息损失更少 中间的ICDM模型生产协同emb时进行了去偏，协同对齐的时候用的是去偏的emb，这是其他论文很少提到的 可改进 太复杂了！3个模块，3大类loss，每类loss又有很多个小loss，总loss数量加起来有十多个。。。 任务太多，各种去偏、对齐loss，真的不会互相影响吗？ 中间的ICDM模块有必要吗？我理解ICDM本质是为了训练产出协同emb，但是因为训练样本本身是点击样本，样本本身已经包含了搜推场景的协同信号，也就是ICDM本身没必要存在了，直接用相同的样本训练两边的semantic id量化模型就行了，也能实现在训练semantic id的过程中，完成协同信号的对齐 生产semantic id的emb来自LLM和PLM，但是这两个大模型都是freeze的，如果把这两个模型也sft，效果会不会更好？其实我原本以为的一阶段就是这样的，这也是我在【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】中提到的一阶段方法。 ","permalink":"http://localhost:1313/posts/2025-10-05-das-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-05-das-paper-reading/das-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System\u003c/li\u003e\n\u003cli\u003e作者单位：快手\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2508.10584\"\u003ehttps://arxiv.org/pdf/2508.10584\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：CIKM 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003eSemantic id生产时，要么没有和协同信号对齐（fig2(1)），要么是两阶段对齐方式（fig2(2)）：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e例如\u003ca href=\"https://arxiv.org/abs/2405.07314\"\u003eLETTER\u003c/a\u003e先生成协同emb，然后和semantic id对齐\u003c/li\u003e\n\u003cli\u003e或者例如\u003ca href=\"https://arxiv.org/pdf/2411.11739\"\u003eQARM\u003c/a\u003e，先协同对齐emb，再生产semantic id\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e把协同对齐和生产semantic id分成两个阶段，天然有信息损失，不是最优的。本文的目的就是把生产协同emb，以及semantic id的协同对齐放到一个模型中联合训练完成，尽量减少信息损失（fig2(3)）。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-05-das-paper-reading/das-fig2.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"主模型\"\u003e主模型\u003c/h1\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-05-das-paper-reading/das-fig3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e主模型如上图所示，中间的ICDM是user和item的双塔模型，用于学习user和item的协同id-based emb；两边分别是生产user和item的semantic id的量化模型。\u003c/p\u003e\n\u003cp\u003e中间的ICDM就是经典的召回双塔模型，使用点击样本进行训练，唯一不同的是，在user和item塔都有流行度去偏模块，用于学习user和item的无偏emb，后续user和item的semantic id协同对齐用的也是无偏的emb。\u003c/p\u003e\n\u003cp\u003e两边分别是user和item的semantic id量化模型，两者比较类似，以item为例：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e先把item的各种信息，如title、desc、ocr等信息用文本构造成prompt，输入到LLM，借助LLM的summary和reasoning能力，产出item的详细描述\u003c/li\u003e\n\u003cli\u003e然后把LLM产出的描述再输入到一个预训练的embedding模型PLM，文中用的是bge m3模型，得到item emb\u003c/li\u003e\n\u003cli\u003e后续就是标准的RQ-VAE过程了\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e需要注意的是，上述前两步，分别用到了LLM和PLM两个大模型，而且看图上这两个模型都是freeze的，也就是说并不微调这两个大模型。后续协同对齐用的emb是RQ-VAE重构emb的中间层结果，即图中的item quantized emb。\u003c/p\u003e\n\u003cp\u003esemantic id的协同对齐方面，有三大类对齐任务：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eU2I对齐：量化user emb和协同item emb对齐、量化item emb和协同user emb对齐\u003c/li\u003e\n\u003cli\u003eU2U和I2I对齐：量化user emb和协同user emb对齐、量化item emb和协同item emb对齐\u003c/li\u003e\n\u003cli\u003eU2U和I2I的共现对齐：点击相同item的两个量化user emb对齐、同一个user点击的两个item的量化item emb对齐\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由于fig3中的协同模型和semantic id模型是联合训练的，总共有3大类loss：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e中间的ICDM的双塔召回模型的loss\u003c/li\u003e\n\u003cli\u003e两边的产semantic id的loss\u003c/li\u003e\n\u003cli\u003e三个模块的对齐loss\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e可借鉴\n\u003cul\u003e\n\u003cli\u003e把semantic id的生产和协同信号对齐统一成一阶段的模式，信息损失更少\u003c/li\u003e\n\u003cli\u003e中间的ICDM模型生产协同emb时进行了去偏，协同对齐的时候用的是去偏的emb，这是其他论文很少提到的\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e可改进\n\u003cul\u003e\n\u003cli\u003e太复杂了！3个模块，3大类loss，每类loss又有很多个小loss，总loss数量加起来有十多个。。。\u003c/li\u003e\n\u003cli\u003e任务太多，各种去偏、对齐loss，真的不会互相影响吗？\u003c/li\u003e\n\u003cli\u003e中间的ICDM模块有必要吗？我理解ICDM本质是为了训练产出协同emb，但是因为训练样本本身是点击样本，样本本身已经包含了搜推场景的协同信号，也就是ICDM本身没必要存在了，直接用相同的样本训练两边的semantic id量化模型就行了，也能实现在训练semantic id的过程中，完成协同信号的对齐\u003c/li\u003e\n\u003cli\u003e生产semantic id的emb来自LLM和PLM，但是这两个大模型都是freeze的，如果把这两个模型也sft，效果会不会更好？其实我原本以为的一阶段就是这样的，这也是我在\u003ca href=\"https://bitjoy.net/posts/2025-10-04-mme-sid-paper-reading/\"\u003e【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】\u003c/a\u003e中提到的一阶段方法。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"论文阅读：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System"},{"content":"\n基本信息 论文标题：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou 作者单位：快手 论文链接：https://arxiv.org/pdf/2411.11739 来源：CIKM 2025 Motivation：论文要解决的问题是什么 多模态emb在搜推场景应用时通常采用如下图的两阶段方式，先预训练多模态emb，然后作为一个冻结特征放到搜推模型中。这种方式存在2个问题：\n表征不对齐：多模态emb预训练的任务通常是图片分类或者文本的MLM，和下游搜推任务不对齐 表征不更新：多模态emb在搜推任务中作为冻结特征，没有更新 本文的方法就是想要解决上述2个问题。 对齐搜推任务的多模态emb预训练 为了解决多模态emb表征不对齐的问题，本文提出的多模态emb预训练任务直接对齐搜推场景，使用U2I和I2I召回模型，挖掘出相似item pair，然后通过对比学习微调多模态大模型。\n具体来说，通过U2I和I2I模型，能够拿到item emb；然后用每一个target item emb去行为流中检索出最相似的商品，作为trigger item emb。\u0026lt;trigger, target\u0026gt;构成一对正样本，然后进行对比学习训练。\n通过召回模型构造的训练样本，和搜推场景的协同信号对齐了，解决了开头提到的第一个问题，即表征不对齐的问题。\nSemantic id生产方法 Semantic id的生产方法如上图右半部分所示，有两种方式：\nVQ：直接圈定一定数量（如N）的item emb作为底池，编号1~N，然后任意来一个item emb，通过对底池emb进行KNN搜索，找出top-k相似商品，假设是(a,b,\u0026hellip;,k)，则VQ编码的semantic id就是(a,b,\u0026hellip;,k)。文中取k=25，感觉挺大的。。。 RQ-Kmeans：对圈定的N个item emb不断进行Kmeans聚类、求残差、残差继续Kmeans聚类的过程。文中取迭代次数为L=6，但是没说每次聚到多少个类。 注意：文中的RQ-Kmeans方法和RQ-VAE还不一样，RQ-Kmeans没有训练过程，也没有重构loss，纯粹是每次进行聚类，然后选聚类中心作为码本的过程。文中也没有对比过为啥不用RQ-VAE。\n产出两套semantic id之后，直接在下游排序任务中进行端到端更新，解决开头提到的表征不更新的问题。具体建模方法比较常规，不是本文的重点，略讲。\n评论 可借鉴 多模态emb预训练任务是i2i的，直接和下游搜推任务对齐 semantic id有两种产出方式，VQ和RQ-Kmeans，尽可能多地保留原始多模态emb的信息 可改进 多模态emb预训练和下游任务对齐，在2025年不算新鲜事了，常规操作。而且文中i2i的构造过程依赖U2I和I2I召回模型，有外部依赖，不够漂亮 VQ的方法，k=25这也太长了吧，相当于一个小型行为流了，会导致下游任务的特征处理更复杂 为什么用RQ-Kmeans而不是RQ-VAE，没有任何说明与对比 从pretrain emb量化成semantic id的过程中，存在严重的信息丢失，这在Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs论文中有讨论 ","permalink":"http://localhost:1313/posts/2025-10-04-qarm-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-qarm-paper-reading/QARM-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou\u003c/li\u003e\n\u003cli\u003e作者单位：快手\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2411.11739\"\u003ehttps://arxiv.org/pdf/2411.11739\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：CIKM 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003e多模态emb在搜推场景应用时通常采用如下图的两阶段方式，先预训练多模态emb，然后作为一个冻结特征放到搜推模型中。这种方式存在2个问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e表征不对齐\u003c/strong\u003e：多模态emb预训练的任务通常是图片分类或者文本的MLM，和下游搜推任务不对齐\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e表征不更新\u003c/strong\u003e：多模态emb在搜推任务中作为冻结特征，没有更新\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e本文的方法就是想要解决上述2个问题。\n\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-qarm-paper-reading/QARM-fig1.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"对齐搜推任务的多模态emb预训练\"\u003e对齐搜推任务的多模态emb预训练\u003c/h1\u003e\n\u003cp\u003e为了解决多模态emb表征不对齐的问题，本文提出的多模态emb预训练任务直接对齐搜推场景，使用U2I和I2I召回模型，挖掘出相似item pair，然后通过对比学习微调多模态大模型。\u003c/p\u003e\n\u003cp\u003e具体来说，通过U2I和I2I模型，能够拿到item emb；然后用每一个target item emb去行为流中检索出最相似的商品，作为trigger item emb。\u0026lt;trigger, target\u0026gt;构成一对正样本，然后进行对比学习训练。\u003c/p\u003e\n\u003cp\u003e通过召回模型构造的训练样本，和搜推场景的协同信号对齐了，解决了开头提到的第一个问题，即表征不对齐的问题。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-qarm-paper-reading/QARM-fig3.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"semantic-id生产方法\"\u003eSemantic id生产方法\u003c/h1\u003e\n\u003cp\u003eSemantic id的生产方法如上图右半部分所示，有两种方式：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eVQ\u003c/strong\u003e：直接圈定一定数量（如N）的item emb作为底池，编号1~N，然后任意来一个item emb，通过对底池emb进行KNN搜索，找出top-k相似商品，假设是(a,b,\u0026hellip;,k)，则VQ编码的semantic id就是(a,b,\u0026hellip;,k)。文中取k=25，感觉挺大的。。。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRQ-Kmeans\u003c/strong\u003e：对圈定的N个item emb不断进行Kmeans聚类、求残差、残差继续Kmeans聚类的过程。文中取迭代次数为L=6，但是没说每次聚到多少个类。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e注意：文中的RQ-Kmeans方法和RQ-VAE还不一样，RQ-Kmeans没有训练过程，也没有重构loss，纯粹是每次进行聚类，然后选聚类中心作为码本的过程。文中也没有对比过为啥不用RQ-VAE。\u003c/p\u003e\n\u003cp\u003e产出两套semantic id之后，直接在下游排序任务中进行端到端更新，解决开头提到的表征不更新的问题。具体建模方法比较常规，不是本文的重点，略讲。\u003c/p\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e可借鉴\n\u003cul\u003e\n\u003cli\u003e多模态emb预训练任务是i2i的，直接和下游搜推任务对齐\u003c/li\u003e\n\u003cli\u003esemantic id有两种产出方式，VQ和RQ-Kmeans，尽可能多地保留原始多模态emb的信息\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e可改进\n\u003cul\u003e\n\u003cli\u003e多模态emb预训练和下游任务对齐，在2025年不算新鲜事了，常规操作。而且文中i2i的构造过程依赖U2I和I2I召回模型，有外部依赖，不够漂亮\u003c/li\u003e\n\u003cli\u003eVQ的方法，k=25这也太长了吧，相当于一个小型行为流了，会导致下游任务的特征处理更复杂\u003c/li\u003e\n\u003cli\u003e为什么用RQ-Kmeans而不是RQ-VAE，没有任何说明与对比\u003c/li\u003e\n\u003cli\u003e从pretrain emb量化成semantic id的过程中，存在严重的信息丢失，这在\u003ca href=\"https://bitjoy.net/posts/2025-10-04-mme-sid-paper-reading/\"\u003eEmpowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs\u003c/a\u003e论文中有讨论\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"论文阅读：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"},{"content":"\n基本信息 论文标题：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs 作者单位：香港城市大学\u0026amp;腾讯 论文链接：https://arxiv.org/pdf/2509.02017 来源：CIKM 2025 Motivation：论文要解决的问题是什么 LLM4SR的基本范式如下，即用LLM直接来做搜推的范式（这种方式在学术界常见，但在工业界不常见）。由于LLM的输入词表范围是有限的（通常比较小），因此其token emb dim通常比较大，比如2048或者4096；而搜推场景的item量级很大，而且在不断更新，因此工业界经典的id-based的搜推模型的item emb dim通常比较小，比如64或128。经典的id-based的搜推模型能比较好地学习到搜推场景的协同信号，为了让LLM模型也能感知这种信息，LLM4SR范式通常会先预训练一个id-based的经典搜推模型，然后将其中的item id emb通过下图的Linear Projection的映射层，映射到LLM token emb的空间，让LLM也能感知搜推的协同信号。\n上述LLM4SR范式存在两个问题：\n维度坍缩：id-based训出来的id emb dim比较小（如64），LLM token emb dim比较大（如4096），在由id emb通过Linear Projection映射到toen emb的过程中，虽然64映射到4096空间了，但扩维后的矩阵存在低秩问题，即还是只利用了4096中的64维的空间。\n论文中，作者分两种情况进行了分析，如果Linear Projection只是一个线性层的话，通过公式推导能得出上述结论；如果Linear Projection包含非线性变换，作者通过实验分析也发现了维度坍缩的现象。 灾难遗忘：除了使用id-based模型产出的id emb，LLM4SR也常用多模态模型产出item emb表征，然后转换成semantic id输入到LLM4SR中。在这种情况下，产出的semantic id通过会遗忘多模态item emb的信息，导致下游LLM4SR的效果不佳。\n论文中，作者用公式9来衡量semantic id保留pretrain多模态emb的信息量。具体来说，如果行为流中的商品序列是{A,B,C,D}，target item是E。使用pretrain多模态emb能计算出E和A~D的相似度，例如相似度\u0026lt;E,A\u0026gt; \u0026gt; \u0026lt;E,B\u0026gt;。如果将pretrain多模态emb转换成semantic id，然后由semantic id恢复出新的A~E的emb之后，再计算E和A~D的相似度，如果仍然有\u0026lt;E,A\u0026gt; \u0026gt; \u0026lt;E,B\u0026gt;，则认为一致（concordant），否则不一致（disconcordant）。这个分析方法挺好的，通过这个指标能估算出转换成semantic id之后，仍然保留原有pretrain多模态emb对搜推场景的序关系的保留程度。 作者发现，转换成semantic id之后，信息只保留了37.14%；进一步，如果semantic id是在下游任务中端到端训练的，则信息只保留了5.5%，也就是说94.5%的pretrain emb的序的信息都丢掉了，也就是灾难遗忘。 Semantic id构建方法 3套emb来源，一套id-based经典搜推模型产出的包含协同信号的emb，另外两套是LLM2CLIP产出的多模态文本和图片emb。作者提到传统CLIP对长文本处理能力较弱，所以升级到LLM2CLIP，能更好地处理长文本。 Semantic id构建方法是经典的RQ-VAE的方法，但有如下两个改进点： 将emb的重构loss由MSE升级成MMD (maximum mean discrepancy)，MSE是计算原始emb和重构emb的欧式距离的误差，而MMD是计算两个分布的diff，实验表明能MMD比MSE能保留更多的pretrain多模态emb信息（即上述公式9），保留44.36% 对量化后的emb做了对齐，因为LLM2CLIP本身进行了图文模态的对齐，所以文中只新增了id emb分别和文本、图片模态的对齐 此外，还有一点论文没提但可能和常规RQ-VAE不同之处，就是原始emb在进行RQ-VAE之前，有一个Encoder升维的操作，在重构loss前对应有一个Decoder降维的操作，而semantic id量化恢复emb是Decoder之前的那个。这一升一降，估计也有助于缓解维度坍缩。 主模型 为了缓解维度坍缩，使用3套emb，一套id-based协同信号emb，另外两套是文本和图片的多模态emb 每套emb既包含原始emb过Linear Projection投影之后的表征（低维投影到高维，存在维度坍缩问题）；也包含由原始emb训练产出的semantic id重构回来的emb（天然高维emb） 为了避免灾难遗忘，semantic id使用上述优化的MMD loss训练产出，并且semantic id emb使用在训练semantic id emb产出的codebook emb进行初始化，然后随着LLM4SR finetuning，而不是完全随机初始化然后端到端训练 最后在LLM4SR输出层，有一个Multimodal Frequency-aware Fusion模块，next token prediction任务相当于一个n分类任务。在这个模块中，对target item也会新增一套emb talbe，这样总共就有4套emb table了。然后词表中每个item会根据热度过一个函数得到四种模态的emb的权重，然后4个emb进行融合。通过这种方式也能一定程度上缓解维度坍缩。 评论 可借鉴 论文的分析方法值得借鉴，例如对维度坍缩的推理分析、灾难遗忘的定量分析等 semantic id训练时的MMD loss缓解灾难遗忘 semantic id emb在下游应用时，使用训练的codebook emb进行初始化，而不是随机初始化，能缓解灾难遗忘 使用多套emb及semantic id，缓解维度坍缩 融合多套emb时，考虑item热度信息，动态调整融合权重 可改进 LLM4SR主要用于学术场景，没有考虑工业场景item id数据量巨大，而且不断更新的情况，因此在工业场景不常见 即使用上MMD loss，pretrain emb信息页只保留了44.36%，如果目标是100%的话，这个绝对差距还很大 没有论证semantic id emb遗忘pretrain emb信息对下游任务的影响，虽然遗忘信息了，但端到端训练也学到新知识了，功过相抵，也许效果不一定差？ semantic id通常通过两阶段训练得到，先预训练emb，然后训练semantic id，两阶段过程天然容易使semantic id遗忘预训练emb的信息，如果将两者合并成一阶段的，即把训练semantic id的网络模块加入到预训练emb的网络中，在预训练emb的过程中，就完成semantic id的训练，那么semantic id遗忘的信息会不会更少？类似的思想在召回双塔模型Poeem（https://arxiv.org/abs/2105.03933）中就有过。 ","permalink":"http://localhost:1313/posts/2025-10-04-mme-sid-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-mme-sid-paper-reading/mme-sid-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs\u003c/li\u003e\n\u003cli\u003e作者单位：香港城市大学\u0026amp;腾讯\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2509.02017\"\u003ehttps://arxiv.org/pdf/2509.02017\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：CIKM 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003eLLM4SR的基本范式如下，即用LLM直接来做搜推的范式（这种方式在学术界常见，但在工业界不常见）。由于LLM的输入词表范围是有限的（通常比较小），因此其token emb dim通常比较大，比如2048或者4096；而搜推场景的item量级很大，而且在不断更新，因此工业界经典的id-based的搜推模型的item emb dim通常比较小，比如64或128。经典的id-based的搜推模型能比较好地学习到搜推场景的协同信号，为了让LLM模型也能感知这种信息，LLM4SR范式通常会先预训练一个id-based的经典搜推模型，然后将其中的item id emb通过下图的Linear Projection的映射层，映射到LLM token emb的空间，让LLM也能感知搜推的协同信号。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-mme-sid-paper-reading/E4SRec.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上述LLM4SR范式存在两个问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e维度坍缩\u003c/strong\u003e：id-based训出来的id emb dim比较小（如64），LLM token emb dim比较大（如4096），在由id emb通过Linear Projection映射到toen emb的过程中，虽然64映射到4096空间了，但扩维后的矩阵存在低秩问题，即还是只利用了4096中的64维的空间。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e论文中，作者分两种情况进行了分析，如果Linear Projection只是一个线性层的话，通过公式推导能得出上述结论；如果Linear Projection包含非线性变换，作者通过实验分析也发现了维度坍缩的现象。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e灾难遗忘\u003c/strong\u003e：除了使用id-based模型产出的id emb，LLM4SR也常用多模态模型产出item emb表征，然后转换成semantic id输入到LLM4SR中。在这种情况下，产出的semantic id通过会遗忘多模态item emb的信息，导致下游LLM4SR的效果不佳。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e论文中，作者用公式9来衡量semantic id保留pretrain多模态emb的信息量。具体来说，如果行为流中的商品序列是{A,B,C,D}，target item是E。使用pretrain多模态emb能计算出E和A~D的相似度，例如相似度\u0026lt;E,A\u0026gt; \u0026gt; \u0026lt;E,B\u0026gt;。如果将pretrain多模态emb转换成semantic id，然后由semantic id恢复出新的A~E的emb之后，再计算E和A~D的相似度，如果仍然有\u0026lt;E,A\u0026gt; \u0026gt; \u0026lt;E,B\u0026gt;，则认为一致（concordant），否则不一致（disconcordant）。这个分析方法挺好的，通过这个指标能估算出转换成semantic id之后，仍然保留原有pretrain多模态emb对搜推场景的\u003cstrong\u003e序\u003c/strong\u003e关系的保留程度。\u003c/li\u003e\n\u003cli\u003e作者发现，转换成semantic id之后，信息只保留了37.14%；进一步，如果semantic id是在下游任务中端到端训练的，则信息只保留了5.5%，也就是说94.5%的pretrain emb的序的信息都丢掉了，也就是灾难遗忘。\n\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-mme-sid-paper-reading/mme-sid-formula9.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"semantic-id构建方法\"\u003eSemantic id构建方法\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e3套emb来源，一套id-based经典搜推模型产出的包含协同信号的emb，另外两套是LLM2CLIP产出的多模态文本和图片emb。作者提到传统CLIP对长文本处理能力较弱，所以升级到LLM2CLIP，能更好地处理长文本。\u003c/li\u003e\n\u003cli\u003eSemantic id构建方法是经典的RQ-VAE的方法，但有如下两个改进点：\u003c/li\u003e\n\u003cli\u003e将emb的重构loss由MSE升级成MMD (maximum mean discrepancy)，MSE是计算原始emb和重构emb的欧式距离的误差，而MMD是计算两个分布的diff，实验表明能MMD比MSE能保留更多的pretrain多模态emb信息（即上述公式9），保留44.36%\u003c/li\u003e\n\u003cli\u003e对量化后的emb做了对齐，因为LLM2CLIP本身进行了图文模态的对齐，所以文中只新增了id emb分别和文本、图片模态的对齐\u003c/li\u003e\n\u003cli\u003e此外，还有一点论文没提但可能和常规RQ-VAE不同之处，就是原始emb在进行RQ-VAE之前，有一个Encoder升维的操作，在重构loss前对应有一个Decoder降维的操作，而semantic id量化恢复emb是Decoder之前的那个。这一升一降，估计也有助于缓解维度坍缩。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-mme-sid-paper-reading/mme-sid-fig2.png\"\u003e\u003c/p\u003e","title":"论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs"},{"content":"博客数据恢复中，敬请期待！\n测试图片： 测试代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // Necessary header files for input output functions #include \u0026lt;iostream\u0026gt; using namespace std; // main() function: where the execution of // C++ program begins int main() { // This statement prints \u0026#34;Hello World\u0026#34; cout \u0026lt;\u0026lt; \u0026#34;Hello World\u0026#34;; return 0; } 测试数学公式： This is an inline \\(a^*=x-b^*\\) equation.\nThese are block equations:\n\\[a^*=x-b^*\\]\\[ a^*=x-b^* \\]\\[ a^*=x-b^* \\]These are also block equations:\n$$a^*=x-b^*$$$$ a^*=x-b^* $$$$ a^*=x-b^* $$","permalink":"http://localhost:1313/posts/2025-08-15-announcement/","summary":"\u003cp\u003e博客数据恢复中，敬请期待！\u003c/p\u003e\n\u003cp\u003e测试图片：\n\u003cimg alt=\"这是图片\" loading=\"lazy\" src=\"/posts/2025-08-15-announcement/myimg.png\"\u003e\u003c/p\u003e\n\u003cp\u003e测试代码：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Necessary header files for input output functions\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e#include\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e\u0026lt;iostream\u0026gt;\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eusing\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enamespace\u003c/span\u003e std;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// main() function: where the execution of\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// C++ program begins\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// This statement prints \u0026#34;Hello World\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    cout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello World\u0026#34;\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e测试数学公式：\nThis is an inline \\(a^*=x-b^*\\) equation.\u003c/p\u003e","title":"公告"},{"content":"这是我的个人博客，数据恢复中\u0026hellip;\n欢迎评论，如需私信请联系: bitjoy@163.com\n","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003e这是我的个人博客，数据恢复中\u0026hellip;\u003c/p\u003e\n\u003cp\u003e欢迎评论，如需私信请联系: \u003ca href=\"mailto:bitjoy@163.com\"\u003ebitjoy@163.com\u003c/a\u003e\u003c/p\u003e","title":"关于"},{"content":"第九十回 驱巨兽六破蛮兵 烧藤甲七擒孟获 孟获第五次失败后，去拉外援了。第六次叫来了另一个洞主木鹿大王，能呼风唤雨，能指挥虎狼豺豹。被诸葛亮用木制巨兽打败。第七次请来了乌戈国帮忙，乌戈国人都穿藤甲能防毒水，于是诸葛亮设计火攻，大败乌戈国人。\n就这样总计七擒七纵，最后孟获心服口服，归顺诸葛亮。\n第九十一回 祭泸水汉相班师 伐中原武侯上表 诸葛亮平定南蛮之地后，准备班师回朝，路过泸水，被水中孤魂野鬼阻挡，不能前进。后来得知要用七七四十九个人头祭奠才能过河。诸葛亮仁慈之人，肯定不会杀人祭河，于是杀了很多牛羊，做成人头的样子，叫做“馒头”，祭奠泸水，这就是馒头的由来。最后孤魂野鬼散去，诸葛亮军队顺利回到成都。\n此时，北方魏国皇帝曹丕病故，享年才40岁，在位仅7年。皇位传给曹丕儿子曹睿。正好雍州、凉州没有太守，司马懿毛遂自荐去当这两个州的太守。诸葛亮利用反间计，散布司马懿要篡位的谣言，离间曹睿和司马懿，最终曹睿罢免司马懿，把司马懿赶回老家种田。\n另一方面，诸葛亮听说曹丕驾崩，幼子曹睿继位，打算北伐魏国。《出师表》就是这个时候写的。于是诸葛亮带着包括赵云在内的一路人马开始北伐。魏国那边就是夏侯渊的儿子夏侯楙前来迎敌。\n第九十二回 赵子龙力斩五将 诸葛亮智取三城 诸葛亮带着赵云等大将来战夏侯楙，夏侯楙那边派出西凉大将韩德及其四个儿子出战，父子一共五个人都被赵云杀了。夏侯楙大败，退守南安城，赵云等三路军马围攻南安城。夏侯楙向周边的天水、安定两个城求解。被诸葛亮使用“诈称魏将”的计策破了南安和安定两个城，捉拿了夏侯楙。只有天水城没有被攻破，而且天水城的谋士姜维（字伯约）识破了诸葛亮的计谋。\n第九十三回 姜伯约归降孔明 武乡侯骂死王朗 接着，诸葛亮带兵来攻打天水城，诸葛亮不知道天水城有姜维这么个文武双全的人才，低估了拿下天水城的难度。同时，姜维也巧妙的使用了兵法，所以在最初的几次战役中，诸葛亮大败。后来诸葛亮摸清了姜维的情况，精心安排了这样一个进攻策略：引一军打姜维老母亲所在地冀县；引一军打囤积粮草之地上邽；引一军打天水城。这个安排一方面把姜维引入到冀县，另一方面分散了天水城的兵力，导致天水城内部大乱。同时，诸葛亮放走夏侯楙，让他去招安姜维；还让人假扮姜维，制造各种姜维已经投降蜀汉的迹象，离间姜维和魏国。最后假戏真做，姜维无路可走，真的投降了蜀汉。在姜维的帮助下，天水城也拿下了。\n诸葛亮拿下汉中三城之后，一路北上，前出祁山，兵临渭水。魏国皇帝曹睿听说后大惊，派出以曹真为大都督，郭淮为副都督，司徒王朗为军师的大军来战诸葛亮。两阵对圆，王朗和诸葛亮出阵，两个人对骂，王朗先出招，骂诸葛亮反贼，不顺应天命；诸葛亮出招，骂王朗汉朝旧臣，都76岁高龄了还出来招摇，助纣为虐，死后有何颜面见二十四帝。最后王朗被诸葛亮骂死。\n这天晚上，曹真算准诸葛亮会乘丧劫寨，诸葛亮也将计就计去劫寨，但是还是诸葛亮老谋深算，劫寨之战以诸葛亮大获全胜。\n第九十四回 诸葛亮乘雪破羌兵 司马懿克日擒孟达 曹真部队失败之后，向西羌求救。西羌自曹操时代开始向曹操进贡，这次听闻魏国求救，于是答应了，派了丞相雅丹和武将越吉共15万大军去偷袭蜀国大本营。诸葛亮听说后，只能分出一部分兵力去抵抗西羌。派出去的关兴和张苞都被勇猛的西羌兵打败了。没办法，诸葛亮留赵云守祁山，自己亲自去西羌前线，巧妙的利用积雪，大破羌兵，杀了越吉，俘虏雅丹。诸葛亮放雅丹回国，西羌事平。\n另一方面，曹真听说诸葛亮退了一部分兵，于是趁机出兵攻打祁山。没想到被镇守的赵云和魏延大败。没办法，曹真求救曹睿皇帝。与此同时，之前关羽败走麦城向孟达求救，孟达没救并投降了魏国。这个时候，孟达又想回到蜀国的怀抱了，诸葛亮同意，让孟达攻打洛阳，诸葛亮攻打长安。\n曹睿如临大敌，太傅钟繇推荐把之前罢免的司马懿请回来。司马懿的军事才能应该不下于诸葛亮，且两个儿子（长子司马师、次子司马昭）素有大志，通晓兵法。司马懿复出的第一战就是大战孟达。结果司马懿获胜，孟达被杀。接着，曹睿正式派司马懿出关破蜀，司马懿带着大将张郃，浩浩荡荡，前来抗诸葛亮。\n第九十五回 马谡拒谏失街亭 武侯弹琴退仲达 诸葛亮算准了司马懿会首先进攻街亭，因为街亭是汉中的咽喉要道，一旦街亭失守，汉中就很危险了。马谡主动请缨去守街亭，于是诸葛亮派马谡主守街亭，同时派魏延、王平等驻扎在街亭附近支援马谡。\n马谡来到街亭之后，觉得这么个小地方，不足为惧，也不听王平的劝诫，非要在山上驻扎。司马懿来了之后，先断了山上游的水源，把山团团围住。马谡军中自乱，最后抵挡不住司马懿的大军，败下阵来。尽管有王平、魏延等的支援，依然不敌司马懿的大军。\n诸葛亮听说马谡大败之后，知道事情很危急，准备从祁山退军回到汉中。同时，为了不制造很大的动静，不让魏军后方的曹真等乘虚而入，诸葛亮各种算计，把关兴、张苞等身边的武将都派出去了。诸葛亮自己带着一帮文官守在西城。司马懿也算到了诸葛亮在西城，于是引大军来打西城。诸葛亮因为实在没有兵了，使出了空城计，把城门大开，军旗尽去，只安排几个士兵扮装老百姓的样子在城门口扫地。司马懿看到后，疑神疑鬼，因为知道诸葛亮向来深思熟虑，这次突然把城门大开，肯定有很大的埋伏，于是退兵了。诸葛亮的空城计成功了。\n在整个混乱的过程中，诸葛亮和司马懿互相算计，总的来说，互有胜负吧。结果就是诸葛亮暂停北伐脚步，退出祁山，回到汉中。司马懿收复陇西诸郡。\n第九十六回 孔明挥泪斩马谡 周鲂断发赚曹休 却说诸葛亮回到汉中之后，清点人马，因马谡之前有军令状，且是因为他不听王平劝诫才导致蜀军大败的，根据军法，挥泪斩马谡。\n话分两头，东吴的周鲂给曹休发了诈降书，且为了让曹休相信自己是真的投降，还把头发割掉了。最后曹休相信周鲂，曹睿派包括曹休在内的三路大军前去攻打东吴。结果很显然，曹休中计，大败而回，气忧成病，回到洛阳就死了。\n第九十七回 讨魏国武侯再上表 破曹兵姜维诈献书 因东吴帮忙击败了曹休，遂发书给诸葛亮让他再次出兵北伐，诸葛亮给后主写了《后出师表》，表明自己鞠躬尽瘁死而后已的精神，再次北伐。就在这时，传来一个坏消息，大将赵云病故，留下两个儿子赵统和赵广。于是诸葛亮北伐只剩几个老将和小将了：魏延、关兴、张苞、王平等。\n魏蜀两军在陈仓口（陈仓口再往北就是街亭）交锋，魏国守陈仓的是郝昭，深沟高垒，诸葛亮使出了各种计谋都没能攻破城池。同时，魏国朝廷派出大将军曹真和身强力壮的王双前来接应。诸葛亮派出的两员大将都被王双打败并杀了。\n最后，姜维使出诈降计，本来想赚曹真的，结果只赚到曹真的一个大将费耀。蜀军大获全胜，虽然没有杀到曹真，杀到费耀也不错。同时曹真损兵折将，蜀军又出祁山（第二次）。\n第九十八回 追汉军王双受诛 袭陈仓武侯取胜 却说诸葛亮第二次出祁山时，由于粮草只够支撑一个月时间，汉中前线只有陈仓这一个地方方便运粮，而陈仓被魏国的郝昭和王双把守着，所以诸葛亮必须速战速决，否则粮草支撑不住。魏国大都督就想了个办法，让部队假装运粮，吸引蜀军来抢粮食，实际粮车里装的都是干柴硫磺等易燃物，等蜀军来抢的时候，把蜀军烧死，同时突袭蜀军营寨。但是这个计策怎么能难倒诸葛亮，诸葛亮算准了，而且将计就计，结果反而把假扮运粮的魏军干掉了。但是蜀军毕竟是没粮了，不能恋战，而且曹睿此时又派了张郃领军来支援，于是，诸葛亮在赢了这一战的情况下，果断撤军。魏军此时刚刚失败，也不敢追诸葛亮。但是魏延军队在陈仓口防御王双，撤离的话，王双肯定是会和魏延干起来的。于是诸葛亮撤军的时候，偷偷给魏延一个锦囊妙计，大部队假装撤军，让王双来追，魏延和一小支部队等王双追出去之后，把王双营寨端了，王双回寨时，魏延又出其不意把王双干掉了。所以诸葛亮第二次出祁山，虽然没有干掉魏国，但是也小挫了一把魏军，同时安全回到汉中。曹真听说王双被杀，忧成疾病。\n此时，吴国孙权称帝了，蜀国派人前去祝贺，同时想联合吴国一起伐魏。吴国假装同意，实则想隔岸观火，坐收渔翁之利。\n又过了一阵子，诸葛亮听到守陈仓的郝昭病危，觉得这是一个进攻陈仓的好机会，于是又率部队去袭击陈仓，没等张郃救兵赶到，诸葛亮大军就把陈仓城攻破了，郝昭本来就病危，被这么一攻，吓死了。等张郃赶到的时候，发现诸葛亮大军把各个路段都占领了，于是退军。随后，诸葛亮又趁胜追击，拿下了散关。这就是诸葛亮第三次出祁山。\n曹睿听说诸葛亮又来了，吓死了，这个时候曹真病又没好，于是就把曹真的大都督职位交给了司马懿。司马懿出兵来与诸葛亮决战。\n第九十九回 诸葛亮大破魏兵 司马懿入寇西蜀 司马懿带着张郃等人前来与诸葛亮决战，但是前几次战争都失败了，于是司马懿决定深沟高垒，不再出战。此时，诸葛亮使用缓兵之计，每隔几天退几十里，给魏军要撤退的错觉。司马懿知道诸葛亮诡计多端，不敢追杀，但是大将张郃不同意，非要去追杀诸葛亮。司马懿没办法，让张郃先去追杀，自己随后就到。诸葛亮算准了司马懿的安排，调兵遣将，不但抵抗住了追杀，还安排部队袭击了司马懿的营寨。司马懿大败而回，虽然诸葛亮没有占领司马懿的营寨，但是歼敌无数。\n在这个过程中，虽然蜀军胜利，但是在一次战争中，张苞（张飞的儿子）不小心骑马掉到河里了，把头磕破了，回成都养病，但是后来死了。诸葛亮悲痛欲绝，决定撤军。诸葛亮第三次出祁山也没有显著成果。\n又过了一段时间，曹真病好了，决定和司马懿率大军进攻蜀国，曹睿同意。于是这两人率40万大军进攻蜀国。诸葛亮算准了接下来一个月都要下雨，蜀国地势又复杂，魏军远道而来，天时地利都不好，所以不用担心。果然这一个月都阴雨连绵，魏军粮草不够，很多士兵都饿死了。没办法，魏军主动撤退。\n第一百回 汉兵劫寨破曹真 武侯斗阵辱仲达 魏军退了之后，诸葛亮并不追杀，而是准备再次出祁山（第四次）。诸葛亮安排兵分两路，一路魏延、张嶷、杜琼、陈式攻打祁山之东箕谷，一路马岱、王平、张冀、马忠攻打祁山之西斜谷，然后会于祁山。司马懿算准了诸葛亮有这个安排，于是也兵分两路，司马懿守箕谷，曹真守斜谷，而且他两还打赌，如果诸葛亮真的来攻打，则司马懿赢，否则曹真赢。\n攻打箕谷的陈式不相信有司马懿的埋伏，不听劝阻，导致被司马懿打败，损兵折将。事后诸葛亮按军法处置，把陈式杀了。而另一方面，曹真不相信诸葛亮会攻打斜谷，消极守军，于是被马岱那路军马打败，诸葛亮顺利出祁山（第四次）。\n有一天，情报说曹真卧病不起，诸葛亮于是写了封信送给曹真看，把曹真骂了一顿，曹真看到信后怀恨而死。魏主曹睿命令司马懿出战。于是两军对峙，司马懿和诸葛亮开始比军事才能（斗阵），司马懿大败。诸葛亮收得胜之兵回祁山，永安城李严派来的送粮官苟安来送粮，但是延误了十天。诸葛亮很生气，按军法该杀，但是手下都求救，诸葛亮就给苟安杖打八十大板放回去了。苟安怀恨在心，投降司马懿。司马懿利用苟安，把苟安放回成都，但是要苟安散布谣言，造谣说诸葛亮要篡后主的位。后主就下令让诸葛亮撤军回成都。\n诸葛亮很伤心，好不容易又出一次祁山，而且战情对自己有利，现在居然被皇上下令撤军，没办法只好撤军了。不过，诸葛亮要杀苟安，后又不杀，杖打八十大板回去，这是很危险的，诸葛亮应该能想到这个小兵会叛变。要不就果断按军法处置杀了，要不就好好安抚。唉，错失良机啊。\n第一百一回 出陇上诸葛妆神 奔剑阁张郃中计 诸葛亮回到成都，查明了事情的真相，后主自己都不好意思了，但是苟安已经逃到魏国去了。\n过了一阵子，诸葛亮又上书要北伐魏国，于是第五次出祁山，兵分两路，王平等四个大将守祁山，诸葛亮带着魏延姜维守卤城。因粮草不够，打算去附近的陇上麦田割点麦子当粮食。司马懿算准了诸葛亮会兵分两路，于是也兵分两路，一路张郃守祁山，司马懿亲自带着人马去陇上。诸葛亮和司马懿在陇上交战，诸葛亮使用“奇门遁甲”之术，“变出”好多个诸葛亮，迷惑司马懿及魏军，然后偷偷安排大部队去陇上割麦。结果，魏军被诸葛亮迷惑得糊里糊涂，还损兵三千，蜀军割麦成功。\n正当诸葛亮优势比较明显时，又出幺蛾子了。诸葛亮突然收到粮草官李严的来信，说东吴接连魏国，打算进攻蜀国大后方。诸葛亮大惊，没办法，决定撤军。退军时，诸葛亮又布下了迷阵，让魏军不敢追杀。但是张郃偏偏要逞能，决定去追杀诸葛亮，在追杀的路上，被诸葛亮的埋伏给杀了。\n诸葛亮安全退回汉中，然后急忙回成都一探究竟，结果吴国并没有要进攻蜀国，而是李严谎报军情，因为李严是粮草官，因凑不齐给诸葛亮北伐的粮草，怕被诸葛亮惩罚，就向诸葛亮撒了这么一个谎，让诸葛亮回来。诸葛亮都快要被气死了，后主也很生气，打算杀了李严，但是朝廷官员说李严是刘备托孤的人之一，还是留一条活路吧，于是后主把李严贬为庶民，用李严的儿子李丰接替为粮草官。\n第一百二回 司马懿占北原渭桥 诸葛亮造木牛流马 却说诸葛亮在蜀国休养了三年，人马雄壮，又准备北伐，有人就劝他，为啥不好好在蜀国待着享受生活呢？诸葛亮说：“臣受先帝知遇之恩，梦寐之间，未尝不设伐魏之策。竭力尽忠，为陛下克复中原，重兴汉室：臣之愿也。”于是第六次出祁山北伐。\n就在诸葛亮出发的时候，听说关羽的儿子关兴病故，诸葛亮痛哭，张飞、关羽的儿子都相继去世，诸葛亮只能带上老将姜维、魏延，和一众小将王平等，共三四十万大军，准备北伐。司马懿方面，带着两个儿子司马师和司马昭，夏侯渊的四个儿子等大将，前来抗蜀。\n此次北伐第一战在渭河附近。诸葛亮谋划好的战略部署，都被司马懿算中了，结果蜀军大败，司马懿守住北原渭桥。诸葛亮写信给东吴孙权，请求支援，孙权同意，起兵三十万北伐。\n诸葛亮在渭水、祁山附近转悠，发现一个上方谷的地方，有点像葫芦，两口小，内部空间大，于是诸葛亮安排人员在里面造木牛流马，造好之后，用木牛流马去剑阁搬运粮草。司马懿知道后，趁蜀军搬运粮草的时候，劫了几个木牛流马回来，依葫芦画瓢，也造了很多木牛流马。诸葛亮知道后，很高兴，他就是要让司马懿学会也用木牛流马来运粮。木牛流马上有一个开关，可以控制木牛流马是否能运行，魏军不知道，只有诸葛亮知道。于是诸葛亮派人去劫魏军的粮草，而且相关的战斗都安排好了。结果蜀军大获全胜，魏军四处逃窜。\n第一百三回 上方谷司马受困 五丈原诸葛禳星 司马懿在渭河，深沟高垒，坚守不出。诸葛亮结合各种计谋，终于引司马懿出战。司马懿兵分两路，一路攻打祁山诸葛亮大本营，一路攻打上方谷。这都是诸葛亮策划好的，两处都有蜀军埋伏。司马懿亲自带着自己的两个儿子攻打上方谷，蜀军魏延引司马懿军队进入上方谷，就谷内放起火来，而且堵住了谷口。本来以为这次司马懿父子三人要被烧死，没想到上天下起雨来，扑灭了大火，救了司马懿。虽然如此，司马懿的两路军队都损失惨重。\n此后，司马懿更是深沟高垒，不敢出战。诸葛亮甚至给司马懿送女人的衣服羞辱司马懿，但是司马懿还是不出战，反倒问起诸葛亮的饮食起居，说诸葛亮吃得少，又日理万机，身体估计要垮掉。诸葛亮听了之后，神思不宁。\n后来，又听说孙权兵分三路攻打魏国，但是都失败了，长叹一声，旧病复发，从此身体就不好了。诸葛亮又夜观天象，发现将星欲坠，担心自己命不久矣。于是每天点蜡烛开始祈禳（祈祷），如果七天之内，蜡烛不灭，则自己的命还可以活很久，否则就要挂了。到第六天的时候，司马懿派人来刺探军情，魏延匆匆忙忙跑进帐篷，本来想向诸葛亮汇报军情的，因为走的太快了，带起来的风把诸葛亮的蜡烛给吹灭了。唉，诸葛亮命不久矣。\n第一百四回 陨大星汉丞相归天 见木像魏都督丧胆 诸葛亮知道自己很快要死了，开始安排后事，丞相一应大事都交给杨仪，用兵秘法各种兵书都传给了姜维，然后告诉杨仪，自己死后，魏延会反叛，你到时候打开我给你的锦囊，自有人收拾魏延的。另一方面，告诉杨仪，自己死后，不要发丧，把自己放在神龛里，这样天上的将星就不会掉下来了，还有当魏军来袭时，把事先准备好的我的木像拿出来，到时候魏军看到肯定要吓死的。没过几天，诸葛亮就死了。\n杨仪、姜维等人，根据诸葛亮的遗嘱，安排徐徐退军，让魏延断后。魏延觉得自己现在被官职比自己小的杨仪管着，不服气，想要联合马岱把杨仪除掉。但是，这时司马懿乘虚而入，以为诸葛亮死了，前来追杀。杨仪抬出事先准备好的诸葛亮的木像，司马懿和魏军吓得半死，拼命撤军，姜维乘势掩杀，魏军被杀自相践踏，死者无数。杨仪、姜维退入栈阁道口。\n第一百五回 武侯预伏锦囊计 魏主拆取承露盘 在杨仪、姜维退入道口的时候，前面遇到魏延反叛，烧断吊桥，挡住去路。魏延和姜维正面交锋，杨仪读了诸葛亮遗留下来的锦囊，对魏延说：“如果你敢在马上连叫三声“谁敢杀我”，我就把汉中地区让给你”。魏延果然在马上大叫“谁敢杀我”，魏延军中的马岱立马把魏延杀了。原来诸葛亮死之前，偷偷跟马岱说了，让马岱跟着魏延假装反叛，实则等到两方交锋时，趁魏延不提防自己，把魏延杀了。\n除掉魏延，杨仪、姜维抬着诸葛亮的灵柩，顺利回到成都。后主、文武百官、平民百姓失声痛哭。根据诸葛亮生前的嘱托，蒋琬为新的丞相，费祎为副丞相，吴懿和姜维守汉中。\n诸葛亮死后几年，魏蜀吴相安无事。魏主曹睿开始骄奢淫逸，大兴土木，听说汉武帝的长安宫有一个铜人，手捧一个承露盘，接天水，汉武帝就是喝这个天水可以长寿、返老返童。曹睿就派人把这个铜人和承露盘拆下来搬到自己建的园林里。另外，曹睿宠幸郭夫人，把原配毛皇后杀了，立郭夫人为新皇后。总之做出各种昏庸残暴的事情。\n第一百六回 公孙渊兵败死襄平 司马懿诈病赚曹爽 正当曹睿各种happy的时候，辽东的公孙渊开始造反了。公孙渊的爸爸是公孙康，当初曹操打袁绍儿子袁尚，袁尚就跑到公孙康那里，然后公孙康把袁尚杀了给了曹操，所以那时候公孙家族还是臣服于曹魏的。传到公孙渊这一代，开始膨胀，居然称帝并主动进攻魏国。曹睿就派司马懿出兵剿灭公孙渊，公孙渊哪是司马懿的对手，交手没几仗，司马懿在襄平把公孙渊及其家族灭了。\n话说有一天半夜三更，魏主曹睿突然看到毛皇后带着几个宫女来要曹睿的命，从那以后，曹睿就病倒了。没过多久，曹睿驾崩，享年36岁，在位13年。皇位传给了曹睿的儿子曹芳，由曹爽和司马懿辅政。曹爽的父亲是曹真，以前和司马懿一起出战防御诸葛亮北伐，那时候曹真和司马懿两人关系不太好。现在曹爽和司马懿一起辅佐曹芳，曹爽看不惯司马懿，于是跟皇上曹芳说可以把司马懿加为太傅。曹芳同意，于是兵权就落到了曹爽的手里。此后，曹爽和门下的幕僚就各种骄奢淫逸。而且曹爽担心司马懿会偷偷算计自己，还派人去太傅府打探司马懿的虚实。司马懿当然知道曹爽心里的小99，于是装聋作哑，装疯卖傻。曹爽信以为真，以为司马懿已经老到不行了，于是不再担心司马懿打击自己，更加得意忘形。\n第一百七回 魏主归政司马氏 姜维兵败牛头山 有一天，魏主曹芳要出城祭祖，曹爽因不再担心司马懿，带着所有手下大将和儿子们，跟着曹芳一起出城潇洒狩猎了。司马懿看准机会，起兵反叛，夺取了京城，并给曹爽送信说我自己想削你的兵权，你如果乖乖交出兵权，我就不追究你的责任。曹爽信以为真，果然交出印绶，徒手回城。司马懿接过印绶之后，立马把曹爽全家老小都抓起来斩首了。从此魏国军权落到了司马懿手里。事实上，如果曹爽那时候不相信司马懿的话，不回城，而是召集其他州的军队，攻打回京城，说不定曹爽能赢呢。Anyway，历史没有如果。\n雍州的夏侯霸和曹爽是亲族，听说曹爽被灭门了，司马懿肯定不会放过自己，就叛乱了，但是打仗又打不赢雍州的郭淮，于是夏侯霸就投降蜀国了。蜀国姜维，听说曹爽被灭门，司马懿掌权，又有夏侯霸来投降，想趁着魏国内乱，去攻打魏国。于是姜维起兵先攻打雍州郭淮，姜维的军事才能较差，在与郭淮的交战中，兵败牛头山。\n第一百八回 丁奉雪中奋短兵 孙峻席间施密计 姜维兵败，回到汉中。后来，司马懿病故，其两个儿子司马师、司马昭子承父业。又东吴陆逊、诸葛瑾接连去世，诸葛恪接替诸葛瑾。孙权也病故，儿子孙亮继位。\n魏国听说孙权死了，想乘机攻打吴国，于是派司马师、司马昭带着三路大军南下攻打吴国。东兴郡是吴国的要害地方，所以魏军先主攻这个地方。诸葛恪安排平北将军丁奉前来应战，当时正是下雪天，丁奉带着三千吴兵，脱掉盔甲，准备使用短兵和魏国互搏，魏军觉得吴军人少，且只用短兵，掉以轻心，结果丁奉带着吴军奋勇杀敌，大败魏军。司马昭转攻为守。\n诸葛恪觉得，既然打了胜仗，不如乘胜追击，北伐魏国。于是一面联系蜀国让联合进攻，一面主攻魏国的要害城市新城。但是呢，诸葛恪对下属太严酷，导致军心涣散，又中了魏军的计谋，最终大败而归。有意思的是，这个诸葛恪打了败仗回来之后，恼羞成怒，把责任都推到手下官兵手里，各种肃清杀替罪羊，还把孙权的亲戚孙峻的掌管御林军的职位给抢了。孙峻不干了，到孙亮面前告状，细数诸葛恪各种罪状，说要把他除掉。孙亮同意，于是孙亮摆了一个鸿门宴，请诸葛恪赴宴，孙峻就席间把诸葛恪杀了，从此吴国军权落到了孙峻手里。\n第一百九回 困司马汉将奇谋 废曹芳魏家果报 话说蜀国姜维接到诸葛恪“联合伐魏”的信号后，起兵出阳平关伐魏。司马师令徐质为先锋，司马昭为大都督，前去抵抗姜维。徐质英勇过人，姜维手下几个人和徐质交手都失败了。后来，姜维使用埋伏计，围困徐质，最后成功杀了徐质，并且把司马昭军队围困在铁笼山。郭淮听说司马昭被困，使用诈降计，骗了姜维的盟友羌人迷当。迷当被俘，投降郭淮。郭淮于是带着迷当的羌人部队，前去救援司马昭。司马昭得救，但是郭淮在和姜维的对战中，被姜维射死。虽然此次战役姜维最终还是失败回汉中，但杀了魏国两员大将：徐质和郭淮，也算是将功补过。\n话说魏主曹芳年幼没有兵权，司马师在魏国，大权在握，越加蛮横无理、目中无人。曹芳忍无可忍，血书衣带诏让人带出宫想召集人马铲除司马师势力。事情败露，被司马师知道了，司马师灭了参与人员家族，并且废了曹芳，另立曹髦为皇。有点像当年曹丕废汉献帝的赶脚，历史总是惊人的相似。\n昔日曹瞒相汉时，其他寡妇与孤儿。 谁知四十余年后，寡妇孤儿亦被欺。\n第一百十回 文鸯单骑退雄兵 姜维背水破大敌 却说司马家族先是杀曹爽，后又废曹芳，引起曹家亲信不满，其中就有镇东将军毋丘俭和扬州刺史文钦，于是他两联合打算讨伐司马师。司马师得了眼疾，刚把眼睛里的瘤子割了，没办法只能亲自出战，留弟弟司马昭掌管朝廷大事。\n在与司马师的对战中，文钦还带着他的儿子文鸯，文鸯骁勇善战，闯入魏军军中，如入无人之境。但是奈何寡不敌众，毋丘俭被杀，文钦知道大势已去，投奔东吴孙峻去了。自此司马师平定淮南叛乱。\n不过这场战争导致司马师眼疾恶化，加上回想起之前自己杀的人，脑子坏掉死了，魏国大权落到了弟弟司马昭的手里。\n蜀国听说司马师死了，司马昭刚刚接任，新旧交替，又打算趁机伐魏。于是姜维又从汉中起兵，攻打魏国。在第一战中，姜维采用背水一战、无路可退的策略，激发士兵战斗意志，打了胜仗。但是在接下来的战争中，毕竟魏国的军事实力还是要强一些，姜维兵败，被迫再次退回汉中。\n第一百十一回 邓士载智取姜伯约 诸葛诞义讨司马昭 姜维回到汉中之后，没过多久，又商议出师伐魏。魏国邓艾、陈泰等人应敌。姜维的计谋被邓艾识破，姜维兵败，再次退回汉中，而且在这次北伐过程中，大将张嶷在救姜维时被魏军杀害。\n魏国司马昭，常怀篡逆之心，想称帝，于是派手下贾充去探访各将军的口风。结果发现镇东将军诸葛诞（和诸葛亮是一个宗族的），不同意司马昭称帝。司马昭于是想除掉诸葛诞，诸葛诞得知之后，揭竿而起，联合东吴讨伐司马昭。司马昭得知之后，想亲自出征讨伐诸葛诞，又担心自己在外时朝廷发生政变，于是要求魏主曹髦和太后和自己一起出征讨伐诸葛诞，没办法，曹髦和太后只能乖乖跟着司马昭出征。\n东吴那边，孙峻病故，从弟孙綝继位，把持朝政。\n第一百十二回 救寿春于诠死节 取长城伯约鏖兵 司马昭和诸葛诞大战寿春城，东吴于诠救援寿春，终于寡不敌众，于诠和诸葛诞兵败被杀，淮南被司马昭平定。\n姜维听说司马昭带着魏主正和诸葛诞东吴大战，觉得这个时候可以乘虚而入，攻打魏国后方。于是再次北伐魏国，这次准备进攻魏国的长城，因魏国的很多粮草都囤积在这里。姜维大败长城将军司马望，司马望坚守不出。后魏将邓艾邓忠父子引兵来救，魏蜀打了一仗，后邓艾父子又深沟高垒，坚守不出，想等司马昭的大部队来救援。后来，姜维听说司马昭打败了诸葛诞，应该会派兵来支援长城，于是退兵回汉中了。\n第一百十三回 丁奉定计斩孙琳 姜维斗阵破邓艾 话说东吴的孙綝把持朝政，吴国皇帝孙亮送出衣带诏想秘密铲除孙綝，结果事发，孙綝杀了所有与之有关的人，罢黜孙亮，另立孙权的第6个儿子孙休为新皇帝。\n孙綝以为只是另立了一个傀儡皇帝，没想到孙休担心自己和孙亮一个下场，也决定铲除孙綝。这次孙休和丁奉秘密布了一个鸿门宴请孙綝赴宴，孙綝自恃把持朝政、掌握兵权，欣然赴约，被孙休灌酒，然后被丁奉、张布等人暗藏的刀斧手杀掉了，铲除了孙綝势力。\n东吴又派使者给蜀国，说司马昭肯定要篡位，到时候一定会入侵吴国、蜀国，希望吴、蜀能早做准备，共同抵御魏国。\n姜维听信后，又出祁山伐魏。姜维因为继承了诸葛亮的阵法，在战场上和魏国的邓艾、司马望等人斗阵法，邓艾、司马望大败。但是司马望想了一个点子，因为蜀国后主刘禅宠信黄皓，日夜以酒色为乐，所以派人去私通黄皓，让她在蜀国散步谣言，说姜维埋怨刘禅，不久要投降魏国了。刘禅听后，把姜维从前线叫回来了。姜维这次北伐又无功而返。哎，刘禅真是猪脑子，昏庸无能还听信谗言，之前把诸葛亮召回来，这次又把姜维召回来。\n第一百十四回 曹髦驱车死南阙 姜维弃粮胜魏兵 却说曹髦看到司马昭把持朝政横行霸道，担心他要篡位，是可忍孰不可忍，曹髦奋起反抗，带着几百个官僮，准备和司马昭大战。可想而知，司马昭掌握了军队，曹髦这是以卵击石。司马昭下令贾允迎战曹髦，贾允命令手下成济动手，成济杀死曹髦。\n赶到现场的司马昭还假惺惺的哭，问到底怎么回事，还下令把刽子手成济全家斩首。真是借刀杀人还卸磨杀驴。皇帝死了，有人就建议司马昭自立为皇帝吧，司马昭说当初曹操篡汉的时候没有称帝，自己篡魏时也不称帝，但是在有意培养自己的接班人儿子司马炎。所以司马昭还是立了一个傀儡皇帝曹操的孙子曹奂。\n姜维听说司马昭弑君，觉得这又是一个师出有名的伐魏的好机会，于是一面给东吴发信，一面起兵出祁山，兵分三路北伐魏国。魏国守祁山的依然是邓艾，邓艾和手下的王瓘商量了一个诈降计，让王瓘去姜维那里诈降，但是被姜维识破了，姜维将计就计，破了邓艾和王瓘，王瓘被蜀兵围困，投江自杀，邓艾也损失惨重。姜维和邓艾各自收兵回国。\n第一百十五回 诏班师后主信谗 托屯田姜维避祸 没过多久，姜维又要北伐魏国。带着夏侯霸、张冀等大将，进攻魏国的洮阳。夏侯霸作为先锋部队，因误入邓艾的空城计，自己和部队都被杀了。后姜维兵分两路，一路继续攻打洮阳，另一路偷袭邓艾的后方祁山寨，大败邓艾部队。就在姜维奋力围攻祁山寨时，收到后主刘禅的诏书，让班师回朝。后主宠幸宦官黄皓，骄奢淫逸。右将军阎宇想夺姜维的大将军职位，巴结黄皓，黄皓就把姜维召回来了。姜维第n次北伐又无功而返。\n姜维回来之后，百般无奈，就跟刘禅说自己打算带兵去沓中屯田，一来让士兵休养生息，二来多种些粮食，准备日后再次伐魏。刘禅同意，于是姜维沿路布下了几十个营寨，防止魏国入侵，自己带着几万部队在沓中屯田。\n司马昭听说蜀国皇帝昏庸无道，姜维也去屯田了，觉得这是个入侵的好机会，于是派邓艾、钟会等大将，准备入侵蜀国。\n第一百十六回 钟会分兵汉中道 武侯显圣定军山 钟会从京城出发，邓艾从祁山出发，浩浩荡荡，进攻蜀国，而且因为兵力充足，各处都安排了进攻路线，比如邓艾攻沓中，诸葛绪断后路等等，安排得妥妥帖帖。姜维赶紧上报朝廷，要求出兵增援，但后主刘禅因听信宦官黄皓，不理姜维。\n没办法，魏军声势浩大，蜀军寡不敌众，最终汉中失守，被钟会夺取。姜维等退入剑阁。\n在这次魏国进攻的战役中，当钟会进攻到定军山这个地方时，出现了好几次灵异事件，搞得钟会军心涣散，担惊受怕，后来问蜀国降将才知道诸葛亮的墓就在定军山，这些灵异事件应该是诸葛武侯显圣了。于是钟会赶紧去祭拜了诸葛亮的墓地，这些灵异事件才消失了。\n第一百十七回 邓士载偷度阴平 诸葛瞻战死绵竹 姜维在撤退的时候使用调虎离山之计成功迷惑了诸葛绪，钟会大怒，本来要杀诸葛绪的，后来众官告免，钟会就让人把诸葛绪押解会京城发落。因为诸葛绪是邓艾的部下，邓艾听到诸葛绪被钟会发落，很生气，决定不和钟会配合，自行从阴平进攻成都。\n阴平的路虽然很险阻，但是邓艾克服困难，最终胜利到达江油城，守城蜀将不战而降。邓艾占据江油，继续进攻绵竹。\n这时刘禅知道事态的严重性，把诸葛亮的儿子诸葛瞻请出来了，诸葛瞻带着儿子诸葛尚前去迎敌。两军在绵竹对战，诸葛瞻终因寡不敌众，父子都战死军中。邓艾大获全胜准备继续进攻成都。\n第一百十八回 哭祖庙一王死孝 入西川二士争功 后主刘禅听说邓艾来势汹汹，和众官商议后决定投降邓艾。刘禅的第五个儿子刘谌听说后，很为刘禅赶到羞愧和愤怒，但又无能为力，于是把自己的妻子儿女都杀了，然后自杀表明自己的气节。\n于是邓艾接受刘禅投降，出榜安民，蜀汉灭亡！\n姜维收到刘禅要求投降的诏书后，军中士兵一片哗然。姜维想了一个点子，也假装投降，但是向钟会投降，而且挑拨离间钟会和邓艾。本来钟会也觉得自己拖住了姜维的军队，邓艾才有机会进攻成都，自己也是有很大的功劳的，现在邓艾反而得了头等功，不服气。于是姜维和钟会一合计，决定向司马昭密告邓艾要盘踞蜀中，密谋造反。司马昭下令让钟会制衡邓艾，同时司马昭自己又怀疑钟会会造反，所以带着魏主曹奂御驾亲征，来视察钟会，这下有好戏看了。\n第一百十九回 假投降巧计成虚话 再受禅依样画葫芦 钟会把邓艾父子监禁，随后准备押回洛阳。后钟会听说司马昭御驾亲征驻扎在长安，准备入蜀，知道自己也要被司马昭杀害，于是和姜维商量计策，姜维说干脆伪造魏主郭太后的命令，讨伐司马昭。但是事情泄露，司马昭派来的监军卫瓘杀了钟会、姜维等人，随后又追杀了邓艾父子。也就是说，灭蜀的两大功臣邓艾和钟会，先是被姜维挑拨离间，后又被上级领导司马昭卸磨杀驴，太惨了。\n从此蜀国彻底灭亡，后主刘禅被押往洛阳，司马昭问刘禅是否思蜀，刘禅说：”此间乐、不思蜀”，悲哀，司马昭就不担心刘禅复国，给他各种好吃好喝，麻痹刘禅。\n没过多久，司马昭突然中风了，然后就死了。临死之前，司马昭立长子司马炎为太子。司马炎继位，废除魏主曹奂，称帝。自此魏国灭亡，晋国开始。\n第一百二十回 荐杜预老将献新谋 降孙皓三分归一统 却说吴主孙休，听说司马炎已经篡位，蜀国灭亡，则晋国一定会南下进攻吴国，忧虑成疾，死了。众官推举孙权的孙子孙皓继位。孙皓荒淫无道，百姓叫苦连天。晋国于是派羊祜南下伐吴，吴国派出陆抗应敌。但是呢，羊祜和陆抗两个奇葩，互相相敬如宾，礼尚往来，大家谁也不进攻，就这样相安无事。\n后来，羊祜都老了，回洛阳见司马炎，跟司马炎说一定要趁孙皓现在荒淫无道，失道寡助的时候，进攻吴国，错过这个机会，等吴国回过神来的时候就晚了。司马炎这才醒悟过来，让羊祜推荐人选，羊祜就推荐了杜预。于是司马炎派杜预出战，杜预率领三军，所向披靡，最后占领东吴。孙皓为了保命，像刘禅一样，投降晋国。自此吴国灭亡，三家归晋！\n有趣的是，魏蜀吴三家最后的皇帝曹奂、刘禅和孙皓，都被司马家族撸到洛阳，最后都善终了。\n","permalink":"http://localhost:1313/posts/2019-07-20-summary-of-the-romance-of-the-three-kingdoms-91-120/","summary":"\u003ch1 id=\"第九十回-驱巨兽六破蛮兵-烧藤甲七擒孟获\"\u003e第九十回 驱巨兽六破蛮兵 烧藤甲七擒孟获\u003c/h1\u003e\n\u003cp\u003e孟获第五次失败后，去拉外援了。第六次叫来了另一个洞主木鹿大王，能呼风唤雨，能指挥虎狼豺豹。被诸葛亮用木制巨兽打败。第七次请来了乌戈国帮忙，乌戈国人都穿藤甲能防毒水，于是诸葛亮设计火攻，大败乌戈国人。\u003c/p\u003e\n\u003cp\u003e就这样总计七擒七纵，最后孟获心服口服，归顺诸葛亮。\u003c/p\u003e\n\u003ch1 id=\"第九十一回-祭泸水汉相班师-伐中原武侯上表\"\u003e第九十一回 祭泸水汉相班师 伐中原武侯上表\u003c/h1\u003e\n\u003cp\u003e诸葛亮平定南蛮之地后，准备班师回朝，路过泸水，被水中孤魂野鬼阻挡，不能前进。后来得知要用七七四十九个人头祭奠才能过河。诸葛亮仁慈之人，肯定不会杀人祭河，于是杀了很多牛羊，做成人头的样子，叫做“馒头”，祭奠泸水，这就是馒头的由来。最后孤魂野鬼散去，诸葛亮军队顺利回到成都。\u003c/p\u003e\n\u003cp\u003e此时，北方魏国皇帝曹丕病故，享年才40岁，在位仅7年。皇位传给曹丕儿子曹睿。正好雍州、凉州没有太守，司马懿毛遂自荐去当这两个州的太守。诸葛亮利用反间计，散布司马懿要篡位的谣言，离间曹睿和司马懿，最终曹睿罢免司马懿，把司马懿赶回老家种田。\u003c/p\u003e\n\u003cp\u003e另一方面，诸葛亮听说曹丕驾崩，幼子曹睿继位，打算北伐魏国。《出师表》就是这个时候写的。于是诸葛亮带着包括赵云在内的一路人马开始北伐。魏国那边就是夏侯渊的儿子夏侯楙前来迎敌。\u003c/p\u003e\n\u003ch1 id=\"第九十二回-赵子龙力斩五将-诸葛亮智取三城\"\u003e第九十二回 赵子龙力斩五将 诸葛亮智取三城\u003c/h1\u003e\n\u003cp\u003e诸葛亮带着赵云等大将来战夏侯楙，夏侯楙那边派出西凉大将韩德及其四个儿子出战，父子一共五个人都被赵云杀了。夏侯楙大败，退守南安城，赵云等三路军马围攻南安城。夏侯楙向周边的天水、安定两个城求解。被诸葛亮使用“诈称魏将”的计策破了南安和安定两个城，捉拿了夏侯楙。只有天水城没有被攻破，而且天水城的谋士姜维（字伯约）识破了诸葛亮的计谋。\u003c/p\u003e\n\u003ch1 id=\"第九十三回-姜伯约归降孔明-武乡侯骂死王朗\"\u003e第九十三回 姜伯约归降孔明 武乡侯骂死王朗\u003c/h1\u003e\n\u003cp\u003e接着，诸葛亮带兵来攻打天水城，诸葛亮不知道天水城有姜维这么个文武双全的人才，低估了拿下天水城的难度。同时，姜维也巧妙的使用了兵法，所以在最初的几次战役中，诸葛亮大败。后来诸葛亮摸清了姜维的情况，精心安排了这样一个进攻策略：引一军打姜维老母亲所在地冀县；引一军打囤积粮草之地上邽；引一军打天水城。这个安排一方面把姜维引入到冀县，另一方面分散了天水城的兵力，导致天水城内部大乱。同时，诸葛亮放走夏侯楙，让他去招安姜维；还让人假扮姜维，制造各种姜维已经投降蜀汉的迹象，离间姜维和魏国。最后假戏真做，姜维无路可走，真的投降了蜀汉。在姜维的帮助下，天水城也拿下了。\u003c/p\u003e\n\u003cp\u003e诸葛亮拿下汉中三城之后，一路北上，前出祁山，兵临渭水。魏国皇帝曹睿听说后大惊，派出以曹真为大都督，郭淮为副都督，司徒王朗为军师的大军来战诸葛亮。两阵对圆，王朗和诸葛亮出阵，两个人对骂，王朗先出招，骂诸葛亮反贼，不顺应天命；诸葛亮出招，骂王朗汉朝旧臣，都76岁高龄了还出来招摇，助纣为虐，死后有何颜面见二十四帝。最后王朗被诸葛亮骂死。\u003c/p\u003e\n\u003cp\u003e这天晚上，曹真算准诸葛亮会乘丧劫寨，诸葛亮也将计就计去劫寨，但是还是诸葛亮老谋深算，劫寨之战以诸葛亮大获全胜。\u003c/p\u003e\n\u003ch1 id=\"第九十四回-诸葛亮乘雪破羌兵-司马懿克日擒孟达\"\u003e第九十四回 诸葛亮乘雪破羌兵 司马懿克日擒孟达\u003c/h1\u003e\n\u003cp\u003e曹真部队失败之后，向西羌求救。西羌自曹操时代开始向曹操进贡，这次听闻魏国求救，于是答应了，派了丞相雅丹和武将越吉共15万大军去偷袭蜀国大本营。诸葛亮听说后，只能分出一部分兵力去抵抗西羌。派出去的关兴和张苞都被勇猛的西羌兵打败了。没办法，诸葛亮留赵云守祁山，自己亲自去西羌前线，巧妙的利用积雪，大破羌兵，杀了越吉，俘虏雅丹。诸葛亮放雅丹回国，西羌事平。\u003c/p\u003e\n\u003cp\u003e另一方面，曹真听说诸葛亮退了一部分兵，于是趁机出兵攻打祁山。没想到被镇守的赵云和魏延大败。没办法，曹真求救曹睿皇帝。与此同时，之前关羽败走麦城向孟达求救，孟达没救并投降了魏国。这个时候，孟达又想回到蜀国的怀抱了，诸葛亮同意，让孟达攻打洛阳，诸葛亮攻打长安。\u003c/p\u003e\n\u003cp\u003e曹睿如临大敌，太傅钟繇推荐把之前罢免的司马懿请回来。司马懿的军事才能应该不下于诸葛亮，且两个儿子（长子司马师、次子司马昭）素有大志，通晓兵法。司马懿复出的第一战就是大战孟达。结果司马懿获胜，孟达被杀。接着，曹睿正式派司马懿出关破蜀，司马懿带着大将张郃，浩浩荡荡，前来抗诸葛亮。\u003c/p\u003e\n\u003ch1 id=\"第九十五回-马谡拒谏失街亭-武侯弹琴退仲达\"\u003e第九十五回 马谡拒谏失街亭 武侯弹琴退仲达\u003c/h1\u003e\n\u003cp\u003e诸葛亮算准了司马懿会首先进攻街亭，因为街亭是汉中的咽喉要道，一旦街亭失守，汉中就很危险了。马谡主动请缨去守街亭，于是诸葛亮派马谡主守街亭，同时派魏延、王平等驻扎在街亭附近支援马谡。\u003c/p\u003e\n\u003cp\u003e马谡来到街亭之后，觉得这么个小地方，不足为惧，也不听王平的劝诫，非要在山上驻扎。司马懿来了之后，先断了山上游的水源，把山团团围住。马谡军中自乱，最后抵挡不住司马懿的大军，败下阵来。尽管有王平、魏延等的支援，依然不敌司马懿的大军。\u003c/p\u003e\n\u003cp\u003e诸葛亮听说马谡大败之后，知道事情很危急，准备从祁山退军回到汉中。同时，为了不制造很大的动静，不让魏军后方的曹真等乘虚而入，诸葛亮各种算计，把关兴、张苞等身边的武将都派出去了。诸葛亮自己带着一帮文官守在西城。司马懿也算到了诸葛亮在西城，于是引大军来打西城。诸葛亮因为实在没有兵了，使出了空城计，把城门大开，军旗尽去，只安排几个士兵扮装老百姓的样子在城门口扫地。司马懿看到后，疑神疑鬼，因为知道诸葛亮向来深思熟虑，这次突然把城门大开，肯定有很大的埋伏，于是退兵了。诸葛亮的空城计成功了。\u003c/p\u003e\n\u003cp\u003e在整个混乱的过程中，诸葛亮和司马懿互相算计，总的来说，互有胜负吧。结果就是诸葛亮暂停北伐脚步，退出祁山，回到汉中。司马懿收复陇西诸郡。\u003c/p\u003e\n\u003ch1 id=\"第九十六回-孔明挥泪斩马谡-周鲂断发赚曹休\"\u003e第九十六回 孔明挥泪斩马谡 周鲂断发赚曹休\u003c/h1\u003e\n\u003cp\u003e却说诸葛亮回到汉中之后，清点人马，因马谡之前有军令状，且是因为他不听王平劝诫才导致蜀军大败的，根据军法，挥泪斩马谡。\u003c/p\u003e\n\u003cp\u003e话分两头，东吴的周鲂给曹休发了诈降书，且为了让曹休相信自己是真的投降，还把头发割掉了。最后曹休相信周鲂，曹睿派包括曹休在内的三路大军前去攻打东吴。结果很显然，曹休中计，大败而回，气忧成病，回到洛阳就死了。\u003c/p\u003e\n\u003ch1 id=\"第九十七回-讨魏国武侯再上表-破曹兵姜维诈献书\"\u003e第九十七回 讨魏国武侯再上表 破曹兵姜维诈献书\u003c/h1\u003e\n\u003cp\u003e因东吴帮忙击败了曹休，遂发书给诸葛亮让他再次出兵北伐，诸葛亮给后主写了《后出师表》，表明自己鞠躬尽瘁死而后已的精神，再次北伐。就在这时，传来一个坏消息，大将赵云病故，留下两个儿子赵统和赵广。于是诸葛亮北伐只剩几个老将和小将了：魏延、关兴、张苞、王平等。\u003c/p\u003e\n\u003cp\u003e魏蜀两军在陈仓口（陈仓口再往北就是街亭）交锋，魏国守陈仓的是郝昭，深沟高垒，诸葛亮使出了各种计谋都没能攻破城池。同时，魏国朝廷派出大将军曹真和身强力壮的王双前来接应。诸葛亮派出的两员大将都被王双打败并杀了。\u003c/p\u003e\n\u003cp\u003e最后，姜维使出诈降计，本来想赚曹真的，结果只赚到曹真的一个大将费耀。蜀军大获全胜，虽然没有杀到曹真，杀到费耀也不错。同时曹真损兵折将，蜀军又出祁山（第二次）。\u003c/p\u003e\n\u003ch1 id=\"第九十八回-追汉军王双受诛-袭陈仓武侯取胜\"\u003e第九十八回 追汉军王双受诛 袭陈仓武侯取胜\u003c/h1\u003e\n\u003cp\u003e却说诸葛亮第二次出祁山时，由于粮草只够支撑一个月时间，汉中前线只有陈仓这一个地方方便运粮，而陈仓被魏国的郝昭和王双把守着，所以诸葛亮必须速战速决，否则粮草支撑不住。魏国大都督就想了个办法，让部队假装运粮，吸引蜀军来抢粮食，实际粮车里装的都是干柴硫磺等易燃物，等蜀军来抢的时候，把蜀军烧死，同时突袭蜀军营寨。但是这个计策怎么能难倒诸葛亮，诸葛亮算准了，而且将计就计，结果反而把假扮运粮的魏军干掉了。但是蜀军毕竟是没粮了，不能恋战，而且曹睿此时又派了张郃领军来支援，于是，诸葛亮在赢了这一战的情况下，果断撤军。魏军此时刚刚失败，也不敢追诸葛亮。但是魏延军队在陈仓口防御王双，撤离的话，王双肯定是会和魏延干起来的。于是诸葛亮撤军的时候，偷偷给魏延一个锦囊妙计，大部队假装撤军，让王双来追，魏延和一小支部队等王双追出去之后，把王双营寨端了，王双回寨时，魏延又出其不意把王双干掉了。所以诸葛亮第二次出祁山，虽然没有干掉魏国，但是也小挫了一把魏军，同时安全回到汉中。曹真听说王双被杀，忧成疾病。\u003c/p\u003e\n\u003cp\u003e此时，吴国孙权称帝了，蜀国派人前去祝贺，同时想联合吴国一起伐魏。吴国假装同意，实则想隔岸观火，坐收渔翁之利。\u003c/p\u003e\n\u003cp\u003e又过了一阵子，诸葛亮听到守陈仓的郝昭病危，觉得这是一个进攻陈仓的好机会，于是又率部队去袭击陈仓，没等张郃救兵赶到，诸葛亮大军就把陈仓城攻破了，郝昭本来就病危，被这么一攻，吓死了。等张郃赶到的时候，发现诸葛亮大军把各个路段都占领了，于是退军。随后，诸葛亮又趁胜追击，拿下了散关。这就是诸葛亮第三次出祁山。\u003c/p\u003e\n\u003cp\u003e曹睿听说诸葛亮又来了，吓死了，这个时候曹真病又没好，于是就把曹真的大都督职位交给了司马懿。司马懿出兵来与诸葛亮决战。\u003c/p\u003e\n\u003ch1 id=\"第九十九回-诸葛亮大破魏兵-司马懿入寇西蜀\"\u003e第九十九回 诸葛亮大破魏兵 司马懿入寇西蜀\u003c/h1\u003e\n\u003cp\u003e司马懿带着张郃等人前来与诸葛亮决战，但是前几次战争都失败了，于是司马懿决定深沟高垒，不再出战。此时，诸葛亮使用缓兵之计，每隔几天退几十里，给魏军要撤退的错觉。司马懿知道诸葛亮诡计多端，不敢追杀，但是大将张郃不同意，非要去追杀诸葛亮。司马懿没办法，让张郃先去追杀，自己随后就到。诸葛亮算准了司马懿的安排，调兵遣将，不但抵抗住了追杀，还安排部队袭击了司马懿的营寨。司马懿大败而回，虽然诸葛亮没有占领司马懿的营寨，但是歼敌无数。\u003c/p\u003e\n\u003cp\u003e在这个过程中，虽然蜀军胜利，但是在一次战争中，张苞（张飞的儿子）不小心骑马掉到河里了，把头磕破了，回成都养病，但是后来死了。诸葛亮悲痛欲绝，决定撤军。诸葛亮第三次出祁山也没有显著成果。\u003c/p\u003e\n\u003cp\u003e又过了一段时间，曹真病好了，决定和司马懿率大军进攻蜀国，曹睿同意。于是这两人率40万大军进攻蜀国。诸葛亮算准了接下来一个月都要下雨，蜀国地势又复杂，魏军远道而来，天时地利都不好，所以不用担心。果然这一个月都阴雨连绵，魏军粮草不够，很多士兵都饿死了。没办法，魏军主动撤退。\u003c/p\u003e\n\u003ch1 id=\"第一百回-汉兵劫寨破曹真-武侯斗阵辱仲达\"\u003e第一百回 汉兵劫寨破曹真 武侯斗阵辱仲达\u003c/h1\u003e\n\u003cp\u003e魏军退了之后，诸葛亮并不追杀，而是准备再次出祁山（第四次）。诸葛亮安排兵分两路，一路魏延、张嶷、杜琼、陈式攻打祁山之东箕谷，一路马岱、王平、张冀、马忠攻打祁山之西斜谷，然后会于祁山。司马懿算准了诸葛亮有这个安排，于是也兵分两路，司马懿守箕谷，曹真守斜谷，而且他两还打赌，如果诸葛亮真的来攻打，则司马懿赢，否则曹真赢。\u003c/p\u003e\n\u003cp\u003e攻打箕谷的陈式不相信有司马懿的埋伏，不听劝阻，导致被司马懿打败，损兵折将。事后诸葛亮按军法处置，把陈式杀了。而另一方面，曹真不相信诸葛亮会攻打斜谷，消极守军，于是被马岱那路军马打败，诸葛亮顺利出祁山（第四次）。\u003c/p\u003e\n\u003cp\u003e有一天，情报说曹真卧病不起，诸葛亮于是写了封信送给曹真看，把曹真骂了一顿，曹真看到信后怀恨而死。魏主曹睿命令司马懿出战。于是两军对峙，司马懿和诸葛亮开始比军事才能（斗阵），司马懿大败。诸葛亮收得胜之兵回祁山，永安城李严派来的送粮官苟安来送粮，但是延误了十天。诸葛亮很生气，按军法该杀，但是手下都求救，诸葛亮就给苟安杖打八十大板放回去了。苟安怀恨在心，投降司马懿。司马懿利用苟安，把苟安放回成都，但是要苟安散布谣言，造谣说诸葛亮要篡后主的位。后主就下令让诸葛亮撤军回成都。\u003c/p\u003e\n\u003cp\u003e诸葛亮很伤心，好不容易又出一次祁山，而且战情对自己有利，现在居然被皇上下令撤军，没办法只好撤军了。不过，诸葛亮要杀苟安，后又不杀，杖打八十大板回去，这是很危险的，诸葛亮应该能想到这个小兵会叛变。要不就果断按军法处置杀了，要不就好好安抚。唉，错失良机啊。\u003c/p\u003e\n\u003ch1 id=\"第一百一回-出陇上诸葛妆神-奔剑阁张郃中计\"\u003e第一百一回 出陇上诸葛妆神 奔剑阁张郃中计\u003c/h1\u003e\n\u003cp\u003e诸葛亮回到成都，查明了事情的真相，后主自己都不好意思了，但是苟安已经逃到魏国去了。\u003c/p\u003e\n\u003cp\u003e过了一阵子，诸葛亮又上书要北伐魏国，于是第五次出祁山，兵分两路，王平等四个大将守祁山，诸葛亮带着魏延姜维守卤城。因粮草不够，打算去附近的陇上麦田割点麦子当粮食。司马懿算准了诸葛亮会兵分两路，于是也兵分两路，一路张郃守祁山，司马懿亲自带着人马去陇上。诸葛亮和司马懿在陇上交战，诸葛亮使用“奇门遁甲”之术，“变出”好多个诸葛亮，迷惑司马懿及魏军，然后偷偷安排大部队去陇上割麦。结果，魏军被诸葛亮迷惑得糊里糊涂，还损兵三千，蜀军割麦成功。\u003c/p\u003e","title":"《三国演义》每回内容梗概（91~120）"},{"content":"第六十一回 赵云截江夺阿斗 孙权遗书退老瞒 却说孙权想乘刘备入川的时候，武力讨回荆州，于是想了个法子，派人去荆州找孙权的妹妹（刘备的老婆，孙夫人），说老母亲病危，让她带着阿斗快点回东吴。孙夫人信以为真，火急火燎带着阿斗走了，也没告诉任何人。走的途中，被赵云和张飞追上，夺回了阿斗，孙夫人一个人回了东吴。\n此时，北方的曹操又南下攻打东吴，孙权只能暂时不管荆州，转而抵御曹操。曹操和孙权互相打了几个月，互有胜负，后来春雨连绵，困苦异常，曹操想撤军，又碍于面子。正好孙权写了一封信给曹操，劝曹操快点撤退吧，给了曹操一个台阶下，于是曹操就撤军了。\n第六十二回 取涪关杨高授首 攻雒城黄魏立功 刘备听说孙夫人回东吴了，曹操南下打孙权，曹操和孙权任何一方赢了，都有可能乘势攻打荆州，于是打算回荆州。张松听说之后写信给刘备，叫他当机立断夺取益州，没想到消息泄露，刘璋知道了刘备和张松的计谋，把张松满门抄斩。同时开始对抗刘备。\n庞统给刘备出了一条计策，先取涪城，后取雒城，最后夺成都。在涪城，刘璋手下杨怀、高沛守官，被庞统和刘备杀害，占领了涪城。在雒城，刘璋手下四员大将镇守（刘璝、泠苞、张任、邓贤），刘备手下黄忠和魏延争功，攻打雒城，杀了邓贤，但还是没有拿下雒城。\n第六十三回 诸葛亮痛哭庞统 张翼德义释严颜 庞统和刘备决定再次对雒城发起进攻，但是庞统夜观天象，发现凶兆，诸葛亮也来信说需要提高警惕，可能要出事。庞统的马也发脾气把庞统摔下马来，刘备好心，把自己的马让给庞统骑，刘备骑庞统的马。\n刘备和庞统分兵进攻，庞统进到一个落凤坡的地方，张任发现前面有一个人骑着刘备的马，以为就是刘备，集中放箭，误把庞统杀死了。庞统道号凤雏，正好在落凤坡牺牲。\n刘备和庞统这一仗算是失败收场，退回涪城。刘备写信给诸葛亮求救，诸葛亮安排关羽留守荆州，带着张飞赵云去增援刘备。张飞打先锋，来到巴郡，巴郡太守严颜深沟高垒不出战。最终，张飞用计把严颜骗出城来，俘虏了严颜，但是觉得严颜是条好汉，放了严颜，严颜也归顺了张飞。\n第六十四回 孔明定计捉张任 杨阜借兵破马超 孔明带着张飞和赵云来援助刘备攻打雒城，最后孔明用计活捉并杀死张任，夺取雒城。刘璋见雒城沦陷，派人求解汉中太守张鲁。\n话分两头，马超自从兵败曹操之后，退守羌地，在羌地占领了一大片地方，但是唯独冀城攻打不下。冀城刺史韦康求救夏侯渊也没被曹操同意。于是韦康就投降马超了，但是马超觉得韦康没诚意，把韦康灭门了。韦康手下杨阜先假装投降马超，然后说妻子死了要回老家奔丧，于是溜出冀城，联合几个表兄弟来反攻马超。此时，夏侯渊也得到曹操允许，要打马超。马超就被一堆人围攻，最后惨败，马超妻子和孩子都被杀了。马超最后仅剩下几个人逃到汉中张鲁手下。\n第六十五回 马超大战葭萌关 刘备自领益州牧 却说刘璋求救于张鲁，马超说自己刚来到张鲁手下，打算立个功，就主动请缨去帮刘璋打刘备吧。张鲁同意。\n于是，马超在葭萌关和张飞大战了几百回合，不分胜负。后来，孔明用计离间了马超和张鲁，然后凭三寸不烂之舌说服马超投降刘备。\n刘璋见大势已去，主动打开成都城门，让出益州，投降刘备。自此，刘备占领四川，成为益州牧。至于刘璋，刘备把他赶出四川，派到一个小地方（南郡公安）住了。\n第六十六回 关云长单刀赴会 伏皇后为国捐生 刘备夺取了益州之后，孙权又打算要回荆州。诸葛亮的哥哥诸葛瑾在孙权手下当谋士，于是把诸葛瑾一家老小都软禁了，跟诸葛瑾说诸葛亮要是再不还荆州，就把诸葛瑾一家灭门。于是派诸葛瑾去成都找诸葛亮讨还荆州。诸葛亮和刘备又演了一场戏，诸葛亮大哭，求刘备，刘备最终同意把荆州的三个郡还给孙权。但是实际镇守荆州的是关羽，关羽又耍赖皮，说将在外军令有所不受，不认刘备的书信。这次讨回荆州又失败了。后来鲁肃又想了一点子，请关羽赴鸿门宴，打算把关羽杀了，武力夺取荆州。关羽霸气十足，单刀赴会。在宴席上和手下周仓又演了一出戏。关羽假装喝醉酒了，仅仅握着鲁肃的手，鲁肃手下怕伤了鲁肃，也不敢动手围攻关羽。就这样，关羽实质上拿鲁肃当了人质，直到关羽上船成功逃离鸿门宴。鲁肃的计谋又失败了，不过这关羽也真够抵赖的。\n话分两头，曹操在许昌更加狂妄自大，还打算自立为魏王，谋士荀彧进谏说这样不好，被曹操训了一顿，荀彧忧愤成疾，死了。。。汉献帝和伏皇后也瑟瑟发抖，伏皇后就秘密派人出去给父亲伏完送信，让他铲除曹贼，结果事情泄漏，伏皇后、伏皇后和汉献帝的两个儿子，和伏完一家都被曹操杀死了。然后，曹操还强行把自己的女儿嫁给了汉献帝。\n第六十七回 曹操平定汉中地 张辽威震逍遥津 话说曹操势力越来越大，商议先取汉中，再收吴蜀。于是曹操亲自起大军攻打张鲁，经过几次战役，使了诈降、离间等计谋，最终夺取汉中地盘，原汉中太守张鲁投降曹操，被曹操封为镇南将军。\n此时曹操想得陇望蜀，顺势把蜀国也端了，四川人民听说曹操拿下汉中， 瑟瑟发抖。诸葛亮想到一个计策，曹操大军都在汉中这边，东边合淝等地空虚，可以再次联合东吴，让东吴进攻合淝，则曹操无暇再攻打西川了。于是这一次，刘备真的就把荆州一半地盘还给孙权了，然后和孙权说明了利害关系，孙权和谋士商量了一下，觉得进攻合淝对自己也有利，于是欣然同意了。孙权起大军进攻合淝。\n合淝等地当前由张辽、李典、乐进等人守关。由于敌众我寡，初期的几次战役都失败了，还丢了皖城。在逍遥津一战中，张辽排兵布阵很有讲究，加上使用诈降等计谋，成功重创了孙权部队，此即张辽威震逍遥津。但是孙权也不气馁，不断从东吴大本营调兵来前线。张辽自知敌众我寡，于是也赶紧去汉中找曹操搬救兵。\n第六十八回 甘宁百骑劫魏营 左慈掷杯戏曹操 曹操带大军来救合淝，和孙权在濡须口对峙。孙权手下大将甘宁自告奋勇，趁曹军初到，晚上只带一百个人去曹营劫寨，狠狠的挫了曹操的锐气。但是曹操毕竟势力强大，孙权在某一次战役中被困垓心，差点丧命。互相对峙了几个月，孙权看短期内取胜无望，决定和曹操求和，同时给曹操上贡。曹操同意，于是各自撤军了。\n曹操回到许昌后，建安二十一年夏，正式称王，即魏王。曹操大老婆丁夫人没生儿子；妾刘氏生曹昂，在南征张绣时战死；卞氏生四个儿子：曹丕，曹彰，曹植，曹熊。立长子曹丕为世子。\n曹操称王之后，有一个左慈来到许昌戏弄曹操，制造了各种灵异事件，比如曹操的橘子，拨开之后是空的；变出龙肝；变出牡丹花；变出松江鲈鱼等等。左慈还给曹操敬酒，曹操让左慈先喝，左慈把杯中酒一分为二，自饮一半，另一半给曹操。曹操大怒，左慈将杯变成一只白鸟飞走了。曹操恼羞成怒，但是无论怎样都杀不死左慈。\n第六十九回 卜周易管辂知机 讨汉贼五臣死节 曹操被左慈吓出病来。有人就请来了另一个“算命的”人管辂帮曹操算命，管辂告诉曹操这是幻术不要担心，曹操心宽，没几天病就好了。\n曹操又叫管辂算算东吴和西蜀，管辂说东吴有一员大将要亡，西蜀要出兵进犯汉中。果然没几日，有人来报东吴鲁肃死了，西蜀要出兵进犯曹操。曹操大怒，想要大举进军西蜀，管辂说且慢，来年春天许昌有火灾，于是曹操没进军，只是派几支军队加强西川防守，同时对首都许昌加强巡逻，防止火灾。\n却说许昌有五个汉朝官员，秘密商议，在元宵节放花灯的情况下，起兵讨伐曹贼。于是元宵节那天，许昌出现火灾，京城大乱。幸好管辂帮曹操算过，曹操也早有提防，叛乱被迅速镇压，五个汉臣也牺牲了。\n第七十回 猛张飞智取瓦口隘 老黄忠计夺天荡山 上回说到管辂算到西蜀出兵进犯汉中，即张飞带领部队去夺取瓦口隘（曹操手下张郃守关），张飞使用激将法激怒张郃，两军交战，张郃大败。后又使用前后包围方法，最终大败张郃，占领瓦口隘。\n张郃败走，曹洪又命令张郃去夺取葭萌关。葭萌关守将霍峻和孟达不敌张郃。刘备派手下黄忠和严颜两员老将前去增援。黄忠使用骄兵之计，大胜张郃。张郃败走，退到天荡山，天荡山是夏侯德镇守。黄忠和严颜使用前后包抄的方法，夺取天荡山，杀了夏侯德。张郃大败，又转投定军山夏侯渊处。定军山是汉中囤积粮草的地方，战略意义重大，所以曹操留夏侯渊在此镇守。\n第七十一回 占对山黄忠逸待劳 据汉水赵云寡胜众 张郃败走定军山之后，曹操知事情严重，亲自带兵四十万来战刘备。这个时候，老将黄忠又主动请缨，要求打定军山。诸葛亮拦不住，就派黄忠和法正一起去了，同时安排赵云在后面接应。\n在定军山一战中，黄忠发现定军山的西边有一座山，比定军山还高，而且可以俯瞰定军山内虚实，于是黄忠趁某个夜黑风高的晚上，占领了这座山。夏侯渊气坏了，被迫出战。法正和黄忠，一个在山顶观察敌情，另一个在山腰准备迎战。两队人马的配合，最终大败了夏侯渊，黄忠杀了夏侯渊，曹军大败，定军山失守，曹军移粮草去北山寨中。\n曹操听说自己的心腹大将夏侯渊被杀，很生气，亲自督战来定军山与夏侯渊报仇。在汉水附近，黄忠和赵云商量，黄忠前去烧粮草，赵云在后面接应，如果规定时间黄忠没回来，则赵云去救援。\n黄忠去烧粮草的时候，被曹操大军围困。赵云带队前去救援，赵云一个人冲入重围，如入无人之境，救出黄忠和副将张著。抵抗住了曹操在汉水的进攻。\n第七十二回 诸葛亮智取汉中 曹阿瞒兵退斜谷 刘备军团和曹操军团，以汉水为界，互相进攻。诸葛亮利用曹操多疑的性格，打败曹操。曹操接连丢失汉水，阳平关，退到斜谷界口。期间，曹操还把杨修杀了，因为杨修各种耍小聪明，曹操很讨厌他，在一次“鸡肋”事件中，以扰乱军心为名，杀了杨修。\n第七十三回 玄德进位汉中王 云长攻拔襄阳郡 曹操在汉中和刘备的战争中，接连失败，最后彻底丢掉了汉中。至此，刘备占据四川、汉中和一半的荆州。刘备手下百官和诸葛亮就商议推举刘备为皇帝，刘备哪里肯自立皇帝，连连推辞。但是大家苦苦相劝，最后刘备自立为汉中王。\n曹操听说刘备自立为王之后，非常生气，打算和孙权摒弃前嫌联合起来灭掉刘备。孙权说你家曹仁就在襄阳，你可先进兵攻打荆州关羽，我然后出兵。诸葛亮得知后，吩咐关羽先下手为强，即抢先进攻曹仁，而不等曹仁来打自己。在关羽和曹仁的战斗中，曹仁接连失败，丢了襄阳城，退兵樊城。\n第七十四回 庞令明抬榇决死战 关云长放水淹七军 曹操得知曹仁失败，派七路大军来救樊城。于禁为总指挥，庞德（庞令明）为先锋。因为庞德的哥哥在蜀国当官，庞德之前的老板马超现在也在蜀国，大家都担心庞德这样的身份去打蜀国，有可能也会反叛。庞德为了表明自己誓死效忠魏国的决心，自制一口棺材，奔赴樊城前线。\n在庞德和关羽的对决中，他两大战一百回合，不分胜负。后来庞德假装使用拖刀计，实则放冷箭，射中关羽，马上就要拿下关羽的时候，于禁担心庞德抢了头功，鸣金收兵，同时把七军都移到川口。庞德想要趁关羽受伤进攻关羽时，于禁也不肯，这个于禁真是坏了魏王的好事啊。\n过了几天关羽伤好了，观察敌情，发现于禁七军都屯于串口，这几日又秋雨连绵，于是打算决汉水水淹七军。于禁不听手下谋士的话，按兵不动，最后被关羽水淹七军。关羽活捉于禁和庞德，把于禁囚禁起来了，斩了庞德。\n第七十五回 关云长刮骨疗毒 吕子明白衣渡江 却说关羽灭了于禁七军，接着来围攻樊城，某一天在樊城北门观察敌情的时候，被城楼上的曹仁军放冷箭，射中右臂。箭上有毒，如果不及时医治，关羽右臂就要废掉。当时华佗听说之后，主动来找关羽，帮他治病。治病的方法就是切开右臂，直至看到骨头，然后把骨头上的毒刮下来，也就是刮骨疗毒。整个过程，关羽没说一点疼，还谈笑风生的下着棋。\n曹操听说关羽擒了于禁，又斩了庞德，害怕了，打算双管齐下，一方面派徐晃再去救樊城，另一方面秘密和孙权通信，因为关羽大部队都带去攻打樊城了，希望孙权能率军攻打荆州，让关羽腹背受敌。孙权同意，一方面陆口的吕蒙假装生病了，被孙权调回去江东养病，另一方面，换了一个没有什么才能的陆逊去守陆口。陆逊新官上任，假装对关羽示好，送各种东西麻痹关羽。果然关羽被麻痹了，觉得荆州陆口可以减少防守，把大部分军队都调往樊城。于是，吕蒙把军队假扮成穿白色衣服的商人，过江进攻并夺取了荆州。还剩下两个城，一个是傅士仁镇守的公安，另一个是糜芳镇守的南郡。没多久，傅士仁和糜芳都投降东吴了。至此，东吴全部收回当初借给刘备的荆州之地。\n第七十六回 徐公明大战沔水 关云长败走麦城 另一方面，曹操派遣的徐晃（徐公明）在樊城和关羽大战于沔水，关羽失败。关羽军中听说老家荆州也被东吴夺取了，军心动摇。关羽面临着前有魏军，后有吴军的困难局面。无奈，只有一个小小的麦城还没沦陷，关羽就带着手下仅剩的几百号人退守麦城了。同时，派手下去成都搬救兵。\n关羽失败的直接原因有两个，一个是前线失败，曹操毕竟军事力量强大，前者于禁庞德来打，庞德还把关羽右臂射伤了，后者徐晃和曹操又来，关羽受伤了还不能上战场，只能让义子关平等人上，军事力量消耗太大了；二个是后方沦陷，后方被东吴乘虚而入。所以关羽腹背受敌，很难不失败。\n第七十七回 玉泉山关公显圣 洛阳城曹操感神 却说关羽被围困麦城，见救兵迟迟不来，就带了两百人亲自冲出重围，去四川搬救兵，留下周仓和王甫等一百人守麦城。关羽去西川的路上，被吕蒙算准路线，设下埋伏，最终关羽父子寡不敌众，被吕蒙部下马忠生擒，然后被孙权杀害！周仓和王甫听说这个消息后，跳楼自杀，麦城失守，至此，荆襄之地回归东吴。\n却说关公魂魄不散，在孙权给吕蒙嘉奖赏酒之时，附体吕蒙，痛骂孙权。然后吕蒙莫名其妙就死了。孙权因害怕刘备报仇，把关羽父子首级送给曹操，一方面开始是曹操请孙权帮忙打关羽的，另一方面是想移祸曹操。在洛阳城，曹操看到关羽首级时，关羽显神。曹操既感慨又害怕，厚葬关羽。\n第七十八回 治风疾神医身死 传遗命奸雄数终 刘备听说关羽被孙权杀害，悲痛欲绝，发誓要灭了东吴，在诸葛亮等人的劝阻下，才稍稍平静了下来。\n却说曹操自从葬了关羽之后，心神不宁，老做噩梦，身体每况愈下。有人就说可以请华佗来医治，曹操同意。华佗说曹操得了风寒，必须把头劈开，取出风寒才能好。曹操大怒，以为华佗之前帮关羽刮骨疗毒，和关羽关系好，这次想帮关羽报仇。于是曹操把华佗关入监狱，华佗不久死在了狱中。\n曹操杀了华佗之后，病情加重，经常做噩梦，梦到之前杀过的人。没多久，曹操就病死了，临死之前，立长子曹丕为世子。\n第七十九回 兄逼弟曹植赋诗 侄陷叔刘封伏法 曹丕继位之后，开始铲除兄弟势力。曹丕的母亲卞氏生四个儿子：曹丕，曹彰，曹植，曹熊，曹丕为长子。老二曹彰主动回来奔丧，交出兵权，回自己封地去了，于是幸免于难。老四曹熊听说曹丕要清算了，吓个半死，上吊自杀了。老三曹植比较狂妄，也不回去奔丧，也不交出兵权。于是曹丕派一支军队去曹植那把曹植押回京城，曹丕为了刁难曹植，让曹植在七步之内作一首诗，作出来则免死贬官，作不出来则赐死。曹植于是作《七步诗》，幸免于难。\n却说刘备集团，打算先打东吴，为关羽报仇，然后进攻中原。但是在此之前，打算先除掉刘封和孟达，因为当初关羽败走麦城时，刘封和孟达没有去救关羽。诸葛亮用离间计，先把刘封和孟达分开了，给刘封升官。孟达知道自己马上要被刘备干掉了，连夜投降了魏国。于是刘备命令刘封攻打孟达，曹丕也命令孟达攻打刘封。刘封和孟达大战，刘封大败回成都，被刘备所杀，孟达因投降曹丕捡回一条小命。\n刘备杀了刘封之后，哀痛关羽，生病了。昔日曹操大将夏侯惇也病故了。天下即将大变。\n第八十回 曹丕废帝篡炎刘 汉王正位继大统 没过多久，曹丕及其部下威逼汉献帝禅让，起初汉献帝不愿意也不甘心，但是架不住曹丕的死亡威胁，还是乖乖把皇帝的位置让出来了。曹丕继位，国号大魏，贬汉献帝为山阳公，逐出许昌。曹丕继位之后，担心许昌宫中多妖怪，迁都洛阳。\n诸葛亮等人听说曹丕篡位，打算拥立刘备为汉王，刘备再三推迟，诸葛亮再三劝告。没办法，刘备继位汉帝。至此，魏国和蜀国正式成立。\n第八十一回 急兄仇张飞遇害 雪弟恨先主兴兵 却说刘备想要为关羽报仇，打算起全国之兵，进攻吴国，百官哭劝不止。张飞听到关羽被害的消息后，也非常悲愤，要给二哥报仇。于是刘备和张飞约了个日子，一起起兵进攻吴国。张飞回到自己的地盘后，想让手下做白旗白甲，挂孝伐吴。因急着起兵，要求手下三日之内必须把白旗白甲做好。手下范疆和张达说三天完不成，要宽限几日。张飞大怒，把他两绑在树上鞭打50下。这两个手下寻思，反正三天之内完不成任务也要被杀，还不如先下手为强，杀了张飞。于是，他两乘着张飞晚上喝醉酒深睡的时候，偷偷杀了张飞，带着首级投降东吴了。\n张飞被害的消息传到刘备那里后，刘备又悲痛欲绝，天天哭泣。化悲痛为力量，命令张飞的义子张苞，关羽的义子关兴，护驾，进攻东吴。\n第八十二回 孙权降魏受九锡 先主征吴赏六军 刘备带着几十万大军，浩浩荡荡向东吴进发。东吴听到后很害怕，诸葛瑾自告奋勇去劝刘备，想说服刘备不要打东吴，而是联合起来打曹丕，刘备不为所动。孙权集团遂向曹丕俯首称臣，希望曹丕能帮东吴，但是曹丕也隔岸观火，无动于衷。\n刘备手下张苞、关兴，作为先锋部队，势如破竹，沿路捷报频传，杀了好多东吴的大将。刘备听闻，犒赏六军。\n第八十三回 战猇亭先主得仇人 守江口书生拜大将 刘备军团在猇亭这个地方，取得了重大胜利：一方面关兴杀了潘璋，糜芳和傅士仁怕死，把杀关羽的马忠杀了，投降刘备，刘备哪里肯放过他们，把糜芳和傅士仁杀了，祭奠关羽，至此关羽的仇已报；另一方面，孙权看到吴军节节败退，把杀张飞的范疆、张达两人和张飞首级送给刘备，刘备杀了范疆和张达，祭奠张飞，至此，张飞的仇已报。\n但是，刘备还是不解气，依然向东吴进发。孙权瑟瑟发抖，因东吴之前的军事首领（大都督）周瑜、鲁肃、吕蒙接连去世，现在国难当头，急需一个大都督。当局讨论了好几天，最后立一个书生陆逊为大都督，吴军中多有不服气的。陆逊上任之后，下令深沟高垒，坚守不出，手下军官都觉得陆逊没本事，不敢与刘备打。但是陆逊的策略是刘备率大军而来，锐不可当，不能与之争锋，所以暂避避刘备的锋芒，等刘备过了这阵子，我们再打。\n果然刘备看陆逊不出战，因夏天酷暑难耐，刘备就把大部队移到附近茂密的树林中，靠近小溪安营扎寨，同时把目前排兵布阵的情况回传给诸葛亮商议。\n第八十四回 陆逊营烧七百里 孔明巧布八阵图 陆逊等到刘备在茂密树林中联营扎寨七百余里，知道反攻的机会来了，令部下用火攻。把蜀军所有帐篷都烧了，死伤无数，刘备在一票人马的护送之下，退守白帝城。刘备大败。孙夫人听到部下有人说刘备大败，已经死于军中，遂望西而泣，投江自尽。\n陆逊沿路追击，快要到白帝城时，路过鱼腹浦，感到杀气冲天，误入诸葛亮入川时布下的八阵图，差点出不来。幸好诸葛亮的岳父黄承彦看到后把他救了出来。\n第八十五回 刘先主遗诏托孤儿 诸葛亮安居平五路 话说刘备兵败猇亭彝陵，退守白帝城。没多久就生病了，眼看着病情一天天加重，刘备就把诸葛亮从成都叫到了白帝城，立了遗嘱，交待了后事，把三个儿子托付给诸葛亮，然后就驾崩了。刘备大儿子刘禅即位，称为后主。\n曹丕听说刘备驾崩，决定和周边四个部落联合，加上曹丕本国部队，一共五路大军进攻蜀国。刘禅听说后慌得要死，赶紧请诸葛亮出主意，但请了好几次诸葛亮都不出丞相府，最后刘禅亲自登门拜访，才知道诸葛亮早已有退兵之计了，为了保密才躲在丞相府不出来。唯一担心的是孙权一路军队，诸葛亮就派邓芝出使东吴，想拉拢东吴，联吴抗魏。\n第八十六回 难张温秦宓逞天辩 破曹丕徐盛用火攻 邓芝出使东吴，不辱使命，吴王同意与蜀国联合抗魏，并派使者张温和邓芝一起去蜀国答谢。张温来到蜀国之后，受到后主、诸葛亮的热情款待。张温因此显得傲慢起来，蜀国学士秦宓和张温展开了激烈的辩论，秦宓获胜，灭了张温威风。诸葛亮担心张温感到羞愧不利于蜀吴联合，又派邓芝和张温一起回东吴再次感谢。自此吴蜀联合。\n魏国听说了，怒了，曹丕亲自带兵攻打吴国，吴国手下徐盛大败曹丕，在这一战中还把曹操旧部张辽杀了。同时蜀国也派赵云出兵阳平关，进攻长安。\n却说突然赵云接到诸葛亮通知，南蛮入侵，让赵云赶紧回来南征，让马超坚守阳平关。\n第八十七回 征南寇丞相大兴师 抗天兵蛮王初受执 诸葛亮带着赵云、魏延等大将，起兵五十万，攻打南蛮。\n原蜀国三郡太守雍闿、朱褒、高定在孟获的引诱下反叛蜀国。诸葛亮使用反间计离间了这三人，高定把其余两人都杀了，并归顺诸葛亮。\n诸葛亮继续南下攻打孟获，孟获派三洞元帅分兵三路到来，诸葛亮通过巧妙的调兵遣将，很快制服了这三路兵马。\n接着，孟获亲自出场，也被诸葛亮的排兵布阵生擒了。孟获不服，诸葛亮放孟获回去，孟获说下次要是再失败，就心服。\n第八十八回 渡泸水再缚番王 识诈降三擒孟获 孟获被诸葛亮放回后，决定以泸水为防线，深沟高垒，希望诸葛亮主动退兵。结果诸葛亮侦查出泸水水流较缓的一段，派军偷渡过河，通过反间计，离间孟获和几个洞主，洞主被孟获鞭打，洞主反戈，生擒孟获给诸葛亮。孟获不服，认为是手下反叛导致自己失败的。诸葛亮放走孟获。\n第二次，孟获让亲弟孟优来诸葛亮寨中诈降，被诸葛亮识破，又抓住孟获。孟获又不服，认为是弟弟孟优诈降不当导致自己失败的。诸葛亮又放走孟获。\n第八十九回 武乡侯四番用计 南蛮王五次遭擒 这次孟获躲到某个好朋友的洞中，该洞有两个入口，一南一北，南口很安全，被孟获和洞主堵住了，蜀军无法进入。北口很危险，沿路有4口泉水，但都有毒，而且早上还有瘴气，非常危险。诸葛亮第一次带队从北口进入时，很多士兵都误喝了毒水，后来通过山上的老叟才知道了破解方法。最后诸葛亮带队攻入北口，与此同时，其他洞主有感于诸葛亮的不杀之恩，主动把孟获抓起来给诸葛亮了。孟获觉得这次也是其他洞主反叛导致自己失败的，不服。诸葛亮第五次放走孟获。\n第九十回 驱巨兽六破蛮兵 烧藤甲七擒孟获 孟获第五次失败后，去拉外援了。第六次叫来了另一个洞主木鹿大王，能呼风唤雨，能指挥虎狼豺豹。被诸葛亮用木制巨兽打败。第七次请来了乌戈国帮忙，乌戈国人都穿藤甲能防毒水，于是诸葛亮设计火攻，大败乌戈国人。\n就这样总计七擒七纵，最后孟获心服口服，归顺诸葛亮。\n","permalink":"http://localhost:1313/posts/2019-07-17-summary-of-the-romance-of-the-three-kingdoms-61-90/","summary":"\u003ch1 id=\"第六十一回-赵云截江夺阿斗-孙权遗书退老瞒\"\u003e第六十一回 赵云截江夺阿斗 孙权遗书退老瞒\u003c/h1\u003e\n\u003cp\u003e却说孙权想乘刘备入川的时候，武力讨回荆州，于是想了个法子，派人去荆州找孙权的妹妹（刘备的老婆，孙夫人），说老母亲病危，让她带着阿斗快点回东吴。孙夫人信以为真，火急火燎带着阿斗走了，也没告诉任何人。走的途中，被赵云和张飞追上，夺回了阿斗，孙夫人一个人回了东吴。\u003c/p\u003e\n\u003cp\u003e此时，北方的曹操又南下攻打东吴，孙权只能暂时不管荆州，转而抵御曹操。曹操和孙权互相打了几个月，互有胜负，后来春雨连绵，困苦异常，曹操想撤军，又碍于面子。正好孙权写了一封信给曹操，劝曹操快点撤退吧，给了曹操一个台阶下，于是曹操就撤军了。\u003c/p\u003e\n\u003ch1 id=\"第六十二回-取涪关杨高授首-攻雒城黄魏立功\"\u003e第六十二回 取涪关杨高授首 攻雒城黄魏立功\u003c/h1\u003e\n\u003cp\u003e刘备听说孙夫人回东吴了，曹操南下打孙权，曹操和孙权任何一方赢了，都有可能乘势攻打荆州，于是打算回荆州。张松听说之后写信给刘备，叫他当机立断夺取益州，没想到消息泄露，刘璋知道了刘备和张松的计谋，把张松满门抄斩。同时开始对抗刘备。\u003c/p\u003e\n\u003cp\u003e庞统给刘备出了一条计策，先取涪城，后取雒城，最后夺成都。在涪城，刘璋手下杨怀、高沛守官，被庞统和刘备杀害，占领了涪城。在雒城，刘璋手下四员大将镇守（刘璝、泠苞、张任、邓贤），刘备手下黄忠和魏延争功，攻打雒城，杀了邓贤，但还是没有拿下雒城。\u003c/p\u003e\n\u003ch1 id=\"第六十三回-诸葛亮痛哭庞统-张翼德义释严颜\"\u003e第六十三回 诸葛亮痛哭庞统 张翼德义释严颜\u003c/h1\u003e\n\u003cp\u003e庞统和刘备决定再次对雒城发起进攻，但是庞统夜观天象，发现凶兆，诸葛亮也来信说需要提高警惕，可能要出事。庞统的马也发脾气把庞统摔下马来，刘备好心，把自己的马让给庞统骑，刘备骑庞统的马。\u003c/p\u003e\n\u003cp\u003e刘备和庞统分兵进攻，庞统进到一个落凤坡的地方，张任发现前面有一个人骑着刘备的马，以为就是刘备，集中放箭，误把庞统杀死了。庞统道号凤雏，正好在落凤坡牺牲。\u003c/p\u003e\n\u003cp\u003e刘备和庞统这一仗算是失败收场，退回涪城。刘备写信给诸葛亮求救，诸葛亮安排关羽留守荆州，带着张飞赵云去增援刘备。张飞打先锋，来到巴郡，巴郡太守严颜深沟高垒不出战。最终，张飞用计把严颜骗出城来，俘虏了严颜，但是觉得严颜是条好汉，放了严颜，严颜也归顺了张飞。\u003c/p\u003e\n\u003ch1 id=\"第六十四回-孔明定计捉张任-杨阜借兵破马超\"\u003e第六十四回 孔明定计捉张任 杨阜借兵破马超\u003c/h1\u003e\n\u003cp\u003e孔明带着张飞和赵云来援助刘备攻打雒城，最后孔明用计活捉并杀死张任，夺取雒城。刘璋见雒城沦陷，派人求解汉中太守张鲁。\u003c/p\u003e\n\u003cp\u003e话分两头，马超自从兵败曹操之后，退守羌地，在羌地占领了一大片地方，但是唯独冀城攻打不下。冀城刺史韦康求救夏侯渊也没被曹操同意。于是韦康就投降马超了，但是马超觉得韦康没诚意，把韦康灭门了。韦康手下杨阜先假装投降马超，然后说妻子死了要回老家奔丧，于是溜出冀城，联合几个表兄弟来反攻马超。此时，夏侯渊也得到曹操允许，要打马超。马超就被一堆人围攻，最后惨败，马超妻子和孩子都被杀了。马超最后仅剩下几个人逃到汉中张鲁手下。\u003c/p\u003e\n\u003ch1 id=\"第六十五回-马超大战葭萌关-刘备自领益州牧\"\u003e第六十五回 马超大战葭萌关 刘备自领益州牧\u003c/h1\u003e\n\u003cp\u003e却说刘璋求救于张鲁，马超说自己刚来到张鲁手下，打算立个功，就主动请缨去帮刘璋打刘备吧。张鲁同意。\u003c/p\u003e\n\u003cp\u003e于是，马超在葭萌关和张飞大战了几百回合，不分胜负。后来，孔明用计离间了马超和张鲁，然后凭三寸不烂之舌说服马超投降刘备。\u003c/p\u003e\n\u003cp\u003e刘璋见大势已去，主动打开成都城门，让出益州，投降刘备。自此，刘备占领四川，成为益州牧。至于刘璋，刘备把他赶出四川，派到一个小地方（南郡公安）住了。\u003c/p\u003e\n\u003ch1 id=\"第六十六回-关云长单刀赴会-伏皇后为国捐生\"\u003e第六十六回 关云长单刀赴会 伏皇后为国捐生\u003c/h1\u003e\n\u003cp\u003e刘备夺取了益州之后，孙权又打算要回荆州。诸葛亮的哥哥诸葛瑾在孙权手下当谋士，于是把诸葛瑾一家老小都软禁了，跟诸葛瑾说诸葛亮要是再不还荆州，就把诸葛瑾一家灭门。于是派诸葛瑾去成都找诸葛亮讨还荆州。诸葛亮和刘备又演了一场戏，诸葛亮大哭，求刘备，刘备最终同意把荆州的三个郡还给孙权。但是实际镇守荆州的是关羽，关羽又耍赖皮，说将在外军令有所不受，不认刘备的书信。这次讨回荆州又失败了。后来鲁肃又想了一点子，请关羽赴鸿门宴，打算把关羽杀了，武力夺取荆州。关羽霸气十足，单刀赴会。在宴席上和手下周仓又演了一出戏。关羽假装喝醉酒了，仅仅握着鲁肃的手，鲁肃手下怕伤了鲁肃，也不敢动手围攻关羽。就这样，关羽实质上拿鲁肃当了人质，直到关羽上船成功逃离鸿门宴。鲁肃的计谋又失败了，不过这关羽也真够抵赖的。\u003c/p\u003e\n\u003cp\u003e话分两头，曹操在许昌更加狂妄自大，还打算自立为魏王，谋士荀彧进谏说这样不好，被曹操训了一顿，荀彧忧愤成疾，死了。。。汉献帝和伏皇后也瑟瑟发抖，伏皇后就秘密派人出去给父亲伏完送信，让他铲除曹贼，结果事情泄漏，伏皇后、伏皇后和汉献帝的两个儿子，和伏完一家都被曹操杀死了。然后，曹操还强行把自己的女儿嫁给了汉献帝。\u003c/p\u003e\n\u003ch1 id=\"第六十七回-曹操平定汉中地-张辽威震逍遥津\"\u003e第六十七回 曹操平定汉中地 张辽威震逍遥津\u003c/h1\u003e\n\u003cp\u003e话说曹操势力越来越大，商议先取汉中，再收吴蜀。于是曹操亲自起大军攻打张鲁，经过几次战役，使了诈降、离间等计谋，最终夺取汉中地盘，原汉中太守张鲁投降曹操，被曹操封为镇南将军。\u003c/p\u003e\n\u003cp\u003e此时曹操想得陇望蜀，顺势把蜀国也端了，四川人民听说曹操拿下汉中， 瑟瑟发抖。诸葛亮想到一个计策，曹操大军都在汉中这边，东边合淝等地空虚，可以再次联合东吴，让东吴进攻合淝，则曹操无暇再攻打西川了。于是这一次，刘备真的就把荆州一半地盘还给孙权了，然后和孙权说明了利害关系，孙权和谋士商量了一下，觉得进攻合淝对自己也有利，于是欣然同意了。孙权起大军进攻合淝。\u003c/p\u003e\n\u003cp\u003e合淝等地当前由张辽、李典、乐进等人守关。由于敌众我寡，初期的几次战役都失败了，还丢了皖城。在逍遥津一战中，张辽排兵布阵很有讲究，加上使用诈降等计谋，成功重创了孙权部队，此即张辽威震逍遥津。但是孙权也不气馁，不断从东吴大本营调兵来前线。张辽自知敌众我寡，于是也赶紧去汉中找曹操搬救兵。\u003c/p\u003e\n\u003ch1 id=\"第六十八回-甘宁百骑劫魏营-左慈掷杯戏曹操\"\u003e第六十八回 甘宁百骑劫魏营 左慈掷杯戏曹操\u003c/h1\u003e\n\u003cp\u003e曹操带大军来救合淝，和孙权在濡须口对峙。孙权手下大将甘宁自告奋勇，趁曹军初到，晚上只带一百个人去曹营劫寨，狠狠的挫了曹操的锐气。但是曹操毕竟势力强大，孙权在某一次战役中被困垓心，差点丧命。互相对峙了几个月，孙权看短期内取胜无望，决定和曹操求和，同时给曹操上贡。曹操同意，于是各自撤军了。\u003c/p\u003e\n\u003cp\u003e曹操回到许昌后，建安二十一年夏，正式称王，即魏王。曹操大老婆丁夫人没生儿子；妾刘氏生曹昂，在南征张绣时战死；卞氏生四个儿子：曹丕，曹彰，曹植，曹熊。立长子曹丕为世子。\u003c/p\u003e\n\u003cp\u003e曹操称王之后，有一个左慈来到许昌戏弄曹操，制造了各种灵异事件，比如曹操的橘子，拨开之后是空的；变出龙肝；变出牡丹花；变出松江鲈鱼等等。左慈还给曹操敬酒，曹操让左慈先喝，左慈把杯中酒一分为二，自饮一半，另一半给曹操。曹操大怒，左慈将杯变成一只白鸟飞走了。曹操恼羞成怒，但是无论怎样都杀不死左慈。\u003c/p\u003e\n\u003ch1 id=\"第六十九回-卜周易管辂知机-讨汉贼五臣死节\"\u003e第六十九回 卜周易管辂知机 讨汉贼五臣死节\u003c/h1\u003e\n\u003cp\u003e曹操被左慈吓出病来。有人就请来了另一个“算命的”人管辂帮曹操算命，管辂告诉曹操这是幻术不要担心，曹操心宽，没几天病就好了。\u003c/p\u003e\n\u003cp\u003e曹操又叫管辂算算东吴和西蜀，管辂说东吴有一员大将要亡，西蜀要出兵进犯汉中。果然没几日，有人来报东吴鲁肃死了，西蜀要出兵进犯曹操。曹操大怒，想要大举进军西蜀，管辂说且慢，来年春天许昌有火灾，于是曹操没进军，只是派几支军队加强西川防守，同时对首都许昌加强巡逻，防止火灾。\u003c/p\u003e\n\u003cp\u003e却说许昌有五个汉朝官员，秘密商议，在元宵节放花灯的情况下，起兵讨伐曹贼。于是元宵节那天，许昌出现火灾，京城大乱。幸好管辂帮曹操算过，曹操也早有提防，叛乱被迅速镇压，五个汉臣也牺牲了。\u003c/p\u003e\n\u003ch1 id=\"第七十回-猛张飞智取瓦口隘-老黄忠计夺天荡山\"\u003e第七十回 猛张飞智取瓦口隘 老黄忠计夺天荡山\u003c/h1\u003e\n\u003cp\u003e上回说到管辂算到西蜀出兵进犯汉中，即张飞带领部队去夺取瓦口隘（曹操手下张郃守关），张飞使用激将法激怒张郃，两军交战，张郃大败。后又使用前后包围方法，最终大败张郃，占领瓦口隘。\u003c/p\u003e\n\u003cp\u003e张郃败走，曹洪又命令张郃去夺取葭萌关。葭萌关守将霍峻和孟达不敌张郃。刘备派手下黄忠和严颜两员老将前去增援。黄忠使用骄兵之计，大胜张郃。张郃败走，退到天荡山，天荡山是夏侯德镇守。黄忠和严颜使用前后包抄的方法，夺取天荡山，杀了夏侯德。张郃大败，又转投定军山夏侯渊处。定军山是汉中囤积粮草的地方，战略意义重大，所以曹操留夏侯渊在此镇守。\u003c/p\u003e\n\u003ch1 id=\"第七十一回-占对山黄忠逸待劳-据汉水赵云寡胜众\"\u003e第七十一回 占对山黄忠逸待劳 据汉水赵云寡胜众\u003c/h1\u003e\n\u003cp\u003e张郃败走定军山之后，曹操知事情严重，亲自带兵四十万来战刘备。这个时候，老将黄忠又主动请缨，要求打定军山。诸葛亮拦不住，就派黄忠和法正一起去了，同时安排赵云在后面接应。\u003c/p\u003e\n\u003cp\u003e在定军山一战中，黄忠发现定军山的西边有一座山，比定军山还高，而且可以俯瞰定军山内虚实，于是黄忠趁某个夜黑风高的晚上，占领了这座山。夏侯渊气坏了，被迫出战。法正和黄忠，一个在山顶观察敌情，另一个在山腰准备迎战。两队人马的配合，最终大败了夏侯渊，黄忠杀了夏侯渊，曹军大败，定军山失守，曹军移粮草去北山寨中。\u003c/p\u003e\n\u003cp\u003e曹操听说自己的心腹大将夏侯渊被杀，很生气，亲自督战来定军山与夏侯渊报仇。在汉水附近，黄忠和赵云商量，黄忠前去烧粮草，赵云在后面接应，如果规定时间黄忠没回来，则赵云去救援。\u003c/p\u003e\n\u003cp\u003e黄忠去烧粮草的时候，被曹操大军围困。赵云带队前去救援，赵云一个人冲入重围，如入无人之境，救出黄忠和副将张著。抵抗住了曹操在汉水的进攻。\u003c/p\u003e\n\u003ch1 id=\"第七十二回-诸葛亮智取汉中-曹阿瞒兵退斜谷\"\u003e第七十二回 诸葛亮智取汉中 曹阿瞒兵退斜谷\u003c/h1\u003e\n\u003cp\u003e刘备军团和曹操军团，以汉水为界，互相进攻。诸葛亮利用曹操多疑的性格，打败曹操。曹操接连丢失汉水，阳平关，退到斜谷界口。期间，曹操还把杨修杀了，因为杨修各种耍小聪明，曹操很讨厌他，在一次“鸡肋”事件中，以扰乱军心为名，杀了杨修。\u003c/p\u003e\n\u003ch1 id=\"第七十三回-玄德进位汉中王-云长攻拔襄阳郡\"\u003e第七十三回 玄德进位汉中王 云长攻拔襄阳郡\u003c/h1\u003e\n\u003cp\u003e曹操在汉中和刘备的战争中，接连失败，最后彻底丢掉了汉中。至此，刘备占据四川、汉中和一半的荆州。刘备手下百官和诸葛亮就商议推举刘备为皇帝，刘备哪里肯自立皇帝，连连推辞。但是大家苦苦相劝，最后刘备自立为汉中王。\u003c/p\u003e","title":"《三国演义》每回内容梗概（61~90）"},{"content":"第三十一回 曹操仓亭破本初 玄德荆州依刘表 袁绍手下主要有三股势力：长子袁谭守青州；次子袁熙守幽州；三子袁尚，后妻刘氏所生，绍最爱之，留身边，守冀州；外甥高干守并州。听说袁绍官渡之战败了，都来支援。于是袁绍聚集四州兵马，屯兵仓亭，准备再和曹操干一仗。结果，曹操谋士程昱献十面埋伏之计，大败袁绍，于是袁绍回老巢，转为防守。\n却说刘备势力趁曹操忙于官渡、仓亭之战，偷袭许昌。曹操打败了袁绍之后，赶紧南下收拾刘备。刘备大败，谋士孙乾建议投靠荆州刘表。刘表谋士蔡瑁进谏：不可。刘备先从吕布，后事曹操，近投袁绍，皆不克终，足可见其为人。今若纳之，曹操必加兵于我，枉动干戈。不如斩孙乾之首，以献曹操，操必重待主公也。不过孙乾凭口才和智勇，打动刘表，刘表同意接受刘备。\n第三十二回 夺冀州袁尚争锋 决漳河许攸献计 却说曹操见刘备依附了刘表，打算转而继续攻打袁绍集团。三子袁尚出阵迎敌，大败而走。袁绍听说袁尚大败，旧病复发，吐血斗升而死。临死之前，刘氏问袁尚能继位吗，袁绍点头。\n曹操谋士郭嘉进谏，袁绍立了三子为后，长子袁谭必定会和袁尚争夺继位权，不如先退兵，让他们兄弟两先内斗，等两败俱伤之后，再进军一举歼灭他们。曹操从其言。果然袁谭和袁尚为了继位权干起来了，袁谭败，遂投降曹操。曹操进军攻打冀州城，袁尚败走，谋士审配守冀州城。曹操谋士许攸献计，可决漳河之水淹冀州城。果然冀州城被淹，城里也没粮食，审配大败，曹操占领冀州城。\n第三十三回 曹丕乘乱纳甄氏 郭嘉遗计定辽东 曹操拿下冀州城之后，他的儿子曹丕进城时发现袁熙的老婆甄氏很漂亮，于是没有杀她，把她留下来当老婆了。\n却说袁谭听说曹操打跑了袁尚，竟然想夺回冀州，不听曹操指挥了。于是袁谭和曹操干了一仗，袁谭被杀。高干所在的并州团队也被曹操灭了，高干被杀。袁尚被曹操逼得走投无路，投奔兄长袁熙，两兄弟又逃命到辽西乌桓。\n却说曹操谋士郭嘉，因北伐路途遥远水土不服，生病了，于是曹操就把他留在了易州养病，自己继续北伐追杀袁尚和袁熙。袁氏兄弟被追的又往辽东跑了。但是路上天气寒冷干旱，军队又缺粮。曹操打算暂时回撤，回到易州时，郭嘉已经死了，给曹操留了一个精囊妙计，告诉曹操不要紧逼袁氏兄弟，自然有人提着他两的头来的。果然，辽东太守公孙康担心袁氏兄弟来到辽东之后，鸠占鹊巢，干脆杀了袁氏兄弟，来投降曹操了。自此曹操彻底平了袁绍集团，占领了北方。\n第三十四回 蔡夫人隔屏听密语 刘皇叔跃马过檀溪 却说刘备依附刘表之后，江夏有人造反，刘备主动请缨去平反，平反的过程中缴获一匹的卢马。刘备本来把马送给了刘表，但刘表手下有人谗言说骑着个马妨主，刘表又把这个马还给刘备了。同时，刘表后妻蔡氏不太喜欢刘备，老担心他篡夺荆州，于是刘表暂且把刘备安排到襄阳的新野县了。在新野的时候，甘夫人还替刘备生了刘禅，因甘夫人晚上做梦梦见吞下北斗星而怀孕，给刘禅取小名阿斗。\n后来有一天，刘表请刘备来荆州喝酒，问刘备到底应该立前妻陈氏生的长子刘琦还是后妻蔡氏生的次子刘琼呢。刘备说自古废长立幼，取乱之道，不如慢慢削弱蔡氏的权力，还是要立长子啊。这话正好被蔡夫人偷听到了，于是蔡夫人和外戚蔡瑁怀恨在心，打算除掉刘备势力，以绝后患。\n于是蔡夫人找了个借口，宴请群臣，顺便把刘备也请去了。蔡瑁把吃饭的地方三面围住，只留西门，因为西门口正好有檀溪阻隔。吃饭的时候，蔡瑁把刘备的手下都灌醉了，准备对刘备下手。正好有一个小罗罗给刘备报信，刘备赶紧骑着的卢马往西门跑，的卢马也不是吃素的，纵身一跃居然跳过了檀溪，救了刘备一命。\n第三十五回 玄德南漳逢隐沦 单福新野遇英主 却说刘备越过檀溪之后，一通乱走来到了南漳，遇到了著名的水镜先生司马徽，司马徽告诉刘备，得卧龙、凤雏其中之一，可安天下。但是司马徽自己不肯出山。\n后来赵云他们找到了刘备，一起回了新野。刘备在街上遇到了谋士单福（其实是后面的徐庶）前来投靠。\n却说曹操平了袁绍后，想南下占领荆州，就派曹仁先去新野打刘备了。正好，单福初露头角，献上一计，破了曹仁。\n第三十六回 玄德用计袭樊城 元直走马荐诸葛 后续，刘备用单福的计谋，两破曹仁，同时占领了曹仁的樊城。曹仁败北之后，回见曹操，说刘备肯定有高人相助，后来程昱调查发现这个单福真名其实是徐庶，因之前杀人了，现在隐姓埋名改成单福，程昱说这个人的才能是自己的十倍。但是徐庶特别孝顺，家里只有一个老母亲。曹操为了把徐庶骗到许都，为自己效力，把徐庶的老母亲骗到许都，同时模仿其母的字迹，给徐庶写了一封信，说自己被曹操软禁，希望徐庶能来许昌救自己。因为徐庶很孝顺，所以泪别刘备，刘备也很不舍。徐庶为了感谢刘备，临走之前，给刘备推荐了襄阳隆中的诸葛亮，同时担心诸葛亮不出山帮刘备，还亲自去隆中跟诸葛亮说了刘备这个人。\n第三十七回 司马徽再荐名士 刘玄德三顾草庐 司马徽听说徐庶在刘备这，特来刘备这找徐庶聊天，没曾想徐庶已经去曹操那了。刘备就顺便和司马徽聊起了卧龙，司马徽特别夸赞和推荐了诸葛亮。于是就出现了著名的刘备三顾茅庐请诸葛亮出山的故事。这三顾，第一次只见到门童，诸葛亮不在；第二次只见到诸葛亮的弟弟诸葛均，诸葛亮排行第二，大哥诸葛瑾，在孙权那当谋士。\n第三十八回 定三分隆中决策 战长江孙氏报仇 刘备第三次顾茅庐终于见到了诸葛亮，诸葛亮虽在隆中，但尽知天下事，和刘备指点江山，分析当前天下局势，然后给刘备出了一个大政方针：将军欲成霸业，北让曹操占天时，南让孙权占地利，将军可占人和。先取荆州为家，后即取西川建基业，以成鼎足之势，然后可图中原也。\n话分两头，吴国第三代领导人孙权，为了给父亲孙坚报仇（孙坚在第七回中被刘表手下杀害），进攻江夏，江夏当时是由刘表的部下黄祖镇守。黄祖大败，被杀，孙权拿下江夏。但是孙权手下谋士张昭说江夏是个孤城，不可守，不如回江东，于是孙权报完仇之后就回江东了。\n第三十九回 荆州城公子三求计 博望坡军师初用兵 黄祖被杀，江夏失守，刘表慌了，请刘备共商御敌大计。这个时候，刘表前妻的儿子，也就是大儿子刘琦来找刘备帮忙，说继母容不下自己，自己的处境很危险。刘备说这个事情你可以找诸葛亮来处理。第二天，刘琦找诸葛亮帮忙，诸葛亮只推刘表自己的家事不便插手，推了三次，刘琦也求了三次。最后诸葛亮终于给了刘琦一计，说现在孙权走了之后，江夏急需人防守，你可以请求调去江夏，这样不在刘表和继母的身边，反倒安全。刘琦照做，刘表果然同意。\n却说曹操听说刘备在新野招兵买马，久留必有后患，可早图之。于是曹操派出以夏侯惇为首的十万大军，直抵博望坡，以窥新野。诸葛亮作为刘备的军师，第一次排兵布阵，安排大家用火攻，大败夏侯惇。这是诸葛亮初出茅庐的第一功。\n第四十回 蔡夫人议献荆州 诸葛亮火烧新野 夏侯惇打了败仗，回报曹操，曹操大怒，亲自带领五十万大军南下攻打荆州，孔融进谏曹操说刘表刘备都是汉室宗亲，不可兴无义之师。曹操大怒，把孔融一家老小全杀了。此孔融即孔融让梨的孔融。\n此时刘表病危，商议要立长子刘琦为荆州之主，让刘备辅佐。蔡夫人听说之后，刘表死后，篡改遗嘱，立次子（自己生的儿子）刘琮为荆州之主，而且不发丧。更可恨的是，听说曹操大军压境，刘琮手下和蔡夫人等人商议投降曹操，保全荆州之主的位置。当派使者去给曹操投降时，被刘备手下的人抓到，刘备这才知道刘表已死，刘琮把荆州卖了。这个时候，诸葛亮献上一计，曹操那么多兵来了，新野是守不住了，不如弃新野，走樊城。于是诸葛亮像上次博望坡一样，提前把新野的老百姓撤离，新野成了一个空城，诱敌入内，然后火烧新野。曹军大败。\n第四十一回 刘玄德携民渡江 赵子龙单骑救主 虽然曹军大败，但毕竟人数众多，曹军一路追击，刘备一路逃难，前后去了樊城→襄阳→江陵，即使情况再危险，刘备也不肯放弃新野的老百姓，所以逃难的速度很慢。一路上被曹军冲杀，大家伙都走散了。刘备安排赵云护送甘夫人、糜夫人和阿斗，但是路上二位夫人也走散了，赵云就返回去，冲入敌军，找他们。路上经过长坂坡等地，找到了甘夫人，后来也找到人糜夫人和阿斗，但是糜夫人受伤了，不肯走，怕耽误赵云和阿斗，于是投井自杀了。于是赵云怀抱着阿斗，不断冲杀，突出重围。\n第四十二回 张翼德大闹长坂桥 刘豫州败走汉津口 赵云救下阿斗之后，开始撤退，回到长坂坡，已经筋疲力尽了，这个时候正好张飞在长坂坡等着，于是请张飞支援。张飞站在长坂桥上，后面曹军追上，张飞厉声大喝：燕人张翼德在此！谁敢来决死战？，连叫了好几次，把曹军吓破胆了，曹军灰溜溜逃走了。\n曹军走之后，张飞把长坂桥拆了，曹军知道后，知道刘备那边没什么兵力，之前是被吓唬到了，所以又开始追击刘备。刘备他们就退走到汉津，关羽守夏口，刘备刘琦去江夏，以成掎角之势。\n第四十三回 诸葛亮舌战群儒 鲁子敬力排众议 这个时候，孙权听说刘表已死，刘备新败，想接连刘备共抗曹操。于是派鲁肃假借给刘表吊丧的名义，想接连刘备刘琦。而刘备也担心仅凭自己的力量难以抵御曹操的进攻，想要向孙权借兵，联合孙权一起抵抗曹操。于是派诸葛亮和鲁肃前往吴地柴桑，向孙权搬救兵。\n而孙权的手下又分为两股势力，张昭等大部分认为为了保全吴国，必须投降曹操；而鲁肃认为，投降曹操之后，对其他大臣没有任何影响，他们是什么官还是什么官，但是对孙权就不同了，孙权就要听命于曹操，相当于势力被削弱了，所以他认为孙权应该起兵共同抵抗曹操。\n在孙权处，诸葛亮和孙权的手下众多谋士进行辩论，凭三寸不烂之舌，把一个个谋士都辩论下去了。而且鲁肃也力排众议，建议孙权联合刘备一起抵抗曹操。\n第四十四回 孔明用智激周瑜 孙权决计破曹操 孙权纠结的时候，吴国太提醒说当年孙策临死之前有言：内事不决问张昭，外事不决问周瑜，为什么不问问周瑜的意见呢。于是孙权把在外的周瑜召回来问建议。周瑜到了之后，主降派和主战派轮番来见周瑜，叫周瑜劝孙权降或战。周瑜是打算降的，轮到诸葛亮和鲁肃来说的时候，诸葛亮用了一个计谋来激将周瑜，说曹操来攻打吴国，目的是为了得到大乔和小乔，好锁到铜雀台欢愉。这可把周瑜气炸了，因为大乔是孙策的老婆，小乔是周瑜的老婆。周瑜一听，自己的老婆可能要被曹操抢走，马上改变主意主战了。\n经过周瑜的劝说，孙权终于决定主战，抵抗曹操了。\n第四十五回 三江口曹操折兵 群英会蒋干中计 有一天曹操派人给周瑜送信，但信封上写着“汉大丞相付周都督开拆”，周瑜大怒，撕信斩来使。曹操知道后大怒，起军攻打周瑜。两军交战于三江口，曹操军事实力：刘表旧将蔡瑁和张允部下，以及青州、徐州的部队，但是由于蔡、张部队久不操练，青、徐不习水性，三江口大战，曹操大败。回去之后，曹操命令蔡、张操练水军。\n周瑜得胜登高的时候，发现北边曹操水寨中灯火通明，于是偷偷跑到曹操水寨窥探，发现他们的水军训练营很有章法，得知是荆州刘表部下的蔡瑁和张允指导训练。因为荆州也多是湖泊，蔡瑁和张允就是很好的水军教练。周瑜就想想办法除掉蔡瑁和张允。\n曹操本来三江口输了，今天又被周瑜窥探水寨，很生气。此时，曹操帐下蒋干自告奋勇，说可以去江东劝降周瑜。蒋干来到周瑜处之后，反中了周瑜的计谋。周瑜知道蒋干要来劝降，于是故意和蒋干喝得酩酊大醉，晚上还要和蒋干一起睡觉。周瑜事先在房间里放了一张伪造的书信，伪造蔡瑁和张允私通周瑜。蒋干趁周瑜睡着的时候，看到了这封信，然后偷偷把信偷走告诉曹操。曹操本来就因为三江口失败有点怪罪蔡瑁和张允，现在蒋干又发现了他两私通周瑜的信，一怒之下，杀了蔡瑁和张允，正中了周瑜的计，除掉了这两个懂水军的指导。不过事后曹操发现自己中计了。\n群英会是因为周瑜为了欢迎蒋干来叙旧，安排了很多精英来陪酒，故名群英会。\n第四十六回 用奇计孔明借箭 献密计黄盖受刑 诸葛亮识破了周瑜的计策，周瑜嫉妒诸葛亮，想借机杀掉诸葛亮。一日，周瑜问水战，用什么武器最好，诸葛亮说用箭。周瑜说军中正好缺少箭，限你十日之内造好十万支箭。诸葛亮说不消十日，三日即可。诸葛亮算好了第三天会有大雾，所以将很多草船连起来，逼近曹操水寨，曹操本来疑心重，又是大雾，不敢出兵，只敢向诸葛亮的草船射箭。就这样，诸葛亮轻松收集到了曹操的十万支箭，完成了周瑜的任务。\n却说曹操丢了十多万支箭，很郁闷，荀攸说必须要派人诈降打入周瑜内部。因曹操之前误杀了蔡瑁，于是派蔡瑁的弟弟蔡中、蔡和去周瑜处诈降。周瑜识破但不说破。周瑜和诸葛亮都认为应该用火攻，但是正愁没人去曹操寨中诈降通报消息。此时，黄盖出场，周瑜使出苦肉计，随便找个借口痛打黄盖，整个过程都让蔡中、蔡和知道。以便日后让曹操相信黄盖是真降而不是诈降。\n第四十七回 阚泽密献诈降书 庞统巧授连环计 阚泽和黄盖是老朋友，决定帮黄盖送诈降书，本来曹操已经识破了黄盖的苦肉计，但是凭借阚泽的口才和镇定，曹操相信了黄盖是真投降。\n但是曹操还是心慌啊，这个时候，蒋干又主动请缨，想去周瑜寨中一探虚实。正好，周瑜安排蒋干和庞统相遇，庞统说自己怀才不遇，被周瑜冷落，蒋干顺势把庞统引荐给曹操。庞统给曹操献上连环计，就是把曹操寨中的所有的船都用锁链连起来，这样船不会颠簸，士兵在船上也不会晕船，方便打仗。周瑜+庞统这个连环计其实是为了日后方便火攻曹操。\n第四十八回 宴长江曹操赋诗 锁战船北军用武 曹操没有识破庞统的连环计，为了庆祝得到庞统的计策和黄盖的投降，在长江口大宴群臣，而且把酒吟诗，作《短歌行》（就是对酒当歌，人生几何那个）。\n曹操军队整装待发，全都准备好要大干一场了。因为曹操担心北军不习水性，才把所有大船连起来了，此时，北军两个中将焦触、张南不服气，说自己虽来自北方，但是也能乘船，主动请缨，要乘小船打前锋。曹操同意，他们乘船打前锋，周瑜派出韩当和周泰御敌，曹军大败，焦触和张南被杀。\n第四十九回 七星坛诸葛祭风 三江口周瑜纵火 周瑜得知曹操的船都已经连起来了，请诸葛亮借东风。诸葛亮于是驻坛借东南风，因为曹军在西北方向，吴军在东南方向，所以需要用东南风火攻曹军。\n诸葛亮借到了东风之后，算到了周瑜小心眼要杀自己，赶紧回江夏和刘备回合，并安排张飞、赵云等人劫住曹操退路，同时安排关羽劫华容道。周瑜那边，也是调军有方，全都安排好了，东风起了之后，黄盖乘船带着各种易燃物，赶往曹军诈降。接近曹寨之后，开始放火，曹军大败，各种逃命。\n第五十回 诸葛亮智算华容 关云长义释曹操 曹军被周瑜放火烧杀了一大半军队，剩下的跟着曹操一路逃命。所到之处，均被诸葛亮算中，提前安排了赵云、张飞等在指定地点截杀曹军。最后，诸葛亮算准了曹军会经过华容道，但可能死不了，于是安排关羽在华容道截杀。曹操遇到关羽，说起之前对关羽的恩情，以及之后没有追究他关羽过五关斩六将的事情，关羽是个重情义的人，最后放了曹操。曹操虽逃过一劫，但最后能跟上的军队只剩27个人了。\n曹操兵败之后，灰溜溜回许昌了，安排曹仁、曹洪、夏侯惇等镇守荆州、合肥、襄阳等南郡的地方。\n第五十一回 曹仁大战东吴兵 孔明一气周公瑾 这个时候，刘备想把荆州、襄阳、南郡等地抢到手，但是赤壁之战毕竟是周瑜赢得，周瑜要拿下南郡易如反掌，肯定不会让刘备得了这个便宜。刘备就和周瑜商量，如果周瑜能打下南郡，就归周瑜，否则，刘备去打。于是，周瑜和曹仁干了几仗，期间有一仗曹仁打开了曹操留给他的锦囊妙计，类似于空城计吧，周瑜被骗，中了毒箭。后来周瑜诈死，反击曹仁，曹仁大败，本来南郡就要到手了，但是这个时候，刘备和诸葛亮窃取了周瑜的胜利果实，吩咐关张赵分别夺取了南郡、荆州、襄阳等地。此即孔明一气周瑜。\n第五十二回 诸葛亮智辞鲁肃 赵子龙计取桂阳 刘备诸葛亮抢了周瑜的胜利果实，鲁肃就去刘备那说理，想要要回南郡等地。但是，诸葛亮说南郡等地本来是刘表的，刘备是皇叔，和刘表也是亲戚，当初刘表就说要把荆州等地给刘备，所以这些地方，本不属于吴国，属于刘表的。鲁肃说刘表死了，他的儿子们也不在，应该归吴国。此时，诸葛亮请出事先请来的刘表的儿子刘琦。这个时候，鲁肃没话说了，刘表儿子在这，荆州这地方就暂且继续让刘琦管着吧。但是，鲁肃说如果刘琦死了，荆州必须给吴国，诸葛亮答应。\n此时，伊籍推荐了荆州有名的马氏兄弟，马谡和马良。马良建议刘备，荆州四面受敌，不可久守，可继续南下先后夺取零陵、武陵、桂阳和长沙。于是，张飞赵云拿下零陵，赵云拿下桂阳。\n第五十三回 关云长义释黄汉升 孙仲谋大战张文远 后张飞主动请缨，拿下武陵。关羽听说张飞赵云都立功了，也主动请缨要去攻打长沙。长沙太守韩玄微不足道，但其手下黄忠很厉害。关羽和黄忠大战一百回合，难分胜负。第一仗，黄忠马失前蹄，关羽放过了黄忠，第二仗，黄忠为了感恩，故意没有射杀关羽。韩玄看出黄忠故意没杀关羽，怒了要斩黄忠，此时，魏延觉得韩玄傲慢无才，带了一帮人把韩玄杀了，和黄忠投降了关羽。至此，平了南方4郡。\n话分两头，孙权和在合肥的张辽干起来了，张辽破了孙权的诈降计，歼灭孙权部将太史慈，孙权兵败。\n在此期间，公子刘琦病故，东吴要来讨回荆州了。\n第五十四回 吴国太佛寺看新郎 刘皇叔洞房续佳偶 东吴派鲁肃来讨要荆州，又被诸葛亮三寸不烂之舌赖皮不给，不过立下字据，说等夺取西川之后再还荆州，诸葛亮和鲁肃都签字了。\n又过了一段时间，甘夫人去世，刘备没了夫人。这个时候，周瑜想出一个鬼点子，打算假意把孙权的妹妹嫁给刘备，但要求刘备入赘，这样在刘备入赘的时候，乘机抓住刘备，威胁使其归还荆州。说办就办，东吴派吕范说媒。诸葛亮将计就计，答应了这桩婚事，派孙乾说媒，同时和赵云一起去东吴办婚事。\n刘备在见孙权之前，先去见了乔国老（二乔的父亲），说明了来意。这乔国老马上把事情告诉了吴国太（即孙权的母亲），这下吴国太可闹翻了天，说只有一个女儿，被孙权拿来使美人计，万一真如孙权计划，假结婚，又杀了刘备，岂不是让女儿守望门寡。骂得孙权狗血淋头。这时乔国老在旁边解围，说刘备好歹也是刘皇叔，一表人才，可以先实际考察一下。于是，吴国太就在甘露寺设宴招待刘备。考察的结果是，吴国太很喜欢刘备，于是这门亲事假戏真做，刘备不但安然无恙，反而还娶到一个妙龄少女当老婆。而周瑜的计谋没有得逞，孙权设置的鸿门宴反被吴国太训斥一番，真是“周郎妙计安天下，赔了夫人又折兵”啊。\n第五十五回 玄德智激孙夫人 孔明二气周公瑾 却说周瑜的计谋失败之后，又想出一招，说既然已经假戏真做，不如把刘备软禁在吴国，香车宝马荣华富贵给刘备伺候着，慢慢的离间刘和关张诸葛亮等人的关系，然后逐个击破。说办就办，这刘备还真每天沉迷酒色，不思荆州了。这时，赵云打开了诸葛亮事先给他的第二个锦囊。赵云就跑去见刘备，谎称曹操为报赤壁之战的仇，率精兵攻打荆州，要刘备快点回去。刘备就跑去和孙夫人商量，说想先回趟荆州，这孙夫人可能也真喜欢刘备，说要跟刘备一块走。于是假装元旦的时候，和刘备面北祭祖，趁机一起逃走了。孙权和周瑜知道后，先后派四路人马追赶，都被孙夫人和赵云骂回去了。于是刘备和孙夫人顺利逃离吴国，回到荆州大本营。这事气得周瑜大叫一声，金疮迸裂，倒于船上。此即孔明二气周瑜。\n第五十六回 曹操大宴铜雀台 孔明三气周公瑾 话分两头，赤壁之战失败之后的曹操回到北方老家，正好铜雀台建好了，于是曹操就在铜雀台大宴文武百官，武官比射箭，文官比作诗，好不热闹。\n回到周瑜这里，周瑜让刘备跑了之后，又派鲁肃去找刘备讨要荆州，被刘备痛哭一通。鲁肃也是个仁慈的人，就回去见周瑜了。周瑜又出了个鬼点子，说既然刘备答应打下四川才还荆州，那我们假装帮他出兵打四川，用打下的四川换回荆州，而实际上当经过荆州的时候，乘机把刘备灭了。但是这条计谋被诸葛亮识破，当周瑜部队经过荆州时，发现荆州城都空了，只留下赵云守城，突然间，关羽、张飞、黄忠、魏延等部分从四面八方杀来，周瑜傻眼了，又收到诸葛亮的恐吓羞辱的信，长叹一声“既生瑜，何生亮”，就这样被诸葛亮气死了。\n第五十七回 柴桑口卧龙吊丧 耒阳县凤雏理事 周瑜被诸葛亮气死之后，诸葛亮又假心假意跑去柴桑口给周瑜吊丧。无论是悼词还是哭泣，诸葛亮都表演得非常成功，反而让人觉得周瑜气量狭窄而诸葛亮多情了。\n在柴桑口的时候，诸葛亮遇到了凤雏庞统，诸葛亮对庞统说如果仕途不顺的话，可以来投奔刘备，还给他写了封推荐信。此时的庞统，被鲁肃引荐给了孙权，但是孙权觉得庞统太狂妄了，没用他。鲁肃又把他推荐给了刘备。刘备看庞统这个人面容丑陋，也不喜欢，随便把他打发到小县城耒阳县当县官（此时庞统故意没有拿出诸葛亮和鲁肃的推荐信）。庞统觉得被怠慢了，在耒阳县天天喝酒不理正事。刘备就派张飞去耒阳县调查情况，结果发现庞统半天就把上百天的正事处理完了，才发现庞统是个人才。最后，张飞把庞统带回刘备的身边了。也就是刘备集齐了卧龙、凤雏两大谋士。\n话分两头，曹操得知刘备羽翼丰满，觉得是个威胁，想攻打刘备，又担心西凉军马腾乘虚而入。于是想出一个计谋，假装把马腾封为征南将军，去讨伐孙权，把他召入京城，乘机除掉。马腾没办法，就把大儿子马超留在西凉，带着另外两个小儿子马休、马铁和侄子马岱进京。曹操派手下黄奎接待，没想到黄奎也受了衣带诏要除曹操的，于是黄奎把曹操的计谋告诉马腾，并打算一起密谋除掉曹操。又没想到计谋被黄奎的妾得知，告诉了曹操，结果曹操把黄奎、马腾和他两个小儿子都灭了。马家只剩留守西凉的马超一个人。\n第五十八回 马孟起兴兵雪恨 曹阿瞒割须弃袍 曹操灭了马腾之后，打算南下攻打孙权。孙权求助于刘备，诸葛亮让刘备写信给马超，让马超攻打曹操，报杀父之仇。于是，马超和逃回来的叔叔马岱兴兵攻打曹操。由于马超武功高强，又有杀父之仇在身，士兵士气很足。进攻很猛，势如破竹。曹操前线的部队节节溃败，没办法，曹操只能暂停南下，转而北上抵御马超。但是，曹操一众武将，还是敌不过马超，被马超打得落花流水。马超可能不认识曹操，只知道穿红袍、留长胡子的人是曹操，曹操为了保命，赶紧把红袍也脱了，长胡子也割了。这就是曹阿瞒割须弃袍的故事。\n第五十九回 许褚裸衣斗马超 曹操抹书间韩遂 在马超和曹操的对抗中，有一仗，曹操手下猛将许褚和马超大战一百回合，不分胜负，许褚在兴头上，干脆脱了衣服和马超打，结果还是输给马超了。\n后来，曹操使用反间计，成功离间马超和韩遂，韩遂被马超砍了左手，但是马超最终也输给了曹操，和马岱等人逃去陇西了。\n此时，汉中太守张鲁听说曹操干掉了西凉，担心曹操南下打汉中，商量着先把旁边益州刘璋灭了，然后抵御曹操。\n第六十回 张永年反难杨修 庞士元议取西蜀 益州刘璋的部下张松（字永年），献计说可以去找曹操帮忙打张鲁，这样张鲁自顾不暇，可解益州之危。于是刘璋就派张松去曹操那搬救兵，张松临走之前还秘密带上了益州的地图。不过张松一来长得太难看，二来态度傲慢，曹操不喜欢他。后来张松还和曹操手下的杨修干起来了，开始互喷。总而言之，张松此行不但没有搬来曹操的救兵，反而惹恼了曹操，曹操也准备南下夺取益州。\n张松从许昌回益州的路上，经过了荆州，刘备热情款待了张松几天几夜。张松被刘备的宽厚仁慈感动了，临走之前，把益州的地图给了刘备，而且还劝刘备来夺取益州，自己可以作为内应。张松回去之后，跟刘璋说了曹操的坏话，然后说可以把同是姓刘的刘备请入益州，抵御张鲁和曹操。傻傻的刘璋果然派人去请刘备入川，刘备也在庞统的极力推荐下，决定入川。于是，刘备就入川了，但是留下诸葛亮、关张赵重兵把守大后方荆州。\n","permalink":"http://localhost:1313/posts/2019-07-11-summary-of-the-romance-of-the-three-kingdoms-31-60/","summary":"\u003ch1 id=\"第三十一回-曹操仓亭破本初-玄德荆州依刘表\"\u003e第三十一回 曹操仓亭破本初 玄德荆州依刘表\u003c/h1\u003e\n\u003cp\u003e袁绍手下主要有三股势力：长子袁谭守青州；次子袁熙守幽州；三子袁尚，后妻刘氏所生，绍最爱之，留身边，守冀州；外甥高干守并州。听说袁绍官渡之战败了，都来支援。于是袁绍聚集四州兵马，屯兵仓亭，准备再和曹操干一仗。结果，曹操谋士程昱献十面埋伏之计，大败袁绍，于是袁绍回老巢，转为防守。\u003c/p\u003e\n\u003cp\u003e却说刘备势力趁曹操忙于官渡、仓亭之战，偷袭许昌。曹操打败了袁绍之后，赶紧南下收拾刘备。刘备大败，谋士孙乾建议投靠荆州刘表。刘表谋士蔡瑁进谏：不可。刘备先从吕布，后事曹操，近投袁绍，皆不克终，足可见其为人。今若纳之，曹操必加兵于我，枉动干戈。不如斩孙乾之首，以献曹操，操必重待主公也。不过孙乾凭口才和智勇，打动刘表，刘表同意接受刘备。\u003c/p\u003e\n\u003ch1 id=\"第三十二回-夺冀州袁尚争锋-决漳河许攸献计\"\u003e第三十二回 夺冀州袁尚争锋 决漳河许攸献计\u003c/h1\u003e\n\u003cp\u003e却说曹操见刘备依附了刘表，打算转而继续攻打袁绍集团。三子袁尚出阵迎敌，大败而走。袁绍听说袁尚大败，旧病复发，吐血斗升而死。临死之前，刘氏问袁尚能继位吗，袁绍点头。\u003c/p\u003e\n\u003cp\u003e曹操谋士郭嘉进谏，袁绍立了三子为后，长子袁谭必定会和袁尚争夺继位权，不如先退兵，让他们兄弟两先内斗，等两败俱伤之后，再进军一举歼灭他们。曹操从其言。果然袁谭和袁尚为了继位权干起来了，袁谭败，遂投降曹操。曹操进军攻打冀州城，袁尚败走，谋士审配守冀州城。曹操谋士许攸献计，可决漳河之水淹冀州城。果然冀州城被淹，城里也没粮食，审配大败，曹操占领冀州城。\u003c/p\u003e\n\u003ch1 id=\"第三十三回-曹丕乘乱纳甄氏-郭嘉遗计定辽东\"\u003e第三十三回 曹丕乘乱纳甄氏 郭嘉遗计定辽东\u003c/h1\u003e\n\u003cp\u003e曹操拿下冀州城之后，他的儿子曹丕进城时发现袁熙的老婆甄氏很漂亮，于是没有杀她，把她留下来当老婆了。\u003c/p\u003e\n\u003cp\u003e却说袁谭听说曹操打跑了袁尚，竟然想夺回冀州，不听曹操指挥了。于是袁谭和曹操干了一仗，袁谭被杀。高干所在的并州团队也被曹操灭了，高干被杀。袁尚被曹操逼得走投无路，投奔兄长袁熙，两兄弟又逃命到辽西乌桓。\u003c/p\u003e\n\u003cp\u003e却说曹操谋士郭嘉，因北伐路途遥远水土不服，生病了，于是曹操就把他留在了易州养病，自己继续北伐追杀袁尚和袁熙。袁氏兄弟被追的又往辽东跑了。但是路上天气寒冷干旱，军队又缺粮。曹操打算暂时回撤，回到易州时，郭嘉已经死了，给曹操留了一个精囊妙计，告诉曹操不要紧逼袁氏兄弟，自然有人提着他两的头来的。果然，辽东太守公孙康担心袁氏兄弟来到辽东之后，鸠占鹊巢，干脆杀了袁氏兄弟，来投降曹操了。自此曹操彻底平了袁绍集团，占领了北方。\u003c/p\u003e\n\u003ch1 id=\"第三十四回-蔡夫人隔屏听密语-刘皇叔跃马过檀溪\"\u003e第三十四回 蔡夫人隔屏听密语 刘皇叔跃马过檀溪\u003c/h1\u003e\n\u003cp\u003e却说刘备依附刘表之后，江夏有人造反，刘备主动请缨去平反，平反的过程中缴获一匹的卢马。刘备本来把马送给了刘表，但刘表手下有人谗言说骑着个马妨主，刘表又把这个马还给刘备了。同时，刘表后妻蔡氏不太喜欢刘备，老担心他篡夺荆州，于是刘表暂且把刘备安排到襄阳的新野县了。在新野的时候，甘夫人还替刘备生了刘禅，因甘夫人晚上做梦梦见吞下北斗星而怀孕，给刘禅取小名阿斗。\u003c/p\u003e\n\u003cp\u003e后来有一天，刘表请刘备来荆州喝酒，问刘备到底应该立前妻陈氏生的长子刘琦还是后妻蔡氏生的次子刘琼呢。刘备说自古废长立幼，取乱之道，不如慢慢削弱蔡氏的权力，还是要立长子啊。这话正好被蔡夫人偷听到了，于是蔡夫人和外戚蔡瑁怀恨在心，打算除掉刘备势力，以绝后患。\u003c/p\u003e\n\u003cp\u003e于是蔡夫人找了个借口，宴请群臣，顺便把刘备也请去了。蔡瑁把吃饭的地方三面围住，只留西门，因为西门口正好有檀溪阻隔。吃饭的时候，蔡瑁把刘备的手下都灌醉了，准备对刘备下手。正好有一个小罗罗给刘备报信，刘备赶紧骑着的卢马往西门跑，的卢马也不是吃素的，纵身一跃居然跳过了檀溪，救了刘备一命。\u003c/p\u003e\n\u003ch1 id=\"第三十五回-玄德南漳逢隐沦-单福新野遇英主\"\u003e第三十五回 玄德南漳逢隐沦 单福新野遇英主\u003c/h1\u003e\n\u003cp\u003e却说刘备越过檀溪之后，一通乱走来到了南漳，遇到了著名的水镜先生司马徽，司马徽告诉刘备，得卧龙、凤雏其中之一，可安天下。但是司马徽自己不肯出山。\u003c/p\u003e\n\u003cp\u003e后来赵云他们找到了刘备，一起回了新野。刘备在街上遇到了谋士单福（其实是后面的徐庶）前来投靠。\u003c/p\u003e\n\u003cp\u003e却说曹操平了袁绍后，想南下占领荆州，就派曹仁先去新野打刘备了。正好，单福初露头角，献上一计，破了曹仁。\u003c/p\u003e\n\u003ch1 id=\"第三十六回-玄德用计袭樊城-元直走马荐诸葛\"\u003e第三十六回 玄德用计袭樊城 元直走马荐诸葛\u003c/h1\u003e\n\u003cp\u003e后续，刘备用单福的计谋，两破曹仁，同时占领了曹仁的樊城。曹仁败北之后，回见曹操，说刘备肯定有高人相助，后来程昱调查发现这个单福真名其实是徐庶，因之前杀人了，现在隐姓埋名改成单福，程昱说这个人的才能是自己的十倍。但是徐庶特别孝顺，家里只有一个老母亲。曹操为了把徐庶骗到许都，为自己效力，把徐庶的老母亲骗到许都，同时模仿其母的字迹，给徐庶写了一封信，说自己被曹操软禁，希望徐庶能来许昌救自己。因为徐庶很孝顺，所以泪别刘备，刘备也很不舍。徐庶为了感谢刘备，临走之前，给刘备推荐了襄阳隆中的诸葛亮，同时担心诸葛亮不出山帮刘备，还亲自去隆中跟诸葛亮说了刘备这个人。\u003c/p\u003e\n\u003ch1 id=\"第三十七回-司马徽再荐名士-刘玄德三顾草庐\"\u003e第三十七回 司马徽再荐名士 刘玄德三顾草庐\u003c/h1\u003e\n\u003cp\u003e司马徽听说徐庶在刘备这，特来刘备这找徐庶聊天，没曾想徐庶已经去曹操那了。刘备就顺便和司马徽聊起了卧龙，司马徽特别夸赞和推荐了诸葛亮。于是就出现了著名的刘备三顾茅庐请诸葛亮出山的故事。这三顾，第一次只见到门童，诸葛亮不在；第二次只见到诸葛亮的弟弟诸葛均，诸葛亮排行第二，大哥诸葛瑾，在孙权那当谋士。\u003c/p\u003e\n\u003ch1 id=\"第三十八回-定三分隆中决策-战长江孙氏报仇\"\u003e第三十八回 定三分隆中决策 战长江孙氏报仇\u003c/h1\u003e\n\u003cp\u003e刘备第三次顾茅庐终于见到了诸葛亮，诸葛亮虽在隆中，但尽知天下事，和刘备指点江山，分析当前天下局势，然后给刘备出了一个大政方针：将军欲成霸业，北让曹操占天时，南让孙权占地利，将军可占人和。先取荆州为家，后即取西川建基业，以成鼎足之势，然后可图中原也。\u003c/p\u003e\n\u003cp\u003e话分两头，吴国第三代领导人孙权，为了给父亲孙坚报仇（孙坚在第七回中被刘表手下杀害），进攻江夏，江夏当时是由刘表的部下黄祖镇守。黄祖大败，被杀，孙权拿下江夏。但是孙权手下谋士张昭说江夏是个孤城，不可守，不如回江东，于是孙权报完仇之后就回江东了。\u003c/p\u003e\n\u003ch1 id=\"第三十九回-荆州城公子三求计-博望坡军师初用兵\"\u003e第三十九回 荆州城公子三求计 博望坡军师初用兵\u003c/h1\u003e\n\u003cp\u003e黄祖被杀，江夏失守，刘表慌了，请刘备共商御敌大计。这个时候，刘表前妻的儿子，也就是大儿子刘琦来找刘备帮忙，说继母容不下自己，自己的处境很危险。刘备说这个事情你可以找诸葛亮来处理。第二天，刘琦找诸葛亮帮忙，诸葛亮只推刘表自己的家事不便插手，推了三次，刘琦也求了三次。最后诸葛亮终于给了刘琦一计，说现在孙权走了之后，江夏急需人防守，你可以请求调去江夏，这样不在刘表和继母的身边，反倒安全。刘琦照做，刘表果然同意。\u003c/p\u003e\n\u003cp\u003e却说曹操听说刘备在新野招兵买马，久留必有后患，可早图之。于是曹操派出以夏侯惇为首的十万大军，直抵博望坡，以窥新野。诸葛亮作为刘备的军师，第一次排兵布阵，安排大家用火攻，大败夏侯惇。这是诸葛亮初出茅庐的第一功。\u003c/p\u003e\n\u003ch1 id=\"第四十回-蔡夫人议献荆州-诸葛亮火烧新野\"\u003e第四十回 蔡夫人议献荆州 诸葛亮火烧新野\u003c/h1\u003e\n\u003cp\u003e夏侯惇打了败仗，回报曹操，曹操大怒，亲自带领五十万大军南下攻打荆州，孔融进谏曹操说刘表刘备都是汉室宗亲，不可兴无义之师。曹操大怒，把孔融一家老小全杀了。此孔融即孔融让梨的孔融。\u003c/p\u003e\n\u003cp\u003e此时刘表病危，商议要立长子刘琦为荆州之主，让刘备辅佐。蔡夫人听说之后，刘表死后，篡改遗嘱，立次子（自己生的儿子）刘琮为荆州之主，而且不发丧。更可恨的是，听说曹操大军压境，刘琮手下和蔡夫人等人商议投降曹操，保全荆州之主的位置。当派使者去给曹操投降时，被刘备手下的人抓到，刘备这才知道刘表已死，刘琮把荆州卖了。这个时候，诸葛亮献上一计，曹操那么多兵来了，新野是守不住了，不如弃新野，走樊城。于是诸葛亮像上次博望坡一样，提前把新野的老百姓撤离，新野成了一个空城，诱敌入内，然后火烧新野。曹军大败。\u003c/p\u003e\n\u003ch1 id=\"第四十一回-刘玄德携民渡江-赵子龙单骑救主\"\u003e第四十一回 刘玄德携民渡江 赵子龙单骑救主\u003c/h1\u003e\n\u003cp\u003e虽然曹军大败，但毕竟人数众多，曹军一路追击，刘备一路逃难，前后去了樊城→襄阳→江陵，即使情况再危险，刘备也不肯放弃新野的老百姓，所以逃难的速度很慢。一路上被曹军冲杀，大家伙都走散了。刘备安排赵云护送甘夫人、糜夫人和阿斗，但是路上二位夫人也走散了，赵云就返回去，冲入敌军，找他们。路上经过长坂坡等地，找到了甘夫人，后来也找到人糜夫人和阿斗，但是糜夫人受伤了，不肯走，怕耽误赵云和阿斗，于是投井自杀了。于是赵云怀抱着阿斗，不断冲杀，突出重围。\u003c/p\u003e\n\u003ch1 id=\"第四十二回-张翼德大闹长坂桥-刘豫州败走汉津口\"\u003e第四十二回 张翼德大闹长坂桥 刘豫州败走汉津口\u003c/h1\u003e\n\u003cp\u003e赵云救下阿斗之后，开始撤退，回到长坂坡，已经筋疲力尽了，这个时候正好张飞在长坂坡等着，于是请张飞支援。张飞站在长坂桥上，后面曹军追上，张飞厉声大喝：燕人张翼德在此！谁敢来决死战？，连叫了好几次，把曹军吓破胆了，曹军灰溜溜逃走了。\u003c/p\u003e\n\u003cp\u003e曹军走之后，张飞把长坂桥拆了，曹军知道后，知道刘备那边没什么兵力，之前是被吓唬到了，所以又开始追击刘备。刘备他们就退走到汉津，关羽守夏口，刘备刘琦去江夏，以成掎角之势。\u003c/p\u003e\n\u003ch1 id=\"第四十三回-诸葛亮舌战群儒-鲁子敬力排众议\"\u003e第四十三回 诸葛亮舌战群儒 鲁子敬力排众议\u003c/h1\u003e\n\u003cp\u003e这个时候，孙权听说刘表已死，刘备新败，想接连刘备共抗曹操。于是派鲁肃假借给刘表吊丧的名义，想接连刘备刘琦。而刘备也担心仅凭自己的力量难以抵御曹操的进攻，想要向孙权借兵，联合孙权一起抵抗曹操。于是派诸葛亮和鲁肃前往吴地柴桑，向孙权搬救兵。\u003c/p\u003e\n\u003cp\u003e而孙权的手下又分为两股势力，张昭等大部分认为为了保全吴国，必须投降曹操；而鲁肃认为，投降曹操之后，对其他大臣没有任何影响，他们是什么官还是什么官，但是对孙权就不同了，孙权就要听命于曹操，相当于势力被削弱了，所以他认为孙权应该起兵共同抵抗曹操。\u003c/p\u003e\n\u003cp\u003e在孙权处，诸葛亮和孙权的手下众多谋士进行辩论，凭三寸不烂之舌，把一个个谋士都辩论下去了。而且鲁肃也力排众议，建议孙权联合刘备一起抵抗曹操。\u003c/p\u003e\n\u003ch1 id=\"第四十四回-孔明用智激周瑜-孙权决计破曹操\"\u003e第四十四回 孔明用智激周瑜 孙权决计破曹操\u003c/h1\u003e\n\u003cp\u003e孙权纠结的时候，吴国太提醒说当年孙策临死之前有言：内事不决问张昭，外事不决问周瑜，为什么不问问周瑜的意见呢。于是孙权把在外的周瑜召回来问建议。周瑜到了之后，主降派和主战派轮番来见周瑜，叫周瑜劝孙权降或战。周瑜是打算降的，轮到诸葛亮和鲁肃来说的时候，诸葛亮用了一个计谋来激将周瑜，说曹操来攻打吴国，目的是为了得到大乔和小乔，好锁到铜雀台欢愉。这可把周瑜气炸了，因为大乔是孙策的老婆，小乔是周瑜的老婆。周瑜一听，自己的老婆可能要被曹操抢走，马上改变主意主战了。\u003c/p\u003e","title":"《三国演义》每回内容梗概（31~60）"},{"content":"第一回 宴桃园豪杰三结义 斩黄巾英雄首立功 黄巾起义，刘关张结为兄弟，大小顺序为刘关张，张飞是卖猪肉的，很有钱。刘备：双股剑；关羽：青龙偃月刀；张飞：丈八蛇矛。\n第二回 张翼德怒鞭督邮 何国舅谋诛宦竖 刘关张镇压黄巾起义有功，但只得到一个很小的县令官，与民秋毫无犯，但被上面下来检查的督邮视察时，因没有贿赂督邮，被督邮穿小鞋，张飞怒不可遏，鞭打督邮。\n此时，朝廷内，十常侍专权，把持朝政。汉灵帝驾崩之后，何进拥立刘辩为皇，因为刘辩是灵帝和何进的姐姐的儿子。同时，把董后鸩杀。同时，何进为了除掉十常侍，引董卓入宫。\n第三回 议温明董卓叱丁原 馈金珠李肃说吕布 十常侍压力山大，为了保命，先下手为强，请何皇后把何进单独召进宫，进宫的路上，把何进杀了。宫内大乱，十常侍被何进部下杀掉。同时，董卓入宫，为彰显威严，欲废少帝辩，立刘协为新皇帝，在温明园讨论废立之事时，忠臣丁原挺身反对，董卓仗势欺人，斥责丁原。无奈丁原背后站着义子吕布，董卓奈何不了丁原。\n次日，丁原携吕布向董卓宣战，吕布骁勇善战，无人能破。此时，董卓部下李肃是吕布的老朋友，带着金银珠宝和董卓的赤兔马来劝说吕布，凭着李肃的三寸不烂之舌和吕布的头脑简单，吕布被说服，杀掉义父丁原，同时投奔董卓麾下。\n第四回 废汉帝陈留践位 谋董贼孟德献刀 董卓既得吕布，态度更加强硬，9月，废少帝，立陈留王刘协为新皇帝。把刘辩打入冷宫，某日，刘辩发牢骚写了首诗，董卓抓住机会，赐鸩酒把刘辩和何太后都杀了。\n曹操为了谋杀奸臣董卓，偷偷带着司徒王允的七宝刀，找了一个机会靠近董卓，本来要刺杀董卓，没成想被董卓从衣镜里发现了，曹操马上改口说有一口很好的宝刀，要献给董卓，然后开溜。董卓后来才get到曹操是要来刺杀自己的。\n第五回 发矫诏诸镇应曹公 破关兵三英战吕布 曹操逃出来之后，被董卓通缉，所以只能发布通告，尽书董卓恶行，招兵买马，准备讨伐董卓。各大诸侯太守都来响应，组成了一个十八路诸侯联盟，盟主是原朝廷重臣袁绍。但是袁绍统领诸侯，调度大军的能力不够，各诸侯也各怀鬼胎，没有凝聚力。\n前锋部队孙坚在进军汜水关时被华雄击败，华雄不可一世，在潘凤等大将接连被华雄斩杀之时，关羽主动请缨前去战华雄，在温酒未冷却的极短时间内斩杀华雄，关羽从此名震诸侯。此即温酒斩华（huà）雄的故事。\n董卓折了华雄，起兵二十万，兵分两路，其中一路由董卓亲自带队，和吕布等人，守住虎牢关，就是标题中的关兵中的关。在虎牢关处，吕布骑着赤兔马，不可一世，盟军无人能敌。最后，刘关张三人亲自出马，大战吕布，吕布败走。\n第六回 焚金阙董卓行凶 匿玉玺孙坚背约 吕布新败，董卓引兵回洛阳，迁都长安，同时把洛阳的宫殿烧毁。孙坚飞奔洛阳，救火的同时，在井中发现了传国玉玺，并私藏起来。后来被盟主袁绍发现，孙坚感到被羞耻了，拔寨离洛阳而去。袁绍大怒，写信给荆州刘表，因为孙坚回老家江东——扬州（？）要经过刘表家，所以袁绍写信给刘表，叫他半路截住孙坚。果然在半路上，孙坚和刘表来了一场恶战，亏孙坚部下三员大将程普、黄盖、韩当死救得脱。自此，孙坚和刘表结怨。\n第七回 袁绍磐河战公孙 孙坚跨江击刘表 袁绍屯兵河内，缺少粮草，向冀州太守韩馥借粮，谋士逢纪说大丈夫落到向别人借粮，可耻啊，冀州乃钱粮广盛之地，不如取而代之。袁绍于是和公孙瓒密谋，让公孙瓒出兵冀州，则韩馥必向袁绍求救，袁绍乘虚而入，占领冀州，然后和公孙瓒平分冀州。\n公孙瓒照做，但是当袁绍占领冀州之后，并没有和公孙瓒平分冀州，而是独占了。公孙瓒派弟弟公孙越去袁绍处，想要分点油水，没想到反被袁绍杀害。于是，公孙瓒大怒，举兵攻打袁绍。两军交战于磐河。此战互有胜负，公孙瓒手下赵云出场，和同在公孙瓒手下的刘关张相见，一见如故，分别时泪如雨下。\n袁绍的弟弟袁术，在南阳，听说哥哥新得冀州，想要哥哥赏赐点马匹，袁绍不给，自此兄弟不睦。袁术又向荆州刘表借粮，刘表也不给。袁术怒了，写信给孙坚，说昔日孙坚私藏玉玺回老家的路上，被刘表伏击，今日，我袁术愿与你结盟，攻打刘表。于是孙坚果然起兵，跨过汉水（长江），攻打刘表。没成想中了刘表部下蒯良的计谋，被杀了，可惜啊。孙坚部下黄盖生擒刘表部下黄祖，于是和刘表交换回孙坚尸体。孙坚大儿子孙策，字伯符；二儿子孙权，字仲谋。\n第八回 王司徒巧使连环计 董太师大闹凤仪亭 董卓在长安听说孙坚死了，更加骄奢淫逸。此时，司徒王允为了江山社稷愁死了，王允府上的歌伎貂蝉，特别漂亮，允以亲女待之。貂蝉想帮王允分担忧愁，于是王允想出了一个连环计。因董卓和其干儿子吕布都是有勇无谋，贪财好色之徒，王允先把貂蝉许配给吕布，然后又悄悄把貂蝉送给董卓。貂蝉从中挑不离间，致使父子二人翻脸。\n因为貂蝉已经被送到董卓府上，有一天，吕布趁着董卓和汉献帝聊天，偷偷来到董卓府上，在府上的凤仪亭看到了貂蝉，和貂蝉搂搂抱抱，被董卓赶回来发现了，董卓大闹凤仪亭，追着吕布打。自此父子二人结怨。\n第九回 除暴徒吕布助司徒 犯长安李傕听贾诩 经过了上面的事情，貂蝉劝董卓搬家，于是董卓和貂蝉搬到郿坞去了。司徒王允和吕布想了一个计策，说汉献帝病刚好，想召集文武百官吃个饭，于是派昔日董卓心腹李肃（就是第三回的李肃，因为董卓没有给李肃升官，李肃对董卓也有怨念），去郿坞宣旨。董卓傻乎乎兴高采烈来到宫内，被早就在此埋伏的王允、吕布、李肃等人杀死，同时去郿坞把董卓全家灭口，包括杀了董卓谋士李儒。吕布得到貂蝉。\n董卓手下四员大将李傕、郭汜、张济、樊稠，听说董卓被杀，打算吃个散伙饭，各自逃命。谋士贾诩说，我们都被通缉，既然自首也要死，不如来个你死我活。四人听了之后认为有道理，于是在西凉起兵，杀奔长安。而且他们制定的军事政策是，其中两个人在山外前后诱杀吕布，但又不恋战，另两个人偷偷起兵直接攻打长安城，让吕布和长安城首尾不能接应。此计果然奏效，董卓余党在长安城内为内应，打开城门，四员大将进入城中，烧杀抢掠，把王允也杀了。吕布弃了家小，投袁术去了。\n第十回 勤王室马腾举义 报父仇曹操兴师 李傕、郭汜、张济、樊稠占领宫内之后，骄奢蛮横，残虐百姓。西凉太守马腾和并州刺史韩遂，密谋贼党，但是都失败了。此时，马腾之子马超出场。樊稠因放过同乡人韩遂，被李傕郭汜杀掉。\n因朝廷昏庸无能，青州黄巾起义又起，太傅朱儁推荐派曹操去剿灭黄巾起义，李傕郭汜同意。东郡太守曹操和济北相鲍信一同破贼。\n曹操在兖州，招贤纳士，群贤毕至。文官：荀彧、荀攸，叔侄二人；程昱；郭嘉；刘晔；满宠；吕虔；毛玠。武官：于禁；典韦。自是曹操部下文有谋臣，武有猛将，威镇山东。\n曹操一高兴，打算把琅琊郡的老父亲曹嵩接过来享天伦之乐，于是，曹嵩和弟弟曹德并一家老小准备赶往兖州。途径徐州，徐州太守陶谦，想讨好曹操，大设宴席款待曹嵩等人，并派部下张闿护送曹嵩。没曾想，张闿原是黄巾余党，在陶谦处没有得到重用，今贼心不改，在路上把曹嵩一家老小全杀了。曹操听到消息，大怒，亲自起兵杀奔徐州。\n第十一回 刘皇叔北海救孔融 吕温侯濮阳破曹操 于是，徐州陶谦，向北海孔融求救，正商议间，黄巾余党管亥来北海攻打孔融。幸好孔融平时人品好，善待城外的一个老奶奶，老奶奶听说孔融有难，叫回来省亲的儿子太史慈去救孔融。太史慈虽然很厉害，但毕竟只有一个人，孔融就叫太史慈杀出重围，请刘备来救援。刘备于是向公孙瓒借了赵云，带着关张来救孔融。\n刘备来了之后，先给曹操写了封信，好言相劝，劝和。正好，这个时候，从宫中逃出来的吕布，攻陷了曹操的老巢兖州和濮阳，于是曹操送刘备一个人情，撤兵回老巢了。\n曹操经过商议之后，准备亲自领兵，去夺回濮阳，在濮阳和吕布进行了恶战，战败，差点被吕布围剿。\n第十二回 陶恭祖三让徐州 曹孟德大战吕布 吕布的谋士陈宫，诡计多端，出了一个点子，诱使曹操进濮阳城，待曹操进城之后，关门放火，差点把曹操灭了，众将死救得脱。\n陶谦在徐州，已经63岁了，儿子又无才，于是，想把徐州送给刘备接管，刘备死活不肯要。就这样来来回回三次，陶谦都要死了，刘备才肯接管徐州。\n曹操自从被吕布打了个败仗，回老家待着。谋士荀彧说，现在收成不好，可以去陈地、汝南、颍川抢占地盘，这些地方都是黄巾余党，乌合之众，轻易可破，又可得粮草。曹操听之，果然占领了这些地方，顺带还收了一员武将许褚。\n某天听说兖州吕布手下大将薛兰、李封都出去劫掠了，可以乘虚而入，夺回兖州。曹操听之，果然夺回兖州，同时六员大将齐战吕布，吕布败走。\n第十三回 李傕郭汜大交兵 杨奉董承双救驾 吕布败走之后，来徐州投奔刘备，屯兵小沛。\n却说李傕郭汜在宫廷横行无忌，太尉杨彪和大司农朱儁密谋诛杀李傕郭汜。杨彪献一反间计：郭汜的妻子妒忌心很强，可派人秘密告诉郭汜妻子，郭汜在和李傕夫人偷情。反间计成功，李傕郭汜反目成仇，李傕劫了天子，郭汜劫了文武百官，每日厮杀。\n因李傕喜欢旁门左道等妖术，谋士贾诩屡谏不止，有怨言。李傕赏罚不分，部下杨奉也有怨言，起兵倒戈，李傕军势渐微。正好张济要来讲和，请天子移驾弘农，李傕卖个人情，开始移驾弘农。路上遇到杨奉劫驾，同时国舅董承也来帮助杨奉，劫驾成功。此即杨奉董承双救驾。\n败走的李傕后来又和郭汜合兵一处，对这两个人无语了。\n第十四回 曹孟德移驾幸许都 吕奉先乘夜袭徐郡 皇帝到了洛阳之后，看到昔日的首都已经变成了断壁残垣，满目疮痍，感叹国运之衰。改年号为建安。此时，李傕郭汜又跑来进犯，太尉杨彪建议把山东的曹操召过来护驾。曹操大喜，起大军来护驾，干翻了李傕郭汜。杨奉、韩暹知道既然曹操来了，肯定容不下自己了，于是借口追击李傕郭汜，引兵囤大梁去了。\n此时，曹操谋士董昭建议，既然已经护驾成功，洛阳一片荒废，可以移驾许都，曹操同意。曹操经过护驾移驾等一系列事迹之后，在朝廷（中原）奠定了地位，手下文武百官很多。文官：荀彧、荀攸、郭嘉、刘晔、曹掾、毛玠、任峻。武官：程昱、范成、董昭、满宠、夏侯惇、夏侯渊、曹仁、曹洪、吕虔、李典、乐进、于禁、徐晃、许褚、典韦。自此，朝廷大事，先禀曹操，后奏天子。这大概就是曹操挟天子以令诸侯的开始吧。\n却说曹操既定大事，担心吕布和刘备联合，恐成心腹大患，于是想离间吕布和刘备，但是失败了。于是离间刘备和袁术，要刘备攻打袁术。刘备没办法，只能起兵攻打袁术，留张飞守徐州。张飞喝酒之后，脾气暴躁，硬要部下曹豹也喝酒，曹豹不喝酒，而且说看在我女婿吕布的面上，不要为难自己。张飞本来就不喜欢吕布，听到曹豹把吕布搬出来，很生气，就下令鞭打曹豹。曹豹很生气，写信给在小沛的吕布，说今晚张飞他们都喝醉了，可以来攻城。吕布果然晚上乘着张飞他们喝醉了，来攻城，而且成功了。张飞败走，逃到刘备和袁术对战的前线盱眙。\n第十五回 太史慈酣斗小霸王 孙伯符大战严白虎 袁术见吕布夺得徐州，叫吕布助攻自己攻打刘备，事成之后，给很多钱粮，吕布照做，但是袁术却没给钱粮，吕布大怒，想攻打袁术，谋士陈宫进谏，袁术兵多粮广，不可攻打，可请刘备回徐州，住小沛。刘备照做，相当于刘备和吕布的位置互换了。\n却说自从孙策父亲孙坚被杀之后，孙策就投奔袁术了，虽然立了不少功，但还是闷闷不乐，想重振父业，于是想借袁术的兵东征刘繇，为了借兵，把孙坚留下来的玉玺都先压在袁术那了，袁术也同意借兵了。\n于是孙策东征刘繇，一路上遇到各种文臣武将，其中就有周瑜。此时，太史慈在刘繇的部下，和孙策大战。孙策武功很高啊，一路上挟死一将，喝死一将，被称为“小霸王”。刘繇军大败，投豫章刘表去了。太史慈后来也归顺孙策了。\n当时东吴归一个叫严白虎的人管，孙策领兵准备顺带把东吴这块地也抢下来。严白虎和会稽太守王朗一起抵抗孙策。但是孙策太厉害了，打败严白虎和王朗，余姚人董袭杀了严白虎来投孙策，王朗去海隅了。自此，江南皆平，孙策奠定了在江南的霸主地位，同时也吸纳了很多文臣武将。\n第十六回 吕奉先射戟辕门 曹孟德败师育水 孙策既然在江南站稳脚跟，想要袁术还自己玉玺，袁术大怒，不给，反而想攻打孙策。谋士杨大将说不可，倒是可以攻打位于小沛的刘备，拿下刘备，再拿下徐州的吕布。于是，袁术进兵刘备，同时想让吕布夹攻刘备，吕布觉得刘备是好人，帮过自己，而且袁术之前答应给吕布的粮草都没给，现在又要帮他打刘备，吕布不肯。于是去前线给袁术和刘备讲和，说如果我能射中辕门外的画戟，你们各自停战收兵，刘备军事实力弱，自然答应，袁术手下纪灵觉得这么远，肯定射不中，所以也答应。没想到吕布弦无虚发，射中了。纪灵只好退兵，于是吕布相当于帮了刘备很大一个忙。\n搞笑的是，刘备在小沛招兵买马时，张飞抢了吕布手下的马匹，吕布大怒，来攻打刘备。刘备没办法，只能弃小沛，投许都曹操。吕布让手下高顺守小沛。\n刘备投曹操之后，曹操给刘备申请了一个豫州牧官位。同时，曹操听说张济张绣来犯首都，曹操大怒，兴兵讨伐张绣。张绣谋士贾诩说曹操势不可挡，不如投降，于是张绣真的就投降曹操了，每天曹操和张绣喝大酒。有一天，曹操发现张济的妻子很漂亮，于是曹操把张济妻子睡了。张绣知道之后，大怒，密谋暗算曹操，趁晚上，张绣部下胡车儿偷了典韦的武器双铁戟，同时大败曹操军队，还把曹操的长子曹昂，曹操的爱将典韦杀了，此即曹操因贪恋美色，败师育水。后来曹操缓过劲来，反败为胜，大败张绣，张绣败走，投奔刘表。\n第十七回 袁公路大起七军 曹孟德会合三将 袁公路，即袁术。却说袁术在淮南，地广粮多，又有孙策抵押的传国玉玺，于是称帝了。同时，催吕布把他的女儿送给袁术的儿子当老婆。但是听说吕布反而把使者送到许都，被曹操杀了，很生气，起七路大军杀奔徐州。吕布谋士陈登献计，破了袁术大军，袁术败回淮南。\n却说这个时候袁术想向孙策借粮，但是孙策因为袁术没有还自己玉玺，怀恨在心，不给，同时上表曹操，劝他南征，共击袁术。袁术因为缺粮，劫掠陈留。曹操也想乘虚攻打袁术，于是联合孙策、刘备和吕布，攻打袁术，此即会合三将。袁术败，丢了寿春城。曹操平了寿春，安排刘备又回到徐州旁边的小沛，叫刘备和吕布不要再闹矛盾了，和睦相处。真实意图是想一起除掉吕布，同时也和吕布的手下陈珪陈登父子私通好了。\n这个时候，张绣接连刘表，又开始兴风作浪了，曹操又来讨伐张绣。\n第十八回 贾文和料敌决胜 夏侯惇拔矢啖睛 张绣谋士贾诩神机妙算，大破曹操军队，曹操回府。\n一日，吕布谋士陈宫在小沛抓到一个使者，搜身发现此人给刘备向曹操送信，密谋杀吕布。吕布知道之后，围攻刘备，刘备赶紧求救于曹操。曹操派夏侯惇、夏侯渊、吕虔、李典等领兵五万来助刘备。夏侯惇和吕布手下高顺对阵时，被高顺手下曹性暗算，射中左眼，夏侯惇拔箭时，把眼珠子带出来了，他认为父精母血，不可弃也，把眼珠子吃了，此即夏侯惇拔矢啖睛。\n第十九回 下邳城曹操鏖兵 白门楼吕布殒命 却说吕布带着张辽、高顺围攻刘关张，形势危急，刘关张只能弃了小沛，开始逃难到许都。曹操率领大军进攻徐州。吕布手下陈登献计（陈珪陈登父子其实暗地里已经归顺曹操了）：徐州被围，下邳有粮可救，不如把家小和钱粮移到下邳。吕布从之。在吕布和曹军厮杀的时候，原吕布谋士糜竺（其实是最开始徐州陶谦的谋士），偷偷夺取徐州大权，不让吕布回徐州城了。吕布没办法，只能去下邳。\n曹操军队又围着下邳城一顿乱打，但是两个月都没攻下。这时，曹操谋士郭嘉和荀彧献计：决沂、泗之水，淹下邳城。曹操从之。另一方面，吕布自己作死，不善待部下将领，只听信两个老婆。搞得手下宋宪、魏续、侯成等人倒戈，偷赤兔马、打开城门、投画戟等，把吕布给卖了。而且还乘吕布睡觉的时候，把吕布绑起来了。吕布被擒，下邳城攻破。吕布和谋士陈宫都被曹操下令杀害。一代枭雄吕布，就此殒命。\n白门楼是下邳城的南大门，因此城门楼使用白色建筑物建造，故得名“白门楼“。\n第二十回 曹阿瞒许田打围 董国舅内阁受诏 曹操灭了吕布势力之后，占领徐州，令车骑将军车胄当徐州首领，带着刘备等人回许都。汉献帝见了刘备，论起辈分，发现刘备是自己叔叔，自此刘备被称为刘皇叔。曹操的谋士都劝曹操乘刚立功拿下徐州，乘机称帝得了，曹操说朝廷老臣众多，未可轻动，不过倒是可以请皇帝出城打猎，借机观察一下皇帝的动向。于是曹操就和皇帝一起在许田打猎，皇帝射一只鹿，连射三箭不中，于是叫曹操试射。曹操向皇帝讨要了皇帝专用的宝雕弓、金鈚箭，一箭射中大鹿。群臣远远的只看见是金鈚箭射中了，以为是皇帝射中的，大呼“万岁”。曹操也正好乘机一马当先，挡在皇帝的前面，接受群臣的祝贺，相当于变相当皇帝。\n皇帝受了这等屈辱，敢怒不敢言，伏皇后父亲伏完推荐国舅董承来帮忙缉拿奸臣曹操。但是朝廷之中，曹操的耳目众多，为了不被曹操发现，皇帝咬破手指，血书密诏，藏在玉带（应该就是腰带？），然后缝起来。第二天，召见董承进宫，明说感谢董承之前护驾有功（第13回），实际上是想送他玉带和锦袍，并告诉他领了玉带和锦袍后，要仔细观察。董承出宫之后，发现了玉带，陆陆续续的，董承召集了一些忠臣，并在白娟上签名，誓死忠于汉室，铲除奸臣曹操，这些人包括：董承、马腾、刘备等。\n第二十一回 曹操煮酒论英雄 关公赚城斩车胄 刘备为了不露锋芒，天天跑到后院种菜。有一天，曹操请刘备喝酒，席间，问：玄德久历四方，必知当世英雄。请试指言之。刘备本想谦虚过去不做评论，但是曹操不肯，非要刘备说。刘备把当世几大势力都说了一遍，曹操都不同意。最后，曹操说：今天下英雄，惟使君与操耳。刘备听到后吃了一惊，筷子掉地上了，正好这个时候下雨天上打雷，刘备就谎称被雷惊吓到了。\n刘备事后和关、张说：“吾之学圃，正欲使操知我无大志；不意操竟指我为英雄，我故失惊落箸。又恐操生疑，故借惧雷以掩饰之耳。”\n又有一天，人报袁绍破了公孙瓒，公孙瓒已自杀身亡。淮南袁术骄奢过度，不恤军民，众皆背反，术使人归帝号于袁绍。绍欲取玉玺，术约亲自送至，见今弃淮南欲归河北。刘备听到这个消息后，一方面寻思不知道公孙瓒手下的赵云现在何处，另一方面想借这个机会脱离曹操，所以对曹操说，袁术从淮南去河北，必经过徐州，我想带兵去徐州半路截住袁术。曹操同意。于是刘关张带着人马去徐州了。在徐州，刘关张和袁术大战，袁术大败，吐血斗余而死。袁术本来是三国势力中比较大的一支，兵粮足备，又有传国玉玺，算是第一个称帝的势力，居然落到这个下场，真是凄惨啊。\n袁术死后，手下徐璆夺得玉玺，赴许昌献于曹操，曹操大喜，封徐璆为高陵太守。\n曹操知道刘备不会回来，就叫徐州太守车胄把刘备势力灭了，关羽献计说可以半夜回城时，假扮曹操军队，引车胄出城，然后灭掉车胄。事情果然和关羽预计的一样，骗车胄开城门之后，刘关张灭了车胄全家，占领徐州。\n此时的中国大概是这样的：http://www.sdmzh.com.cn/map/sgst.html，199年。\n第二十二回 袁曹各起马步三军 关张共擒王刘二将 刘备违抗曹操命令，又杀了车胄占了徐州，担心曹操大军来讨伐，于是陈登献计联合袁绍共抗曹操。虽然刘备刚杀了袁绍的弟弟袁术，但刘备请袁绍的三世通家郑玄帮忙通融，袁绍考虑再三，决定出兵帮助刘备攻打曹操，于是起兵马、步、水三军。曹操闻讯，也起马、步、水三军进行抵抗，同时派刘岱、王忠去徐州攻打刘备。\n袁绍军中，谋臣不和：原来许攸不乐审配领兵，沮授又恨绍不用其谋，各不相和，不图进取。袁绍心怀疑惑，不思进兵。所以两军在前线驻扎，也不开火，就这样耗着，曹操干脆就回许都了。\n刘岱、王忠两个草包部队，跑去徐州，一个被关羽生擒，一个被张飞生擒，不过刘备都把他们放回去了。刘备担心曹操来犯，于是令关羽守下邳，孙乾、简雍、糜竺、糜芳守徐州，刘备和张飞守小沛，以为掎角之势。注意，这个时候，刘关张不在同一个地方了。\n第二十三回 祢正平裸衣骂贼 吉太医下毒遭刑 话说曹操想起兵打刘备，孔融建议可先使人招安张绣、刘表。张绣被招安，曹操令张绣招安刘表，但刘景升好结纳名流，今必得一有文名之士往说之，方可降耳。孔融推荐才子祢衡，字正平。但是曹操怠慢了祢衡，祢衡嘲笑曹操手下无能人，曹操为了羞辱祢衡，每天上朝的时候让祢衡击鼓。\n但是祢衡击鼓的时候没有更衣，被训斥了，祢衡干脆把衣服全脱了，辱骂曹贼。曹操后派祢衡去说刘表，刘表被祢衡羞辱，刘表推荐去说黄祖，黄祖也被羞辱，黄祖遂杀了祢衡。刘表暂未归顺曹操。\n却说国舅董承病了，名医吉太来帮董承治病时，听到董承说梦话要杀曹贼，于是和董承商量，可以在帮曹操治病的时候，乘机下毒杀了曹操。但是，某天董承发现家奴和侍妾在窃窃私语，不知是怀疑他们偷情还是怎么了，把家奴打了四十大板，家奴怀恨在心，半夜逃出董承家，去给曹操报信了。曹操得知此事之后，诈称自己有病，破了吉太和董承的阴谋，杀了吉太，同时搜出了皇帝的衣带诏，一举杀了董承、王子服等有关的所有人。城中官民见者，无不下泪。\n第二十四回 国贼行凶杀贵妃 皇叔败走投袁绍 曹操杀了董承等人还不解气，连带杀了董贵妃，董贵妃是董承的妹妹，当时已经有身孕了，依然被曹操杀害。\n此时，在衣带诏上签名的只剩下西凉太守马腾和徐州刘备了。曹操决定起兵打刘备，刘备求救于袁绍，本来袁绍这个时候如果乘机袭击曹操老巢许都的话，曹操腹背受敌肯定招架不住，但是袁绍因自己最爱的小儿子生病了，没心思打仗，不打算出兵帮刘备，只说如果刘备走投无路了可以来投袁绍。\n却说刘关张这个时候肯定不是曹操大军的对手，三人带三个小部队，都被打散了，张飞被迫去了芒砀山，刘备投了袁绍，关羽带着刘备的妻小死守下邳，徐州被曹操攻破。\n第二十五回 屯土山关公约三事 救白马曹操解重围 却说曹操毕竟是大规模正规军，攻势很猛，关羽被迫退到了一座土山包上。这个时候，曹操想招安关羽，派张辽去说关羽，张辽口才还是不错，最后，关羽和曹操定了三个约定，才愿意暂时归顺曹操账下，这三个约定分别是：1. 只降汉帝，不降曹操；2. 二嫂处请给皇叔俸禄养赡，一应上下人等，皆不许到门；3. 但知刘皇叔去向，不管千里万里，便当辞去。曹操答应。\n曹操为了感化笼络关羽，对关羽非常好，甚至把从吕布那缴获的赤兔马送给了关羽。\n这个时候，袁绍突然想攻打曹操了，真是可笑，谋士田丰进谏说之前曹操去打徐州时，许都空虚，那个时候不打许都，现在跑去打许都，不是送死吗。但是袁绍不听，甚至还把田丰囚禁起来了。\n袁绍带着颜良、文丑等大将，首先进攻白马。颜良和文丑武力高超啊，曹操连派几元大将，都被颜良和文丑杀了。曹操最后让关羽出马，关羽胜出，杀了颜良，帮曹操解了白马之围。\n第二十六回 袁本初败兵折将 关云长挂印封金 袁绍见颜良被杀，又派出文丑出战，又被关羽杀了。两次败军回报都说是被一个美髯公杀了，袁绍猜是关羽，于是怀疑刘备和关羽串通好了，两次要杀刘备，都被刘备解释过去了，说徐州一战，兄弟三人都打散了，也不知道对方是不是关羽，我可以写封信让人送给关羽，这样关羽就可以归到袁绍账下了，一个关羽可远远强于被杀的颜良文丑啊。袁绍真是傻啊。\n这个时候正在逃难的孙乾偷偷跑到关羽寨下，跟关羽说刘备好像就在袁绍那里。又有一天，袁绍部下南阳陈震送来了刘备的信，跟关羽说他现在在袁绍处，可速来。于是，关羽挂印封金（曹操之前给关羽一个汉寿亭侯的官当），不要任何金银珠宝和官爵，带着甘、糜二夫人，准备去袁绍处找刘备。\n第二十七回 美髯公千里走单骑 汉寿侯五关斩六将 话说关羽离开曹操的时候，多次想向曹操当面辞别，但曹操为了留住关羽，都没有见关羽。关羽只得留下一封信给曹操，独自上路。这也就导致关羽没有得到的曹操的书面授权的通关文书。关羽在去往袁绍的路上，虽然骑着赤兔马，但要护送甘、糜二夫人，速度也不是很快。沿路遇到了五处关卡，守关之人皆以关羽没有得到曹操的授权而阻拦，结果是，这些人都被关羽杀了，斩将六员，此即过五关斩六将。\n第二十八回 斩蔡阳兄弟释疑 会古城主臣聚义 话说张飞逃难的一个叫古城的地方，并且占领了这个山城。关羽一行人也正好到这个地方。两人见面，张飞气不打一处来，和关羽干起来了，因为他觉得关羽不顾桃园结义，降了曹操，又被封侯赐爵，是忘恩负义之人，不论关羽怎么解释，张飞就是不听。\n突然秦琪的舅舅蔡阳来为秦琪报仇，因为关羽过五关斩六将的时候杀了秦琪。关羽为了告诉张飞，自己是清白的，亲自来战蔡阳，并把蔡阳杀了，同时抓了一个小兵，让他跟张飞解释自己在曹操处时是怎样不忘刘备的。最后，张飞信了，兄弟两冰释前嫌，握手言和。\n于是，他们商量好，张飞依然守着古城，关羽和孙乾去袁绍处找刘备。刘备和他们见面之后，找了个借口要摆脱袁绍，说：”刘表镇守荆州，我可以去诏安他，因为我刘备和刘表都是汉室后代，他应该会听我的话。同时，可以派孙乾再去诏安关羽（这其实是幌子，为了让孙乾也和刘备一起离开）。”这时候，简雍说我也可以和刘备一起走，可以监视刘备，防止他跑了（其实简雍这样说也是为了一块跑）。\n这样，刘备同伙一帮人就这样顺利的脱离了袁绍，袁绍真是太傻了。刘备等人在去古城的路上，正好遇到了赵云，原来公孙瓒死了之后，赵云无处可走，听说张飞在古城，就来古城找张飞，正好遇到了刘备等人。就这样，刘关张和赵云等人顺利在古城再次会师。\n第二十九回 小霸王怒斩于吉 碧眼儿坐领江东 却说孙策自霸江东，兵精粮足，还和曹操结了亲家，曹仁之女许配给孙策幼弟孙匡。但是孙策向曹操要个大司马的官当当，曹操不同意，孙策怀恨在心，常有袭许都之心。吴郡太守许贡偷偷想向曹操报信，被孙策发现并截住了。孙策杀了许贡，但是某天，许贡的三个家客给许贡报仇，毒箭射中了孙策。医生说孙策必须静养百日才能好，而且不能动怒。\n有一天，孙策发现一个仙人于吉在街上走着，老百姓都焚香伏道而拜，孙策觉得这是妖魔鬼怪，抓住于吉并杀了他。但是于吉真是仙人，被无辜杀害之后，又一直出现在孙策面前，不过别人看不见，只有孙策看得见。就这样，孙策不断被于吉激怒，又干不掉于吉。最终，毒性发作，金仓迸裂，死了。孙策死前，把江山交给了弟弟孙权，告诉他内事不决问张昭，外事不决问周瑜。孙策的老婆是大乔。\n孙权继承父兄业绩之后，吸纳了很多人才，比如鲁肃、诸葛瑾、顾雍等人。从此，孙权威震江东，深得民心，孙权的时代开始了。\n第三十回 战官渡本初败绩 劫乌巢孟德烧粮 却说袁绍兴兵往官渡进发，原本曹操的兵力是要少于袁绍的，但是袁绍听谋士建议，不体恤部下官兵，导致谋士许攸叛变，投奔曹操；手下大将张邰、高览也倒戈，投奔曹操。袁绍的粮草都囤在乌巢，许攸建议曹操可以派一小支精锐部队，把乌巢的粮草都烧了，这样袁军大乱，曹操可以乘势掩杀大败袁绍。\n曹操采纳了许攸的建议，果然烧了袁绍的粮草，大败袁绍，曹操大获全胜，这是三国上有名的三大战役之一，也是著名的以弱胜强的战役之一。袁绍太不爱惜人才了，而且不体恤属下；自己没智慧无计谋，而且还不听取采纳谋士的建议，不失败才怪。\n百度百科上有一段胜败分析：\n官渡之战是袁曹双方力量转变，使当时中国北部由分裂走向统一的一次关键性战役，对于三国历史的发展有着极其重要的影响。此战曹军的胜利不是偶然的，袁曹间的兼并战争，虽属于封建割据势力之间的争斗，但它实现了地区统一，客观上符合人民的愿望。 官渡之战是汉末乃至中国史上有名的以少胜多的战役，也是曹操与袁绍争夺北方霸权的转折点。官渡一战之后，曹操终于一反之前对袁绍的劣势，为自己统一北方奠定了基础。曹操在战事初期处于劣势，当中全赖三人为曹操扭转困局——荀彧、荀攸、许攸。\n1、曹操于黎阳与袁绍相持，本欲还兵再作打算，荀攸献计：“今兵少不敌，分其势乃可。公到延津，若将渡兵向其后者，绍必西应之，然后轻兵袭白马，掩其不备，颜良可擒也。”曹操依计行事，果然大破袁军，斩杀颜良。\n2、建安五年八月始，两军再次相持于官渡，双方互有胜负。其后曹操军中缺粮，适逢袁绍谋士许攸与营中将士不和，投奔曹操。许攸献计烧袁绍军粮，使袁绍不战自败。\n3、曹操曾经在交战之时想过放弃，写信给许都的荀彧。而荀彧却提醒了曹操：“在战争双方都疲惫不堪时，谁后退谁被动，谁放弃谁灭亡。战机就在这时出现。”最后帮助曹操寻回信心，继续坚持。\n曹操能接纳能人之言，取得最终的胜利，这全在于用人之道。荀攸、许攸皆是人才，献上计谋，有化险为夷之功；荀彧则具备长远的战略眼光，能够鼓励和帮助曹操在关键时期坚持战斗，这是更高层次的人才。由此观之，人才的妥善任用应该可说是“一计敌万人”。至于曹操，他是一个懂得运用人才的人才，能接纳他人之言，故袁绍兵多也不足为惧，正所谓兵不在多，在乎能否调遣。\n","permalink":"http://localhost:1313/posts/2019-07-10-summary-of-the-romance-of-the-three-kingdoms-1-30/","summary":"\u003ch1 id=\"第一回-宴桃园豪杰三结义-斩黄巾英雄首立功\"\u003e第一回 宴桃园豪杰三结义 斩黄巾英雄首立功\u003c/h1\u003e\n\u003cp\u003e黄巾起义，刘关张结为兄弟，大小顺序为刘关张，张飞是卖猪肉的，很有钱。刘备：双股剑；关羽：青龙偃月刀；张飞：丈八蛇矛。\u003c/p\u003e\n\u003ch1 id=\"第二回-张翼德怒鞭督邮-何国舅谋诛宦竖\"\u003e第二回 张翼德怒鞭督邮 何国舅谋诛宦竖\u003c/h1\u003e\n\u003cp\u003e刘关张镇压黄巾起义有功，但只得到一个很小的县令官，与民秋毫无犯，但被上面下来检查的督邮视察时，因没有贿赂督邮，被督邮穿小鞋，张飞怒不可遏，鞭打督邮。\u003c/p\u003e\n\u003cp\u003e此时，朝廷内，十常侍专权，把持朝政。汉灵帝驾崩之后，何进拥立刘辩为皇，因为刘辩是灵帝和何进的姐姐的儿子。同时，把董后鸩杀。同时，何进为了除掉十常侍，引董卓入宫。\u003c/p\u003e\n\u003ch1 id=\"第三回-议温明董卓叱丁原-馈金珠李肃说吕布\"\u003e第三回 议温明董卓叱丁原 馈金珠李肃说吕布\u003c/h1\u003e\n\u003cp\u003e十常侍压力山大，为了保命，先下手为强，请何皇后把何进单独召进宫，进宫的路上，把何进杀了。宫内大乱，十常侍被何进部下杀掉。同时，董卓入宫，为彰显威严，欲废少帝辩，立刘协为新皇帝，在温明园讨论废立之事时，忠臣丁原挺身反对，董卓仗势欺人，斥责丁原。无奈丁原背后站着义子吕布，董卓奈何不了丁原。\u003c/p\u003e\n\u003cp\u003e次日，丁原携吕布向董卓宣战，吕布骁勇善战，无人能破。此时，董卓部下李肃是吕布的老朋友，带着金银珠宝和董卓的赤兔马来劝说吕布，凭着李肃的三寸不烂之舌和吕布的头脑简单，吕布被说服，杀掉义父丁原，同时投奔董卓麾下。\u003c/p\u003e\n\u003ch1 id=\"第四回-废汉帝陈留践位-谋董贼孟德献刀\"\u003e第四回 废汉帝陈留践位 谋董贼孟德献刀\u003c/h1\u003e\n\u003cp\u003e董卓既得吕布，态度更加强硬，9月，废少帝，立陈留王刘协为新皇帝。把刘辩打入冷宫，某日，刘辩发牢骚写了首诗，董卓抓住机会，赐鸩酒把刘辩和何太后都杀了。\u003c/p\u003e\n\u003cp\u003e曹操为了谋杀奸臣董卓，偷偷带着司徒王允的七宝刀，找了一个机会靠近董卓，本来要刺杀董卓，没成想被董卓从衣镜里发现了，曹操马上改口说有一口很好的宝刀，要献给董卓，然后开溜。董卓后来才get到曹操是要来刺杀自己的。\u003c/p\u003e\n\u003ch1 id=\"第五回-发矫诏诸镇应曹公-破关兵三英战吕布\"\u003e第五回 发矫诏诸镇应曹公 破关兵三英战吕布\u003c/h1\u003e\n\u003cp\u003e曹操逃出来之后，被董卓通缉，所以只能发布通告，尽书董卓恶行，招兵买马，准备讨伐董卓。各大诸侯太守都来响应，组成了一个十八路诸侯联盟，盟主是原朝廷重臣袁绍。但是袁绍统领诸侯，调度大军的能力不够，各诸侯也各怀鬼胎，没有凝聚力。\u003c/p\u003e\n\u003cp\u003e前锋部队孙坚在进军汜水关时被华雄击败，华雄不可一世，在潘凤等大将接连被华雄斩杀之时，关羽主动请缨前去战华雄，在温酒未冷却的极短时间内斩杀华雄，关羽从此名震诸侯。此即温酒斩华（huà）雄的故事。\u003c/p\u003e\n\u003cp\u003e董卓折了华雄，起兵二十万，兵分两路，其中一路由董卓亲自带队，和吕布等人，守住虎牢关，就是标题中的关兵中的关。在虎牢关处，吕布骑着赤兔马，不可一世，盟军无人能敌。最后，刘关张三人亲自出马，大战吕布，吕布败走。\u003c/p\u003e\n\u003ch1 id=\"第六回-焚金阙董卓行凶-匿玉玺孙坚背约\"\u003e第六回 焚金阙董卓行凶 匿玉玺孙坚背约\u003c/h1\u003e\n\u003cp\u003e吕布新败，董卓引兵回洛阳，迁都长安，同时把洛阳的宫殿烧毁。孙坚飞奔洛阳，救火的同时，在井中发现了传国玉玺，并私藏起来。后来被盟主袁绍发现，孙坚感到被羞耻了，拔寨离洛阳而去。袁绍大怒，写信给荆州刘表，因为孙坚回老家江东——扬州（？）要经过刘表家，所以袁绍写信给刘表，叫他半路截住孙坚。果然在半路上，孙坚和刘表来了一场恶战，亏孙坚部下三员大将程普、黄盖、韩当死救得脱。自此，孙坚和刘表结怨。\u003c/p\u003e\n\u003ch1 id=\"第七回-袁绍磐河战公孙-孙坚跨江击刘表\"\u003e第七回 袁绍磐河战公孙 孙坚跨江击刘表\u003c/h1\u003e\n\u003cp\u003e袁绍屯兵河内，缺少粮草，向冀州太守韩馥借粮，谋士逢纪说大丈夫落到向别人借粮，可耻啊，冀州乃钱粮广盛之地，不如取而代之。袁绍于是和公孙瓒密谋，让公孙瓒出兵冀州，则韩馥必向袁绍求救，袁绍乘虚而入，占领冀州，然后和公孙瓒平分冀州。\u003c/p\u003e\n\u003cp\u003e公孙瓒照做，但是当袁绍占领冀州之后，并没有和公孙瓒平分冀州，而是独占了。公孙瓒派弟弟公孙越去袁绍处，想要分点油水，没想到反被袁绍杀害。于是，公孙瓒大怒，举兵攻打袁绍。两军交战于磐河。此战互有胜负，公孙瓒手下赵云出场，和同在公孙瓒手下的刘关张相见，一见如故，分别时泪如雨下。\u003c/p\u003e\n\u003cp\u003e袁绍的弟弟袁术，在南阳，听说哥哥新得冀州，想要哥哥赏赐点马匹，袁绍不给，自此兄弟不睦。袁术又向荆州刘表借粮，刘表也不给。袁术怒了，写信给孙坚，说昔日孙坚私藏玉玺回老家的路上，被刘表伏击，今日，我袁术愿与你结盟，攻打刘表。于是孙坚果然起兵，跨过汉水（长江），攻打刘表。没成想中了刘表部下蒯良的计谋，被杀了，可惜啊。孙坚部下黄盖生擒刘表部下黄祖，于是和刘表交换回孙坚尸体。孙坚大儿子孙策，字伯符；二儿子孙权，字仲谋。\u003c/p\u003e\n\u003ch1 id=\"第八回-王司徒巧使连环计-董太师大闹凤仪亭\"\u003e第八回 王司徒巧使连环计 董太师大闹凤仪亭\u003c/h1\u003e\n\u003cp\u003e董卓在长安听说孙坚死了，更加骄奢淫逸。此时，司徒王允为了江山社稷愁死了，王允府上的歌伎貂蝉，特别漂亮，允以亲女待之。貂蝉想帮王允分担忧愁，于是王允想出了一个连环计。因董卓和其干儿子吕布都是有勇无谋，贪财好色之徒，王允先把貂蝉许配给吕布，然后又悄悄把貂蝉送给董卓。貂蝉从中挑不离间，致使父子二人翻脸。\u003c/p\u003e\n\u003cp\u003e因为貂蝉已经被送到董卓府上，有一天，吕布趁着董卓和汉献帝聊天，偷偷来到董卓府上，在府上的凤仪亭看到了貂蝉，和貂蝉搂搂抱抱，被董卓赶回来发现了，董卓大闹凤仪亭，追着吕布打。自此父子二人结怨。\u003c/p\u003e\n\u003ch1 id=\"第九回-除暴徒吕布助司徒-犯长安李傕听贾诩\"\u003e第九回 除暴徒吕布助司徒 犯长安李傕听贾诩\u003c/h1\u003e\n\u003cp\u003e经过了上面的事情，貂蝉劝董卓搬家，于是董卓和貂蝉搬到郿坞去了。司徒王允和吕布想了一个计策，说汉献帝病刚好，想召集文武百官吃个饭，于是派昔日董卓心腹李肃（就是第三回的李肃，因为董卓没有给李肃升官，李肃对董卓也有怨念），去郿坞宣旨。董卓傻乎乎兴高采烈来到宫内，被早就在此埋伏的王允、吕布、李肃等人杀死，同时去郿坞把董卓全家灭口，包括杀了董卓谋士李儒。吕布得到貂蝉。\u003c/p\u003e\n\u003cp\u003e董卓手下四员大将李傕、郭汜、张济、樊稠，听说董卓被杀，打算吃个散伙饭，各自逃命。谋士贾诩说，我们都被通缉，既然自首也要死，不如来个你死我活。四人听了之后认为有道理，于是在西凉起兵，杀奔长安。而且他们制定的军事政策是，其中两个人在山外前后诱杀吕布，但又不恋战，另两个人偷偷起兵直接攻打长安城，让吕布和长安城首尾不能接应。此计果然奏效，董卓余党在长安城内为内应，打开城门，四员大将进入城中，烧杀抢掠，把王允也杀了。吕布弃了家小，投袁术去了。\u003c/p\u003e\n\u003ch1 id=\"第十回-勤王室马腾举义-报父仇曹操兴师\"\u003e第十回 勤王室马腾举义 报父仇曹操兴师\u003c/h1\u003e\n\u003cp\u003e李傕、郭汜、张济、樊稠占领宫内之后，骄奢蛮横，残虐百姓。西凉太守马腾和并州刺史韩遂，密谋贼党，但是都失败了。此时，马腾之子马超出场。樊稠因放过同乡人韩遂，被李傕郭汜杀掉。\u003c/p\u003e\n\u003cp\u003e因朝廷昏庸无能，青州黄巾起义又起，太傅朱儁推荐派曹操去剿灭黄巾起义，李傕郭汜同意。东郡太守曹操和济北相鲍信一同破贼。\u003c/p\u003e\n\u003cp\u003e曹操在兖州，招贤纳士，群贤毕至。文官：荀彧、荀攸，叔侄二人；程昱；郭嘉；刘晔；满宠；吕虔；毛玠。武官：于禁；典韦。自是曹操部下文有谋臣，武有猛将，威镇山东。\u003c/p\u003e\n\u003cp\u003e曹操一高兴，打算把琅琊郡的老父亲曹嵩接过来享天伦之乐，于是，曹嵩和弟弟曹德并一家老小准备赶往兖州。途径徐州，徐州太守陶谦，想讨好曹操，大设宴席款待曹嵩等人，并派部下张闿护送曹嵩。没曾想，张闿原是黄巾余党，在陶谦处没有得到重用，今贼心不改，在路上把曹嵩一家老小全杀了。曹操听到消息，大怒，亲自起兵杀奔徐州。\u003c/p\u003e\n\u003ch1 id=\"第十一回-刘皇叔北海救孔融-吕温侯濮阳破曹操\"\u003e第十一回 刘皇叔北海救孔融 吕温侯濮阳破曹操\u003c/h1\u003e\n\u003cp\u003e于是，徐州陶谦，向北海孔融求救，正商议间，黄巾余党管亥来北海攻打孔融。幸好孔融平时人品好，善待城外的一个老奶奶，老奶奶听说孔融有难，叫回来省亲的儿子太史慈去救孔融。太史慈虽然很厉害，但毕竟只有一个人，孔融就叫太史慈杀出重围，请刘备来救援。刘备于是向公孙瓒借了赵云，带着关张来救孔融。\u003c/p\u003e\n\u003cp\u003e刘备来了之后，先给曹操写了封信，好言相劝，劝和。正好，这个时候，从宫中逃出来的吕布，攻陷了曹操的老巢兖州和濮阳，于是曹操送刘备一个人情，撤兵回老巢了。\u003c/p\u003e\n\u003cp\u003e曹操经过商议之后，准备亲自领兵，去夺回濮阳，在濮阳和吕布进行了恶战，战败，差点被吕布围剿。\u003c/p\u003e\n\u003ch1 id=\"第十二回-陶恭祖三让徐州-曹孟德大战吕布\"\u003e第十二回 陶恭祖三让徐州 曹孟德大战吕布\u003c/h1\u003e\n\u003cp\u003e吕布的谋士陈宫，诡计多端，出了一个点子，诱使曹操进濮阳城，待曹操进城之后，关门放火，差点把曹操灭了，众将死救得脱。\u003c/p\u003e\n\u003cp\u003e陶谦在徐州，已经63岁了，儿子又无才，于是，想把徐州送给刘备接管，刘备死活不肯要。就这样来来回回三次，陶谦都要死了，刘备才肯接管徐州。\u003c/p\u003e\n\u003cp\u003e曹操自从被吕布打了个败仗，回老家待着。谋士荀彧说，现在收成不好，可以去陈地、汝南、颍川抢占地盘，这些地方都是黄巾余党，乌合之众，轻易可破，又可得粮草。曹操听之，果然占领了这些地方，顺带还收了一员武将许褚。\u003c/p\u003e\n\u003cp\u003e某天听说兖州吕布手下大将薛兰、李封都出去劫掠了，可以乘虚而入，夺回兖州。曹操听之，果然夺回兖州，同时六员大将齐战吕布，吕布败走。\u003c/p\u003e\n\u003ch1 id=\"第十三回-李傕郭汜大交兵-杨奉董承双救驾\"\u003e第十三回 李傕郭汜大交兵 杨奉董承双救驾\u003c/h1\u003e\n\u003cp\u003e吕布败走之后，来徐州投奔刘备，屯兵小沛。\u003c/p\u003e\n\u003cp\u003e却说李傕郭汜在宫廷横行无忌，太尉杨彪和大司农朱儁密谋诛杀李傕郭汜。杨彪献一反间计：郭汜的妻子妒忌心很强，可派人秘密告诉郭汜妻子，郭汜在和李傕夫人偷情。反间计成功，李傕郭汜反目成仇，李傕劫了天子，郭汜劫了文武百官，每日厮杀。\u003c/p\u003e","title":"《三国演义》每回内容梗概（1~30）"},{"content":"这一讲是上一讲的补充，内容比较零碎，包括：Word2vec回顾、优化、基于统计的词向量、GloVe、词向量评价、词义等，前两个内容没必要再介绍了，下面逐一介绍后四个内容。\n基于统计的词向量 词向量的目的就是希望通过低维稠密向量来表示词的含义，而词的分布式语义表示方法认为词的含义由其上下文语境决定。Word2vec把中心词和临近词抽取出来，通过预测的方式训练得到词向量。在Word2vec之前，传统的方式通过统计词的共现性来得到词向量，即一个词的词向量表示为其临近词出现的频率，如果两个词的含义很相近，则其临近词分布会比较像，得到的词向量也比较像。其具体计算过程在第一次作业中有详细的描述，这里再简单回顾如下。\n假设一个语料库中包含三个句子，共有8个特异词（包括点号），对于每个词，统计其前后一个词的词频（临近窗口为1），由此能得到一个8×8的对称矩阵，其每一行（或每一列）表示该词的词向量。比如对于like这个词，在三个句子中，其左右共出现2次I，1次deep和1次NLP，所以like对应的词向量中，I、deep和NLP维的值分别为2,1,1。\n这种基于词频统计的方法很简单，但是它有如下不足：\n特异的词很多，所以矩阵很大，维度很高，需要的存储空间也很大 特异词的数目是在不断增长的，则词向量的维度也在不断增长 矩阵很稀疏，即词向量很稀疏，会导致很多NLP任务会遇到稀疏计算的问题 所以需要把上述计数矩阵转换为一个低维稠密的矩阵，方法就是SVD分解。上述矩阵原本是一个\\(n\\times n\\)的矩阵，SVD分解后能得到一个\\(n\\times k\\)的矩阵，其中\\(k\\ll n\\)。即原本的词向量是一个\\(n\\)维的高维稀疏向量，变成了\\(k\\)维的低维稠密向量，而且还不会损失太多的信息。\n2005年的一篇文章对上述简单的计数方法进行了改进，包括去掉停用词、使用倾斜窗口、使用皮尔逊相关系数等，提出了COALS模型，该模型得到的词向量效果也不错，也具有句法特征和语义特征。使用统计的方法和使用预测的方法训练词向量，两者的对比如下。基于统计计数的方法的主要特点是：训练速度快，能充分利用统计信息，主要用来捕获词的相似性。基于预测的方法的主要特点是：对语料库的大小可扩展，没有充分利用统计信息，能捕获除了词的相似性之外的其他复杂特征。\nGloVe GloVe的全称是GloVe: Global Vectors for Word Representation，正是这门课的老师Christopher D. Manning的研究成果。有关GloVe论文的详细解读，可以看这篇博客。GloVe希望能综合上述基于统计和基于预测的两种方法的优点。\nGloVe的基本思想依然是基于统计的方法，当统计得到共现矩阵X之后，可以计算得到词\\(k\\)是词\\(i\\)的临近词的概率：\n$$P_{i,k}=\\dfrac{X_{i,k}}{X_{i}}$$再定义两个\\(P\\)的比值：\n$$ratio_{i,j,k}=\\dfrac{P_{i,k}}{P_{j,k}}$$如果词\\(k\\)在两个词\\(i\\)和\\(j\\)的临近概率相同，无论是同样大（water）还是同样小（fashion），经过比值计算后，\\(ratio_{i,j,k}\\)都约等于1了，说明在维度water和fashion上，无法区分ice和steam。而在维度solid和gash上，由于概率\\(P\\)的差异，导致\\(ratio_{i,j,k}\\)很大或者很小，这是有意义的，说明在solid和gas维度上，可以区分ice和steam的语义。\n基于这样的观察，GloVe首先统计语料库中三元组\\(i,j,k\\)的\\(ratio_{i,j,k}\\)，然后初始化词向量\\(v\\)，构造函数\\(g\\)，使得利用词向量计算得到的\\(g(v_{i},v_{j},v_{k})\\)和真实\\(ratio_{i,j,k}\\)尽量接近。\n$$\\dfrac{P_{i,k}}{P_{j,k}}=ratio_{i,j,k}=g(v_{i},v_{j},v_{k})$$$$J=\\sum_{i,j,k}^N(\\dfrac{P_{i,k}}{P_{j,k}}-g(v_{i},v_{j},v_{k}))^2$$但是上述方法的复杂度太高了，对于一个\\(N\\times N\\)的共现矩阵，上述算法需要计算所有的三元组，复杂度是\\(N\\times N\\times N\\)。GloVe文章通过各种转换技巧，把复杂度降为了一个\\(N\\times N\\)的问题，具体过程可以看上面提到的博客或者paper原文。\n总的来说，基于共现矩阵这种统计的方法，能捕获整个语料库全局的信息；而类似word2vec的预测的方法，则主要捕获局部的滑动窗口内的共现信息，两种方法训练得到的词向量效果都不错。\n词向量评价 评价词向量的好坏主要有两个尺度，一是内部任务评价（intrinsic），一是外部任务评价（extrinsic），两者的主要特点如下。大概意思是内部任务是根据词本身具有的性质，比如近义、反义等，评价词向量本身的性能。外部任务是指词向量对NLP下游任务的性能的影响，比如同样是一个文本分类问题，换不同的词向量，对文本分类任务的性能的影响能反映出词向量的性能。\n常见的内部任务评价是词的类比推理（Word Vector Analogies），就是类似man:woman :: king:queen这种，word2vec还专门整理出了这样的测试数据：word2vec/trunk/questions-words.txt。另一个内部任务评价是使用训练得到的词向量计算的词相似度，和人类认为的相似度做比较，有团队专门整理出了人类对两个词的相似度打分，具体可以看这里。\n对词向量的外部任务评价就很多了，几乎所有的NLP任务都可以用来作为词向量的外部任务评价，比如命名实体识别、文本分类等等，这里不再展开。\n词义 一个词往往具有多个含义（word senses），特别是对于常用的词或者存在很久的词。那么一个词向量能同时包含这个词的多个语义吗？有文章把一个词的多个语义通过线性加权的方式叠加到一个词向量中，然后还能通过稀疏编码的方式求解出每个语义的词向量，具体可以看下图中的参考文献。\n","permalink":"http://localhost:1313/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/","summary":"\u003cp\u003e这一讲是上一讲的补充，内容比较零碎，包括：Word2vec回顾、优化、基于统计的词向量、GloVe、词向量评价、词义等，前两个内容没必要再介绍了，下面逐一介绍后四个内容。\u003c/p\u003e\n\u003ch1 id=\"基于统计的词向量\"\u003e基于统计的词向量\u003c/h1\u003e\n\u003cp\u003e词向量的目的就是希望通过低维稠密向量来表示词的含义，而词的分布式语义表示方法认为词的含义由其上下文语境决定。Word2vec把中心词和临近词抽取出来，通过预测的方式训练得到词向量。在Word2vec之前，传统的方式通过统计词的共现性来得到词向量，即一个词的词向量表示为其临近词出现的频率，如果两个词的含义很相近，则其临近词分布会比较像，得到的词向量也比较像。其具体计算过程在\u003ca href=\"https://github.com/01joy/stanford-cs224n-winter-2019/blob/master/1.8/assignment/a1/exploring_word_vectors.ipynb\"\u003e第一次作业\u003c/a\u003e中有详细的描述，这里再简单回顾如下。\u003c/p\u003e\n\u003cp\u003e假设一个语料库中包含三个句子，共有8个特异词（包括点号），对于每个词，统计其前后一个词的词频（临近窗口为1），由此能得到一个8×8的对称矩阵，其每一行（或每一列）表示该词的词向量。比如对于like这个词，在三个句子中，其左右共出现2次I，1次deep和1次NLP，所以like对应的词向量中，I、deep和NLP维的值分别为2,1,1。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/co-occurrence_matrix.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这种基于词频统计的方法很简单，但是它有如下不足：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特异的词很多，所以矩阵很大，维度很高，需要的存储空间也很大\u003c/li\u003e\n\u003cli\u003e特异词的数目是在不断增长的，则词向量的维度也在不断增长\u003c/li\u003e\n\u003cli\u003e矩阵很稀疏，即词向量很稀疏，会导致很多NLP任务会遇到稀疏计算的问题\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e所以需要把上述计数矩阵转换为一个低维稠密的矩阵，方法就是SVD分解。上述矩阵原本是一个\\(n\\times n\\)的矩阵，SVD分解后能得到一个\\(n\\times k\\)的矩阵，其中\\(k\\ll n\\)。即原本的词向量是一个\\(n\\)维的高维稀疏向量，变成了\\(k\\)维的低维稠密向量，而且还不会损失太多的信息。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://pdfs.semanticscholar.org/73e6/351a8fb61afc810a8bb3feaa44c41e5c5d7b.pdf\"\u003e2005年的一篇文章\u003c/a\u003e对上述简单的计数方法进行了改进，包括去掉停用词、使用倾斜窗口、使用皮尔逊相关系数等，提出了COALS模型，该模型得到的词向量效果也不错，也具有句法特征和语义特征。使用统计的方法和使用预测的方法训练词向量，两者的对比如下。基于统计计数的方法的主要特点是：训练速度快，能充分利用统计信息，主要用来捕获词的相似性。基于预测的方法的主要特点是：对语料库的大小可扩展，没有充分利用统计信息，能捕获除了词的相似性之外的其他复杂特征。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/count_based_vs_direct_prediction.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"glove\"\u003eGloVe\u003c/h1\u003e\n\u003cp\u003eGloVe的全称是\u003ca href=\"https://nlp.stanford.edu/projects/glove/\"\u003eGloVe: Global Vectors for Word Representation\u003c/a\u003e，正是这门课的老师Christopher D. Manning的研究成果。有关GloVe论文的详细解读，可以看\u003ca href=\"https://blog.csdn.net/coderTC/article/details/73864097\"\u003e这篇博客\u003c/a\u003e。GloVe希望能综合上述基于统计和基于预测的两种方法的优点。\u003c/p\u003e\n\u003cp\u003eGloVe的基本思想依然是基于统计的方法，当统计得到共现矩阵X之后，可以计算得到词\\(k\\)是词\\(i\\)的临近词的概率：\u003c/p\u003e\n$$P_{i,k}=\\dfrac{X_{i,k}}{X_{i}}$$\u003cp\u003e再定义两个\\(P\\)的比值：\u003c/p\u003e\n$$ratio_{i,j,k}=\\dfrac{P_{i,k}}{P_{j,k}}$$\u003cp\u003e如果词\\(k\\)在两个词\\(i\\)和\\(j\\)的临近概率相同，无论是同样大（water）还是同样小（fashion），经过比值计算后，\\(ratio_{i,j,k}\\)都约等于1了，说明在维度water和fashion上，无法区分ice和steam。而在维度solid和gash上，由于概率\\(P\\)的差异，导致\\(ratio_{i,j,k}\\)很大或者很小，这是有意义的，说明在solid和gas维度上，可以区分ice和steam的语义。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/GloVe.png\"\u003e\u003c/p\u003e\n\u003cp\u003e基于这样的观察，GloVe首先统计语料库中三元组\\(i,j,k\\)的\\(ratio_{i,j,k}\\)，然后初始化词向量\\(v\\)，构造函数\\(g\\)，使得利用词向量计算得到的\\(g(v_{i},v_{j},v_{k})\\)和真实\\(ratio_{i,j,k}\\)尽量接近。\u003c/p\u003e\n$$\\dfrac{P_{i,k}}{P_{j,k}}=ratio_{i,j,k}=g(v_{i},v_{j},v_{k})$$$$J=\\sum_{i,j,k}^N(\\dfrac{P_{i,k}}{P_{j,k}}-g(v_{i},v_{j},v_{k}))^2$$\u003cp\u003e但是上述方法的复杂度太高了，对于一个\\(N\\times N\\)的共现矩阵，上述算法需要计算所有的三元组，复杂度是\\(N\\times N\\times N\\)。GloVe文章通过各种转换技巧，把复杂度降为了一个\\(N\\times N\\)的问题，具体过程可以看上面提到的博客或者paper原文。\u003c/p\u003e\n\u003cp\u003e总的来说，基于共现矩阵这种统计的方法，能捕获整个语料库全局的信息；而类似word2vec的预测的方法，则主要捕获局部的滑动窗口内的共现信息，两种方法训练得到的词向量效果都不错。\u003c/p\u003e\n\u003ch1 id=\"词向量评价\"\u003e词向量评价\u003c/h1\u003e\n\u003cp\u003e评价词向量的好坏主要有两个尺度，一是内部任务评价（intrinsic），一是外部任务评价（extrinsic），两者的主要特点如下。大概意思是内部任务是根据词本身具有的性质，比如近义、反义等，评价词向量本身的性能。外部任务是指词向量对NLP下游任务的性能的影响，比如同样是一个文本分类问题，换不同的词向量，对文本分类任务的性能的影响能反映出词向量的性能。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/word_vector_evaluation.png\"\u003e\u003c/p\u003e\n\u003cp\u003e常见的内部任务评价是词的类比推理（Word Vector Analogies），就是类似man:woman :: king:queen这种，word2vec还专门整理出了这样的测试数据：\u003ca href=\"https://code.google.com/archive/p/word2vec/source/default/source\"\u003eword2vec/trunk/questions-words.txt\u003c/a\u003e。另一个内部任务评价是使用训练得到的词向量计算的词相似度，和人类认为的相似度做比较，有团队专门整理出了人类对两个词的相似度打分，具体可以看\u003ca href=\"http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e对词向量的外部任务评价就很多了，几乎所有的NLP任务都可以用来作为词向量的外部任务评价，比如命名实体识别、文本分类等等，这里不再展开。\u003c/p\u003e\n\u003ch1 id=\"词义\"\u003e词义\u003c/h1\u003e\n\u003cp\u003e一个词往往具有多个含义（word senses），特别是对于常用的词或者存在很久的词。那么一个词向量能同时包含这个词的多个语义吗？有文章把一个词的多个语义通过线性加权的方式叠加到一个词向量中，然后还能通过稀疏编码的方式求解出每个语义的词向量，具体可以看下图中的参考文献。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/word_senses.png\"\u003e\u003c/p\u003e","title":"CS224N（1.10）Word Vectors 2 and Word Senses"},{"content":"今天开始介绍大名鼎鼎的NLP网课Stanford-CS224N。第一讲内容为课程简介和词向量。\n词向量即用来表示这个词的含义的向量。早期的NLP常用one-hot编码来表示词向量，假如词典中共有10000个词，则这个one-hot向量长度就是10000，该词在词典中所处位置对应的值为1，其他值为0。\none-hot表示方法虽然简单，但其有诸多缺点：1. 词典中的词是不断增多的，比如英语，通过对原有的词增加前缀和后缀，可以变换出很多不同的词，one-hot编码会导致向量维度非常大，且每个向量是稀疏的；2. 不同词的one-hot编码向量是垂直的，在向量空间中无法表示近似关系，即使两个含义相近的词，它们的词向量点积也为0。\n既然one-hot编码有这么多缺点，那我们就换一种编码，one-hot是高维稀疏向量，那新的编码就改用低维稠密向量，这样就解决了上述问题，那么怎样得到一个词的低维稠密的词向量呢？这就是word2vec算法。\nword2vec采用了分布式语义的方法来表示一个词的含义。本质上，一个词的含义就是这个词所处的上下文语境。回想一下我们高中做英语完形填空时，一篇短文，挖了好多空，让我们根据空缺词的上下文语境选择合适的词。也就是说上下文语境已经能够确定这个词的含义了，如果选词正确，也就意味着我们理解了这个空缺词的含义。\nword2vec算法发表于2013年，包括两种训练算法Skip-grams (SG)和Continuous Bag of Words (CBOW)，这两种方法很类似，其中CBOW和上述介绍到的英语完形填空几乎是一样的，由上下文词预测中心词；而SG则和CBOW正好相反，由中心词预测上下文词，本文主要介绍SG算法。\n给定一个语料库，这个语料库包含了很多文章，每篇文章又包含很多句子，每个句子又包含很多词语。所以一个语料库是一个天然的标注集，因为对于每一个选定的中心词，我们都知道其临近的词是什么。这样一个（中心词，临近词）对就构成了一个标注集。SG算法的中心思想就是对于每个选定的中心词，尽量准确的预测其周围可能出现的词的概率分布。具体来说，SG算法首先随机初始化每个词的词向量；然后预测不同临近词出现的概率，最后最大化实际临近词出现的概率。\n形式化来说，就是用极大似然估计的方法，求解每个词的词向量。其目标函数如下，其中\\(\\theta\\)是待求解的参数；\\(t\\)为选定的中心词位置；对于每个\\(t\\)（外层\\(\\prod\\)），估计其邻域\\(\\pm m\\)个词出现的概率（内层\\(\\prod\\)）。\n求解极大似然估计的方法比较成熟，一般先把极大似然转换为最小化-log似然，然后用梯度下降求解。所以核心问题就变成了如何求解\\(P(w_{t+j}|w_t;\\theta)\\)。\n对于每个词\\(w\\)，定义其两个词向量：\\(v_w\\)表示当\\(w\\)为中心词时的词向量，\\(u_w\\)表示当\\(w\\)为其他词的临近词时的词向量。则对于一个中心词\\(c\\)和其临近词\\(o\\)，有：\n上式本质是一个softmax函数，因为给定\\(c\\)，\\(o\\)相当于是标注结果，所以把它们的点积作为分子，希望分子越大越好；而分母则是所有可能的\\(u_w\\)和\\(v_c\\)的点积之和，起到归一化作用。\n题外话：讲这张幻灯片时，还提到softmax的一个形象解释。softmax包括max和soft两层含义。假设对于一个数组[1,2,3,4]，直接max也就是hard max的结果是保留最大值，其他全变为0，即[0,0,0,4]。但是softmax对他们求\\(\\frac{exp(x_i)}{\\sum_{j=1}^nexp(x_j)}\\)，变成了[0.03, 0.09, 0.24, 0.64]，最大的还是第4个数，但第四个数的优势被放大了，原来4只是1的4倍，现在0.64是0.03的21倍。所以softmax不但保留了max的属性，还变得更soft了，原来小的数不会被抹为0，只不过拉大了差异。\n使用梯度下降还需要求解\\(P\\)对参数\\(\\theta\\)的梯度，在这里\\(\\theta\\)代表了所有词的中心词向量和临近词向量。对于上式，\\(u_o\\)、\\(v_c\\)等就是\\(\\theta\\)的一部分。不断利用求导的链式法则，容易得到：\n$$\\begin{eqnarray}\\frac{\\partial P(o|c)}{\\partial v_c}=u_o-\\sum_{w\\in V}P(w|c)u_w.\\tag{1}\\end{eqnarray}$$最后算出来的梯度很有意思，\\(u_o\\)表示观察到的上下文词向量（o表示observed），减号后面的是这个位置的期望的词向量，期望=概率*值。差值就是真实观察词向量减去期望词向量，这就是梯度。当它们的差值很小时，说明给定\\(c\\)能很好的预测其临近词的概率分布。\nOK，当以上内容都准备妥当之后，我们就可以开始求解词向量了。首先随机初始化每个词\\(w\\)的中心词向量\\(v_w\\)和临近词向量\\(u_w\\)；然后求解-log损失函数\\(J(\\theta)\\)；最后根据梯度下降更新所有参数\\(\\theta\\)。\n上述word2vec算法简单，直观，但写代码实现比较复杂。在实际应用场景中，人们往往使用神经网络的方法来求解词向量，具体教程请看这里： http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ 。\n我们把训练词向量的问题转换为端到端的文本分类问题。如下图所示，对于语料库中的一个句子，假设临近窗口为前后两个词，则可以抽取出如下图右边所示的训练样本，每个训练样本是一个（中心词，临近词）的pair。比如对于（the, quick）训练样本，我们希望神经网络在输入the这个中心词时，能以较高的概率预测出quick这个词。\n网络的结构如下图所示，也非常简单，是仅含一个隐藏层的全连接网络。比如上图的一组训练数据是（the, quick），表示输入是the的one-hot编码，输出是quick的one-hot编码。假设词典里有10,000个不同的词，则one-hot编码长度为10,000。有一个隐藏层的全连接网络，对应权重构成两个权重矩阵，和输入层连接的矩阵为\\(V\\)，其每一行表示词作为中心词时的词向量。输入行向量乘以\\(V\\)正好得到输入词的词向量，这对应课上讲的作为中心词的词向量\\(v_c\\)。\n隐层和输出层连接的权重矩阵为\\(U\\)，其每一列表示输出层的词的临近词词向量。隐层的行向量\\(v_c\\)乘以矩阵\\(U\\)，得到词\\(c\\)的临近词的概率分布，再经过softmax激活，进行归一化。其实反过来看，从输出往隐层看，相当于输出层的行向量乘以\\(U\\)的转置，得到隐层词向量。这其实就是另一种训练词向量的方法CBOW，即英语完形填空，用临近词来预测中心词。\n对于下图的神经网络，输出用softmax激活，损失函数使用-log损失，训练网络时使用梯度下降，其效果正好是课上讲的使用极大似然估计的方法！\n另一方面，上图的这种结构是skip-gram模型，如果把对应的箭头反一下，输入变输出，输出变输入，其实就变成了CBOW模型了。\n上述全连接网络虽然能很方便的计算词向量，但存在两个问题：1. 网络过于庞大，参数量太多；2. 训练样本太多，每个样本都更新所有参数，训练速度慢。针对这两个问题，作者分别提出了 subsampling 和 negative sampling 的技巧，具体请看教程： http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ 。\n第一个问题，网络参数量太多。假设有1w个特异的词，词向量长度为300，整个网络就有两个300w的矩阵（上图的V和U）参数需要优化。另一方面，训练语料库往往是很大的，随随便便就是成百上千万的文章，由此拆分得到的训练词组对就更大了，很容易到上亿的级别。几百万的参数，几亿的训练数据， 导致网络太过庞大，训练不动。\nsubsampling技巧是指，每个词有一个保留概率p，以这个概率p保留其在训练数据中，以1-p删掉这个词。比如上面的例子，删掉了fox，则fox对应的4个训练数据也一并删掉了，所以能减少较多的训练数据。对于词\\(w_i\\)，其概率\\(P(w_i)\\)公式如下，其中\\(z(w_i)\\)是词\\(w\\)的词频。概率p和这个词在语料库中的词频有关，词频越大，保留概率越低，即被删掉的概率越大，所以subsampling之后应该能较大的减少训练数据。\n$$\\begin{eqnarray}P(w_i) = (\\sqrt{\\frac{z(w_i)}{0.001}} + 1) \\cdot \\frac{0.001}{z(w_i)} .\\tag{2}\\end{eqnarray}$$\n第二个问题，原来的网络在训练时，对于每个输入中心词，都会对两个很大的参数矩阵V和U（和上面假设一样，300w）进行轻微的更新，更新操作太多了。\nnegative sampling技巧，只更新一小部分参数。比如对于(“fox”, “quick”)，期望的输出是quick的one-hot，即只有quick对应位为1，其他为0。但网络的softmax输出肯定不可能是完完全全的quick为1，其他为0；有可能是quick为0.8，其他位有些0.001，0.03之类的非0值，这就导致输出层的所有神经元都有误差。按照传统的方法，输出层所有神经元对应的U矩阵的权重都要更新。negative sampling技巧是，只更新和quick连接的U权重以及随机选5个输出神经元的连接权重进行更新，这样一下把需要更新的U权重个数从300w降到了6*300=1800，只需要更新0.06%的参数，大大减小了参数更新量！\n5个随机选中的神经元（输出位置，即对应1w维的某个词）被称为negative sample，被选中的概率和词频成正比，词频越大的词被选中的概率越大，和上面subsampling类似。概率公式如下，其中\\(f(w_i)\\)应该和(2)中的\\(z(w_i)\\)一样，都表示词频。\n$$\\begin{eqnarray}P(w_i) = \\frac{ {f(w_i)}^{3/4} }{\\sum_{j=0}^{n}\\left( {f(w_j)}^{3/4} \\right) }.\\tag{3}\\end{eqnarray}$$ 作业请见GitHub： https://github.com/01joy/stanford-cs224n-winter-2019/blob/master/1.8/assignment/a1/exploring_word_vectors.ipynb\n复习SVD分解请看： https://blog.csdn.net/u010099080/article/details/68060274 以及 https://www.zhihu.com/question/34143886/answer/131046490\n","permalink":"http://localhost:1313/posts/2019-06-22-cs224n-1-8-introduction-and-word-vectors/","summary":"\u003cp\u003e今天开始介绍大名鼎鼎的NLP网课Stanford-CS224N。第一讲内容为课程简介和词向量。\u003c/p\u003e\n\u003cp\u003e词向量即用来表示这个词的含义的向量。早期的NLP常用one-hot编码来表示词向量，假如词典中共有10000个词，则这个one-hot向量长度就是10000，该词在词典中所处位置对应的值为1，其他值为0。\u003c/p\u003e\n\u003cp\u003eone-hot表示方法虽然简单，但其有诸多缺点：1. 词典中的词是不断增多的，比如英语，通过对原有的词增加前缀和后缀，可以变换出很多不同的词，one-hot编码会导致向量维度非常大，且每个向量是稀疏的；2. 不同词的one-hot编码向量是垂直的，在向量空间中无法表示近似关系，即使两个含义相近的词，它们的词向量点积也为0。\u003c/p\u003e\n\u003cp\u003e既然one-hot编码有这么多缺点，那我们就换一种编码，one-hot是高维稀疏向量，那新的编码就改用低维稠密向量，这样就解决了上述问题，那么怎样得到一个词的低维稠密的词向量呢？这就是word2vec算法。\u003c/p\u003e\n\u003cp\u003eword2vec采用了分布式语义的方法来表示一个词的含义。本质上，一个词的含义就是这个词所处的上下文语境。回想一下我们高中做英语完形填空时，一篇短文，挖了好多空，让我们根据空缺词的上下文语境选择合适的词。也就是说上下文语境已经能够确定这个词的含义了，如果选词正确，也就意味着我们理解了这个空缺词的含义。\u003c/p\u003e\n\u003cp\u003eword2vec算法发表于2013年，包括两种训练算法Skip-grams (SG)和Continuous Bag of Words (CBOW)，这两种方法很类似，其中CBOW和上述介绍到的英语完形填空几乎是一样的，由上下文词预测中心词；而SG则和CBOW正好相反，由中心词预测上下文词，本文主要介绍SG算法。\u003c/p\u003e\n\u003cp\u003e给定一个语料库，这个语料库包含了很多文章，每篇文章又包含很多句子，每个句子又包含很多词语。所以一个语料库是一个天然的标注集，因为对于每一个选定的中心词，我们都知道其临近的词是什么。这样一个（中心词，临近词）对就构成了一个标注集。SG算法的中心思想就是对于每个选定的中心词，尽量准确的预测其周围可能出现的词的概率分布。具体来说，SG算法首先随机初始化每个词的词向量；然后预测不同临近词出现的概率，最后最大化实际临近词出现的概率。\u003c/p\u003e\n\u003cp\u003e形式化来说，就是用极大似然估计的方法，求解每个词的词向量。其目标函数如下，其中\\(\\theta\\)是待求解的参数；\\(t\\)为选定的中心词位置；对于每个\\(t\\)（外层\\(\\prod\\)），估计其邻域\\(\\pm m\\)个词出现的概率（内层\\(\\prod\\)）。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-22-cs224n-1-8-introduction-and-word-vectors/word2vec_objective_function.png\"\u003e\u003c/p\u003e\n\u003cp\u003e求解极大似然估计的方法比较成熟，一般先把极大似然转换为最小化-log似然，然后用梯度下降求解。所以核心问题就变成了如何求解\\(P(w_{t+j}|w_t;\\theta)\\)。\u003c/p\u003e\n\u003cp\u003e对于每个词\\(w\\)，定义其两个词向量：\\(v_w\\)表示当\\(w\\)为中心词时的词向量，\\(u_w\\)表示当\\(w\\)为其他词的临近词时的词向量。则对于一个中心词\\(c\\)和其临近词\\(o\\)，有：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-22-cs224n-1-8-introduction-and-word-vectors/word2vec_prediction_function.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上式本质是一个softmax函数，因为给定\\(c\\)，\\(o\\)相当于是标注结果，所以把它们的点积作为分子，希望分子越大越好；而分母则是所有可能的\\(u_w\\)和\\(v_c\\)的点积之和，起到归一化作用。\u003c/p\u003e\n\u003cp\u003e题外话：讲这张幻灯片时，还提到softmax的一个形象解释。softmax包括max和soft两层含义。假设对于一个数组[1,2,3,4]，直接max也就是hard max的结果是保留最大值，其他全变为0，即[0,0,0,4]。但是softmax对他们求\\(\\frac{exp(x_i)}{\\sum_{j=1}^nexp(x_j)}\\)，变成了[0.03, 0.09, 0.24, 0.64]，最大的还是第4个数，但第四个数的优势被放大了，原来4只是1的4倍，现在0.64是0.03的21倍。所以softmax不但保留了max的属性，还变得更soft了，原来小的数不会被抹为0，只不过拉大了差异。\u003c/p\u003e\n\u003cp\u003e使用梯度下降还需要求解\\(P\\)对参数\\(\\theta\\)的梯度，在这里\\(\\theta\\)代表了所有词的中心词向量和临近词向量。对于上式，\\(u_o\\)、\\(v_c\\)等就是\\(\\theta\\)的一部分。不断利用求导的链式法则，容易得到：\u003c/p\u003e\n$$\\begin{eqnarray}\\frac{\\partial P(o|c)}{\\partial v_c}=u_o-\\sum_{w\\in V}P(w|c)u_w.\\tag{1}\\end{eqnarray}$$\u003cp\u003e最后算出来的梯度很有意思，\\(u_o\\)表示观察到的上下文词向量（o表示observed），减号后面的是这个位置的期望的词向量，期望=概率*值。差值就是真实观察词向量减去期望词向量，这就是梯度。当它们的差值很小时，说明给定\\(c\\)能很好的预测其临近词的概率分布。\u003c/p\u003e\n\u003cp\u003eOK，当以上内容都准备妥当之后，我们就可以开始求解词向量了。首先随机初始化每个词\\(w\\)的中心词向量\\(v_w\\)和临近词向量\\(u_w\\)；然后求解-log损失函数\\(J(\\theta)\\)；最后根据梯度下降更新所有参数\\(\\theta\\)。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e上述word2vec算法简单，直观，但写代码实现比较复杂。在实际应用场景中，人们往往使用神经网络的方法来求解词向量，具体教程请看这里： \u003ca href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\"\u003ehttp://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\u003c/a\u003e 。\u003c/p\u003e\n\u003cp\u003e我们把训练词向量的问题转换为端到端的文本分类问题。如下图所示，对于语料库中的一个句子，假设临近窗口为前后两个词，则可以抽取出如下图右边所示的训练样本，每个训练样本是一个（中心词，临近词）的pair。比如对于（the, quick）训练样本，我们希望神经网络在输入the这个中心词时，能以较高的概率预测出quick这个词。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-22-cs224n-1-8-introduction-and-word-vectors/training_data.png\"\u003e\u003c/p\u003e\n\u003cp\u003e网络的结构如下图所示，也非常简单，是仅含一个隐藏层的全连接网络。比如上图的一组训练数据是（the, quick），表示输入是the的one-hot编码，输出是quick的one-hot编码。假设词典里有10,000个不同的词，则one-hot编码长度为10,000。有一个隐藏层的全连接网络，对应权重构成两个权重矩阵，和输入层连接的矩阵为\\(V\\)，其每一行表示词作为中心词时的词向量。输入行向量乘以\\(V\\)正好得到输入词的词向量，这对应课上讲的作为中心词的词向量\\(v_c\\)。\u003c/p\u003e\n\u003cp\u003e隐层和输出层连接的权重矩阵为\\(U\\)，其每一列表示输出层的词的临近词词向量。隐层的行向量\\(v_c\\)乘以矩阵\\(U\\)，得到词\\(c\\)的临近词的概率分布，再经过softmax激活，进行归一化。其实反过来看，从输出往隐层看，相当于输出层的行向量乘以\\(U\\)的转置，得到隐层词向量。这其实就是另一种训练词向量的方法CBOW，即英语完形填空，用临近词来预测中心词。\u003c/p\u003e\n\u003cp\u003e对于下图的神经网络，输出用softmax激活，损失函数使用-log损失，训练网络时使用梯度下降，其效果正好是课上讲的使用极大似然估计的方法！\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-22-cs224n-1-8-introduction-and-word-vectors/skip_gram_net_arch2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e另一方面，上图的这种结构是skip-gram模型，如果把对应的箭头反一下，输入变输出，输出变输入，其实就变成了CBOW模型了。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e上述全连接网络虽然能很方便的计算词向量，但存在两个问题：1. 网络过于庞大，参数量太多；2. 训练样本太多，每个样本都更新所有参数，训练速度慢。针对这两个问题，作者分别提出了 subsampling 和 negative sampling 的技巧，具体请看教程：\n\u003ca href=\"http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\"\u003ehttp://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\u003c/a\u003e 。\u003c/p\u003e\n\u003cp\u003e第一个问题，网络参数量太多。假设有1w个特异的词，词向量长度为300，整个网络就有两个300w的矩阵（上图的V和U）参数需要优化。另一方面，训练语料库往往是很大的，随随便便就是成百上千万的文章，由此拆分得到的训练词组对就更大了，很容易到上亿的级别。几百万的参数，几亿的训练数据， 导致网络太过庞大，训练不动。\u003c/p\u003e\n\u003cp\u003esubsampling技巧是指，每个词有一个保留概率p，以这个概率p保留其在训练数据中，以1-p删掉这个词。比如上面的例子，删掉了fox，则fox对应的4个训练数据也一并删掉了，所以能减少较多的训练数据。对于词\\(w_i\\)，其概率\\(P(w_i)\\)公式如下，其中\\(z(w_i)\\)是词\\(w\\)的词频。概率p和这个词在语料库中的词频有关，词频越大，保留概率越低，即被删掉的概率越大，所以subsampling之后应该能较大的减少训练数据。\u003c/p\u003e\n$$\\begin{eqnarray}P(w_i) = (\\sqrt{\\frac{z(w_i)}{0.001}} + 1) \\cdot \\frac{0.001}{z(w_i)} .\\tag{2}\\end{eqnarray}$$\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/mccormickml.com/assets/word2vec/subsample_func_plot.png\"\u003e\u003c/p\u003e\n\u003cp\u003e第二个问题，原来的网络在训练时，对于每个输入中心词，都会对两个很大的参数矩阵V和U（和上面假设一样，300w）进行轻微的更新，更新操作太多了。\u003c/p\u003e\n\u003cp\u003enegative sampling技巧，只更新一小部分参数。比如对于(“fox”, “quick”)，期望的输出是quick的one-hot，即只有quick对应位为1，其他为0。但网络的softmax输出肯定不可能是完完全全的quick为1，其他为0；有可能是quick为0.8，其他位有些0.001，0.03之类的非0值，这就导致输出层的所有神经元都有误差。按照传统的方法，输出层所有神经元对应的U矩阵的权重都要更新。negative sampling技巧是，只更新和quick连接的U权重以及随机选5个输出神经元的连接权重进行更新，这样一下把需要更新的U权重个数从300w降到了6*300=1800，只需要更新0.06%的参数，大大减小了参数更新量！\u003c/p\u003e\n\u003cp\u003e5个随机选中的神经元（输出位置，即对应1w维的某个词）被称为negative sample，被选中的概率和词频成正比，词频越大的词被选中的概率越大，和上面subsampling类似。概率公式如下，其中\\(f(w_i)\\)应该和(2)中的\\(z(w_i)\\)一样，都表示词频。\u003c/p\u003e\n$$\\begin{eqnarray}P(w_i) = \\frac{ {f(w_i)}^{3/4} }{\\sum_{j=0}^{n}\\left( {f(w_j)}^{3/4} \\right) }.\\tag{3}\\end{eqnarray}$$\u003chr\u003e\n\u003cp\u003e作业请见GitHub：\n\u003ca href=\"https://github.com/01joy/stanford-cs224n-winter-2019/blob/master/1.8/assignment/a1/exploring_word_vectors.ipynb\"\u003ehttps://github.com/01joy/stanford-cs224n-winter-2019/blob/master/1.8/assignment/a1/exploring_word_vectors.ipynb\u003c/a\u003e\u003c/p\u003e","title":"CS224N（1.8）Introduction and Word Vectors"},{"content":"相信很多学CS的同学之前都没听说过“蛋白质结构预测”这个问题，直到2018年12月初，一则劲爆消息瞬间引爆了CSer的朋友圈，那就是Google Deepmind团队开发的AlphaFold一举拿下当年的CASP比赛冠军，而且远远甩开了第二名。我当时就转载过类似的公众号文章，大家可以阅读并想象当时朋友圈的欢呼声：阿尔法狗再下一城 | 蛋白结构预测AlphaFold大胜传统人类模型。\n当时，很多同学也转载过类似的文章，但其实很少有人真正明白“蛋白质结构预测”这个问题是什么，它的难度有多大，CASP是个什么比赛，以及AlphaFold的内部原理是什么。当然，对于这一连串的问题，我当时也是懵逼的。不过自己好歹也是个跟蛋白质有关的PhD，如此热点事件，自然是要关注的。不过之后一直没时间，直到今年相关顶级文章再次爆出，我就借着准备文献讲评的机会了解了相关的知识，在这里跟大家分享一下。\nhttps://upload.wikimedia.org/wikipedia/commons/a/a9/Protein_folding.png\n蛋白质结构分为四级，分别是一级结构、二级结构、三级结构和四级结构，下面分别描述。\n一级结构 蛋白质的一级结构可以理解为一条线性的字符串，比如MSFIKTFSGKHFYYDKINKDDIVINDIAVSLSNICR。其基本组成单元是一个个的氨基酸，即一个个的字母。氨基酸有单字母表示和三字母表示，为了简洁，本文使用单字母表示，下图的例子是三字母表示。常见的氨基酸只有20种，所以一级结构的字符串通常只包含20种字母，不包含的6种字母是BJOUXZ。\nhttp://oregonstate.edu/instruct/bb450/450material/schedule450s17e.html 本文大部分蛋白质基础知识都来源于此\n20种氨基酸的结构符合一个通式，如下图所示，中间的碳原子称为Cα碳原子，表示它处在α位；左边连了一个氨基-NH2，称为N端；右边连了一个羧基-COOH，称为C端。20种不同氨基酸的差别就在于Cα上连接的侧链基团R，具体的差别网上一搜就能查到。 https://upload.wikimedia.org/wikipedia/commons/c/ce/AminoAcidball.svg\n20种氨基酸连接的方式为脱水缩合，即一个氨基酸的羧基-COOH和另一个氨基酸的氨基-NH2反应，丢掉一个H2O，形成一个肽键-CO-NH-，如下图所示。丢掉了羧基和氨基的氨基酸被称为氨基酸残基，这个名词很形象，氨基酸缺胳膊少腿，所以变成了“残”基。 二级结构 二级结构就是在一级结构的字符串的基础上，肽链怎样进行盘旋、折叠等变换，形成一种局部的三维结构，这种局部的三维结构通常由氢键支撑。常见的二级结构有α螺旋和β折叠，如下图所示。其中α螺旋的每个残基的-NH的H和临近的第4个残基的-CO的O形成氢键，由此支撑α螺旋的结构稳定性，如下图的箭头所指虚线。β折叠则是两条肽链，平行排列，对应残基的-NH的H和-CO的O形成氢键，由此形成两股β折叠的结构，多股β折叠形成类似手风琴的样子。β折叠分为平行和反平行排列，我们前面介绍到肽段分为N端和C端，如果形成β折叠的两股链都是从N到C（或从C到N），则称为平行排列，否则是反平行排列。每股β折叠都有一个大箭头表示其方向。\n细分的话，蛋白质的二级结构总共有8种，包括转角、无规则卷曲等。目前常采用DSSP的分类方法，有些文献会把8种结构粗分为α螺旋、β折叠和转角这三种结构。\n由上图可知，蛋白质的二级结构极大的决定了其三级结构（下面介绍），所以有很多工作是研究怎样准确预测蛋白质的二级结构的，即预测每个氨基酸残基处于哪一种二级结构中。形式化表示就是，对于一个蛋白质一级结构字符串\\(A_1A_2A_3A_4A_5…\\)，输出\\(a_1a_2a_3a_4a_5…\\)，其中\\(a_i\\)∈{α螺旋，β折叠，转角}。所以，蛋白质的二级结构是一个端到端的问题，很像机器翻译，目前很多文章都会用深度学习NLP的方法来预测蛋白质的二级结构。\n三级结构 简单理解，三级结构就是把多个二级结构拼接到一起，折叠成一个完整的蛋白质三维结构，如下图所示。维持蛋白质三级结构的力比较多样，除了氢键之外，还有二硫键、金属键等。\n四级结构 简单理解，四级结构就是多个三级结构分子组合成一个复合物，就是四级结构。\nhttps://en.wikipedia.org/wiki/Protein_quaternary_structure\n对于CSer来说，由于四级结构仅仅是多个三级结构组合到一起，我们常说的蛋白质三维结构预测问题，通常是指预测蛋白质的三级结构。问题是，构成蛋白质链的原子非常多，我们怎样形式化描述一条蛋白质的三维结构呢？这还要从最原始的一级结构说起。\n蛋白质结构预测问题 前面提到，两个氨基酸通过脱水缩合的方式形成肽键从而连接到一起形成一级结构（本文图四），肽键虽然是单键，但它具有类似双键的特点，即难以旋转（比如羧基中的-C=O键就是双键，无法旋转）。所以，由肽键及周围的6个原子形成了一个固定的肽键平面，这6个原子分别是-C-CO-NH-C-，如下图所示，箭头所指的红色键就是肽键，它周围画出了一个平面，就是肽键平面。\n肽键平面的存在极大的简化了蛋白质结构，可以认为这6个原子的相对位置是固定的了！另一方面，跟这个平面相连的左右两个C原子的两个键是单键，所以他们可以旋转，旋转的角度称为扭转角ϕ和ψ，为了更直观的感受肽链的肽键平面和两个扭转角，可以看下面的动画：K0045879-Rotation_around_amide_bonds_in_protein.mp4（来自https://www.sciencephoto.com/media/639617/view）\n事实上，扭转角ϕ和ψ并不是在360°范围内随机均匀分布的，1963年就有科学家统计过扭转角ϕ和ψ的分布，他们发现稳定的蛋白质结构的ϕ和ψ通常只分布在一小部分区域，如下图的拉氏图所示，这些区域正好对应了常见的α螺旋和β折叠的结构。\n最后，我们还需要介绍一个角度，那就是ω。前面提到，虽然肽键具有双键的特点，难以旋转，但它在少数情况下还是可以旋转的。假设通常情况下，肽键的角度定义为ω=0°，如下图所示，红色的键即为肽键，这种结构的好处是它能让形成肽键的两个残基的侧链R（图中黑色基团）离得尽量的远，这样能保持比较稳定的结构。如果肽键旋转为ω=180°，变为下图的样子，则两个侧链R很靠近，就产生位阻效应，就不稳定，所以这种情况比较少见。但不管怎么说，肽键的扭转角ω也是一个变量因素。\n综上所述，对于一条肽链，如果知道每个残基的三个扭转角ϕ、ψ和ω，则可以重构出肽链的主干部分的三维结构，这就像将极坐标转换为直角坐标一样容易。需要提醒的是，本文提到的蛋白质三维结构预测问题，对蛋白质的结构进行了简化，包括：1. 仅预测蛋白质或肽链的主干结构，不考虑侧链R的结构；2. 假设肽链主干中每个键的长度是固定的；3. 不考虑键的角度，比如对于上图的肽键，仅考虑肽键绕肽键轴本身的旋转，不考虑肽键绕着某一端原子的旋转，比如固定左边的蓝色小球，肽键和右边的红色小球旋转出平面了。 下图的肽键平面，详细的标识出了各个相对固定的值。\nFigure 8-1 from Fundamentals of Biochemistry\n所以，对于CSer来说，蛋白质的三维结构预测问题，就可以看成一个端到端的学习问题，输入是一个字符串，输出是每个字符（残基）对应的三个扭转角ϕ、ψ和ω，问题看起来非常的简洁漂亮。而且，这个问题和NLP中的序列标注、机器翻译等问题很像，所以很多NLP的技术可以用来预测蛋白质的三维结构。下图的插画就是最近发表在Cell Sytems上的一篇用LSTM预测蛋白质三维结构的文章，我会在下一篇博客中和大家分享这篇文章。\nhttps://www.sciencedirect.com/science/article/pii/S2405471219300766?via%3Dihub\n有关“蛋白质结构预测”本身的最后一个问题是，为什么能仅仅通过一级结构的序列信息，预测得到其三级结构呢？也就是说蛋白质结构预测这个问题是否可解，如果蛋白质的三级结构还由其他因素决定，那么即使Deeplearning玩出花了，在生物上也是不可行的。所以，每遇到一个新问题，都要自问一下，这个问题从原理上是否可解。对于“蛋白质结构预测”这个问题，最开始也有人进行了类似的自问，得到的答案是可行的：\n1965年，安芬森（Anfinsen）基于还原变性的牛胰RNase在不需其他任何物质帮助下，仅通过去除变性剂和还原剂就使其恢复天然结构的实验结果，提出了“多肽链的氨基酸序列包含了形成其热力学上稳定的天然构象所必需的全部信息”的“自组装学说”，随后这个学说又得到一些补充。这些学说表明：氨基酸序列确定其空间构象，从而为蛋白质结构预测提供了可行性。\nhttp://chinaxiv.org/user/download.htm?id=6478\nCASP比赛 提到蛋白质三级结构预测，不得不提的是CASP这个比赛。CASP的全称是The Critical Assessment of protein Structure Prediction (CASP)，即蛋白质结构预测的关键评估，被誉为蛋白质结构预测的奥林匹克竞赛。CASP从1994年开始举办，每两年一届，最近的一届是2018年的CASP13。\n每一届CASP比赛，都会提供大约100条未知结构的蛋白质序列，让所有参赛者进行结构预测，比赛结束之后，主办方会通过生化方法测定这些蛋白质的三维结构，然后和参赛者预测的结果进行比对，然后给出预测得分。提供的蛋白质序列分为两类：一类序列和PDB数据库中已有结构的序列有相似性，由此可以基于模板预测，准确度比较高，这类算法称为Template-Based Modeling；另一类序列和PDB库已知结构的序列相似度很低，可以认为是全新的蛋白质，因为无法利用已有模板信息，需要进行从头测序（De novo或ab initio或Free Modeling），目前的准确率比较低。参赛选手也分为两组，一组是servers only，即仅允许算法参赛，给定3天的时间；另一组是human and servers，即允许人和算法合作，共同预测蛋白质结构，给定3周的时间。\nCASP同时提供多种比赛项目，比如常规的结构预测（Regular targets）、数据辅助预测（Data-Assisted targets）和蛋白质接触面预测（Contact predictions）等，其中数据辅助预测中提供了核磁数据（NMR）、交联数据（XLMS）等，对的，交联数据就是我目前研究的pLink处理的数据。\nhttp://predictioncenter.org/casp13/index.cgi\nAlphaFold参加了CASP13的humans and servers组，第一次参赛就一鸣惊人，获得冠军并甩开第二名好多。\nhttp://predictioncenter.org/casp13/zscores_final.cgi\n在蛋白质结构预测领域，活跃着很多华人学者，比如密西根大学张阳团队在servers only组获得七连冠CASP7~CASP13，14年霸主地位！ （I-TASSER (as Zhang-Server) and QUARK from Zhang Lab）。大家可以围观一下张老师实验室的机房以及张老师在清华的一个访谈。另外，芝加哥大学的许锦波团队开发的RaptorX在每届的CASP上也取得了不错的成绩。国内方面，中科院计算所的卜东波老师和上海交大的沈红斌老师也有相关的研究。\nhttp://www.predictioncenter.org/casp13/zscores_final.cgi?model_type=first\u0026gr_type=server_only\nOK，以上就是有关“蛋白质结构预测”这个问题的入门介绍，由于本人并非研究这个领域，理解有误的地方还请大家指出。最后，在入门这个领域时，我查了很多资料，上面的图片也都来自网络并注明了出处，下面汇总列出供大家参考：\n蛋白质结构基础知识： 维基百科：https://en.wikipedia.org/wiki/Protein_structure 俄勒冈州立大学课程General Biochemistry BB 450/550：http://oregonstate.edu/instruct/bb450/450material/schedule450s17e.html csbsju课程Biochemistry Online：http://employees.csbsju.edu/hjakubowski/classes/ch331/protstructure/olunderstandconfo.html 蛋白质：http://refer.biovip.com/doc-view-334.html 肽键平面动画： https://www.sciencephoto.com/media/639617/view https://employees.csbsju.edu/hjakubowski/classes/ch331/protstructure/pp180to0.gif https://employees.csbsju.edu/hjakubowski/classes/ch331/protstructure/pp0to180.gif https://www.youtube.com/watch?v=Kewhg5spUjs https://www.youtube.com/watch?v=meNEUTn9Atg 蛋白质结构预测入门综述： 蛋白质二级结构预测方法的评价（唐一源，计算机与应用化学）：http://www.cnki.com.cn/Article/CJFDTotal-JSYH200306003.htm 蛋白质结构预测（张阳，物理学报）：http://www.cnki.com.cn/Article/CJFDTOTAL-WLXB201617012.htm 蛋白质三级结构预测算法综述（卜东波，计算机学报）：http://www.cnki.com.cn/Article/CJFDTOTAL-JSJX201804002.htm 基于深度学习的八类蛋白质二级结构预测算法（杨伟，计算机应用）：http://www.cnki.com.cn/Article/CJFDTOTAL-JSJY201705054.htm 蛋白质结构预测：梦想与现实（卜东波，信息技术快报）：http://chinaxiv.org/user/download.htm?id=6478 蛋白质结构预测团队： DeepMind：https://deepmind.com/blog/alphafold/ 密西根大学张阳团队：https://zhanglab.ccmb.med.umich.edu/ 芝加哥大学许锦波团队：https://ttic.uchicago.edu/~jinbo/ 哈佛医学院Mohammed AlQuraishi：https://scholar.harvard.edu/alquraishi 中科院计算所卜东波团队：http://bioinfo.ict.ac.cn/~dbu/ 上海交大沈红斌团队：http://www.csbio.sjtu.edu.cn/ ","permalink":"http://localhost:1313/posts/2019-05-25-introduction-to-protein-structure-prediction/","summary":"\u003cp\u003e相信很多学CS的同学之前都没听说过“蛋白质结构预测”这个问题，直到2018年12月初，一则劲爆消息瞬间引爆了CSer的朋友圈，那就是Google Deepmind团队开发的AlphaFold一举拿下当年的CASP比赛冠军，而且远远甩开了第二名。我当时就转载过类似的公众号文章，大家可以阅读并想象当时朋友圈的欢呼声：\u003ca href=\"https://mp.weixin.qq.com/s?__biz=MzUyOTcxNDA2MA==\u0026amp;mid=2247484072\u0026amp;idx=1\u0026amp;sn=4ced43b28439e193e2a88f402c81cb2f\u0026amp;chksm=fa5d9c4bcd2a155df9c5d7450b7d6f0424af39fcfc8634c2345ff0b01300192b4ea6c0565341\u0026amp;mpshare=1\u0026amp;scene=1\u0026amp;srcid=1203qx87iYulfkOp7jvZx5uJ\u0026amp;key=e93dd65619c9c8dbcde90e46cec14857b94dda95e137fa444083b63060f0711ed2b7bd7e3b247f8c8ab36b68388bad7974855e5725ecc112fdd999179387a498599090c13da887f08290f51de8576a8d\u0026amp;ascene=1\u0026amp;uin=MTgzOTAyODQw\u0026amp;devicetype=Windows\u0026#43;10\u0026amp;version=62060739\u0026amp;lang=zh_CN\u0026amp;pass_ticket=HhfIKFFBQhij05w1DGJ%2BS23fCqJLztSSaonfMvhvStM%3D\"\u003e阿尔法狗再下一城 | 蛋白结构预测AlphaFold大胜传统人类模型\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e当时，很多同学也转载过类似的文章，但其实很少有人真正明白“蛋白质结构预测”这个问题是什么，它的难度有多大，CASP是个什么比赛，以及AlphaFold的内部原理是什么。当然，对于这一连串的问题，我当时也是懵逼的。不过自己好歹也是个跟蛋白质有关的PhD，如此热点事件，自然是要关注的。不过之后一直没时间，直到今年相关顶级文章再次爆出，我就借着准备文献讲评的机会了解了相关的知识，在这里跟大家分享一下。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/a/a9/Protein_folding.png\"\u003e\n\u003ca href=\"https://upload.wikimedia.org/wikipedia/commons/a/a9/Protein_folding.png\"\u003ehttps://upload.wikimedia.org/wikipedia/commons/a/a9/Protein_folding.png\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e蛋白质结构分为四级，分别是一级结构、二级结构、三级结构和四级结构，下面分别描述。\u003c/p\u003e\n\u003ch1 id=\"一级结构\"\u003e一级结构\u003c/h1\u003e\n\u003cp\u003e蛋白质的一级结构可以理解为一条线性的字符串，比如MSFIKTFSGKHFYYDKINKDDIVINDIAVSLSNICR。其基本组成单元是一个个的氨基酸，即一个个的字母。氨基酸有单字母表示和三字母表示，为了简洁，本文使用单字母表示，下图的例子是三字母表示。常见的氨基酸只有20种，所以一级结构的字符串通常只包含20种字母，不包含的6种字母是BJOUXZ。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/PrimaryProteinStructure.png\"\u003e\n\u003ca href=\"http://oregonstate.edu/instruct/bb450/450material/schedule450s17e.html\"\u003ehttp://oregonstate.edu/instruct/bb450/450material/schedule450s17e.html\u003c/a\u003e 本文大部分蛋白质基础知识都来源于此\u003c/p\u003e\n\u003cp\u003e20种氨基酸的结构符合一个通式，如下图所示，中间的碳原子称为Cα碳原子，表示它处在α位；左边连了一个氨基-NH2，称为N端；右边连了一个羧基-COOH，称为C端。20种不同氨基酸的差别就在于Cα上连接的侧链基团R，具体的差别网上一搜就能查到。\n\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/c/ce/AminoAcidball.svg\"\u003e\n\u003ca href=\"https://upload.wikimedia.org/wikipedia/commons/c/ce/AminoAcidball.svg\"\u003ehttps://upload.wikimedia.org/wikipedia/commons/c/ce/AminoAcidball.svg\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e20种氨基酸连接的方式为脱水缩合，即一个氨基酸的羧基-COOH和另一个氨基酸的氨基-NH2反应，丢掉一个H2O，形成一个肽键-CO-NH-，如下图所示。丢掉了羧基和氨基的氨基酸被称为氨基酸残基，这个名词很形象，氨基酸缺胳膊少腿，所以变成了“残”基。\n\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/PeptideBonds.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"二级结构\"\u003e二级结构\u003c/h1\u003e\n\u003cp\u003e二级结构就是在一级结构的字符串的基础上，肽链怎样进行盘旋、折叠等变换，形成一种\u003cstrong\u003e局部\u003c/strong\u003e的三维结构，这种局部的三维结构通常由氢键支撑。常见的二级结构有α螺旋和β折叠，如下图所示。其中α螺旋的每个残基的-NH的H和临近的第4个残基的-CO的O形成氢键，由此支撑α螺旋的结构稳定性，如下图的箭头所指虚线。β折叠则是两条肽链，平行排列，对应残基的-NH的H和-CO的O形成氢键，由此形成两股β折叠的结构，多股β折叠形成类似手风琴的样子。β折叠分为平行和反平行排列，我们前面介绍到肽段分为N端和C端，如果形成β折叠的两股链都是从N到C（或从C到N），则称为平行排列，否则是反平行排列。每股β折叠都有一个大箭头表示其方向。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/AlphaHelix.png\"\u003e\n\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/BetaSheets.png\"\u003e\u003c/p\u003e\n\u003cp\u003e细分的话，蛋白质的二级结构总共有8种，包括转角、无规则卷曲等。目前常采用\u003ca href=\"https://en.wikipedia.org/wiki/Protein_secondary_structure#DSSP_classification\"\u003eDSSP的分类方法\u003c/a\u003e，有些文献会把8种结构粗分为α螺旋、β折叠和转角这三种结构。\u003c/p\u003e\n\u003cp\u003e由上图可知，蛋白质的二级结构极大的决定了其三级结构（下面介绍），所以有很多工作是研究怎样准确预测蛋白质的二级结构的，即预测每个氨基酸残基处于哪一种二级结构中。形式化表示就是，对于一个蛋白质一级结构字符串\\(A_1A_2A_3A_4A_5…\\)，输出\\(a_1a_2a_3a_4a_5…\\)，其中\\(a_i\\)∈{α螺旋，β折叠，转角}。所以，蛋白质的二级结构是一个端到端的问题，很像机器翻译，目前很多文章都会用深度学习NLP的方法来预测蛋白质的二级结构。\u003c/p\u003e\n\u003ch1 id=\"三级结构\"\u003e三级结构\u003c/h1\u003e\n\u003cp\u003e简单理解，三级结构就是把多个二级结构拼接到一起，折叠成一个完整的蛋白质三维结构，如下图所示。维持蛋白质三级结构的力比较多样，除了氢键之外，还有二硫键、金属键等。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/TertiaryStructure.png\"\u003e\n\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/ForcesStabilizingTertiaryStructure.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"四级结构\"\u003e四级结构\u003c/h1\u003e\n\u003cp\u003e简单理解，四级结构就是多个三级结构分子组合成一个复合物，就是四级结构。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/ProteinQuaternaryStructure.png\"\u003e\n\u003ca href=\"https://en.wikipedia.org/wiki/Protein_quaternary_structure\"\u003ehttps://en.wikipedia.org/wiki/Protein_quaternary_structure\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e对于CSer来说，由于四级结构仅仅是多个三级结构组合到一起，我们常说的蛋白质三维结构预测问题，通常是指预测蛋白质的三级结构。问题是，构成蛋白质链的原子非常多，我们怎样形式化描述一条蛋白质的三维结构呢？这还要从最原始的一级结构说起。\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"蛋白质结构预测问题\"\u003e蛋白质结构预测问题\u003c/h1\u003e\n\u003cp\u003e前面提到，两个氨基酸通过脱水缩合的方式形成肽键从而连接到一起形成一级结构（本文图四），肽键虽然是单键，但它具有类似双键的特点，即难以旋转（比如羧基中的-C=O键就是双键，无法旋转）。所以，由肽键及周围的6个原子形成了一个固定的肽键平面，这6个原子分别是-C-CO-NH-C-，如下图所示，箭头所指的红色键就是肽键，它周围画出了一个平面，就是肽键平面。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/MultiplePeptideBondPlanes.png\"\u003e\n\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/PhiandPsiAngles.png\"\u003e\u003c/p\u003e\n\u003cp\u003e肽键平面的存在极大的简化了蛋白质结构，可以认为这6个原子的相对位置是固定的了！另一方面，跟这个平面相连的左右两个C原子的两个键是单键，所以他们可以旋转，旋转的角度称为扭转角ϕ和ψ，为了更直观的感受肽链的肽键平面和两个扭转角，可以看下面的动画：\u003ca href=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/K0045879-Rotation_around_amide_bonds_in_protein.mp4\"\u003eK0045879-Rotation_around_amide_bonds_in_protein.mp4\u003c/a\u003e（来自\u003ca href=\"https://www.sciencephoto.com/media/639617/view\"\u003ehttps://www.sciencephoto.com/media/639617/view\u003c/a\u003e）\u003c/p\u003e\n\u003cp\u003e事实上，扭转角ϕ和ψ并不是在360°范围内随机均匀分布的，1963年就有科学家统计过扭转角ϕ和ψ的分布，他们发现稳定的蛋白质结构的ϕ和ψ通常只分布在一小部分区域，如下图的拉氏图所示，这些区域正好对应了常见的α螺旋和β折叠的结构。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/RamachandranPlotLabeled.png\"\u003e\u003c/p\u003e\n\u003cp\u003e最后，我们还需要介绍一个角度，那就是ω。前面提到，虽然肽键具有双键的特点，难以旋转，但它在少数情况下还是可以旋转的。假设通常情况下，肽键的角度定义为ω=0°，如下图所示，红色的键即为肽键，这种结构的好处是它能让形成肽键的两个残基的侧链R（图中黑色基团）离得尽量的远，这样能保持比较稳定的结构。如果肽键旋转为ω=180°，变为下图的样子，则两个侧链R很靠近，就产生\u003ca href=\"https://baike.baidu.com/item/%E7%A9%BA%E9%97%B4%E4%BD%8D%E9%98%BB%E6%95%88%E5%BA%94\"\u003e位阻效应\u003c/a\u003e，就不稳定，所以这种情况比较少见。但不管怎么说，肽键的扭转角ω也是一个变量因素。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/StericHindrance.png\"\u003e\u003c/p\u003e\n\u003cp\u003e综上所述，对于一条肽链，如果知道每个残基的三个扭转角ϕ、ψ和ω，则可以重构出肽链的主干部分的三维结构，这就像将极坐标转换为直角坐标一样容易。需要提醒的是，本文提到的蛋白质三维结构预测问题，对蛋白质的结构进行了简化，包括：1. 仅预测蛋白质或肽链的主干结构，不考虑侧链R的结构；2. 假设肽链主干中每个键的长度是固定的；3. 不考虑键的角度，比如对于上图的肽键，仅考虑肽键绕肽键轴本身的旋转，不考虑肽键绕着某一端原子的旋转，比如固定左边的蓝色小球，肽键和右边的红色小球旋转出平面了。 下图的肽键平面，详细的标识出了各个相对固定的值。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://github.com/01joy/bitjoy.net/raw/master/blog/2019/05/AmidePlane.jpg\"\u003e\nFigure 8-1 from Fundamentals of Biochemistry\u003c/p\u003e\n\u003cp\u003e所以，对于CSer来说，蛋白质的三维结构预测问题，就可以看成一个端到端的学习问题，输入是一个字符串，输出是每个字符（残基）对应的三个扭转角ϕ、ψ和ω，问题看起来非常的简洁漂亮。而且，这个问题和NLP中的序列标注、机器翻译等问题很像，所以很多NLP的技术可以用来预测蛋白质的三维结构。下图的插画就是最近发表在Cell Sytems上的一篇用LSTM预测蛋白质三维结构的文章，我会在下一篇博客中和大家分享这篇文章。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/ProteinStructurePrediction.png\"\u003e\n\u003ca href=\"https://www.sciencedirect.com/science/article/pii/S2405471219300766?via%3Dihub\"\u003ehttps://www.sciencedirect.com/science/article/pii/S2405471219300766?via%3Dihub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e有关“蛋白质结构预测”本身的最后一个问题是，为什么能仅仅通过一级结构的序列信息，预测得到其三级结构呢？也就是说蛋白质结构预测这个问题是否可解，如果蛋白质的三级结构还由其他因素决定，那么即使Deeplearning玩出花了，在生物上也是不可行的。所以，每遇到一个新问题，都要自问一下，这个问题从原理上是否可解。对于“蛋白质结构预测”这个问题，最开始也有人进行了类似的自问，得到的答案是可行的：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e1965年，安芬森（Anfinsen）基于还原变性的牛胰RNase在不需其他任何物质帮助下，仅通过去除变性剂和还原剂就使其恢复天然结构的实验结果，提出了“多肽链的氨基酸序列包含了形成其热力学上稳定的天然构象所必需的全部信息”的“自组装学说”，随后这个学说又得到一些补充。这些学说表明：氨基酸序列确定其空间构象，从而为蛋白质结构预测提供了可行性。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"http://chinaxiv.org/user/download.htm?id=6478\"\u003ehttp://chinaxiv.org/user/download.htm?id=6478\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"casp比赛\"\u003eCASP比赛\u003c/h1\u003e\n\u003cp\u003e提到蛋白质三级结构预测，不得不提的是CASP这个比赛。CASP的全称是The Critical Assessment of protein Structure Prediction (CASP)，即蛋白质结构预测的关键评估，被誉为蛋白质结构预测的奥林匹克竞赛。CASP从1994年开始举办，每两年一届，最近的一届是2018年的CASP13。\u003c/p\u003e\n\u003cp\u003e每一届CASP比赛，都会提供大约100条未知结构的蛋白质序列，让所有参赛者进行结构预测，比赛结束之后，主办方会通过生化方法测定这些蛋白质的三维结构，然后和参赛者预测的结果进行比对，然后给出预测得分。提供的蛋白质序列分为两类：一类序列和PDB数据库中已有结构的序列有相似性，由此可以基于模板预测，准确度比较高，这类算法称为Template-Based Modeling；另一类序列和PDB库已知结构的序列相似度很低，可以认为是全新的蛋白质，因为无法利用已有模板信息，需要进行从头测序（De novo或ab initio或Free Modeling），目前的准确率比较低。参赛选手也分为两组，一组是servers only，即仅允许算法参赛，给定3天的时间；另一组是human and servers，即允许人和算法合作，共同预测蛋白质结构，给定3周的时间。\u003c/p\u003e\n\u003cp\u003eCASP同时提供多种比赛项目，比如常规的结构预测（Regular targets）、数据辅助预测（Data-Assisted targets）和蛋白质接触面预测（Contact predictions）等，其中数据辅助预测中提供了核磁数据（NMR）、交联数据（XLMS）等，对的，交联数据就是我目前研究的pLink处理的数据。\u003c/p\u003e","title":"“蛋白质结构预测”问题描述"},{"content":"由于本书成书较早（2015），作者当时使用的是Theano，但Theano已不再维护，所以本博客使用当下流行的Pytorch框架讲解MNIST图片分类的代码实现，具体就是Pytorch官方给出的MNIST代码：https://github.com/pytorch/examples/tree/master/mnist。\n使用该工具在线制作：http://alexlenail.me/NN-SVG/LeNet.html\n下面，我首先贴出经过我注释的Pytorch MNIST代码，然后对一些关键问题进行解释。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 from __future__ import print_function import argparse import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms # 所有网络类要继承nn.Module class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 调用父类构造函数 self.conv1 = nn.Conv2d(1, 20, 5, 1) # (in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) self.conv2 = nn.Conv2d(20, 50, 5, 1) # 这一层的in_channels正好是上一层的out_channels self.fc1 = nn.Linear(4*4*50, 500) self.fc2 = nn.Linear(500, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 2) # kernel_size=2, stride=2，pooling之后的大小除以2 x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 2) x = x.view(-1, 4*4*50) # 展开成 (z, 4*4*50)，其中z是通过自动推导得到的，所以这里设置为-1，这里相当于展开成行向量，便于后续全连接 x = F.relu(self.fc1(x)) x = self.fc2(x) return F.log_softmax(x, dim=1) # log_softmax 即 log(softmax(x))；dim=1对行进行softmax，因为上面x.view展开成行向量了，log_softmax速度和数值稳定性都比softmax好一些 def train(args, model, device, train_loader, optimizer, epoch): model.train() # 告诉pytorch，这是训练阶段 https://stackoverflow.com/a/51433411/2468587 for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() # 每个batch的梯度重新累加 output = model(data) loss = F.nll_loss(output, target) # 这里的nll_loss就是Michael Nielsen在ch3提到的log-likelihood cost function，配合softmax使用，batch的梯度/loss要求均值mean loss.backward() # 求loss对参数的梯度dw optimizer.step() # 梯度下降，w\u0026#39;=w-η*dw if batch_idx % args.log_interval == 0: print(\u0026#39;Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\u0026#39;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) def test(args, model, device, test_loader): model.eval() # 告诉pytorch，这是预测（评价）阶段 test_loss = 0 correct = 0 with torch.no_grad(): # 预测时不需要误差反传，https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/2 for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += F.nll_loss(output, target, reduction=\u0026#39;sum\u0026#39;).item() # sum up batch loss，预测时的loss求sum，L54再求均值 pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print(\u0026#39;\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\u0026#39;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) def plot1digit(data_loader): import numpy as np import matplotlib.pyplot as plt examples = enumerate(data_loader) batch_idx, (Xs, ys) = next(examples) # 读取到的是一个batch的所有数据 X=Xs[0].numpy()[0] # Xs[0]取出batch中的第一个数据，由tensor转换为numpy，因为pytorch tensor的格式是[channel, height, width]，所以最后[0]取出其第一个通道的[h,w] y=ys[0].numpy() # y没有通道，就一个标量值 np.savetxt(\u0026#39;../../../fig/%d.csv\u0026#39;%y, X, delimiter=\u0026#39;,\u0026#39;) plt.imshow(X, cmap=\u0026#39;Greys\u0026#39;) # or \u0026#39;Greys_r\u0026#39; plt.savefig(\u0026#39;../../../fig/%d.png\u0026#39;%y) plt.show() def main(): # Training settings parser = argparse.ArgumentParser(description=\u0026#39;PyTorch MNIST Example\u0026#39;) parser.add_argument(\u0026#39;--batch-size\u0026#39;, type=int, default=64, metavar=\u0026#39;N\u0026#39;, help=\u0026#39;input batch size for training (default: 64)\u0026#39;) parser.add_argument(\u0026#39;--test-batch-size\u0026#39;, type=int, default=1000, metavar=\u0026#39;N\u0026#39;, help=\u0026#39;input batch size for testing (default: 1000)\u0026#39;) parser.add_argument(\u0026#39;--epochs\u0026#39;, type=int, default=10, metavar=\u0026#39;N\u0026#39;, help=\u0026#39;number of epochs to train (default: 10)\u0026#39;) parser.add_argument(\u0026#39;--lr\u0026#39;, type=float, default=0.01, metavar=\u0026#39;LR\u0026#39;, help=\u0026#39;learning rate (default: 0.01)\u0026#39;) parser.add_argument(\u0026#39;--momentum\u0026#39;, type=float, default=0.5, metavar=\u0026#39;M\u0026#39;, help=\u0026#39;SGD momentum (default: 0.5)\u0026#39;) parser.add_argument(\u0026#39;--no-cuda\u0026#39;, action=\u0026#39;store_true\u0026#39;, default=False, help=\u0026#39;disables CUDA training\u0026#39;) parser.add_argument(\u0026#39;--seed\u0026#39;, type=int, default=1, metavar=\u0026#39;S\u0026#39;, help=\u0026#39;random seed (default: 1)\u0026#39;) parser.add_argument(\u0026#39;--log-interval\u0026#39;, type=int, default=10, metavar=\u0026#39;N\u0026#39;, help=\u0026#39;how many batches to wait before logging training status\u0026#39;) parser.add_argument(\u0026#39;--save-model\u0026#39;, action=\u0026#39;store_true\u0026#39;, default=False, help=\u0026#39;For Saving the current Model\u0026#39;) args = parser.parse_args() use_cuda = not args.no_cuda and torch.cuda.is_available() torch.manual_seed(args.seed) device = torch.device(\u0026#34;cuda\u0026#34; if use_cuda else \u0026#34;cpu\u0026#34;) kwargs = {\u0026#39;num_workers\u0026#39;: 1, \u0026#39;pin_memory\u0026#39;: True} if use_cuda else {} train_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;../data\u0026#39;, train=True, download=True, transform=transforms.Compose([ # https://discuss.pytorch.org/t/can-some-please-explain-how-the-transforms-work-and-why-normalize-the-data/2461/3 transforms.ToTensor(), # 把[0,255]的(H,W,C)的图片转换为[0,1]的(channel,height,width)的图片 transforms.Normalize((0.1307,), (0.3081,)) # 进行z-score标准化，这两个数分别是MNIST的均值和标准差 ])), batch_size=args.batch_size, shuffle=True, **kwargs) test_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;../data\u0026#39;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=args.test_batch_size, shuffle=True, **kwargs) # plot1digit(train_loader) model = Net().to(device) optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) for epoch in range(1, args.epochs + 1): train(args, model, device, train_loader, optimizer, epoch) test(args, model, device, test_loader) if (args.save_model): torch.save(model.state_dict(),\u0026#34;mnist_cnn.pt\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: main() 首先是MNIST数据格式的问题，在L108~L120，我们使用Pytorch的DataLoader载入了训练和测试数据，数据格式本质上和本系列博客的第一篇博客介绍的是一致的，即每张图片都是28*28的灰度图片，因为是灰度图片，所以只有一个通道数，默认格式是(H,W,C)，且值域范围是[0,255]。但上述代码对原始图片进行了两个变换，分别是ToTensor和Normalize。ToTensor将[0,255]的灰度图片(H,W,C)转换为[0,1]的灰度图片(C,H,W)，即Pytorch对2D图片的格式要求都是channel在前。所以经过这一转换，一张图片的shape是(1,28,28)，是一个三维矩阵；如果是彩色图片的话，有R,G,B三个通道，C=3。Normalize对图片数据进行z-score标准化，即减去均值再除以标准差；L112的两个值就是预先计算的MNIST数据集的均值和标准差。这些操作的好处是能让模型更加平稳快速收敛。同第一篇博客一样，我们可以把Pytorch格式的图片打印出来以便直观理解，L61的plot1digit函数就是这个作用。\nPytorch中自定义的网络类都要继承nn.Module这个基类（L10），自定义的网络类需要实现两个函数，分别是构造函数__init__和前向传播函数forward。在__init__中，定义需要用到的成员变量，往往是一些layer，比如上述代码中定义了两个卷积层和两个全连接层。在forward中，完成了实际的网络搭建过程，传入图片x，x流过整个网络，最后返回网络的输出。\n上述代码的网络结构如本博客开篇的图片所示，依次是：输入层、卷积层（包含ReLU激活）、池化层、卷积层（包含ReLU激活）、池化层、全连接层（包含ReLU激活）、全连接层（包含logsoftmax输出）。网络的构建过程很像搭积木，在forward函数和开篇的图片中都能很直观的看出来。\n需要稍微解释一下的是Conv2d类的初始化参数，前4个参数分别是in_channels, out_channels, kernel_size, stride，分别表示传入通道数、传出通道数、卷积核大小、卷积步长，对于第一个卷积核（L13），由于直接和输入层相连，MNIST图片是28*28的单通道图片，所以in_channels=1；out_channels的大小表示所用卷积核的数目，比如这里设置为20就表示有20个卷积核；kernel_size=5表示5*5的卷积核；stride=1表示卷积移动的步长为1。(1,28,28)的图片，经过上述卷积之后，得到的feature map大小变为了(20,24,24)，即有20个大小为24*24的feature map。经过max_pooling之后变为(20,12,12)，只改变了feature map大小，没有改变其channels数。所以第二个卷积核的in_channels等于第一个卷积核的out_channels，等于20（L14）。以此类推，第二个max_pooling输出的feature map就是(50,4,4)，所以第一个全连接层的输入维度是4*4*50。需要稍微注意的是在由feature map和下一层进行全连接时，需要先展开成一个行向量（L23），变成类似于BP网络的输入格式。\nPytorch的官方文档对每个函数都有详细的解释，甚至还给出了公式说明怎样计算卷积层和池化层之后的feature map的size，非常良心，所以遇到任何问题，一定要先仔细看官方文档。如果你仔细看文档的话，会发现nn和nn.functional下会有很多同名的类和函数，比如nn.Conv2d和nn.functional.conv2d同时存在，有关它们的区别，简单来说，前者表示类，后者表示函数，像卷积层、全连接层等需要保存学习参数的layer，建议使用nn；而像ReLU和max_pooling等不需要保存学习参数或功能比较简单的layer，建议直接用nn.functional，具体的区别和建议请看：https://www.zhihu.com/question/66782101/answer/579393790。\n其他还有一些小细节，比如Pytorch模型在训练和预测时，需要分别调用model.train()和model.eval()告诉Pytorch此时是训练和预测阶段了，因为在train阶段，会使用dropout、batchnorm等技术，而在预测时不会调用，所以需要显式告诉Pytorch现在是训练还是预测。还有就是训练阶段，每个batch需要梯度清零（optimizer.zero_grad）、求梯度（loss.backward）、梯度下降更新参数（optimizer.step）等步骤。在预测阶段，不用求梯度（torch.no_grad）等。具体可以看上述代码注释。\n最后总结，使用Pytorch构建深度学习模型是非常简单的，注意Pytorch的基本规则，仿照MNIST例子依葫芦画瓢就可以搭建自己的模型了。建议大家看过官方代码之后，自己重写一遍，检查一下是否学习到位。\n至此，Neural Networks and Deep Learning这本书的学习笔记到此结束，算是入门了神经网络和深度学习，接下来请开启深度学习实战的炼丹之旅。\n","permalink":"http://localhost:1313/posts/2019-05-19-neural-networks-and-deep-learning-appendix-pytorch-mnist-tutorial/","summary":"\u003cp\u003e由于本书成书较早（2015），作者当时使用的是Theano，但Theano已不再维护，所以本博客使用当下流行的Pytorch框架讲解MNIST图片分类的代码实现，具体就是Pytorch官方给出的MNIST代码：\u003ca href=\"https://github.com/pytorch/examples/tree/master/mnist\"\u003ehttps://github.com/pytorch/examples/tree/master/mnist\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-19-neural-networks-and-deep-learning-appendix-pytorch-mnist-tutorial/nn.png\"\u003e\n使用该工具在线制作：\u003ca href=\"http://alexlenail.me/NN-SVG/LeNet.html\"\u003ehttp://alexlenail.me/NN-SVG/LeNet.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e下面，我首先贴出经过我注释的Pytorch MNIST代码，然后对一些关键问题进行解释。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 46\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 47\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 48\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 49\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 50\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 51\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 52\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 53\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 54\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 55\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 56\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 57\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 58\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 59\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 60\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 61\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 62\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 63\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 64\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 65\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 66\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 67\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 68\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 69\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 70\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 71\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 72\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 73\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 74\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 75\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 76\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 77\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 78\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 79\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 80\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 81\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 82\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 83\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 84\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 85\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 86\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 87\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 88\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 89\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 90\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 91\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 92\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 93\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 94\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 95\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 96\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 97\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 98\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 99\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e100\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e101\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e102\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e103\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e104\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e105\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e106\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e107\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e108\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e109\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e110\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e111\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e112\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e113\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e114\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e115\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e116\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e117\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e118\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e119\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e120\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e121\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e122\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e123\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e124\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e125\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e126\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e127\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e128\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e129\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e130\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e131\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e132\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e133\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e134\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e135\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e __future__ \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e print_function\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e argparse\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch.nn \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e nn\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch.nn.functional \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e F\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch.optim \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e optim\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e torchvision \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e datasets, transforms\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 所有网络类要继承nn.Module\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eNet\u003c/span\u003e(nn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eModule):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        super(Net, self)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e() \u003cspan style=\"color:#75715e\"\u003e# 调用父类构造函数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econv1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e nn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConv2d(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# (in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econv2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e nn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConv2d(\u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# 这一层的in_channels正好是上一层的out_channels\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efc1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e nn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eLinear(\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e500\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efc2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e nn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eLinear(\u003cspan style=\"color:#ae81ff\"\u003e500\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eforward\u003c/span\u003e(self, x):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erelu(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econv1(x))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax_pool2d(x, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# kernel_size=2, stride=2，pooling之后的大小除以2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erelu(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econv2(x))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax_pool2d(x, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e x\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eview(\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# 展开成 (z, 4*4*50)，其中z是通过自动推导得到的，所以这里设置为-1，这里相当于展开成行向量，便于后续全连接\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erelu(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efc1(x))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efc2(x)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elog_softmax(x, dim\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# log_softmax 即 log(softmax(x))；dim=1对行进行softmax，因为上面x.view展开成行向量了，log_softmax速度和数值稳定性都比softmax好一些\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e     \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etrain\u003c/span\u003e(args, model, device, train_loader, optimizer, epoch):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etrain() \u003cspan style=\"color:#75715e\"\u003e# 告诉pytorch，这是训练阶段 https://stackoverflow.com/a/51433411/2468587\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e batch_idx, (data, target) \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e enumerate(train_loader):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        data, target \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device), target\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        optimizer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ezero_grad() \u003cspan style=\"color:#75715e\"\u003e# 每个batch的梯度重新累加\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        output \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(data)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        loss \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enll_loss(output, target) \u003cspan style=\"color:#75715e\"\u003e# 这里的nll_loss就是Michael Nielsen在ch3提到的log-likelihood cost function，配合softmax使用，batch的梯度/loss要求均值mean\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        loss\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebackward() \u003cspan style=\"color:#75715e\"\u003e# 求loss对参数的梯度dw\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        optimizer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estep() \u003cspan style=\"color:#75715e\"\u003e# 梯度下降，w\u0026#39;=w-η*dw\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e batch_idx \u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e args\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elog_interval \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Train Epoch: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e [\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e (\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{:.0f}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%)]\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003eLoss: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{:.6f}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eformat(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                epoch, batch_idx \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e len(data), len(train_loader\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edataset),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#ae81ff\"\u003e100.\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e batch_idx \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e len(train_loader), loss\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitem()))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etest\u003c/span\u003e(args, model, device, test_loader):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eeval()  \u003cspan style=\"color:#75715e\"\u003e# 告诉pytorch，这是预测（评价）阶段\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    test_loss \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    correct \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad(): \u003cspan style=\"color:#75715e\"\u003e# 预测时不需要误差反传，https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e data, target \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e test_loader:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            data, target \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device), target\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            output \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(data)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            test_loss \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enll_loss(output, target, reduction\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;sum\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitem() \u003cspan style=\"color:#75715e\"\u003e# sum up batch loss，预测时的loss求sum，L54再求均值\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            pred \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e output\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eargmax(dim\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, keepdim\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# get the index of the max log-probability\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            correct \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e pred\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eeq(target\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eview_as(pred))\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esum()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitem()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    test_loss \u003cspan style=\"color:#f92672\"\u003e/=\u003c/span\u003e len(test_loader\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edataset)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003eTest set: Average loss: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{:.4f}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e, Accuracy: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e (\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{:.0f}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%)\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eformat(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        test_loss, correct, len(test_loader\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edataset),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ae81ff\"\u003e100.\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e correct \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e len(test_loader\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edataset)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eplot1digit\u003c/span\u003e(data_loader):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e plt\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    examples \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e enumerate(data_loader)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    batch_idx, (Xs, ys) \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e next(examples) \u003cspan style=\"color:#75715e\"\u003e# 读取到的是一个batch的所有数据\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    X\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eXs[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enumpy()[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#75715e\"\u003e# Xs[0]取出batch中的第一个数据，由tensor转换为numpy，因为pytorch tensor的格式是[channel, height, width]，所以最后[0]取出其第一个通道的[h,w]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    y\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eys[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enumpy() \u003cspan style=\"color:#75715e\"\u003e# y没有通道，就一个标量值\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esavetxt(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../../../fig/\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e.csv\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003ey, X, delimiter\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;,\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e     \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eimshow(X, cmap\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Greys\u0026#39;\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# or \u0026#39;Greys_r\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esavefig(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../../../fig/\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e.png\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003ey)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Training settings\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e argparse\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eArgumentParser(description\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;PyTorch MNIST Example\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--batch-size\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eint, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e64\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;N\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;input batch size for training (default: 64)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--test-batch-size\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eint, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1000\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;N\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;input batch size for testing (default: 1000)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--epochs\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eint, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;N\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;number of epochs to train (default: 10)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--lr\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003efloat, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.01\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;LR\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;learning rate (default: 0.01)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--momentum\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003efloat, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;M\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;SGD momentum (default: 0.5)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--no-cuda\u0026#39;\u003c/span\u003e, action\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;store_true\u0026#39;\u003c/span\u003e, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;disables CUDA training\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--seed\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eint, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;S\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;random seed (default: 1)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--log-interval\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eint, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;N\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;how many batches to wait before logging training status\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e     \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--save-model\u0026#39;\u003c/span\u003e, action\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;store_true\u0026#39;\u003c/span\u003e, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;For Saving the current Model\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    args \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eparse_args()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    use_cuda \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e args\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_cuda \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecuda\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eis_available()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emanual_seed(args\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eseed)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    device \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edevice(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e use_cuda \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;cpu\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    kwargs \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;num_workers\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;pin_memory\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e} \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e use_cuda \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    train_loader \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eutils\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataLoader(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        datasets\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eMNIST(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../data\u0026#39;\u003c/span\u003e, train\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, download\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                       transform\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003etransforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eCompose([ \u003cspan style=\"color:#75715e\"\u003e# https://discuss.pytorch.org/t/can-some-please-explain-how-the-transforms-work-and-why-normalize-the-data/2461/3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                           transforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eToTensor(), \u003cspan style=\"color:#75715e\"\u003e# 把[0,255]的(H,W,C)的图片转换为[0,1]的(channel,height,width)的图片\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                           transforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eNormalize((\u003cspan style=\"color:#ae81ff\"\u003e0.1307\u003c/span\u003e,), (\u003cspan style=\"color:#ae81ff\"\u003e0.3081\u003c/span\u003e,)) \u003cspan style=\"color:#75715e\"\u003e# 进行z-score标准化，这两个数分别是MNIST的均值和标准差\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                       ])),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        batch_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eargs\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebatch_size, shuffle\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003ekwargs)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    test_loader \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eutils\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataLoader(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        datasets\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eMNIST(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../data\u0026#39;\u003c/span\u003e, train\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e, transform\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003etransforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eCompose([\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                           transforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eToTensor(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                           transforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eNormalize((\u003cspan style=\"color:#ae81ff\"\u003e0.1307\u003c/span\u003e,), (\u003cspan style=\"color:#ae81ff\"\u003e0.3081\u003c/span\u003e,))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                       ])),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        batch_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eargs\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etest_batch_size, shuffle\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003ekwargs)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# plot1digit(train_loader)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Net()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    optimizer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e optim\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSGD(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eparameters(), lr\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eargs\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elr, momentum\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eargs\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emomentum)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e epoch \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, args\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eepochs \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        train(args, model, device, train_loader, optimizer, epoch)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        test(args, model, device, test_loader)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (args\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esave_model):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esave(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estate_dict(),\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mnist_cnn.pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;__main__\u0026#39;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    main()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e首先是MNIST数据格式的问题，在L108~L120，我们使用Pytorch的DataLoader载入了训练和测试数据，数据格式本质上和本系列博客的\u003ca href=\"https://bitjoy.net/posts/2018-11-25-neutral-networks-and-deep-learning-1-mnist-dataset/\"\u003e第一篇博客\u003c/a\u003e介绍的是一致的，即每张图片都是28*28的灰度图片，因为是灰度图片，所以只有一个通道数，默认格式是(H,W,C)，且值域范围是[0,255]。但上述代码对原始图片进行了两个变换，分别是ToTensor和Normalize。ToTensor将[0,255]的灰度图片(H,W,C)转换为[0,1]的灰度图片(C,H,W)，即Pytorch对2D图片的格式要求都是channel在前。所以经过这一转换，一张图片的shape是(1,28,28)，是一个三维矩阵；如果是彩色图片的话，有R,G,B三个通道，C=3。Normalize对图片数据进行z-score标准化，即减去均值再除以标准差；L112的两个值就是预先计算的MNIST数据集的均值和标准差。这些操作的好处是能让模型更加平稳快速收敛。同第一篇博客一样，我们可以把Pytorch格式的图片打印出来以便直观理解，L61的plot1digit函数就是这个作用。\u003c/p\u003e","title":"Neural Networks and Deep Learning（七）番外篇·Pytorch MNIST教程"},{"content":"简化版 叶文洁向宇宙发射了一个信号 三体人接收到了这个信号 三体人计划逃离水深火热的三体星系，殖民太阳系 地球人在保卫太阳系的末日之战中，被三体舰队团灭，太阳系岌岌可危 罗辑参透了黑暗森林法则，并假借雪地工程实现了对三体星系的威慑，三体撤军，太阳系幸存 罗辑年老体衰，程心接替罗辑成为新的执剑人 三体人预料到程心心慈手软，不敢实施黑暗森林打击 三体人果断进军太阳系，程心果然没有实施黑暗森林打击，地球沦为三体人的殖民地 在太空执行任务的地球飞船发射了三体坐标，三体再次撤军，并随后遭到黑暗森林打击，三体星系灭亡 发射三体坐标也暴露了太阳系坐标，太阳系遭到更高级的打击——降维打击，太阳系被二维化 程心借助光速飞船逃离太阳系来到了云天明送给她的类地行星蓝星上 程心又进入了云天明送给她的平行小宇宙，企图躲避大宇宙的归零大爆炸 太多的小宇宙导致大宇宙质量流失，无法归零 大宇宙向所有宇宙发布回归运动声明，请求小宇宙归还质量 程心最终归还质量，回到了大宇宙 大宇宙坍缩成奇点，完成大爆炸，宇宙开始了新的轮回 扩充版 《三体I·地球往事》 叶文洁经历了父亲在文革中被迫害致死、在大兴安岭被出卖等种种事件之后，对人类的恶彻底绝望了，她决定在红岸基地度过一生。在红岸基地，她意外发现可以利用太阳作为放大器把信号发往其他星球，于是她向宇宙发出了地球的第一个信号，希望外星文明来拯救罪恶的人类。隔壁的三体文明正处于水生火热之中，收到地球信号后，决定冲出三体星球，占领地球。三体人首先派出了两个质子（智子），封锁地球的基础研究，然后派出大型三体舰队进军地球。在地球上，分两个大阵营：一个是以叶文洁为领袖的地球三体组织，可以认为是地球的叛军；另一个是各国组织的政府军，准备消灭地球叛军并对战三体人。地球三体组织又分为三个派系，降临派、拯救派和幸存派。政府军能打败地球叛军并阻止三体人的进攻吗，请看下回分解。\n《三体II·黑暗森林》 地球人为了抵抗三体舰队的入侵，利用三体人思维透明的弱点，选定了四个人开展面壁者计划，其中三人相继失败。200年后，三体星球派来的水滴团灭了地球舰队的舰队方阵，足足有两千多艘几个足球场大的战舰，在一个小时内团灭。侥幸逃离的几艘战舰之间为了维持自身的生存，开始自相残杀，地球文明面临灭顶之灾。罗辑，唯一没有被识破的面壁人，参透了黑暗森林法则：\n宇宙就是一座黑暗森林，每个文明都是带枪的猎人，像幽灵般潜行于林间，轻轻拨开挡路的树枝，竭力不让脚步发出一点儿声音，连呼吸都小心翼翼……他必须小心，因为林中到处都有与他一样潜行的猎人。如果他发现了别的生命，不管是不是猎人，不管是天使还是魔鬼，不管是娇嫩的婴儿还是步履蹒跚的老人，也不管是天仙般的少女还是天神般的男孩，能做的只有一件事：开枪消灭之。在这片森林中，他人就是地狱，就是永恒的威胁，任何暴露自己存在的生命都将很快被消灭。这就是宇宙文明的图景，这就是对费米悖论的解释。”\n罗辑假借雪地工程，制造了一个和三体文明同归于尽的方案，即在太阳周围精心安排一层油膜，使得从宇宙其他文明的视角来看，透过油膜的点点亮光，表示三体星系的坐标。只要这个坐标发射，三体文明就会遭到黑暗森林打击。罗辑通过雪地工程，使地球文明第一次获得了和三体文明谈判的资格，在此之前，地球文明卑微如一只蚂蚁。罗辑成功了，三体文明接受了罗辑的谈判条件，地球文明幸存了下来，并且三体智子解除了对地球的科技封锁。接下来地球文明和三体文明又会发生怎样惊心动魄的故事呢，请听下回分解。\n《三体III·死神永生》 《三体II》之后，罗辑拯救了地球文明，地球和三体处在互相制衡的状态，地球处于威慑纪元。由于罗辑掌握发射三体坐标的开关，决定着两个文明的生死存亡，罗辑被称为执剑人。渐渐的，罗辑老了，需要新人接替罗辑成为执剑人，程心最终竞选成功，成为新的执剑人。在这期间三体文明和地球文明交流密切，关系融洽，似乎一切都那么的平静和美好。\n突然，意想不到的事情发生了，三体派出大批舰队进攻地球，而此时的执剑人程心却没能说服自己发射三体坐标（因为如果发射三体坐标，也会同时暴露地球坐标，导致地球遭受打击，作为圣母心的程心自然是受不了的）。就这样，地球沦陷，三体舰队全面占领地球，把地球人圈养在澳大利亚。\n就在地球文明生死存亡之际，在外太空执行任务的“万有引力”号飞船广播了三体坐标，三体文明自知死路一条，撤离地球，地球再一次得救，处于广播纪元。不久，三体遭受黑暗森林打击，三体文明毁灭。广播三体坐标也暴露了地球的坐标，所以地球人开始探索拯救地球免于黑暗森林打击的方案。\n三体文明虽然被毁灭，但由于文明发达，仍有三体人得以逃往外太空。在三体智子和地球告别之际，智子安排程心和云天明会面，云天明是程心的大学同学，暗恋程心，买下一颗遥远的恒星并送给程心，程心却在不知情的情况下把云天明的大脑发射到三体人手中。云天明被三体人复活，并被安排和程心会面，在和程心会面过程中，云天明给程心讲了三个故事，通过多重隐喻的方式传达了拯救地球的方案。 地球人通过对三个故事的研究，总结出拯救地球的三个方案：\n安全声明，降低太阳系的光速，使太阳系变成一个低光速黑域，地球人把自己锁死在太阳系，永远也无法逃出。通过这种方案，让地外文明觉得太阳系不是威胁，打消进攻的念头。 超光速飞船，制造超光速飞船，飞离被暴露的太阳系，寻找新的家园。 掩体计划，将地球人迁移到类木行星的背阳面，由于类木行星距离太阳较远，当黑暗森林打击到来时，用类木行星作为盾牌，抵挡太阳爆炸发射的冲击波。 经过不断的争论和调整，地球人最终选定掩体计划，因为安全声明方案需要降低光速，难度太大，而超光速飞船即使研制出来，肯定只能让少数人逃生，由此会引发普通阶层的不满，导致地球内乱。于是，地球进入掩体纪元。\n随着掩体计划的实施，地球人陆续搬迁到类木行星背阳面的太空近地轨道居住，地球人又过上了幸福的生活。可好景不长，太阳系的坐标终究是暴露了，被高级得多的歌者文明发现，他们自然知道使用常规的黑暗森林打击无法消灭躲在类木行星后面的地球人，于是他们启用了更高级的武器——降维打击！他们向太阳系发射了一张小纸条，不久这张小纸条扩大成一张二维平面，这张二维平面就像一个超级黑洞一样，把周围的三维物体吸到它的平面上，压扁，变成一张静态的二维图片。就这样，太阳系的行星包括太阳本身不断被吸到这个二维平面，坍缩成一张死去了的二维图片。要想逃躲被二维化的命运，必须以超光速飞离太阳系，但是之前的超光速飞船计划已经被明令禁止了。通常被公开禁止的东西，都有人在私底下偷偷流通，超光速飞船也不例外。程心的公司，因为各种原因，私底下偷偷研制成功了超光速的曲率驱动飞船。于是，程心和她的助理艾AA乘坐超光速飞船逃离了太阳系，来到了云天明送给她的那颗恒星的一个类地行星蓝星上，程心等人进入了银河纪元。\n没想到，蓝星上有人！是之前逃离太阳系的万有引力号上的成员关一帆。在蓝星上，关一帆检测到旁边的行星灰星有飞船迹象，以为是云天明，于是和程心乘坐飞船前往灰星，艾AA就留在了蓝星。在前往灰星的路上，关一帆告诉程心，太阳系向二维平面的跌落会永远进行下去，直到整个宇宙都跌入到二维。实际上，宇宙原本是十维空间，但是由于星际战争，不断有文明使用降维打击，慢慢的，宇宙的维度就被打成了三维，现在又将被打成二维。当宇宙被星际战争打成零维之后，宇宙重启，就像把时针拨过12点一样。比起降维打击，之前人类参透的黑暗森林打击不值一提，在星际战争中，黑暗森林打击就像狙击手之间的阵地战，对于整个战争来说是件小事，而最有威力的武器是利用宇宙规律，比如降低维度用来攻击，降低光速用来防御，真是太可怕了。\n关一帆和程心来到灰星之后，发现了曲率驱动飞船留下的尾迹——死线，这五根死线非常粗非常黑，只有很高级的飞船才能产生如此粗和黑的死线，关一帆猜测是归零者的飞船留下来的，归零者是一群智慧个体，想重启宇宙回到田园时代。这些死线（很粗的圆柱体）是绝对的光速为零的黑域，任何东西只要进去了，就逃不出来，必死无疑。这些死线还有一个特点是如果周围有其他曲率驱动飞船，则产生的死线会和已有的死线发生干扰，使得黑域扩散。\n所以非常不巧的是，归零者来到了灰星，而云天明来到了蓝星，而程心他们却去了灰星。更可怕的是，云天明的曲率驱动飞船产生的尾迹和归零者的死线产生了干扰，导致黑域扩散，关一帆和程心的飞船跌入黑域，光速变慢。在黑域里，电子计算机和量子计算机失效，关一帆启动了神经元计算机，同时，由于氧气不足，他们两进入了冬眠。经过几天的航行，他们的飞船终于回到了蓝星，但因为他们的光速变慢了，所以他们的几天，对于处在蓝星上的艾AA和云天明来说已经是几千万年之后了。关一帆和程心在蓝星上找到了艾AA和云天明留给他们的礼物，一扇门，一扇通往另一个平行小宇宙的门，当然，这个小宇宙也是云天明送给他们的。关一帆和程心来到了这个小宇宙，很巧的是，智子也在这个小宇宙里，作为该小宇宙的管家。智子告诉两位，这个小宇宙是时间之外的宇宙，和之前的宇宙是平行的，能躲过之前大宇宙的坍缩。当大宇宙坍缩到奇点然后大爆炸形成新的大宇宙之后，他们就可以从这个小宇宙回到新的大宇宙，开始新的田园生活了。\n原本以为关一帆和程心会在小宇宙中幸福的生活下去，没想到，他们突然收到了大宇宙的超膜广播，用一百多万种语言写成的广播，广播内容是回归运动声明：\n回归运动声明：我们宇宙的总质量减少至临界值以下，宇宙将由封闭转变为开放，宇宙将在永恒的膨胀中死去，所有的生命和记忆都将死去。请归还你们拿走的质量，只把记忆体送往新宇宙。\n即有太多的文明发现了可以制造小宇宙来躲避大宇宙的坍缩，导致大宇宙的质量减小到临界值而无法完成归零的大爆炸，大宇宙将由封闭转变为开放，在永恒的膨胀中死去。该声明请求所有小宇宙归还他们拿走的质量，以完成大宇宙的归零。\n在经历了几百年的星际战争，在亲眼目睹了太阳系母亲的坍缩和宇宙的黑暗之后，程心和关一帆内心平静，他们决定响应回归运动，将小宇宙的所有质量，包括天、地、太阳、飞船等等一切质量，都拆卸下来归还给了大宇宙。最后，关一帆、程心和智子，手拉手，离开了小宇宙，进入了大宇宙，开始了宇宙新一轮轮回。死神永生！\n读后感：佩服大刘巨大的脑洞！全书看完，完全不觉得是科幻小说，所有物理、生物、计算机的知识，运用得天衣无缝，毫无破绽，觉得这就是地球、太阳系、宇宙的未来。科幻作家首先要是一名合格的作家，本文的文学性毫不弱于其科幻性，我贫乏的语言已经不足以表达这部作品的伟大了。《三体》系列完全可以拍成一部不输于冰与火之歌的史诗巨作！推荐看完全书的同学去B站看文曰小强的速读视频，这个up主也是厉害，如此硬核的小说，用84分钟就讲完了。如果没看过原书就不推荐看了，因为小说本身的信息密度就很高，再经过小强加工压缩到84分钟，信息密度就更高了，很可能会看得一头雾水。总之，膜拜大刘，一举把中国的科幻水平提高到世界水准。\n","permalink":"http://localhost:1313/posts/2019-05-18-introduction-of-the-three-body-problem/","summary":"\u003ch1 id=\"简化版\"\u003e简化版\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e叶文洁向宇宙发射了一个信号\u003c/li\u003e\n\u003cli\u003e三体人接收到了这个信号\u003c/li\u003e\n\u003cli\u003e三体人计划逃离水深火热的三体星系，殖民太阳系\u003c/li\u003e\n\u003cli\u003e地球人在保卫太阳系的末日之战中，被三体舰队团灭，太阳系岌岌可危\u003c/li\u003e\n\u003cli\u003e罗辑参透了黑暗森林法则，并假借雪地工程实现了对三体星系的威慑，三体撤军，太阳系幸存\u003c/li\u003e\n\u003cli\u003e罗辑年老体衰，程心接替罗辑成为新的执剑人\u003c/li\u003e\n\u003cli\u003e三体人预料到程心心慈手软，不敢实施黑暗森林打击\u003c/li\u003e\n\u003cli\u003e三体人果断进军太阳系，程心果然没有实施黑暗森林打击，地球沦为三体人的殖民地\u003c/li\u003e\n\u003cli\u003e在太空执行任务的地球飞船发射了三体坐标，三体再次撤军，并随后遭到黑暗森林打击，三体星系灭亡\u003c/li\u003e\n\u003cli\u003e发射三体坐标也暴露了太阳系坐标，太阳系遭到更高级的打击——降维打击，太阳系被二维化\u003c/li\u003e\n\u003cli\u003e程心借助光速飞船逃离太阳系来到了云天明送给她的类地行星蓝星上\u003c/li\u003e\n\u003cli\u003e程心又进入了云天明送给她的平行小宇宙，企图躲避大宇宙的归零大爆炸\u003c/li\u003e\n\u003cli\u003e太多的小宇宙导致大宇宙质量流失，无法归零\u003c/li\u003e\n\u003cli\u003e大宇宙向所有宇宙发布回归运动声明，请求小宇宙归还质量\u003c/li\u003e\n\u003cli\u003e程心最终归还质量，回到了大宇宙\u003c/li\u003e\n\u003cli\u003e大宇宙坍缩成奇点，完成大爆炸，宇宙开始了新的轮回\u003c/li\u003e\n\u003c/ol\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://images.weserv.nl/?url=https://img1.doubanio.com/view/subject/l/public/s2768378.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://images.weserv.nl/?url=https://img3.doubanio.com/view/subject/l/public/s3078482.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://images.weserv.nl/?url=https://img3.doubanio.com/view/subject/l/public/s26012674.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"扩充版\"\u003e扩充版\u003c/h1\u003e\n\u003ch2 id=\"三体i地球往事\"\u003e《三体I·地球往事》\u003c/h2\u003e\n\u003cp\u003e叶文洁经历了父亲在文革中被迫害致死、在大兴安岭被出卖等种种事件之后，对人类的恶彻底绝望了，她决定在红岸基地度过一生。在红岸基地，她意外发现可以利用太阳作为放大器把信号发往其他星球，于是她向宇宙发出了地球的第一个信号，希望外星文明来拯救罪恶的人类。隔壁的三体文明正处于水生火热之中，收到地球信号后，决定冲出三体星球，占领地球。三体人首先派出了两个质子（智子），封锁地球的基础研究，然后派出大型三体舰队进军地球。在地球上，分两个大阵营：一个是以叶文洁为领袖的地球三体组织，可以认为是地球的叛军；另一个是各国组织的政府军，准备消灭地球叛军并对战三体人。地球三体组织又分为三个派系，降临派、拯救派和幸存派。政府军能打败地球叛军并阻止三体人的进攻吗，请看下回分解。\u003c/p\u003e\n\u003ch2 id=\"三体ii黑暗森林\"\u003e《三体II·黑暗森林》\u003c/h2\u003e\n\u003cp\u003e地球人为了抵抗三体舰队的入侵，利用三体人思维透明的弱点，选定了四个人开展面壁者计划，其中三人相继失败。200年后，三体星球派来的水滴团灭了地球舰队的舰队方阵，足足有两千多艘几个足球场大的战舰，在一个小时内团灭。侥幸逃离的几艘战舰之间为了维持自身的生存，开始自相残杀，地球文明面临灭顶之灾。罗辑，唯一没有被识破的面壁人，参透了黑暗森林法则：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e宇宙就是一座黑暗森林，每个文明都是带枪的猎人，像幽灵般潜行于林间，轻轻拨开挡路的树枝，竭力不让脚步发出一点儿声音，连呼吸都小心翼翼……他必须小心，因为林中到处都有与他一样潜行的猎人。如果他发现了别的生命，不管是不是猎人，不管是天使还是魔鬼，不管是娇嫩的婴儿还是步履蹒跚的老人，也不管是天仙般的少女还是天神般的男孩，能做的只有一件事：开枪消灭之。在这片森林中，他人就是地狱，就是永恒的威胁，任何暴露自己存在的生命都将很快被消灭。这就是宇宙文明的图景，这就是对费米悖论的解释。”\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e罗辑假借雪地工程，制造了一个和三体文明同归于尽的方案，即在太阳周围精心安排一层油膜，使得从宇宙其他文明的视角来看，透过油膜的点点亮光，表示三体星系的坐标。只要这个坐标发射，三体文明就会遭到黑暗森林打击。罗辑通过雪地工程，使地球文明第一次获得了和三体文明谈判的资格，在此之前，地球文明卑微如一只蚂蚁。罗辑成功了，三体文明接受了罗辑的谈判条件，地球文明幸存了下来，并且三体智子解除了对地球的科技封锁。接下来地球文明和三体文明又会发生怎样惊心动魄的故事呢，请听下回分解。\u003c/p\u003e\n\u003ch2 id=\"三体iii死神永生\"\u003e《三体III·死神永生》\u003c/h2\u003e\n\u003cp\u003e《三体II》之后，罗辑拯救了地球文明，地球和三体处在互相制衡的状态，地球处于威慑纪元。由于罗辑掌握发射三体坐标的开关，决定着两个文明的生死存亡，罗辑被称为执剑人。渐渐的，罗辑老了，需要新人接替罗辑成为执剑人，程心最终竞选成功，成为新的执剑人。在这期间三体文明和地球文明交流密切，关系融洽，似乎一切都那么的平静和美好。\u003c/p\u003e\n\u003cp\u003e突然，意想不到的事情发生了，三体派出大批舰队进攻地球，而此时的执剑人程心却没能说服自己发射三体坐标（因为如果发射三体坐标，也会同时暴露地球坐标，导致地球遭受打击，作为圣母心的程心自然是受不了的）。就这样，地球沦陷，三体舰队全面占领地球，把地球人圈养在澳大利亚。\u003c/p\u003e\n\u003cp\u003e就在地球文明生死存亡之际，在外太空执行任务的“万有引力”号飞船广播了三体坐标，三体文明自知死路一条，撤离地球，地球再一次得救，处于广播纪元。不久，三体遭受黑暗森林打击，三体文明毁灭。广播三体坐标也暴露了地球的坐标，所以地球人开始探索拯救地球免于黑暗森林打击的方案。\u003c/p\u003e\n\u003cp\u003e三体文明虽然被毁灭，但由于文明发达，仍有三体人得以逃往外太空。在三体智子和地球告别之际，智子安排程心和云天明会面，云天明是程心的大学同学，暗恋程心，买下一颗遥远的恒星并送给程心，程心却在不知情的情况下把云天明的大脑发射到三体人手中。云天明被三体人复活，并被安排和程心会面，在和程心会面过程中，云天明给程心讲了三个故事，通过多重隐喻的方式传达了拯救地球的方案。 地球人通过对三个故事的研究，总结出拯救地球的三个方案：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e安全声明，降低太阳系的光速，使太阳系变成一个低光速黑域，地球人把自己锁死在太阳系，永远也无法逃出。通过这种方案，让地外文明觉得太阳系不是威胁，打消进攻的念头。\u003c/li\u003e\n\u003cli\u003e超光速飞船，制造超光速飞船，飞离被暴露的太阳系，寻找新的家园。\u003c/li\u003e\n\u003cli\u003e掩体计划，将地球人迁移到类木行星的背阳面，由于类木行星距离太阳较远，当黑暗森林打击到来时，用类木行星作为盾牌，抵挡太阳爆炸发射的冲击波。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e经过不断的争论和调整，地球人最终选定掩体计划，因为安全声明方案需要降低光速，难度太大，而超光速飞船即使研制出来，肯定只能让少数人逃生，由此会引发普通阶层的不满，导致地球内乱。于是，地球进入掩体纪元。\u003c/p\u003e\n\u003cp\u003e随着掩体计划的实施，地球人陆续搬迁到类木行星背阳面的太空近地轨道居住，地球人又过上了幸福的生活。可好景不长，太阳系的坐标终究是暴露了，被高级得多的歌者文明发现，他们自然知道使用常规的黑暗森林打击无法消灭躲在类木行星后面的地球人，于是他们启用了更高级的武器——降维打击！他们向太阳系发射了一张小纸条，不久这张小纸条扩大成一张二维平面，这张二维平面就像一个超级黑洞一样，把周围的三维物体吸到它的平面上，压扁，变成一张静态的二维图片。就这样，太阳系的行星包括太阳本身不断被吸到这个二维平面，坍缩成一张死去了的二维图片。要想逃躲被二维化的命运，必须以超光速飞离太阳系，但是之前的超光速飞船计划已经被明令禁止了。通常被公开禁止的东西，都有人在私底下偷偷流通，超光速飞船也不例外。程心的公司，因为各种原因，私底下偷偷研制成功了超光速的曲率驱动飞船。于是，程心和她的助理艾AA乘坐超光速飞船逃离了太阳系，来到了云天明送给她的那颗恒星的一个类地行星蓝星上，程心等人进入了银河纪元。\u003c/p\u003e\n\u003cp\u003e没想到，蓝星上有人！是之前逃离太阳系的万有引力号上的成员关一帆。在蓝星上，关一帆检测到旁边的行星灰星有飞船迹象，以为是云天明，于是和程心乘坐飞船前往灰星，艾AA就留在了蓝星。在前往灰星的路上，关一帆告诉程心，太阳系向二维平面的跌落会永远进行下去，直到整个宇宙都跌入到二维。实际上，宇宙原本是十维空间，但是由于星际战争，不断有文明使用降维打击，慢慢的，宇宙的维度就被打成了三维，现在又将被打成二维。当宇宙被星际战争打成零维之后，宇宙重启，就像把时针拨过12点一样。比起降维打击，之前人类参透的黑暗森林打击不值一提，在星际战争中，黑暗森林打击就像狙击手之间的阵地战，对于整个战争来说是件小事，而最有威力的武器是利用宇宙规律，比如降低维度用来攻击，降低光速用来防御，真是太可怕了。\u003c/p\u003e\n\u003cp\u003e关一帆和程心来到灰星之后，发现了曲率驱动飞船留下的尾迹——死线，这五根死线非常粗非常黑，只有很高级的飞船才能产生如此粗和黑的死线，关一帆猜测是归零者的飞船留下来的，归零者是一群智慧个体，想重启宇宙回到田园时代。这些死线（很粗的圆柱体）是绝对的光速为零的黑域，任何东西只要进去了，就逃不出来，必死无疑。这些死线还有一个特点是如果周围有其他曲率驱动飞船，则产生的死线会和已有的死线发生干扰，使得黑域扩散。\u003c/p\u003e\n\u003cp\u003e所以非常不巧的是，归零者来到了灰星，而云天明来到了蓝星，而程心他们却去了灰星。更可怕的是，云天明的曲率驱动飞船产生的尾迹和归零者的死线产生了干扰，导致黑域扩散，关一帆和程心的飞船跌入黑域，光速变慢。在黑域里，电子计算机和量子计算机失效，关一帆启动了神经元计算机，同时，由于氧气不足，他们两进入了冬眠。经过几天的航行，他们的飞船终于回到了蓝星，但因为他们的光速变慢了，所以他们的几天，对于处在蓝星上的艾AA和云天明来说已经是几千万年之后了。关一帆和程心在蓝星上找到了艾AA和云天明留给他们的礼物，一扇门，一扇通往另一个平行小宇宙的门，当然，这个小宇宙也是云天明送给他们的。关一帆和程心来到了这个小宇宙，很巧的是，智子也在这个小宇宙里，作为该小宇宙的管家。智子告诉两位，这个小宇宙是时间之外的宇宙，和之前的宇宙是平行的，能躲过之前大宇宙的坍缩。当大宇宙坍缩到奇点然后大爆炸形成新的大宇宙之后，他们就可以从这个小宇宙回到新的大宇宙，开始新的田园生活了。\u003c/p\u003e\n\u003cp\u003e原本以为关一帆和程心会在小宇宙中幸福的生活下去，没想到，他们突然收到了大宇宙的超膜广播，用一百多万种语言写成的广播，广播内容是回归运动声明：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e回归运动声明：我们宇宙的总质量减少至临界值以下，宇宙将由封闭转变为开放，宇宙将在永恒的膨胀中死去，所有的生命和记忆都将死去。请归还你们拿走的质量，只把记忆体送往新宇宙。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e即有太多的文明发现了可以制造小宇宙来躲避大宇宙的坍缩，导致大宇宙的质量减小到临界值而无法完成归零的大爆炸，大宇宙将由封闭转变为开放，在永恒的膨胀中死去。该声明请求所有小宇宙归还他们拿走的质量，以完成大宇宙的归零。\u003c/p\u003e\n\u003cp\u003e在经历了几百年的星际战争，在亲眼目睹了太阳系母亲的坍缩和宇宙的黑暗之后，程心和关一帆内心平静，他们决定响应回归运动，将小宇宙的所有质量，包括天、地、太阳、飞船等等一切质量，都拆卸下来归还给了大宇宙。最后，关一帆、程心和智子，手拉手，离开了小宇宙，进入了大宇宙，开始了宇宙新一轮轮回。死神永生！\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e读后感：佩服大刘巨大的脑洞！全书看完，完全不觉得是科幻小说，所有物理、生物、计算机的知识，运用得天衣无缝，毫无破绽，觉得这就是地球、太阳系、宇宙的未来。科幻作家首先要是一名合格的作家，本文的文学性毫不弱于其科幻性，我贫乏的语言已经不足以表达这部作品的伟大了。《三体》系列完全可以拍成一部不输于冰与火之歌的史诗巨作！推荐看完全书的同学去B站看文曰小强的速读视频，这个up主也是厉害，如此硬核的小说，用84分钟就讲完了。如果没看过原书就不推荐看了，因为小说本身的信息密度就很高，再经过小强加工压缩到84分钟，信息密度就更高了，很可能会看得一头雾水。总之，膜拜大刘，一举把中国的科幻水平提高到世界水准。\u003c/p\u003e","title":"《三体》始末"},{"content":"今天我们终于进入到了本书的重头戏——深度学习。其实，这一章的深度学习主要介绍的是卷积神经网络，即CNN。\n本书之前的章节介绍的都是如下图的全连接网络，虽然全连接网络已经能够在MNIST数据集上取得98%以上的测试准确率，但有两个比较大的缺点：1. 训练参数太多，容易过拟合；2. 难以捕捉图片的局部信息。第一点很好理解，参数一多，网络就难以训练，难以加深。对于第二点，因为全连接的每个神经元都和上一层的所有神经元相连，无论距离远近，也就是说网络不会捕捉图片的局部信息和空间结构信息。\n本章要介绍的卷积神经网络，相对于全连接网络，有如下三个特点：1. 局部感知local receptive fields 2. 权值共享shared weights 3. 池化pooling，下面分别介绍这三部分内容。\n局部感知 对于MNIST的一张28*28灰度图片，全连接网络的输入把图片展开成一个维度为784的向量，这就天然丢失了图片的空间结构信息。而CNN的输入保持了图片28*28的二维空间结构信息，相应的，CNN的中间层也是二维的。这就涉及到输入层的二维图片和隐藏层的二维图片如何对应的问题。\nCNN使用一个被称为“卷积核”的东西，把输入图片转换为隐藏层的特征图（feature map），如下图所示，假设卷积核大小为5*5，则输入图片每5*5的一个小区域被转换为隐藏层的一个神经元（像素），这个小区域就称为局部感受野。\n当卷积核不断的在输入图片中移动时，假设每次移动一格（stride=1），则原来28*28的图片，经过一次卷积后，得到的feature map大小为24*24，相比输入图片小了一圈。\n权值共享 那么，这个卷积操作具体是怎样执行的呢，非常简单。5*5的卷积核本质是一个5*5的矩阵，矩阵中的每个值相当于这个卷积核的参数，或者说权值w。每次卷积时，5*5的矩阵和输入图片中5*5的感受野对应位相乘再相加得到隐藏层的一个值。\n下图是一个缩小版的动图例子，左图的绿色大图相当于输入的5*5图片，移动的黄色小图相当于当前卷积的感受野，大小为3*3。在这个3*3的感受野中，每个单元格居中的数字是输入图片的像素值，右下角的红色小字表示卷积核的权值。每次卷积操作，感受野内的图片像素和卷积核权值相乘再相加，得到右图红色小图中的一个单元格的值，这就完成了一次卷积。当黄色感受野不断在输入图片中移动时，右边的feature map也不断被填充，直到一轮卷积完成。整个过程进行了9次卷积，feature map的大小为3*3=9卷积次数。\nhttps://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59\n这里又涉及到CNN的第二个特点——权值共享。注意到，对于上图的一轮卷积操作，不同感受野内右下角的权值矩阵是一样的，也就是说9次卷积的卷积核权值是一样的。权值共享有两个好处，一是特征位置无关，二是参数量大大下降。\n对于特征位置无关 。这个3*3的卷积核相当于一个特征提取器或者说滤波器，比如这个特征提取器能够提取“猫”这个特征，则无论猫在输入图片的左上角还是右下角，“猫”这个特征都能被提取出来，因为卷积核在小范围移动，无论“猫”位于图片的哪个区域，当卷积核移动到这个区域时，卷积得到的输出比较大，被激活，得到“猫”这个特征。所以CNN对位置不敏感，这对图像处理尤其有利。正因为这个特点，经过卷积核卷积操作之后的小图片（上图右边的红色图片）被称为特征图（feature map），因为它就是用卷积核提取出来的符合这个卷积核描述的一个特征。\n对于参数量大大下降。事实上，一次卷积操作除了上面动图显示的卷积核与感受野内的图片相乘再相加之外，还会对加和之后的值做一个激活输出。回到我们的MNIST例子，一次卷积操作用公式来表示就是：\n$$\\begin{eqnarray}\\sigma\\left(b + \\sum_{l=0}^4 \\sum_{m=0}^4 w_{l,m} a_{j+l, k+m} \\right).\\tag{1}\\end{eqnarray}$$\\(w\\)表示卷积核权值矩阵，\\(a\\)表示感受野内的输入图片，两个累加\\(\\sum\\)就是上面动图显示的相乘相加过程，得到和之后，还会加上一个偏移量\\(b\\)，最后进行激活输出\\(\\sigma\\)。所以一个5*5的卷积核，参数量为5*5+1=26。如果有20个卷积核，参数总量为20*26=520。但如果是全连接网络，假设隐藏层有30个，则参数量为784*30+30=23550。所以仅考虑隐藏层的参数量，CNN就比全连接网络少了45倍的参数，参数量少了，就能加快训练，网络也有可能加深。\n池化 池化就很好理解了，对于卷积得到的feature map，再画一个框（类似于卷积层的感受野），把框内的最大值取出来作为池化之后的值，这就是max-pooling。池化的目的是用来简化信息的，相当于降维。池化的框也可以称为核kernel，如果kernel的大小是2*2的，则一个24*24的feature map，经过max-pooling之后就变成了12*12了，维度瞬间降了一半， 把原来的feature map变成了一个紧凑的feature map。\n池化层往往跟在卷积层的后面，下图表示一张28*28的图片，使用3个5*5的卷积核之后，得到了3个24*24的feature map，再经过2*2的max-pooling，得到3个12*12的feature map。\n到这里，CNN的三大特点就介绍完毕了。对于上图，三个卷积核相当于提取了三种特征，我们还需要完成最终的分类任务，这时候还得把全连接网络请过来。经过max-pooling之后，我们再接一个包含10个神经元的全连接层，作为输出层，完整的网络结果如下：\n最后的全连接层和我们前面介绍的全连接网络是完全一样的，只不过全连接的输入是3个经过max-pooling之后的feature map，再和输出层相连时，可以想象成先把3个12*12的feature map展开并首尾相连，得到一个3*12*12=432的向量，再和输出层的10个神经元进行全连接。这就是一个非常简单的CNN网络，包含一个输入层、一个卷积层、一个池化层和一个输出层。\n本文的代码示例network3.py中，构建了一个和上图类似的简单的CNN网络，如下图所示，使用了20个卷积核，相当于提取了20种特征；max-pooling之后使用了两个全连接层，前一层包含100个隐藏神经元，使用sigmoid激活；后一层包含10个神经元，使用softmax激活，作为输出层。就是这么一个简单的CNN网络，其在测试集上的准确率达到了98.78%，超过了本文之前构建的所有的全连接网络。\n由于原文使用的是已经不再维护的Theano，本博客不打算详细介绍其代码实现，我将在稍后的博文中分享Pytorch的CNN代码。不过我还是把原文对CNN的优化过程总结如下，用测试集的准确率作为性能指标：\n上图简单的CNN网络，98.78% 增加一个卷积层，且把激活函数换成ReLU，99.23% 数据增强，把原有的5000张图片，上下左右各平移一个像素，增加了4倍数据，99.37% 增加一个全连接层，且全连接层神经元增加为1000个，使用dropout=0.5，epoch相应减少到40个，99.6%。因为卷积层有权值共享，天然参数少防止过拟合，所以dropout一般只用于全连接层 模型融合ensemble，5个上述模型，采用majority vote，99.67%，已接近人类水平 虽然经过上述5步，准确率没有达到100%，但那些分类错误的图片，真的很难说分错了，因为图片看起来就不是它标注的结果（右上角），就应该是分错的结果（右下角）。总的来说，我觉得已经非常不错了。\n稍微解释两个问题。\n一、为什么ReLU比Sigmoid性能好，目前还没有一个令人信服的答案，只是很多人在很多标准数据集上测试，发现ReLU性能比Sigmoid好，这个消息传开之后，大家都开始用ReLU了。通常的解释是ReLU的max(0,z)在z很大时，梯度依然为1，不存在梯度消失的问题；而Sigmoid在z很大时，\\(\\sigma'(z)\\)梯度几乎为0，存在梯度消失的问题。\n二、为什么CNN可以做深？怎样解决上一章提到的梯度消失或者梯度爆炸问？事实上，CNN并没有解决上一章的问题，而是绕过了这个问题：1) 卷积层大大减少了模型参数，使得网络更加简单，可以训练得更快；2）强大的正则减少过拟合：dropout和卷积层；3）使用ReLU而不是sigmoid，可以加速训练过程3~5倍；4）使用GPU，并且愿意训练更长的时间，因为sigmoid的梯度消失只不过是梯度很小更新很慢，如果训练时间足够长的话，应该也能收敛。而如果用GPU的话，训练速度加快了，相同时间就可以训练更多的epoch。组合3和4，相当于比之前的网络训练时长增加了30倍，收敛效果自然会更好。\n除了CNN，文章最后还简单提到了图像识别的进展、深度学习的其他应用以及神经网络的未来等内容，本博客就不详细介绍了，有兴趣的可以去看原文。\n附录：LeNet-5和多通道卷积 这部分内容是我自己加的，书中提到network3.py是模仿著名的LeNet-5，我于是就查了查LeNet-5的文章和相关介绍。\nhttps://blog.csdn.net/saw009/article/details/80590245\n上图就是非常经典的LeNet-5网络结构图，包含2个卷积层、2个池化层、2个全连接层和1个Gaussian connections。目前比较流行的做法中已经把最后一个Gaussian connections换成了全连接层。本博客前面介绍的卷积层输入只有一张图片，比较好理解，但是LeNet-5的C3即第二个卷积层，面对的是S2得到的6张feature map，相当于有6张输入图片，如何做卷积。\n这就涉及到“多通道”卷积的概念，可以把多通道理解为S2的6个feature map之于C3，一个feature map理解为一个通道。事实上，如果输入图片是彩色的，则输入图片就包含R、G、B三个通道，相当于输入了3张feature map，此时在C1中就会遇到“多通道”卷积的问题。\n假设输入有R、G、B三个通道，则每个通道都需要有一个不同的卷积核进行卷积，最后把三个通道的卷积结果加起来，进行激活，如下面动图所示。\nhttps://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\nhttps://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n回到LeNet-5，S2有6个通道，C3得到了16个feature map，说明C3有16个卷积核。但是，C3每个卷积核并不只包含一个卷积矩阵，而是包含三个卷积矩阵，即每个卷积核也是三通道的，而且这三个通道的卷积矩阵参数是不同的，即不共享，它们只共享一个偏移量\\(b\\)，如下图所示。\nhttps://blog.csdn.net/saw009/article/details/80590245\n但是S2有6个feature map，三个通道不够用，所以LeNet-5的论文中还规定了每个卷积核的三通道卷积S2中的哪3个feature map，如下图所示。\nhttps://blog.csdn.net/saw009/article/details/80590245\n如果还不理解的话，CS231N还有一个更直观的例子： https://cs231n.github.io/assets/conv-demo/index.html 可以点击按钮暂停，然后手动计算看看是否和动图的结果一致。\nhttps://cs231n.github.io/assets/conv-demo/index.html\n有关CNN和LeNet-5的参考资料：\nCNN直观解释：https://www.zhihu.com/question/39022858/answer/224446917 可视化卷积：https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59 可视化“多通道”卷积：https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1 动图展示“多通道”卷积：https://cs231n.github.io/convolutional-networks/ LeNet-5原文：http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf LeNet-5网络解读：https://blog.csdn.net/saw009/article/details/80590245 ","permalink":"http://localhost:1313/posts/2019-05-04-neural-networks-and-deep-learning-6-dl/","summary":"\u003cp\u003e今天我们终于进入到了本书的重头戏——深度学习。其实，这一章的深度学习主要介绍的是卷积神经网络，即CNN。\u003c/p\u003e\n\u003cp\u003e本书之前的章节介绍的都是如下图的全连接网络，虽然全连接网络已经能够在MNIST数据集上取得98%以上的测试准确率，但有两个比较大的缺点：1. 训练参数太多，容易过拟合；2. 难以捕捉图片的局部信息。第一点很好理解，参数一多，网络就难以训练，难以加深。对于第二点，因为全连接的每个神经元都和上一层的所有神经元相连，无论距离远近，也就是说网络不会捕捉图片的局部信息和空间结构信息。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz41.png\"\u003e\u003c/p\u003e\n\u003cp\u003e本章要介绍的卷积神经网络，相对于全连接网络，有如下三个特点：1. 局部感知local receptive fields 2. 权值共享shared weights 3. 池化pooling，下面分别介绍这三部分内容。\u003c/p\u003e\n\u003ch1 id=\"局部感知\"\u003e局部感知\u003c/h1\u003e\n\u003cp\u003e对于MNIST的一张28*28灰度图片，全连接网络的输入把图片展开成一个维度为784的向量，这就天然丢失了图片的空间结构信息。而CNN的输入保持了图片28*28的二维空间结构信息，相应的，CNN的中间层也是二维的。这就涉及到输入层的二维图片和隐藏层的二维图片如何对应的问题。\u003c/p\u003e\n\u003cp\u003eCNN使用一个被称为“卷积核”的东西，把输入图片转换为隐藏层的特征图（feature map），如下图所示，假设卷积核大小为5*5，则输入图片每5*5的一个小区域被转换为隐藏层的一个神经元（像素），这个小区域就称为局部感受野。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz44.png\"\u003e\u003c/p\u003e\n\u003cp\u003e当卷积核不断的在输入图片中移动时，假设每次移动一格（stride=1），则原来28*28的图片，经过一次卷积后，得到的feature map大小为24*24，相比输入图片小了一圈。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz45.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"权值共享\"\u003e权值共享\u003c/h1\u003e\n\u003cp\u003e那么，这个卷积操作具体是怎样执行的呢，非常简单。5*5的卷积核本质是一个5*5的矩阵，矩阵中的每个值相当于这个卷积核的参数，或者说权值w。每次卷积时，5*5的矩阵和输入图片中5*5的感受野对应位相乘再相加得到隐藏层的一个值。\u003c/p\u003e\n\u003cp\u003e下图是一个缩小版的动图例子，左图的绿色大图相当于输入的5*5图片，移动的黄色小图相当于当前卷积的感受野，大小为3*3。在这个3*3的感受野中，每个单元格居中的数字是输入图片的像素值，右下角的红色小字表示卷积核的权值。每次卷积操作，感受野内的图片像素和卷积核权值相乘再相加，得到右图红色小图中的一个单元格的值，这就完成了一次卷积。当黄色感受野不断在输入图片中移动时，右边的feature map也不断被填充，直到一轮卷积完成。整个过程进行了9次卷积，feature map的大小为3*3=9卷积次数。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-04-neural-networks-and-deep-learning-6-dl/e7ff1-1zcjpufrb6ehpri4eyp6aaa.gif\"\u003e\n\u003ca href=\"https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59\"\u003ehttps://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e这里又涉及到CNN的第二个特点——权值共享。注意到，对于上图的一轮卷积操作，不同感受野内右下角的权值矩阵是一样的，也就是说9次卷积的卷积核权值是一样的。权值共享有两个好处，一是特征位置无关，二是参数量大大下降。\u003c/p\u003e\n\u003cp\u003e对于特征位置无关 。这个3*3的卷积核相当于一个特征提取器或者说\u003ca href=\"https://www.zhihu.com/question/39022858/answer/224446917\"\u003e滤波器\u003c/a\u003e，比如这个特征提取器能够提取“猫”这个特征，则无论猫在输入图片的左上角还是右下角，“猫”这个特征都能被提取出来，因为卷积核在小范围移动，无论“猫”位于图片的哪个区域，当卷积核移动到这个区域时，卷积得到的输出比较大，被激活，得到“猫”这个特征。所以CNN对位置不敏感，这对图像处理尤其有利。正因为这个特点，经过卷积核卷积操作之后的小图片（上图右边的红色图片）被称为特征图（feature map），因为它就是用卷积核提取出来的符合这个卷积核描述的一个特征。\u003c/p\u003e\n\u003cp\u003e对于参数量大大下降。事实上，一次卷积操作除了上面动图显示的卷积核与感受野内的图片相乘再相加之外，还会对加和之后的值做一个激活输出。回到我们的MNIST例子，一次卷积操作用公式来表示就是：\u003c/p\u003e\n$$\\begin{eqnarray}\\sigma\\left(b + \\sum_{l=0}^4 \\sum_{m=0}^4 w_{l,m} a_{j+l, k+m} \\right).\\tag{1}\\end{eqnarray}$$\u003cp\u003e\\(w\\)表示卷积核权值矩阵，\\(a\\)表示感受野内的输入图片，两个累加\\(\\sum\\)就是上面动图显示的相乘相加过程，得到和之后，还会加上一个偏移量\\(b\\)，最后进行激活输出\\(\\sigma\\)。所以一个5*5的卷积核，参数量为5*5+1=26。如果有20个卷积核，参数总量为20*26=520。但如果是全连接网络，假设隐藏层有30个，则参数量为784*30+30=23550。所以仅考虑隐藏层的参数量，CNN就比全连接网络少了45倍的参数，参数量少了，就能加快训练，网络也有可能加深。\u003c/p\u003e\n\u003ch1 id=\"池化\"\u003e池化\u003c/h1\u003e\n\u003cp\u003e池化就很好理解了，对于卷积得到的feature map，再画一个框（类似于卷积层的感受野），把框内的最大值取出来作为池化之后的值，这就是max-pooling。池化的目的是用来简化信息的，相当于降维。池化的框也可以称为核kernel，如果kernel的大小是2*2的，则一个24*24的feature map，经过max-pooling之后就变成了12*12了，维度瞬间降了一半， 把原来的feature map变成了一个紧凑的feature map。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz47.png\"\u003e\u003c/p\u003e\n\u003cp\u003e池化层往往跟在卷积层的后面，下图表示一张28*28的图片，使用3个5*5的卷积核之后，得到了3个24*24的feature map，再经过2*2的max-pooling，得到3个12*12的feature map。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz48.png\"\u003e\u003c/p\u003e\n\u003cp\u003e到这里，CNN的三大特点就介绍完毕了。对于上图，三个卷积核相当于提取了三种特征，我们还需要完成最终的分类任务，这时候还得把全连接网络请过来。经过max-pooling之后，我们再接一个包含10个神经元的全连接层，作为输出层，完整的网络结果如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz49.png\"\u003e\u003c/p\u003e\n\u003cp\u003e最后的全连接层和我们前面介绍的全连接网络是完全一样的，只不过全连接的输入是3个经过max-pooling之后的feature map，再和输出层相连时，可以想象成先把3个12*12的feature map展开并首尾相连，得到一个3*12*12=432的向量，再和输出层的10个神经元进行全连接。这就是一个非常简单的CNN网络，包含一个输入层、一个卷积层、一个池化层和一个输出层。\u003c/p\u003e\n\u003cp\u003e本文的代码示例network3.py中，构建了一个和上图类似的简单的CNN网络，如下图所示，使用了20个卷积核，相当于提取了20种特征；max-pooling之后使用了两个全连接层，前一层包含100个隐藏神经元，使用sigmoid激活；后一层包含10个神经元，使用softmax激活，作为输出层。就是这么一个简单的CNN网络，其在测试集上的准确率达到了98.78%，超过了本文之前构建的所有的全连接网络。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/simple_conv.png\"\u003e\u003c/p\u003e\n\u003cp\u003e由于原文使用的是已经不再维护的Theano，本博客不打算详细介绍其代码实现，我将在稍后的博文中分享Pytorch的CNN代码。不过我还是把原文对CNN的优化过程总结如下，用测试集的准确率作为性能指标：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e上图简单的CNN网络，98.78%\u003c/li\u003e\n\u003cli\u003e增加一个卷积层，且把激活函数换成ReLU，99.23%\u003c/li\u003e\n\u003cli\u003e数据增强，把原有的5000张图片，上下左右各平移一个像素，增加了4倍数据，99.37%\u003c/li\u003e\n\u003cli\u003e增加一个全连接层，且全连接层神经元增加为1000个，使用dropout=0.5，epoch相应减少到40个，99.6%。因为卷积层有权值共享，天然参数少防止过拟合，所以dropout一般只用于全连接层\u003c/li\u003e\n\u003cli\u003e模型融合ensemble，5个上述模型，采用majority vote，99.67%，已接近人类水平\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e虽然经过上述5步，准确率没有达到100%，但那些分类错误的图片，真的很难说分错了，因为图片看起来就不是它标注的结果（右上角），就应该是分错的结果（右下角）。总的来说，我觉得已经非常不错了。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/ensemble_errors.png\"\u003e\u003c/p\u003e\n\u003cp\u003e稍微解释两个问题。\u003c/p\u003e","title":"Neural Networks and Deep Learning（六）深度学习"},{"content":"本章我们将分析一下为什么深度神经网络难以训练的问题。\n首先来看问题：如果神经网络的层次不断加深，则在BP误差反向传播的过程中，网络前几层的梯度更新会非常慢，导致前几层的权重无法学习到比较好的值，这就是梯度消失问题（The vanishing gradient problem）。\n以我们在第三章学习的network2.py为例（交叉熵损失函数+Sigmoid激活函数），我们可以计算每个神经元中误差对偏移量\\(b\\)的偏导\\(\\partial C/ \\partial b\\)，根据第二章BP网络的知识，\\(\\partial C/ \\partial b\\)也是\\(\\partial C/ \\partial w\\)的一部分（BP3和BP4的关系），所以如果\\(\\partial C/ \\partial b\\)的绝对值大，则说明梯度大，在误差反向传播的时候，\\(b\\)和\\(w\\)更新就快。\n假设network2的网络结构是[784,30,30,10]，即有两个隐藏层，则我们可以画出在误差反向传播过程中，隐藏层每个神经元的\\(\\partial C/ \\partial b\\)的大小，用柱子长度表示。由下图可知，我们发现第二个隐藏层的梯度普遍大于第一个隐藏层的梯度，这会是一般现象吗，还是偶然现象？\n既然梯度出现了层与层的差异，则可以定义第\\(l\\)层的梯度（如不加说明，则默认是误差\\(C\\)对偏移量\\(b\\)的梯度）向量的长度为\\(\\| \\delta^l \\|\\)，比如\\(\\| \\delta^1 \\|\\)表示第一个隐藏层中每个神经元的\\(\\partial C/ \\partial b\\)的绝对值之和，就是一范数，如果\\(\\| \\delta^l \\|\\)越大，则说明这一层权重的更新越快。\n由此，我们可以画出当有两个隐藏层时，\\(\\| \\delta^l \\|\\)随epoch的变化情况：\n当有三个隐藏层时：\n当有四个隐藏层时：\n我们发现，规律是惊人的一致，即越靠近输出层的隐藏层，\\(\\| \\delta^l \\|\\)越大，即梯度更新越快；越靠近输入层的隐藏层，\\(\\| \\delta^l \\|\\)越小，即梯度更新越慢。\n这就会导致梯度消失的问题（The vanishing gradient problem）：即在误差反向传播过程中，刚开始权重更新比较快，越到后面（越靠近输入层），则权重更新变得很慢，无法搜索到比较优的值。\n所以，对于同样的network2，其他参数都不变，只是单纯增加网络层数，验证集上的准确率反而会下降！按理说网络层数增加，验证集上的准确率会上升，或者不变，至少不应该下降啊，因为最不济增加的网络层什么都不做，准确率应该一样才对，为什么反而下降了呢。虽然层数增加了，但因为上述梯度消失问题，靠近输入层的权重反而没学好，因为权重是随机初始化的，所以验证集上的准确率反而下降了。\n那么，为什么层数增加会导致梯度消失问题呢，我们可以从BP的更新公式中一探究竟。\n为了简化问题，假设我们的网络每一层只有一个神经元：\n则根据BP的更新公式，可以计算得到\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b_1} = \\sigma'(z_1) \\, w_2 \\sigma'(z_2) \\,w_3 \\sigma'(z_3) \\, w_4 \\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}.\\tag{1}\\end{eqnarray}$$计算过程其实很简单，对照本博客开头的那张图，\\(\\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}\\)就是(BP1)，把(BP1)带入(BP2)，就是不断乘以\\(w^{l+1} \\sigma'(z^l)\\)，然后就能得到下图的公式。\n我们在第三章时也曾介绍到梯度消失问题，当时提出的应对策略是采用交叉熵损失函数+Sigmoid，在求解梯度时可以把\\(\\sigma’\\)抵消掉，以此来解决梯度消失的问题。但是请注意，当时求解的仅仅是误差对输出层的梯度，可以通过交叉熵损失函数+Sigmoid抵消\\(\\sigma’\\)，对应到上图就是仅仅最后两项的\\(\\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}\\)可以抵消\\(\\sigma’\\)，即只有BP公式中的(BP1)可以抵消\\(\\sigma’\\)。而如果误差继续反向传播，则其他层的梯度依然包含\\(\\sigma’\\)项，由上图就可以看到，\\(\\frac{\\partial C}{\\partial b_1}\\)除了最后的\\(\\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}\\)抵消了一个\\(\\sigma’\\)，前面还有3个\\(\\sigma’\\)连乘。如果\\(\\sigma\\)是Sigmoid激活函数的话，很容易就导致在Sigmoid两端，梯度更新缓慢的问题，然后又通过乘法放大了梯度消失的问题。\n观察公式(1)，我们发现梯度中包含很多\\(w_j \\sigma'(z_j)\\)项相乘，如果\\(\\sigma\\)是Sigmoid，则\\(\\sigma'(z_j) \\leq 1/4\\)，等号在\\(z_j=0\\)时取得；又因为权重随机初始化自\\(w_j\\sim N(0,1)\\)，所以很容易有\\(|w_j| \u003c 1\\)，这就导致\\(|w_j \\sigma'(z_j)| \u003c 1/4\\)，多个\\(w_j \\sigma'(z_j)\\)项相乘，越乘越小，导致梯度消失。\n由下图可知，不同层的梯度，有部分是相同的，区别就在于\\(w_j \\sigma'(z_j)\\)项乘的多少，所以这就能说明为什么反向传播得越多（越靠近输入层），梯度更新越慢。\n上述\\(w_j \\sigma'(z_j)\\)项连乘的问题不但会导致梯度消失问题，有时候还可能导致梯度爆炸（The exploding gradient problem）。比如假设令\\(w_1 = w_2 = w_3 = w_4 = 100\\)，令\\(b_i = -100 * a_{i-1}\\)，使得\\(z_i = w_i a_{i-1} + b_i = 0\\)，这就有\\(\\sigma'(z_j) = 1/4\\)，进而有\\(w_j\\sigma'(z_j)=100 * \\frac{1}{4} = 25\u003e1\\)。多个\\(w_j \\sigma'(z_j)\\)项相乘就会越乘越大，导致梯度爆炸。\n所以根源还是出现在多个\\(w_j \\sigma'(z_j)\\)项相乘的问题上，导致BP梯度更新不稳定，有时候可能梯度消失，有时候可能梯度爆炸，这就导致深度神经网络训练起来有难度。也许把Sigmoid激活函数换成ReLU可以解决这个问题？\n","permalink":"http://localhost:1313/posts/2019-04-14-neural-networks-and-deep-learning-5-why-are-nn-hard-to-train/","summary":"\u003cp\u003e本章我们将分析一下为什么深度神经网络难以训练的问题。\u003c/p\u003e\n\u003cp\u003e首先来看问题：如果神经网络的层次不断加深，则在BP误差反向传播的过程中，网络前几层的梯度更新会非常慢，导致前几层的权重无法学习到比较好的值，这就是梯度消失问题（The vanishing gradient problem）。\u003c/p\u003e\n\u003cp\u003e以我们在\u003ca href=\"https://bitjoy.net/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/\"\u003e第三章学习的network2.py\u003c/a\u003e为例（交叉熵损失函数+Sigmoid激活函数），我们可以计算每个神经元中误差对偏移量\\(b\\)的偏导\\(\\partial C/ \\partial b\\)，根据\u003ca href=\"https://bitjoy.net/posts/2018-12-14-neutral-networks-and-deep-learning-2-bp/\"\u003e第二章BP网络\u003c/a\u003e的知识，\\(\\partial C/ \\partial b\\)也是\\(\\partial C/ \\partial w\\)的一部分（BP3和BP4的关系），所以如果\\(\\partial C/ \\partial b\\)的绝对值大，则说明梯度大，在误差反向传播的时候，\\(b\\)和\\(w\\)更新就快。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz21.png\"\u003e\u003c/p\u003e\n\u003cp\u003e假设network2的网络结构是[784,30,30,10]，即有两个隐藏层，则我们可以画出在误差反向传播过程中，隐藏层每个神经元的\\(\\partial C/ \\partial b\\)的大小，用柱子长度表示。由下图可知，我们发现第二个隐藏层的梯度普遍大于第一个隐藏层的梯度，这会是一般现象吗，还是偶然现象？\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-14-neural-networks-and-deep-learning-5-why-are-nn-hard-to-train/ch5.1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e既然梯度出现了层与层的差异，则可以定义第\\(l\\)层的梯度（如不加说明，则默认是误差\\(C\\)对偏移量\\(b\\)的梯度）向量的长度为\\(\\| \\delta^l \\|\\)，比如\\(\\| \\delta^1 \\|\\)表示第一个隐藏层中每个神经元的\\(\\partial C/ \\partial b\\)的绝对值之和，就是一范数，如果\\(\\| \\delta^l \\|\\)越大，则说明这一层权重的更新越快。\u003c/p\u003e\n\u003cp\u003e由此，我们可以画出当有两个隐藏层时，\\(\\| \\delta^l \\|\\)随epoch的变化情况：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/training_speed_2_layers.png\"\u003e\u003c/p\u003e\n\u003cp\u003e当有三个隐藏层时：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/training_speed_3_layers.png\"\u003e\u003c/p\u003e\n\u003cp\u003e当有四个隐藏层时：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/training_speed_4_layers.png\"\u003e\u003c/p\u003e\n\u003cp\u003e我们发现，规律是惊人的一致，即越靠近输出层的隐藏层，\\(\\| \\delta^l \\|\\)越大，即梯度更新越快；越靠近输入层的隐藏层，\\(\\| \\delta^l \\|\\)越小，即梯度更新越慢。\u003c/p\u003e\n\u003cp\u003e这就会导致梯度消失的问题（The vanishing gradient problem）：即在误差\u003cstrong\u003e反向\u003c/strong\u003e传播过程中，刚开始权重更新比较快，越到后面（越靠近输入层），则权重更新变得很慢，无法搜索到比较优的值。\u003c/p\u003e\n\u003cp\u003e所以，对于同样的network2，其他参数都不变，只是单纯增加网络层数，验证集上的准确率反而会下降！按理说网络层数增加，验证集上的准确率会上升，或者不变，至少不应该下降啊，因为最不济增加的网络层什么都不做，准确率应该一样才对，为什么反而下降了呢。虽然层数增加了，但因为上述梯度消失问题，靠近输入层的权重反而没学好，因为权重是随机初始化的，所以验证集上的准确率反而下降了。\u003c/p\u003e\n\u003cp\u003e那么，为什么层数增加会导致梯度消失问题呢，我们可以从BP的更新公式中一探究竟。\u003c/p\u003e\n\u003cp\u003e为了简化问题，假设我们的网络每一层只有一个神经元：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz37.png\"\u003e\u003c/p\u003e\n\u003cp\u003e则根据BP的更新公式，可以计算得到\u003c/p\u003e\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b_1} = \\sigma'(z_1) \\, w_2 \\sigma'(z_2) \\,w_3 \\sigma'(z_3) \\, w_4 \\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}.\\tag{1}\\end{eqnarray}$$\u003cp\u003e计算过程其实很简单，对照本博客开头的那张图，\\(\\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}\\)就是(BP1)，把(BP1)带入(BP2)，就是不断乘以\\(w^{l+1} \\sigma'(z^l)\\)，然后就能得到下图的公式。\u003c/p\u003e","title":"Neural Networks and Deep Learning（五）为什么深度神经网络难以训练"},{"content":"我们应该都听说过神经网络强大到能拟合任意一个函数，但细究起来很少有人能论证这个观点，这一章就用通俗易懂的图解方式来证明神经网络为什么能拟合任意一个函数。\n开始介绍之前，有两点需要注意：\n并不是说神经网络可以精确计算任意一个函数\\(f(x)\\)，而是说当隐藏层神经元增加时，可以无限逼近\\(f(x)\\)，比如对于任何一个输入\\(x\\)，网络的输出\\(g(x)\\)和正确值\\(f(x)\\)的差小于某个阈值，\\(|g(x) – f(x)| \u003c \\epsilon\\)； 神经网络拟合的是连续函数，而不是那种不连续、离散、急剧变化的函数。 假设给定一个下图的连续函数，函数形式未知，本章将用图解的方式来证明，一个单隐层的神经网络就可以很好的拟合这个未知函数。\n首先，假设我们的隐藏层只有两个神经元，激活函数使用Sigmoid，并且我们暂时只关注上面那个神经元的参数和输出。则通过调整该神经元的\\(w\\)和\\(b\\)，可以得到不同形状的Sigmoid函数形式。\n极端情况下，如果\\(w\\)很大而\\(b\\)很小，则可以用Sigmoid函数模拟阶梯函数：\n如果令\\(s = -b/w\\)，则只用一个\\(s\\)就可以确定Sigmoid的函数图像：\n如果把隐藏层下面那个神经元也考虑进来，并且令隐藏层的两个神经元和输出层的神经元的连接权重互为相反数，则输出层未激活值\\(z=w_1 a_1 + w_2 a_2\\)的函数图像变成了一个神奇的鼓包，这个鼓包就是我们后续拟合任意函数的基本单元。根据严格的函数形式，还可以知道\\(w_1\\)和\\(w_2\\)的绝对值控制着鼓包的高度，\\(s_1\\)和\\(s_2\\)的值控制着鼓包的位置和宽度。大家可以去原始网页上体验一下作者给出的可交互版本，很有意思。\n有了这个基本单元之后，我们可以通过增加隐藏层神经元的个数来增加鼓包的个数，比如再增加一对隐层神经元，可增加一个鼓包。虽然下图的例子中两个鼓包相互独立，但通过调整4个\\(s\\)，可以让两个鼓包相连甚至交错，大家可以去原网页试一试。\n继续增加隐层神经元个数，则可以继续增加鼓包的数量，如下图所示。\n到这里想必大家马上知道了为什么神经网络能拟合任何一个函数了，如果隐层神经元足够多，则右图的小鼓包可以足够密，通过调整每个鼓包的高度，则无穷多个鼓包的顶点连线可以拟合任意一个函数。这和我们求函数积分（函数下方面积）时使用多个小矩形近似是一个道理！\n所以对于本章开头的未知函数，我们通过调整不同鼓包的高度，可以使得小矩形面积之和与真实积分的差在\\( \\epsilon=0.4\\)以内。如果无限增加隐层神经元个数，则可以无限逼近真实值。这就说明神经网络确实可以拟合任意一个函数。\n上述推导稍微需要注意的一点是，右图的输出是未激活函数值\\(\\sum_j w_j a_j\\)，而网络真正的输出是激活值\\(\\sigma(\\sum_j w_j a_j + b)\\)。这没有太大的关系，因为上面已经说明未激活输出能拟合任意函数，激活函数也是一个函数。增加激活函数就要求右图需要拟合激活函数和真实函数的嵌套函数。既然未激活输出能拟合任意函数，肯定能拟合这个嵌套函数\\(\\sigma^{-1} \\circ f(x)\\)，再用激活函数作用一下\\(\\sigma\\circ\\sigma^{-1} \\circ f(x)\\)，激活函数抵消了，正好得到\\(f(x)\\)。\n如果输入是多维，或者输出是多维，都是类似的道理。这就说明神经网络确实可以拟合任意函数，真的很强大哦。\n","permalink":"http://localhost:1313/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/","summary":"\u003cp\u003e我们应该都听说过神经网络强大到能拟合任意一个函数，但细究起来很少有人能论证这个观点，这一章就用通俗易懂的图解方式来证明神经网络为什么能拟合任意一个函数。\u003c/p\u003e\n\u003cp\u003e开始介绍之前，有两点需要注意：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e并不是说神经网络可以精确计算任意一个函数\\(f(x)\\)，而是说当隐藏层神经元增加时，可以无限逼近\\(f(x)\\)，比如对于任何一个输入\\(x\\)，网络的输出\\(g(x)\\)和正确值\\(f(x)\\)的差小于某个阈值，\\(|g(x) – f(x)| \u003c \\epsilon\\)；\u003c/li\u003e\n\u003cli\u003e神经网络拟合的是连续函数，而不是那种不连续、离散、急剧变化的函数。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e假设给定一个下图的连续函数，函数形式未知，本章将用图解的方式来证明，一个单隐层的神经网络就可以很好的拟合这个未知函数。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e首先，假设我们的隐藏层只有两个神经元，激活函数使用Sigmoid，并且我们暂时只关注上面那个神经元的参数和输出。则通过调整该神经元的\\(w\\)和\\(b\\)，可以得到不同形状的Sigmoid函数形式。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e极端情况下，如果\\(w\\)很大而\\(b\\)很小，则可以用Sigmoid函数模拟阶梯函数：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/high_weight_function.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e如果令\\(s = -b/w\\)，则只用一个\\(s\\)就可以确定Sigmoid的函数图像：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e如果把隐藏层下面那个神经元也考虑进来，并且令隐藏层的两个神经元和输出层的神经元的连接权重互为相反数，则输出层未激活值\\(z=w_1 a_1 + w_2 a_2\\)的函数图像变成了一个神奇的鼓包，这个鼓包就是我们后续拟合任意函数的基本单元。根据严格的函数形式，还可以知道\\(w_1\\)和\\(w_2\\)的绝对值控制着鼓包的高度，\\(s_1\\)和\\(s_2\\)的值控制着鼓包的位置和宽度。\u003ca href=\"http://neuralnetworksanddeeplearning.com/chap4.html#bump_fn\"\u003e大家可以去原始网页上体验一下作者给出的可交互版本，很有意思\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/bump_function.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e有了这个基本单元之后，我们可以通过增加隐藏层神经元的个数来增加鼓包的个数，比如再增加一对隐层神经元，可增加一个鼓包。虽然下图的例子中两个鼓包相互独立，但通过调整4个\\(s\\)，可以让两个鼓包相连甚至交错，\u003ca href=\"http://neuralnetworksanddeeplearning.com/chap4.html#double_bump\"\u003e大家可以去原网页试一试\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.4.png\"\u003e\u003c/p\u003e\n\u003cp\u003e继续增加隐层神经元个数，则可以继续增加鼓包的数量，如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.5.png\"\u003e\u003c/p\u003e\n\u003cp\u003e到这里想必大家马上知道了为什么神经网络能拟合任何一个函数了，如果隐层神经元足够多，则右图的小鼓包可以足够密，通过调整每个鼓包的高度，则无穷多个鼓包的顶点连线可以拟合任意一个函数。这和我们求函数积分（函数下方面积）时使用多个小矩形近似是一个道理！\u003c/p\u003e\n\u003cp\u003e所以对于本章开头的未知函数，我们通过调整不同鼓包的高度，可以使得小矩形面积之和与真实积分的差在\\( \\epsilon=0.4\\)以内。如果无限增加隐层神经元个数，则可以无限逼近真实值。这就说明神经网络确实可以拟合任意一个函数。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.6.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上述推导稍微需要注意的一点是，右图的输出是未激活函数值\\(\\sum_j w_j a_j\\)，而网络真正的输出是激活值\\(\\sigma(\\sum_j w_j a_j + b)\\)。这没有太大的关系，因为上面已经说明未激活输出能拟合任意函数，激活函数也是一个函数。增加激活函数就要求右图需要拟合激活函数和真实函数的嵌套函数。既然未激活输出能拟合任意函数，肯定能拟合这个嵌套函数\\(\\sigma^{-1} \\circ f(x)\\)，再用激活函数作用一下\\(\\sigma\\circ\\sigma^{-1} \\circ f(x)\\)，激活函数抵消了，正好得到\\(f(x)\\)。\u003c/p\u003e\n\u003cp\u003e如果输入是多维，或者输出是多维，都是类似的道理。这就说明神经网络确实可以拟合任意函数，真的很强大哦。\u003c/p\u003e","title":"Neural Networks and Deep Learning（四）图解神经网络为什么能拟合任意函数"},{"content":"权重初始化 在之前的章节中，我们都是用一个标准正态分布\\(N(0,1^2)\\)初始化所有的参数\\(w\\)和\\(b\\)，但是当神经元数量比较多时，会出现意想不到的问题。\n假设一个神经网络的输入层有1000个神经元，且某个样本的1000维输入中，恰好有500维是0，另500维是1。我们目前考察隐藏层的第一个神经元，则该神经元为激活的输出为\\(z = \\sum_j w_j x_j+b\\)，因为输入中的500维是0，所以\\(z\\)相当于有501个来自\\(N(0,1^2)\\)的随机变量相加。因为\\(w_j\\)和\\(b\\)的初始化都是独立同分布的，所以\\(z\\)也是一个正态分布，均值为0，但方差变成了\\(\\sqrt{501} \\approx 22.4\\)，即\\(z\\sim N(0,\\sqrt{501}^2)\\)。我们知道对于正态分布，如果方差越小，则分布的形状是高廋型的；如果方差越大，则分布的形状是矮胖型的。所以\\(z\\)有很大的概率取值会远大于1或远小于-1。又因为激活函数是sigmoid，当\\(z\\)远大于1或远小于-1时，\\(\\sigma (z)\\)趋近于1或者0，且导数趋于0，变化缓慢，导致梯度消失。\n请注意，这里的梯度消失和之前介绍得梯度消失稍有不同，之前是说在误差反向传播过程中，损失函数对权重的导数中包含梯度消失项，所以可以通过更换损失函数来解决。但是这里的梯度消失并不是在误差反向传播过程中产生的，而是在正向传播产生的，跟损失函数没关系。\n解决这个问题的方法很简单，根据上面的分析，如果输入\\(x_j\\)全为1，\\(w\\)和\\(b\\)都来自\\(N(0,1^2)\\)，则\\(z\\sim N(0, \\sqrt{n+1}^2)\\)，其中\\(n\\)为输入样本的维度。要减小\\(z\\)的方差，减小\\(w\\)和\\(b\\)的方差就可以了。因为\\(b\\)只有一个，对整体的影响不大，可以不修改\\(b\\)的分布，\\(b\\)依然来自\\(N(0,1^2)\\)。把\\(w_j\\)的分布修改为\\(N(0, (\\frac{1}{\\sqrt{n}})^2)\\)，此时\\(z\\sim N(0, \\sqrt{2}^2)\\)，\\(\\sqrt{2}=1.414\\)就非常接近1了，\\(z\\)的分布也变成了一个高廋型的，梯度消失问题也就不存在了。\n如果是开头的例子，输入维度为1000，其中500为0，500为1，\\(w_j\\sim N(0, (\\frac{1}{\\sqrt{1000}})^2)\\)，\\(b\\sim N(0,1^2)\\)，则\\(z\\sim N(0, \\sqrt{\\frac{3}{2}}^2)\\)，\\(\\sqrt{3/2} = 1.22\\ldots\\)也是高廋型的，不会有梯度消失的问题。\n由下图可知，在新的权重初始化策略下，网络很快就收敛了，比之前的方法快很多。\n怎样选择超参数 大原则：在网络优化的前期，尽量使网络结构、问题简单，以便快速得实验结果，不断尝试超参数取值，当找到正确的优化方向后，再慢慢把网络和问题变复杂，精细调整超参数。比如MNIST问题，开始可以减少训练数据，只取0和1的图片，做二分类；同时可以减少网络层数，验证集大小等，以便快速得到网络输出，判断网络性能变化。这样可以快速尝试新的超参数。\n学习率\\(\\eta\\) 在误差反向传播中，学习率太大，虽然可以加速学习，但在后期可能导致网络震荡，无法收敛；学习率太小，导致学习速度太慢，训练时间过长。\n确定学习率的方法是：首先随便选定一个值，比如0.01，然后不断增大10倍：0.1, 1, 10, 100…如果发现cost曲线在震荡，说明选大了，要降低，直到找到一个比较合适的值。这个过程只要找到合适的数量级就可以了，不一定要非常精确。比如发现0.1是比较合适的，那么可以再尝试0.2,0.3…，如果发现0.5不错，可以设学习率为0.5的一半0.25，这样可以使得在后续epoch中，不容易发生震荡。最好的方法是可变学习率，即前期学习率稍大（0.5），后期学习率稍小（0.1）之类的。\nepoch no-improvement-in-ten rule，就是说如果模型在最近的10个epoch中，验证集的accuracy都没有提高，则可以stop了。在早期实验中可以这么做，后续精细优化时可以改变ten，比如no-improvement-in-20/30等。\n正则化参数\\(\\lambda\\) 首先不要正则（\\(\\lambda=0\\)），使用上面提到的方法确定学习率\\(\\eta\\)，在确定的学习率情况下，正则\\(\\lambda=1\\)开始进行优化，比如每次乘以10或者除以10，观察验证集上的accuracy指标，找到正则化所在的合适的数量级，然后再fine-tune。\nMini-batch size 太小了，无法利用现有软件包的矩阵操作的优势，速度会很慢。极端情况下，如果mini-batch size=1，就是说每次只用一个sample做BP，则100次mini-batch=1会比一次mini-batch=100操作慢很多，因为很多软件包对矩阵操作有优化，而没有对for循环优化。太大了，则一次BP要很久，参数更新的次数也比较少。\n其他技术 随机梯度下降SGD的变种 海森矩阵法 SGD优化的目标就是最小化损失函数\\(C\\)，\\(C\\)是所有参数\\(w = w_1, w_2, \\ldots\\)的函数，即\\(C=C(w)\\)。希望能够通过改变\\(w\\)，不断最小化\\(C\\)，即找一个\\(\\Delta w\\)，使得\\(C(w+\\Delta w)\\)最小化。把\\(C(w+\\Delta w)\\)泰勒展开得到：\n$$\\begin{eqnarray}C(w+\\Delta w) \u0026 = \u0026 C(w) + \\sum_j \\frac{\\partial C}{\\partial w_j} \\Delta w_j\\nonumber \\\\ \u0026 \u0026 + \\frac{1}{2} \\sum_{jk} \\Delta w_j \\frac{\\partial^2 C}{\\partial w_j\\partial w_k} \\Delta w_k + \\ldots\\tag{1}\\end{eqnarray}$$写成矩阵形式就是：\n$$\\begin{eqnarray}C(w+\\Delta w) = C(w) + \\nabla C \\cdot \\Delta w +\\frac{1}{2} \\Delta w^T H \\Delta w + \\ldots,\\tag{2}\\end{eqnarray}$$其中的\\(\\nabla C\\)就是常规的梯度向量，\\(H\\)就是著名的海森矩阵，其中\\(H_{jk}=\\partial^2 C / \\partial w_j \\partial w_k\\)。如果把(2)中的高阶项扔掉，得到如下的近似等式：\n$$\\begin{eqnarray} C(w+\\Delta w) \\approx C(w) + \\nabla C \\cdot \\Delta w +\\frac{1}{2} \\Delta w^T H \\Delta w.\\tag{3}\\end{eqnarray}$$最小化(3)的右边，得：\n$$\\begin{eqnarray}\\Delta w = -H^{-1} \\nabla C.\\tag{4}\\end{eqnarray}$$所以我们可以通过如下方式更新\\(w\\)已达到最小化\\(C\\)的目的，当然也可以给\\(\\Delta w\\)乘上学习率\\(\\eta\\)。\n海森矩阵法有点像通过解析的方式最小化\\(C\\)，感觉上比SGD方法更精确靠谱。事实上，海森矩阵法确实比SGD方法收敛速度更快，但是因为在公式(4)中需要求解海森矩阵\\(H\\)的逆矩阵\\(H^{-1}\\)，当网络的参数量很大时，求解过程会非常慢，导致海森矩阵法不实用。\n基于动量的梯度下降 增加速度这个变量，个人不是太理解：\n$$\\begin{eqnarray} v \u0026 \\rightarrow \u0026 v’ = \\mu v – \\eta \\nabla C \\tag{5}\\\\w \u0026 \\rightarrow \u0026 w’ = w+v’.\\tag{6}\\end{eqnarray}$$不同的激活函数 tanh是和sigmoid很像的一个激活函数，其函数形式为：\n$$\\begin{eqnarray}\\tanh(z) \\equiv \\frac{e^z-e^{-z}}{e^z+e^{-z}}.\\tag{7}\\end{eqnarray}$$事实上，sigmoid函数\\(\\sigma(z)\\)和\\(\\tanh(z)\\)有线性关系：\n$$\\begin{eqnarray} \\sigma(z) = \\frac{1+\\tanh(z/2)}{2},\\tag{8}\\end{eqnarray}$$\\(\\tanh(z)\\)的函数图像如下，和sigmoid非常类似：\n\\(\\tanh(z)\\)和sigmoid的主要区别就是值域不一样，前者值域为[-1,1]，后者值域为[0,1]。这会导致什么差异呢？观察(BP4)这个公式，对于第\\(l\\)层的第\\(j\\)个神经元和第\\(l-1\\)层的所有神经元的连接权重\\(w_{jk}^l\\)，如果使用sigmoid激活，则\\(a_k^{l-1}\\)都是非负的，而这些梯度共用一个\\(\\delta_j^l\\)，所以对于固定的\\(j\\)，不同的\\(k\\)，所有的梯度\\(\\frac{\\partial C}{\\partial w^l_{jk}}\\)方向是一样的！这在无形中就减小了搜索空间。而如果用\\(\\tanh(z)\\)激活的话，不同的\\(k\\)的\\(a^{l-1}_k\\)正负号可能就不一样，搜索空间更大，更容易收敛。\n另一个比较常见的激活函数是ReLU激活函数，其函数形式如下：\n$$ReLU(z)=max(0, z)\\tag{9}$$函数图像如下：\nReLU和Sigmoid、tanh很不一样，ReLU在\\(z\u003e0\\)的方向上不会有梯度消失的问题。\n好了，第三章的内容就全部介绍完毕了，这一章介绍了很多调试神经网络的经验法则，没有太多的理论基础，相信随着这个领域的发展，神经网络的黑盒子会被慢慢打开。\n","permalink":"http://localhost:1313/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/","summary":"\u003ch1 id=\"权重初始化\"\u003e权重初始化\u003c/h1\u003e\n\u003cp\u003e在之前的章节中，我们都是用一个标准正态分布\\(N(0,1^2)\\)初始化所有的参数\\(w\\)和\\(b\\)，但是当神经元数量比较多时，会出现意想不到的问题。\u003c/p\u003e\n\u003cp\u003e假设一个神经网络的输入层有1000个神经元，且某个样本的1000维输入中，恰好有500维是0，另500维是1。我们目前考察隐藏层的第一个神经元，则该神经元为激活的输出为\\(z = \\sum_j w_j x_j+b\\)，因为输入中的500维是0，所以\\(z\\)相当于有501个来自\\(N(0,1^2)\\)的随机变量相加。因为\\(w_j\\)和\\(b\\)的初始化都是独立同分布的，所以\\(z\\)也是一个正态分布，均值为0，但方差变成了\\(\\sqrt{501} \\approx 22.4\\)，即\\(z\\sim N(0,\\sqrt{501}^2)\\)。我们知道对于正态分布，如果方差越小，则分布的形状是高廋型的；如果方差越大，则分布的形状是矮胖型的。所以\\(z\\)有很大的概率取值会远大于1或远小于-1。又因为激活函数是sigmoid，当\\(z\\)远大于1或远小于-1时，\\(\\sigma (z)\\)趋近于1或者0，且导数趋于0，变化缓慢，导致梯度消失。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz32.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/ch3.8.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e请注意，这里的梯度消失和\u003ca href=\"https://bitjoy.net/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/\"\u003e之前介绍得梯度消失\u003c/a\u003e稍有不同，之前是说在误差反向传播过程中，损失函数对权重的导数中包含梯度消失项，所以可以通过更换损失函数来解决。但是这里的梯度消失并不是在误差反向传播过程中产生的，而是在正向传播产生的，跟损失函数没关系。\u003c/p\u003e\n\u003cp\u003e解决这个问题的方法很简单，根据上面的分析，如果输入\\(x_j\\)全为1，\\(w\\)和\\(b\\)都来自\\(N(0,1^2)\\)，则\\(z\\sim N(0, \\sqrt{n+1}^2)\\)，其中\\(n\\)为输入样本的维度。要减小\\(z\\)的方差，减小\\(w\\)和\\(b\\)的方差就可以了。因为\\(b\\)只有一个，对整体的影响不大，可以不修改\\(b\\)的分布，\\(b\\)依然来自\\(N(0,1^2)\\)。把\\(w_j\\)的分布修改为\\(N(0, (\\frac{1}{\\sqrt{n}})^2)\\)，此时\\(z\\sim N(0, \\sqrt{2}^2)\\)，\\(\\sqrt{2}=1.414\\)就非常接近1了，\\(z\\)的分布也变成了一个高廋型的，梯度消失问题也就不存在了。\u003c/p\u003e\n\u003cp\u003e如果是开头的例子，输入维度为1000，其中500为0，500为1，\\(w_j\\sim N(0, (\\frac{1}{\\sqrt{1000}})^2)\\)，\\(b\\sim N(0,1^2)\\)，则\\(z\\sim N(0, \\sqrt{\\frac{3}{2}}^2)\\)，\\(\\sqrt{3/2} = 1.22\\ldots\\)也是高廋型的，不会有梯度消失的问题。\u003c/p\u003e\n\u003cp\u003e由下图可知，在新的权重初始化策略下，网络很快就收敛了，比之前的方法快很多。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/weight_initialization_30.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"怎样选择超参数\"\u003e怎样选择超参数\u003c/h1\u003e\n\u003cp\u003e大原则：在网络优化的前期，尽量使网络结构、问题简单，以便快速得实验结果，不断尝试超参数取值，当找到正确的优化方向后，再慢慢把网络和问题变复杂，精细调整超参数。比如MNIST问题，开始可以减少训练数据，只取0和1的图片，做二分类；同时可以减少网络层数，验证集大小等，以便快速得到网络输出，判断网络性能变化。这样可以快速尝试新的超参数。\u003c/p\u003e\n\u003ch1 id=\"学习率\"\u003e学习率\\(\\eta\\)\u003c/h1\u003e\n\u003cp\u003e在误差反向传播中，学习率太大，虽然可以加速学习，但在后期可能导致网络震荡，无法收敛；学习率太小，导致学习速度太慢，训练时间过长。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/multiple_eta.png\"\u003e\u003c/p\u003e\n\u003cp\u003e确定学习率的方法是：首先随便选定一个值，比如0.01，然后不断增大10倍：0.1, 1, 10, 100…如果发现cost曲线在震荡，说明选大了，要降低，直到找到一个比较合适的值。这个过程只要找到合适的数量级就可以了，不一定要非常精确。比如发现0.1是比较合适的，那么可以再尝试0.2,0.3…，如果发现0.5不错，可以设学习率为0.5的一半0.25，这样可以使得在后续epoch中，不容易发生震荡。最好的方法是可变学习率，即前期学习率稍大（0.5），后期学习率稍小（0.1）之类的。\u003c/p\u003e\n\u003ch1 id=\"epoch\"\u003eepoch\u003c/h1\u003e\n\u003cp\u003eno-improvement-in-ten rule，就是说如果模型在最近的10个epoch中，验证集的accuracy都没有提高，则可以stop了。在早期实验中可以这么做，后续精细优化时可以改变ten，比如no-improvement-in-20/30等。\u003c/p\u003e\n\u003ch1 id=\"正则化参数\"\u003e正则化参数\\(\\lambda\\)\u003c/h1\u003e\n\u003cp\u003e首先不要正则（\\(\\lambda=0\\)），使用上面提到的方法确定学习率\\(\\eta\\)，在确定的学习率情况下，正则\\(\\lambda=1\\)开始进行优化，比如每次乘以10或者除以10，观察验证集上的accuracy指标，找到正则化所在的合适的数量级，然后再fine-tune。\u003c/p\u003e\n\u003ch1 id=\"mini-batch-size\"\u003eMini-batch size\u003c/h1\u003e\n\u003cp\u003e太小了，无法利用现有软件包的矩阵操作的优势，速度会很慢。极端情况下，如果mini-batch size=1，就是说每次只用一个sample做BP，则100次mini-batch=1会比一次mini-batch=100操作慢很多，因为很多软件包对矩阵操作有优化，而没有对for循环优化。太大了，则一次BP要很久，参数更新的次数也比较少。\u003c/p\u003e\n\u003ch1 id=\"其他技术\"\u003e其他技术\u003c/h1\u003e\n\u003ch2 id=\"随机梯度下降sgd的变种\"\u003e随机梯度下降SGD的变种\u003c/h2\u003e\n\u003ch3 id=\"海森矩阵法\"\u003e海森矩阵法\u003c/h3\u003e\n\u003cp\u003eSGD优化的目标就是最小化损失函数\\(C\\)，\\(C\\)是所有参数\\(w = w_1, w_2, \\ldots\\)的函数，即\\(C=C(w)\\)。希望能够通过改变\\(w\\)，不断最小化\\(C\\)，即找一个\\(\\Delta w\\)，使得\\(C(w+\\Delta w)\\)最小化。把\\(C(w+\\Delta w)\\)泰勒展开得到：\u003c/p\u003e\n$$\\begin{eqnarray}C(w+\\Delta w) \u0026 = \u0026 C(w) + \\sum_j \\frac{\\partial C}{\\partial w_j} \\Delta w_j\\nonumber \\\\ \u0026 \u0026 + \\frac{1}{2} \\sum_{jk} \\Delta w_j \\frac{\\partial^2 C}{\\partial w_j\\partial w_k} \\Delta w_k + \\ldots\\tag{1}\\end{eqnarray}$$\u003cp\u003e写成矩阵形式就是：\u003c/p\u003e","title":"Neural Networks and Deep Learning（三·三）权重初始化及其他"},{"content":"过拟合介绍 首先介绍一下神经网络中不同数据集的功能，包括训练集、验证集和测试集。\n训练集是用来训练网络参数的。当觉得在训练集上训练得差不多时，就可以在验证集上进行测试，如果验证集上的性能不好，则需要调整网络结构或者超参数，重新在训练集上训练。所以本质上验证集指导训练过程，也参与了训练和调参。为了防止网络对验证集过拟合，当网络在训练集和验证集上表现都不错时，就可以在测试集上进行测试了。测试集上的性能代表了模型的最终性能。\n当然如果发现网络在测试集上性能不好，可能还会反过来去优化网络，重新训练和验证，这么说测试集最终也变相参与了调优。如果一直这么推下去的话，就没完没了了，所以一般还是认为用验证集对模型进行优化，用测试集对模型性能进行测试。\n过拟合的含义就是网络在训练集上性能很好，但是在验证集（或者测试集）上的性能较差，这说明网络在训练集上训练过头了，对训练集产生了过拟合。为了便于叙述，本文没有验证集，直接使用测试集作为验证集对模型进行调优，所以主要考察网络在训练集和测试集上的性能表现。\n判断网络是否过拟合的方法就是观察网络在训练集和测试集上的accuracy和loss的变化曲线。对于accuracy，如果训练集的accuracy很高接近100%且收敛了，但测试集上的accuracy和训练集上的accuracy相差较大也收敛了（如下图收敛到82%左右），说明网络过拟合了。对于loss，如果训练集的loss一直在下降，但测试集的loss先下降后又上升，也说明网络过拟合了。这两种现象，虽然指标不同，但含义是一样的，即网络在训练集上的性能一直在提高甚至到完美水平，但在测试集上的性能提高到一定水平后不再变化甚至下降了。\n不过下面几张图反应的过拟合epoch时间可能不一样，比如对于测试集上的accuracy，可能在280左右过拟合，但是对于测试集上的loss，在15和280左右都可以认为是过拟合了，尤其是15，loss最低，之后loss反升，可以认为是一个合理的过拟合的点。具体哪个epoch之后过拟合，取决于问题本身关注哪个指标，比如MNIST分类问题，可能关注分类accuracy，所以可重点关注测试集上的accuracy那个图，认为是280左右过拟合，因为200~280的accuracy还一直有提升，虽然提升很有限。\n应对过拟合最好的方法就是增加训练数据，如果能把所有可能的数据都收集到，对所有数据产生过拟合，那相当于对所有数据都能预测得很好，那问题本质上已经解决了。\n但是，在实际应用场景中，不可能收集到所有数据，而且数据往往是严重不足的，此时，应对过拟合主要有三种方法：正则化、Dropout和数据增强，下面分别介绍这三个部分。\n正则化 正则化的思路就是修改损失函数，使损失函数考虑模型复杂度。考虑正则化的损失函数的通用公式如下：\n$$\\begin{eqnarray} C = C_0(w,x,y) + \\lambda\\Omega(w)\\tag{1}\\end{eqnarray}$$其中\\(C_0\\)为原始的没有正则化项的损失函数，比如MSE或者交叉熵损失等，\\(\\Omega(w)\\)表示正则化项，即用来惩罚模型复杂度的，\\(\\lambda\\)表示正则化参数，用来平衡\\(C_0\\)和\\(\\Omega(w)\\)的重要性。\n正则化又分为L2正则和L1正则，它们很类似，先详细介绍下L2正则。\n举个例子，L2正则化后的损失函数如下：\n$$\\begin{eqnarray} C = C_0 + \\frac{\\lambda}{2n}\\sum_w w^2,\\tag{2}\\end{eqnarray}$$前半部分就是普通的损失函数（比如MSE或者交叉熵损失），后半部分就是L2正则。L2正则是对网络中的所有权重\\(w\\)求平方和（\\(\\vec w\\)的L2范数，所以叫L2正则），然后除以\\(2n\\)，其中\\(n\\)是训练样本数，除以2应该是为了后面求导方便。\n(2)式的直观含义是，\\(\\min C\\)的过程中，我不但希望损失函数本身\\(C_0\\)足够小，还希望网络的权重\\(w\\)也比较小，最好不要出现很大的\\(w\\)。如果\\(\\lambda\\)越大，表示正则化越厉害，对大的\\(w\\)惩罚越严重。\n加入L2正则后的梯度也很容易计算，如下：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w} \u0026 = \u0026 \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\tag{3}\\\\ \\frac{\\partial C}{\\partial b} \u0026 = \u0026 \\frac{\\partial C_0}{\\partial b}.\\tag{4}\\end{eqnarray}$$对应的参数更新公式如下：\n$$\\begin{eqnarray}b \u0026 \\rightarrow \u0026 b -\\eta \\frac{\\partial C_0}{\\partial b}.\\tag{5}\\end{eqnarray}$$$$\\begin{eqnarray} w \u0026 \\rightarrow \u0026 w-\\eta \\frac{\\partial C_0}{\\partial w}-\\frac{\\eta \\lambda}{n} w \\tag{6}\\\\ \u0026 = \u0026 \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial C_0}{\\partial w}. \\tag{7}\\end{eqnarray}$$由(5)可知，偏移量\\(b\\)的梯度更新和没有正则化时是一样的，因为正则化并没有惩罚\\(b\\)，这个后面会解释为什么。由(7)可知，对\\(w\\)的梯度更新和没有正则化时很类似，只不过需要先对\\(w\\)进行缩放，缩放因子为\\(1-\\frac{\\eta\\lambda}{n}\\)，因为训练样本\\(n\\)往往很大，所以缩放因子在(0,1)，即先对\\(w\\)进行缩小，然后正常梯度下降，这种操作也被称为权值衰减。\\(\\lambda\\)最好根据\\(n\\)的大小进行调整，如果\\(n\\)非常大的话，\\(\\lambda\\)最好也大一些，否则权值衰减因子就会很小，正则化效果就不明显。\n如果是mini-batch进行更新的话，公式也类似，梯度对mini-batch中的\\(m\\)个样本进行平均。\n$$\\begin{eqnarray} w \\rightarrow \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m}\\sum_x \\frac{\\partial C_x}{\\partial w}, \\tag{8}\\end{eqnarray}$$$$\\begin{eqnarray}b \\rightarrow b – \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial b},\\tag{9}\\end{eqnarray}$$通过简单的L2正则，就能解决文章开头的过拟合问题，使得测试集上的accuracy在200~400个epoch时一直有提升。\n正则化不仅能解决过拟合问题，还能使结果更加稳定。我们都知道，神经网络的参数是随机初始化的，很有可能不同的随机初始值收敛到的网络权重不一样，导致最终模型性能有差异。加入正则化后，对所有权重都有正则化约束，使得梯度下降在探索不同\\(w\\)的方向时，都能得到一定程度的更新，而不会说某个方向比较好就一直在那个方向探索。\n那么正则化为什么能解决过拟合问题呢？从公式(1)可知，加入正则化之后，除了要优化网络对训练数据预测的准确度之外，还需要使得网络的权重\\(w\\)尽量小，权重越小则网络越简单，极端情况下如果权重等于0，则相当于少了一个参数，网络肯定更简单了。所以加入正则化之后，网络的复杂度降低了，网络的泛化能力就更强了，也就更不容易过拟合。\n以一个很简单的(x,y)的二维数据拟合为例，我们既可以用一个九次函数\\(y = a_0 x^9 + a_1 x^8 + \\ldots + a_9\\)完美拟合，也可以用一个一次函数\\(y = 2x\\)拟合。虽然九次函数能完美拟合，但是其最高次幂为9，一旦真实数据中有一些噪声，即x有一点噪声，则预测得到的y就会相差特别大，这其实就是非常严重的过拟合，即在训练数据上性能非常好，但在测试数据上性能可能较差，而且对噪声很不稳定。而一次函数虽然在训练集上不能完美拟合，但效果也不差，而且预想到它在应对有噪声的数据x时，也能预测得比较准确，所以一次函数的泛化效果更好，更不容易过拟合。\n对应到神经网络中，\\(w\\)的大小可以类比成上面例子中的最高次幂，如果\\(w\\)很大，则x的微小扰动都可能造成网络输出的很大变化，就类似于上面的9次函数，很可能学到了数据中的局部噪声的特征。而\\(w\\)较小的网络，可能就没学到这些局部噪声的特征，而只是学到了数据中全局的（或者大部分数据都有的）、高频的特征，这其实是好事，这种网络在新的数据上的泛化能力就会更好，不易被噪声干扰。\n其实哲学界还有一个“奥卡姆剃刀”原则，就是说如果一个数据集既能用简单模型解释，也能用复杂模型解释，那就尽量选择简单模型吧，简单就是美啊。\n对于神经网络来说，还有一个很神奇的事情，就拿本文构造的神经网络为例，如果隐藏层有100个神经元，则大概有80000个参数，但是我们的训练数据集中只有50000张谱图，训练数据小于模型参数个数，按理说会导致严重的过拟合，但实际效果并没有，网络在测试集上的accuracy也很高。所以这很神奇，好像和机器学习中的经验风险最小化定理冲突。有人猜测这可能和多层神经网络有“自正则化”的效果有关。\n最后，解释一下为什么正则化项中只有\\(w\\)没有\\(b\\)。因为每个神经元的操作是\\(\\sigma(wx+b)\\)，\\(w\\)和\\(x\\)是乘法，影响更大，\\(b\\)对神经元输出的影响并不大，没必要正则化，当然强行对\\(b\\)正则也可以，只是和没对\\(b\\)正则效果差不多。说多了都是玄学，经验之谈。\nL1正则和L2正则类似，只不过换成了对\\(w\\)的一范数，损失函数如下：\n$$\\begin{eqnarray} C = C_0 + \\frac{\\lambda}{n} \\sum_w |w|.\\tag{10}\\end{eqnarray}$$对\\(w\\)的权重更新公式如下，和(7)很类似，不细说了。\n$$\\begin{eqnarray} w \\rightarrow w’ =w-\\frac{\\eta \\lambda}{n} \\mbox{sgn}(w) – \\eta \\frac{\\partial C_0}{\\partial w},\\tag{11}\\end{eqnarray}$$L1正则和L2正则都有一个权重衰减因子，虽然因子不同，但目的都是为了让\\(w\\)衰减，尽量小。他们的差别是L2衰减的是\\(w\\)的一个比例（公式(7)，\\(w\\left(1 – \\frac{\\eta \\lambda}{n} \\right)\\)），而L1衰减的是一个固定值（公式(11)，\\(w-\\frac{\\eta \\lambda}{n} \\mbox{sgn}(w)\\)）。这就导致如果\\(w\\)本身很大的话，L1减的相对更少，而L2减的相对更多；如果\\(w\\)本身很小的的话，L1减的相对就更多，而L2减的相对更少。所以L1倾向于把小的\\(w\\)直接减到0，而对大的\\(w\\)不怎么减，而L2对所有\\(w\\)都一视同仁的减一定的比例。所以L1倾向于保留大的\\(w\\)，而直接把小的\\(w\\)正则为0，所以L1得到的非0权重可能更少，能得到稀疏模型，有利于特征选择。\n对L1和L2正则的区别，比较常见的解释是二维情况下的图解，网上很多，比如：https://blog.csdn.net/jinping_shi/article/details/52433975。\nDropout Dropout的做法和L1正则、L2正则很不一样，它不对损失函数进行修改，而是对网络结构进行修改。Dropout会在每个mini-batch训练时，随机删掉网络中一半的神经元（并不是真正的删除，而是暂时把对应的\\(w\\)设置为0）。它的效果相当于每次训练只用了一半的神经元，那么这次mini-batch和下次mini-batch相当于训练了不同的网络，最后预测时恢复所有的神经元。这样得到的预测结果相当于多个神经网络进行了average或者voting，使得预测结果更加鲁棒。比如有5个模型，其中3个模型预测对了，2个模型预测错了，做一下voting的话，就能得到正确的结果，而如果只是一个模型的话，如果刚好预测错了，那就错了。\nDropout的另一个解释时，因为网络在训练时会随机删掉一半的节点，那么节点间的依赖关系就减弱了，迫使神经元依赖较少的信息也要得到比较好的预测结果，所以网络会更加鲁棒。Dropout在深层神经网络中，特别有用，能有效防止过拟合。不过也正因为此，dropout会增加训练收敛的时间，这是可以理解的。\n数据增强 本博客开头就提到，解决过拟合最有效的方法就是增加训练数据集，但收集到的数据很少，有没有办法根据已收集的数据集，产生新的数据集呢，数据增强就是干这个的。\n对图片数据来说，可做的数据增强包括：旋转、移位、翻转等。比如MNIST数据集，对一张手写数字图片4旋转15度，这张图片依然表示数字4，虽然人眼很容易识别旋转15度之后依然是4，但对网络来说，这个变化非常大，可以认为是一个新的样本。不过旋转角度不能太大了，比如6旋转变成9了，就不能标注成6了。\n对于语音识别任务来说，数据增强可做的包括对语音添加背景噪声，快进，慢放等。总之，针对不同的任务，可以有不同的数据增强方法，基本原则就是增强之后的数据确实是现实世界中存在的数据，比如手写数字图片的轻微旋转，现实中就是有人写字写歪了一点；对语音进行快进，现实中就是有人说话快一点等。\n最后总结一下，防止过拟合的方法包括：L2正则、L1正则、Dropout和数据增强。\n","permalink":"http://localhost:1313/posts/2019-03-24-neural-networks-and-deep-learning-3-2-overfitting-and-regularization/","summary":"\u003ch1 id=\"过拟合介绍\"\u003e过拟合介绍\u003c/h1\u003e\n\u003cp\u003e首先介绍一下神经网络中不同数据集的功能，包括训练集、验证集和测试集。\u003c/p\u003e\n\u003cp\u003e训练集是用来训练网络参数的。当觉得在训练集上训练得差不多时，就可以在验证集上进行测试，如果验证集上的性能不好，则需要调整网络结构或者超参数，重新在训练集上训练。所以本质上验证集指导训练过程，也参与了训练和调参。为了防止网络对验证集过拟合，当网络在训练集和验证集上表现都不错时，就可以在测试集上进行测试了。测试集上的性能代表了模型的最终性能。\u003c/p\u003e\n\u003cp\u003e当然如果发现网络在测试集上性能不好，可能还会反过来去优化网络，重新训练和验证，这么说测试集最终也变相参与了调优。如果一直这么推下去的话，就没完没了了，所以一般还是认为用验证集对模型进行优化，用测试集对模型性能进行测试。\u003c/p\u003e\n\u003cp\u003e过拟合的含义就是网络在训练集上性能很好，但是在验证集（或者测试集）上的性能较差，这说明网络在训练集上训练过头了，对训练集产生了过拟合。为了便于叙述，本文没有验证集，直接使用测试集作为验证集对模型进行调优，所以主要考察网络在训练集和测试集上的性能表现。\u003c/p\u003e\n\u003cp\u003e判断网络是否过拟合的方法就是观察网络在训练集和测试集上的accuracy和loss的变化曲线。对于accuracy，如果训练集的accuracy很高接近100%且收敛了，但测试集上的accuracy和训练集上的accuracy相差较大也收敛了（如下图收敛到82%左右），说明网络过拟合了。对于loss，如果训练集的loss一直在下降，但测试集的loss先下降后又上升，也说明网络过拟合了。这两种现象，虽然指标不同，但含义是一样的，即网络在训练集上的性能一直在提高甚至到完美水平，但在测试集上的性能提高到一定水平后不再变化甚至下降了。\u003c/p\u003e\n\u003cp\u003e不过下面几张图反应的过拟合epoch时间可能不一样，比如对于测试集上的accuracy，可能在280左右过拟合，但是对于测试集上的loss，在15和280左右都可以认为是过拟合了，尤其是15，loss最低，之后loss反升，可以认为是一个合理的过拟合的点。具体哪个epoch之后过拟合，取决于问题本身关注哪个指标，比如MNIST分类问题，可能关注分类accuracy，所以可重点关注测试集上的accuracy那个图，认为是280左右过拟合，因为200~280的accuracy还一直有提升，虽然提升很有限。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/overfitting4.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/overfitting2.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/overfitting1.png\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/overfitting3.png\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e应对过拟合最好的方法就是增加训练数据，如果能把所有可能的数据都收集到，对所有数据产生过拟合，那相当于对所有数据都能预测得很好，那问题本质上已经解决了。\u003c/p\u003e\n\u003cp\u003e但是，在实际应用场景中，不可能收集到所有数据，而且数据往往是严重不足的，此时，应对过拟合主要有三种方法：正则化、Dropout和数据增强，下面分别介绍这三个部分。\u003c/p\u003e\n\u003ch1 id=\"正则化\"\u003e正则化\u003c/h1\u003e\n\u003cp\u003e正则化的思路就是修改损失函数，使损失函数考虑模型复杂度。考虑正则化的损失函数的通用公式如下：\u003c/p\u003e\n$$\\begin{eqnarray} C = C_0(w,x,y) + \\lambda\\Omega(w)\\tag{1}\\end{eqnarray}$$\u003cp\u003e其中\\(C_0\\)为原始的没有正则化项的损失函数，比如MSE或者交叉熵损失等，\\(\\Omega(w)\\)表示正则化项，即用来惩罚模型复杂度的，\\(\\lambda\\)表示正则化参数，用来平衡\\(C_0\\)和\\(\\Omega(w)\\)的重要性。\u003c/p\u003e\n\u003cp\u003e正则化又分为L2正则和L1正则，它们很类似，先详细介绍下L2正则。\u003c/p\u003e\n\u003cp\u003e举个例子，L2正则化后的损失函数如下：\u003c/p\u003e\n$$\\begin{eqnarray} C = C_0 + \\frac{\\lambda}{2n}\\sum_w w^2,\\tag{2}\\end{eqnarray}$$\u003cp\u003e前半部分就是普通的损失函数（比如MSE或者交叉熵损失），后半部分就是L2正则。L2正则是对网络中的所有权重\\(w\\)求平方和（\\(\\vec w\\)的L2范数，所以叫L2正则），然后除以\\(2n\\)，其中\\(n\\)是训练样本数，除以2应该是为了后面求导方便。\u003c/p\u003e\n\u003cp\u003e(2)式的直观含义是，\\(\\min C\\)的过程中，我不但希望损失函数本身\\(C_0\\)足够小，还希望网络的权重\\(w\\)也比较小，最好不要出现很大的\\(w\\)。如果\\(\\lambda\\)越大，表示正则化越厉害，对大的\\(w\\)惩罚越严重。\u003c/p\u003e\n\u003cp\u003e加入L2正则后的梯度也很容易计算，如下：\u003c/p\u003e\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w} \u0026 = \u0026 \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\tag{3}\\\\ \\frac{\\partial C}{\\partial b} \u0026 = \u0026 \\frac{\\partial C_0}{\\partial b}.\\tag{4}\\end{eqnarray}$$\u003cp\u003e对应的参数更新公式如下：\u003c/p\u003e\n$$\\begin{eqnarray}b \u0026 \\rightarrow \u0026 b -\\eta \\frac{\\partial C_0}{\\partial b}.\\tag{5}\\end{eqnarray}$$$$\\begin{eqnarray} w \u0026 \\rightarrow \u0026 w-\\eta \\frac{\\partial C_0}{\\partial w}-\\frac{\\eta \\lambda}{n} w \\tag{6}\\\\ \u0026 = \u0026 \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial C_0}{\\partial w}. \\tag{7}\\end{eqnarray}$$\u003cp\u003e由(5)可知，偏移量\\(b\\)的梯度更新和没有正则化时是一样的，因为正则化并没有惩罚\\(b\\)，这个后面会解释为什么。由(7)可知，对\\(w\\)的梯度更新和没有正则化时很类似，只不过需要先对\\(w\\)进行缩放，缩放因子为\\(1-\\frac{\\eta\\lambda}{n}\\)，因为训练样本\\(n\\)往往很大，所以缩放因子在(0,1)，即先对\\(w\\)进行缩小，然后正常梯度下降，这种操作也被称为权值衰减。\\(\\lambda\\)最好根据\\(n\\)的大小进行调整，如果\\(n\\)非常大的话，\\(\\lambda\\)最好也大一些，否则权值衰减因子就会很小，正则化效果就不明显。\u003c/p\u003e","title":"Neural Networks and Deep Learning（三·二）过拟合与正则化"},{"content":"原文的第三章内容较多，本博客将分三个部分进行介绍：梯度消失、过拟合与正则化、权重初始化及其他，首先介绍梯度消失问题。\n为简单起见，假设网络只包含一个输入和一个神经元，网络的损失是均方误差损失MSE，激活函数是Sigmoid函数。则该网络的参数只包含权重\\(w\\)和偏移量\\(b\\)。我们想训练这个网络，使得当输入为1时，输出0。\n假设我们随机初始化\\(w_0=0.6\\)，\\(b_0=0.9\\)，则网络的损失随着训练的epoch变化曲线如下，看起来挺好的，一开始损失下降很快，随着epoch增加，损失下降逐渐平缓，直至收敛。\n但是，如果随机初始化\\(w_0=2.0\\)，\\(b_0=2.0\\)，则网络的损失一开始下降得很缓慢，要训练到快200个epoch时，损失才快速下降。可以看到同样是300个epoch，由于初始化权重的差别，损失下降的趋势完全不一样，而且对于下面这种情况，到300个epoch时，损失还有下降的空间，所以期望的output不如上面的接近目标值0。\n为什么同样的网络，只是因为初始化权重的差异，损失的变化曲线却相差这么多呢，这和我们选择的损失函数与激活函数有关。\n回顾一下，我们在上一讲的末尾介绍到如果损失函数是MSE且激活函数是Sigmoid时，有\\(\\delta^L = (a^L-y) \\odot \\{\\sigma(z^L)(1-\\sigma(z^L))\\}\\)，又因为网络只有一个神经元，所以梯度如下：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w} \u0026 = \u0026 (a-y)\\sigma'(z) x = a \\sigma'(z),\\tag{1}\\\\\\frac{\\partial C}{\\partial b} \u0026 = \u0026 (a-y)\\sigma'(z) = a \\sigma'(z)\\tag{2}\\end{eqnarray}$$其中第二个等号是把\\(x=1\\)和\\(y=0\\)带入得到的。由此可见，误差对两个参数\\(w\\)和\\(b\\)的梯度都和激活函数的导数有关，因为激活函数是Sigmoid，当神经元的输出接近0或1时，梯度几乎为0，误差反向传播就会非常慢，导致上图出现损失下降非常慢的现象。这就是梯度消失的原因。\n为了解决这个问题，我们可以采取两种策略，一是替换损失函数，一是替换激活函数。\n第一种方法是将MSE的损失函数替换为交叉熵损失函数，激活函数依然是Sigmoid。我们考虑一个比本文开头更复杂的网络，仍然是一个输出神经元，但包含多个输入神经元。\n此时，交叉熵损失函数定义如下，其中的\\(n\\)表示训练样本数，\\(\\frac{1}{n}\\sum_x\\)表示对所有输入样本\\(x\\)的交叉熵损失求均值。\n$$\\begin{eqnarray}C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right]\\tag{3}\\end{eqnarray}$$我们首先考察为什么(3)可以是一个损失函数，损失函数需要满足如下两个条件：\n非负； 当网络输出和目标答案越接近，损失越小；反之损失越大。 简单代入几组不同的样本很容易验证交叉熵满足上述两个条件 ，所以交叉熵可以作为一个损失函数。\n下面我们再考察一下为什么交叉熵损失函数+Sigmoid激活函数可以解决梯度消失的问题。首先推导交叉熵损失\\(C\\)对权重\\(w_j\\)和\\(b\\)的梯度：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w_j} \u0026 = \u0026 -\\frac{1}{n} \\sum_x \\left(\\frac{y }{\\sigma(z)} -\\frac{(1-y)}{1-\\sigma(z)} \\right)\\frac{\\partial \\sigma}{\\partial w_j} \\tag{4}\\\\\u0026 = \u0026 -\\frac{1}{n} \\sum_x \\left(\\frac{y}{\\sigma(z)}-\\frac{(1-y)}{1-\\sigma(z)} \\right)\\sigma'(z) x_j\\tag{5}\\\\\u0026 = \u0026 \\frac{1}{n}\\sum_x \\frac{\\sigma'(z) x_j}{\\sigma(z) (1-\\sigma(z))}(\\sigma(z)-y).\\tag{6}\\end{eqnarray}$$上式分子Sigmoid的导数正好可以和分母抵消，得到：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w_j} = \\frac{1}{n} \\sum_x x_j(\\sigma(z)-y).\\tag{7}\\end{eqnarray}$$类似的，可得：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b} = \\frac{1}{n} \\sum_x (\\sigma(z)-y).\\tag{8}\\end{eqnarray}$$非常神奇的，我们发现交叉熵损失函数对参数的梯度中不存在Sigmoid的导数了！这样就不受Sigmoid两端梯度消失的影响了！而且从(7)和(8)还可以发现，当网络的输出\\(\\sigma(z)\\)和目标结果\\(y\\)差距越大，梯度越大；差距越小，梯度越小。这和人类的学习过程很相似，当犯错越大，教训越大，得到的经验也越多，提升也越明显。\n换成交叉熵损失函数+Sigmoid激活函数的组合后，我们再看看之前的两个例子：\n和预期的结果一样，无论哪一种情况，由于刚开始时网络的输出和预期答案差距较大，所以梯度也大，损失下降也很快，不再出现梯度消失的问题了。\n上面的讨论是基于输出层只有一个神经元的情况，如果输出层有\\(j\\)个神经元，交叉熵公式如下，结论是一样的。\n$$\\begin{eqnarray} C = -\\frac{1}{n} \\sum_x\\sum_j \\left[y_j \\ln a^L_j + (1-y_j) \\ln (1-a^L_j) \\right].\\tag{9}\\end{eqnarray}$$$$\\begin{eqnarray} \\delta^L = a^L-y.\\tag{10}\\end{eqnarray}$$$$\\begin{eqnarray} \\frac{\\partial C}{\\partial w^L_{jk}} \u0026 = \u0026 \\frac{1}{n} \\sum_x a^{L-1}_k (a^L_j-y_j).\\tag{11}\\end{eqnarray}$$如果输出层神经元的激活函数都是线性的，不再是Sigmoid函数，此时依然使用MSE损失函数也没有梯度消失的问题，公式如下：\n$$\\begin{eqnarray}a^L_j = z^L_j.\\tag{12}\\end{eqnarray}$$$$\\begin{eqnarray}\\delta^L = a^L-y.\\tag{13}\\end{eqnarray}$$$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w^L_{jk}} \u0026 = \u0026 \\frac{1}{n} \\sum_x a^{L-1}_k (a^L_j-y_j) \\tag{14}\\\\\\frac{\\partial C}{\\partial b^L_{j}} \u0026 = \u0026 \\frac{1}{n} \\sum_x (a^L_j-y_j).\\tag{15}\\end{eqnarray}$$ 那么交叉熵这个损失函数是怎么来的呢，是碰巧有人发现使用交叉熵能抵消Sigmoid的梯度消失问题吗？事实上，我们可以从MSE+Sigmoid的梯度消失问题中推导得到交叉熵损失函数。还是以开头只有一个神经元的网络为例，对于公式(1)和(2)，如果希望能抵消掉激活函数的导数\\(\\sigma'(z)\\)，则我们的目标为：\n$$\\begin{eqnarray} \\frac{\\partial C}{\\partial w_j} \u0026 = \u0026 x_j(a-y) \\tag{16}\\\\\\frac{\\partial C}{\\partial b } \u0026 = \u0026 (a-y).\\tag{17}\\end{eqnarray}$$又根据原始推导和Sigmoid导数有：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial a} \\sigma'(z)= \\frac{\\partial C}{\\partial a} a(1-a).\\tag{18}\\end{eqnarray}$$对比公式(17)有：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial a} = \\frac{a-y}{a(1-a)}.\\tag{19}\\end{eqnarray}$$据此可推导得到损失函数为：\n$$\\begin{eqnarray}C = -[y \\ln a + (1-y) \\ln (1-a)]+ {\\rm constant},\\tag{20}\\end{eqnarray}$$如果有多个输入样本，则损失函数为：\n$$\\begin{eqnarray}C = -\\frac{1}{n} \\sum_x [y \\ln a +(1-y) \\ln(1-a)] + {\\rm constant},\\tag{21}\\end{eqnarray}$$可以看到，上式本质上和公式(3)是一样的，这就是交叉熵损失函数！所以交叉熵并不神秘，不是拍脑袋凭空想出来的，而是可以根据目标求解出来的一个损失函数。\n上面介绍了第一种替换损失函数但保留Sigmoid激活函数来解决梯度消失的方法，第二种方法就是把损失函数和激活函数都换掉的方法，损失函数替换为log似然损失，激活函数替换为Softmax。\nSoftmax激活函数的公式如下：\n$$\\begin{eqnarray} a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}},\\tag{22}\\end{eqnarray}$$此时，某个神经元的输出\\(a_j^L\\)不再只跟它自己未激活的\\(z_j^L\\)有关，而是和所有的未激活值\\(z_k^L\\)有关。而且\\(a_j^L\\)有一个很好的特性，它用所有未激活值的和做了归一化，所以每个输出\\(a_j^L\\)可以看成输出为\\(j\\)的概率值，满足：\n$$\\begin{eqnarray}\\sum_j a^L_j \u0026 = \u0026 \\frac{\\sum_j e^{z^L_j}}{\\sum_k e^{z^L_k}} = 1.\\tag{23}\\end{eqnarray}$$log似然损失函数如下：\n$$\\begin{eqnarray}C \\equiv -\\ln a^L_y.\\tag{24}\\end{eqnarray}$$比如在手写数字识别问题中，如果输入是一个\\(7\\)的图片，那么对应这个样本的损失就是\\(-\\ln a^L_7\\)，如果输出的第7个神经元的概率\\(a_7^L\\)很高接近于1，则损失\\(-\\ln a^L_7\\)会很小；反之，如果\\(a_7^L\\)很低接近于0，则损失\\(-\\ln a^L_7\\)会很大。\n同样的，我们可以推导得到使用log似然损失+Softmax也能解决梯度消失的问题，对应的梯度如下，和公式(14)和(15)是一样的。\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b^L_j} \u0026 = \u0026 a^L_j-y_j \\tag{25}\\\\\\frac{\\partial C}{\\partial w^L_{jk}} \u0026 = \u0026 a^{L-1}_k (a^L_j-y_j) \\tag{26}\\end{eqnarray}$$事实上，Softmax相当于Sigmoid在多元情况下的推广，交叉熵损失(9)的每一项也就是log似然损失(24)。回顾逻辑回归那篇博客，有非常多的相似点。\n最后总结，对于一个神经网络，判断是否有梯度消失的问题，最根本的方法就是求解损失对参数的梯度，如果梯度中包含可能出现梯度消失的项，则存在梯度消失的问题，否则不存在。而梯度求解又和所选的损失函数和激活函数有关，目前比较好的组合是MSE+线性激活、交叉熵+Sigmoid、log似然+Softmax，这几种组合都不会有梯度消失的问题。\n","permalink":"http://localhost:1313/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/","summary":"\u003cp\u003e原文的第三章内容较多，本博客将分三个部分进行介绍：梯度消失、过拟合与正则化、权重初始化及其他，首先介绍梯度消失问题。\u003c/p\u003e\n\u003cp\u003e为简单起见，假设网络只包含一个输入和一个神经元，网络的损失是均方误差损失MSE，激活函数是Sigmoid函数。则该网络的参数只包含权重\\(w\\)和偏移量\\(b\\)。我们想训练这个网络，使得当输入为1时，输出0。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz28.png\"\u003e\u003c/p\u003e\n\u003cp\u003e假设我们随机初始化\\(w_0=0.6\\)，\\(b_0=0.9\\)，则网络的损失随着训练的epoch变化曲线如下，看起来挺好的，一开始损失下降很快，随着epoch增加，损失下降逐渐平缓，直至收敛。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/ch3.1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是，如果随机初始化\\(w_0=2.0\\)，\\(b_0=2.0\\)，则网络的损失一开始下降得很缓慢，要训练到快200个epoch时，损失才快速下降。可以看到同样是300个epoch，由于初始化权重的差别，损失下降的趋势完全不一样，而且对于下面这种情况，到300个epoch时，损失还有下降的空间，所以期望的output不如上面的接近目标值0。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/ch3.2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e为什么同样的网络，只是因为初始化权重的差异，损失的变化曲线却相差这么多呢，这和我们选择的损失函数与激活函数有关。\u003c/p\u003e\n\u003cp\u003e回顾一下，我们在\u003ca href=\"https://bitjoy.net/posts/2018-12-14-neutral-networks-and-deep-learning-2-bp/\"\u003e上一讲\u003c/a\u003e的末尾介绍到如果损失函数是MSE且激活函数是Sigmoid时，有\\(\\delta^L = (a^L-y) \\odot \\{\\sigma(z^L)(1-\\sigma(z^L))\\}\\)，又因为网络只有一个神经元，所以梯度如下：\u003c/p\u003e\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w} \u0026 = \u0026 (a-y)\\sigma'(z) x = a \\sigma'(z),\\tag{1}\\\\\\frac{\\partial C}{\\partial b} \u0026 = \u0026 (a-y)\\sigma'(z) = a \\sigma'(z)\\tag{2}\\end{eqnarray}$$\u003cp\u003e其中第二个等号是把\\(x=1\\)和\\(y=0\\)带入得到的。由此可见，误差对两个参数\\(w\\)和\\(b\\)的梯度都和激活函数的导数有关，因为激活函数是Sigmoid，当神经元的输出接近0或1时，梯度几乎为0，误差反向传播就会非常慢，导致上图出现损失下降非常慢的现象。这就是梯度消失的原因。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\"\u003e\u003c/p\u003e\n\u003cp\u003e为了解决这个问题，我们可以采取两种策略，一是替换损失函数，一是替换激活函数。\u003c/p\u003e\n\u003cp\u003e第一种方法是将MSE的损失函数替换为交叉熵损失函数，激活函数依然是Sigmoid。我们考虑一个比本文开头更复杂的网络，仍然是一个输出神经元，但包含多个输入神经元。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz29.png\"\u003e\u003c/p\u003e\n\u003cp\u003e此时，交叉熵损失函数定义如下，其中的\\(n\\)表示训练样本数，\\(\\frac{1}{n}\\sum_x\\)表示对所有输入样本\\(x\\)的交叉熵损失求均值。\u003c/p\u003e\n$$\\begin{eqnarray}C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right]\\tag{3}\\end{eqnarray}$$\u003cp\u003e我们首先考察为什么(3)可以是一个损失函数，损失函数需要满足如下两个条件：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e非负；\u003c/li\u003e\n\u003cli\u003e当网络输出和目标答案越接近，损失越小；反之损失越大。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e简单代入几组不同的样本很容易验证交叉熵满足上述两个条件 ，所以交叉熵可以作为一个损失函数。\u003c/p\u003e\n\u003cp\u003e下面我们再考察一下为什么交叉熵损失函数+Sigmoid激活函数可以解决梯度消失的问题。首先推导交叉熵损失\\(C\\)对权重\\(w_j\\)和\\(b\\)的梯度：\u003c/p\u003e\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w_j} \u0026 = \u0026 -\\frac{1}{n} \\sum_x \\left(\\frac{y }{\\sigma(z)} -\\frac{(1-y)}{1-\\sigma(z)} \\right)\\frac{\\partial \\sigma}{\\partial w_j} \\tag{4}\\\\\u0026 = \u0026 -\\frac{1}{n} \\sum_x \\left(\\frac{y}{\\sigma(z)}-\\frac{(1-y)}{1-\\sigma(z)} \\right)\\sigma'(z) x_j\\tag{5}\\\\\u0026 = \u0026 \\frac{1}{n}\\sum_x \\frac{\\sigma'(z) x_j}{\\sigma(z) (1-\\sigma(z))}(\\sigma(z)-y).\\tag{6}\\end{eqnarray}$$\u003cp\u003e上式分子Sigmoid的导数正好可以和分母抵消，得到：\u003c/p\u003e","title":"Neural Networks and Deep Learning（三·一）梯度消失"},{"content":"这一讲介绍误差反向传播（backpropagation）网络，简称BP网络。\n以上一讲介绍的MNIST手写数字图片分类问题为研究对象，首先明确输入输出：输入就是一张28×28的手写数字图片，展开后可以表示成一个长度为784的向量；输出可以表示为一个长度为10的one-hot向量，比如输入是一张“3”的图片，则输出向量为(0,0,0,1,0,0,0,0,0,0,0)。\n然后构造一个如下的三层全连接网络。第一层为输入层，包含784个神经元，正好对应输入的一张28×28的图片。第二层为隐藏层，假设隐藏层有15个神经元。第三层为输出层，正好10个神经元，对应该图片的one-hot结果。\n全连接网络表示上一层的每个神经元都和下一层的每个神经元有连接，即每个神经元的输入来自上一层所有神经元的输出，每个神经元的输出连接到下一层的所有神经元。每条连边上都有一个权重w。\n每个神经元执行的操作非常简单，就是把跟它连接的每个输入乘以边上的权重，然后累加起来。\n比如上面的一个神经元，它的输出就是：\n$$\\begin{eqnarray}\\mbox{output} = \\left\\{ \\begin{array}{ll}0 \u0026 \\mbox{if} \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\1 \u0026 \\mbox{if} \\sum_j w_j x_j \u003e \\mbox{threshold}\\end{array}\\right.\\tag{1}\\end{eqnarray}$$其中的threshold就是该神经元激活的阈值，如果累加值超过threshold，则该神经元被激活，输出为1，否则为0。这就是最原始的感知机网络。感知机网络也可以写成如下的向量形式，用激活阈值b代替threshold，然后移到左边。神经网络中，每条边具有权重w，每个神经元具有激活阈值b。\n$$\\begin{eqnarray}\\mbox{output} = \\left\\{ \\begin{array}{ll} 0 \u0026 \\mbox{if } w\\cdot x + b \\leq 0 \\\\1 \u0026 \\mbox{if } w\\cdot x + b \u003e 0\\end{array}\\right.\\tag{2}\\end{eqnarray}$$ 但是感知机网络的这种激活方式不够灵活，它在threshold左右有一个突变，如果输入或者某个边上的权重稍微有一点变化，输出结果可能就千差万别了。于是后来人们提出了用sigmoid函数来当激活函数，它在0附近的斜率较大，在两边的斜率较小，能达到和阶梯函数类似的效果，而且函数光滑可导。sigmoid的函数形式如下，其中\\(z\\equiv w \\cdot x + b\\)为神经元激活之前的值。\n$$\\begin{eqnarray} \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}\\tag{3}\\end{eqnarray}$$sigmmoid函数还有一个优点就是它的导数很好计算，可以用它本身来表示：\n$$\\begin{eqnarray}\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\tag{4}\\end{eqnarray}$$BP网络的参数就是所有连线上的权重w和所有神经元中的激活阈值b，如果知道这些参数，给定一个输入x，则可以很容易的通过正向传播（feedforward）的方法计算到输出，即不断的执行\\(w \\cdot x + b\\)操作，然后用sigmoid激活，再把上一层的输出传递给下一层作为输入，直到最后一层。\n1 2 3 4 5 def feedforward(self, a): \u0026#34;\u0026#34;\u0026#34;Return the output of the network if ``a`` is input.\u0026#34;\u0026#34;\u0026#34; for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a 同时，网络的误差可以用均方误差（mean squared error, MSE）表示，即网络在最后一层的激活值（即网络的输出值）\\(a\\)和对应训练集输入\\(x\\)的正确答案\\(y(x)\\)的差的平方。有\\(n\\)个输入则误差取平均，\\(\\dfrac{1}{2}\\)是为了后续求导方便。\n$$\\begin{eqnarray} C(w,b) \\equiv\\frac{1}{2n} \\sum_x \\| y(x) – a\\|^2\\tag{5}\\end{eqnarray}$$训练BP网络的过程就是不断调整参数——权重weights和阈值biases——使得网络的误差\\(C\\)最小。使用的方法是梯度下降，即误差\\(C\\)对每个参数求偏导（梯度），然后参数向梯度的反方向更新，这样更新能保证误差\\(C\\)下降得最快，至于为什么下降最快，直观上很好理解，数学证明可以看原文，证明也很好理解，而且我认为很精彩。\n$$\\begin{eqnarray}w_k \u0026 \\rightarrow \u0026 w_k’ = w_k-\\eta \\frac{\\partial C}{\\partial w_k} \\tag{6}\\\\b_l \u0026 \\rightarrow \u0026 b_l’ = b_l-\\eta \\frac{\\partial C}{\\partial b_l}.\\tag{7}\\end{eqnarray}$$ 下面我们就来求解梯度下降中最重要的梯度，即\\(C\\)对每个参数\\(w\\)和\\(b\\)的偏导，先正式定义一些记号。\n创建网络时，可以对w和b进行随机初始化，代码如下。如果以本文开头的网络结构为例，用[748, 15, 10]输入到下面函数，随机初始化b和w。[748, 15, 10]分别表示第一、二、三层的神经元个数为748，15和10。\n1 2 3 4 5 6 def __init__(self, sizes): self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] 请注意第一层（输入层）神经元是没有阈值b的，所以biases列表只有两项，第一项为15×1的列向量，表示中间层的阈值；第二项为10×1的列向量，表示输出层的阈值。在网络中用\\(b^l_j\\)表示第\\(l\\)层的第\\(j\\)个神经元的阈值，如下\\(b_3^2\\)表示第2层的第3个神经元的阈值。\n权重由于关系到两层之间，所以本文的三层网络的weights列表也只有两项，第一项为15×748的矩阵，表示中间层和输入层的权重矩阵；第二项为10×15的矩阵，表示输出层和中间层的权重矩阵。在网络中用\\(w_{jk}^l\\)表示第\\(l-1\\)层的第\\(k\\)个神经元和第\\(l\\)层的第\\(j\\)个神经元的连接权重，这种表示方法和常规的顺序下标表示相反，但是在矩阵相乘时\\(z^l=w^la^{l-1}+b^l\\)（下方公式10），不用转置，更方便。\n类似的，我们再定义一个神经元激活之前的值为\\(z\\)，激活之后的值为\\(a=\\sigma(z)\\)，上下标的表示方法和\\(b\\)的表示方法是一样的。具体来说，\n$$\\begin{eqnarray} z^{l}_j = \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\tag{8}\\end{eqnarray}$$$$\\begin{eqnarray} a^{l}_j =\\sigma (z^{l}_j) = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right)\\tag{9}\\end{eqnarray}$$向量形式为：\n$$\\begin{eqnarray} z^l=w^la^{l-1}+b^l \\tag{10}\\end{eqnarray}$$$$\\begin{eqnarray} a^{l} = \\sigma (z^l) = \\sigma\\left( w^la^{l-1}+b^l \\right)\\tag{11}\\end{eqnarray}$$对于输入层神经元来说，输入就等于输出，没有激活一说，所以\\(a^1=x\\)。对于单个输入样本\\(x\\)来说，网络的误差如下，\\(L\\)表示网络的总层数，所以\\(a^L\\)表示网络最终的输出。\n$$\\begin{eqnarray}C = \\frac{1}{2} \\|y-a^L\\|^2 = \\frac{1}{2} \\sum_j (y_j-a^L_j)^2.\\tag{12}\\end{eqnarray}$$我们的目标就是求出\\(C\\)对每个参数的梯度。\n首先，引入一个很重要的中间量\\(\\delta^l_j\\)，表示误差\\(C\\)对第\\(l\\)层的第\\(j\\)个神经元的未激活值\\(z^l_j\\)的偏导：\n$$\\begin{eqnarray} \\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j}.\\tag{13}\\end{eqnarray}$$根据定义，对于输出层（假设共有\\(L\\)层），使用链式法则和公式(9)，有下面的等式。\n$$\\begin{eqnarray} \\delta^L_j = \\frac{\\partial C}{\\partial z^L_j} = \\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_j}{\\partial z^L_j} =\\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j).\\tag{14}\\end{eqnarray}$$上式\\(\\frac{\\partial C}{\\partial a^L_j}\\)就是损失\\(C\\)对最后一层输出\\(a^L\\)的偏导，可以用\\(\\nabla_a C\\)表示。输出层的\\(\\delta^L_j\\)只和损失以及输出层激活前后的值有关。整体用向量表示为：\n$$\\begin{eqnarray} \\delta^L = \\nabla_a C \\odot \\sigma'(z^L).\\tag{BP1}\\end{eqnarray}$$其中的\\(\\odot\\)表示哈达玛乘积（Hadamard product），就是向量或矩阵的对应位相乘，比如\\((s \\odot t)_j = s_j t_j\\)，或：\n$$\\begin{eqnarray}\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right] \\odot \\left[\\begin{array}{c} 3 \\\\ 4\\end{array} \\right]= \\left[ \\begin{array}{c} 1 * 3 \\\\ 2 * 4 \\end{array} \\right]= \\left[ \\begin{array}{c} 3 \\\\ 8 \\end{array} \\right].\\tag{15}\\end{eqnarray}$$如果不是最后一层，则使用链式法则有：\n$$\\begin{eqnarray}\\delta^l_j \u0026 = \u0026 \\frac{\\partial C}{\\partial z^l_j} \\tag{16}\\\\\u0026 = \u0026 \\sum_k \\frac{\\partial C}{\\partial z^{l+1}_k} \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \\tag{17}\\\\ \u0026 = \u0026 \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \\delta^{l+1}_k,\\tag{18}\\end{eqnarray}$$又因为\n$$\\begin{eqnarray}z^{l+1}_k = \\sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \\sum_j w^{l+1}_{kj} \\sigma(z^l_j) +b^{l+1}_k.\\tag{19}\\end{eqnarray}$$所以\n$$\\begin{eqnarray}\\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = w^{l+1}_{kj} \\sigma'(z^l_j).\\tag{20}\\end{eqnarray}$$将(20)代入(18)得到：\n$$\\begin{eqnarray}\\delta^l_j = \\sum_k w^{l+1}_{kj} \\delta^{l+1}_k \\sigma'(z^l_j).\\tag{21}\\end{eqnarray}$$写成向量形式就是：\n$$\\begin{eqnarray} \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\\tag{BP2}\\end{eqnarray}$$从(BP2)可知，中间层的\\(\\delta^l\\)可通过下一层的\\(\\delta^{l+1}\\)递推计算得到。这正体现了误差反向传播的思想，即误差从输出层反向一层一层传到输入层，所以正好可以通过(BP1)和(BP2)计算到所有层的\\(\\delta^l\\)。\n我们推导(BP1)和(BP2)是为了便于求解\\(C\\)对\\(w\\)和\\(b\\)的梯度，进而可以根据公式(6)和(7)更新参数。首先求解\\(C\\)对\\(b\\)的梯度：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial b^l_j} = \\delta^l_j \\tag{BP3}\\end{eqnarray}$$上式中间使用链式法则，中间左式正好是\\(\\delta^l_j\\)的定义(13)，中间右式根据(8)可知等于1，所以误差\\(C\\)对神经元阈值\\(b^l_j\\)的梯度正好等于\\(\\delta^l_j\\)。然后求解\\(C\\)对\\(w\\)的梯度：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w^l_{jk}} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\\tag{BP4}\\end{eqnarray}$$同样利用链式法则和(13)、(8)很容易就能得到上式。可以看到，借助中间量\\(\\delta^l_j\\)，\\(C\\)对\\(w\\)和\\(b\\)的梯度能很简洁清晰的表示出来，这就是为什么一开始要推导\\(\\delta^l_j\\)的原因。\n以上我们就求到了误差\\(C\\)对所有参数的梯度。总结一下就是：\n所以，对于单个输入样本\\(x\\)，网络的损失对所有参数的偏导可以用如下算法求解：\n在真实场景中，样本数往往很多，如果一个样本更新一次参数，则效率比较低，目前通常将一小批样本（batch）一起算一个累加的梯度，然后更新一次，这个过程称为一次迭代（iteration）。当所有样本都迭代过之后，称为一次epoch。下面是针对一次iteration的BP算法，其中的\\(\\eta\\)为学习率，通俗理解为往梯度下降的方向走多远，\\(\\eta\\)太大可能引起震荡无法收敛，\\(\\eta\\)太小训练又太慢，所以需要人工调参。\n在BP算法的4个公式中，最重要的是(BP1)，因为(BP1)计算了误差开始传播的初始值。(BP1)中的两个分量\\(\\nabla_a C\\)和\\(\\sigma'(z^L)\\)又和网络的设计有关。如果使用的损失是均方误差损失(12)，使用的激活函数是sigmoid函数(3)，则(BP1)可具体写成下面的等式，所以BP算法的4个公式中，所有变量都是已知的了。\n$$\\begin{eqnarray} \\delta^L = (a^L-y) \\odot \\{\\sigma(z^L)(1-\\sigma(z^L))\\}.\\tag{22}\\end{eqnarray}$$了解了BP的原理之后，代码就非常好理解了，具体可以看GitHub项目，删掉所有注释，真正的核心代码只有74行。使用[784, 30, 10]作为输入，即三层网络，每层神经元个数分别为784，30和10，调用如下代码（表示训练30个epochs，每个batch有10个样本，学习率为3.0），网络在测试集上的分类准确率能达到95%以上。\n1 net.SGD(training_data, 30, 10, 3.0, test_data=test_data) ","permalink":"http://localhost:1313/posts/2018-12-14-neutral-networks-and-deep-learning-2-bp/","summary":"\u003cp\u003e这一讲介绍误差反向传播（backpropagation）网络，简称BP网络。\u003c/p\u003e\n\u003cp\u003e以上一讲介绍的MNIST手写数字图片分类问题为研究对象，首先明确输入输出：输入就是一张28×28的手写数字图片，展开后可以表示成一个长度为784的向量；输出可以表示为一个长度为10的one-hot向量，比如输入是一张“3”的图片，则输出向量为(0,0,0,1,0,0,0,0,0,0,0)。\u003c/p\u003e\n\u003cp\u003e然后构造一个如下的三层全连接网络。第一层为输入层，包含784个神经元，正好对应输入的一张28×28的图片。第二层为隐藏层，假设隐藏层有15个神经元。第三层为输出层，正好10个神经元，对应该图片的one-hot结果。\u003c/p\u003e\n\u003cp\u003e全连接网络表示上一层的每个神经元都和下一层的每个神经元有连接，即每个神经元的输入来自上一层所有神经元的输出，每个神经元的输出连接到下一层的所有神经元。每条连边上都有一个权重w。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz12.png\"\u003e\u003c/p\u003e\n\u003cp\u003e每个神经元执行的操作非常简单，就是把跟它连接的每个输入乘以边上的权重，然后累加起来。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz0.png\"\u003e\u003c/p\u003e\n\u003cp\u003e比如上面的一个神经元，它的输出就是：\u003c/p\u003e\n$$\\begin{eqnarray}\\mbox{output} = \\left\\{ \\begin{array}{ll}0 \u0026 \\mbox{if} \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\1 \u0026 \\mbox{if} \\sum_j w_j x_j \u003e \\mbox{threshold}\\end{array}\\right.\\tag{1}\\end{eqnarray}$$\u003cp\u003e其中的threshold就是该神经元激活的阈值，如果累加值超过threshold，则该神经元被激活，输出为1，否则为0。这就是最原始的感知机网络。感知机网络也可以写成如下的向量形式，用激活阈值b代替threshold，然后移到左边。神经网络中，每条边具有权重w，每个神经元具有激活阈值b。\u003c/p\u003e\n$$\\begin{eqnarray}\\mbox{output} = \\left\\{ \\begin{array}{ll} 0 \u0026 \\mbox{if } w\\cdot x + b \\leq 0 \\\\1 \u0026 \\mbox{if } w\\cdot x + b \u003e 0\\end{array}\\right.\\tag{2}\\end{eqnarray}$$\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/d/d9/Dirac_distribution_CDF.svg\"\u003e\n\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\"\u003e\u003c/p\u003e\n\u003cp\u003e但是感知机网络的这种激活方式不够灵活，它在threshold左右有一个突变，如果输入或者某个边上的权重稍微有一点变化，输出结果可能就千差万别了。于是后来人们提出了用sigmoid函数来当激活函数，它在0附近的斜率较大，在两边的斜率较小，能达到和阶梯函数类似的效果，而且函数光滑可导。sigmoid的函数形式如下，其中\\(z\\equiv w \\cdot x + b\\)为神经元激活之前的值。\u003c/p\u003e\n$$\\begin{eqnarray} \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}\\tag{3}\\end{eqnarray}$$\u003cp\u003esigmmoid函数还有一个优点就是它的导数很好计算，可以用它本身来表示：\u003c/p\u003e\n$$\\begin{eqnarray}\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\tag{4}\\end{eqnarray}$$\u003cp\u003eBP网络的参数就是所有连线上的权重w和所有神经元中的激活阈值b，如果知道这些参数，给定一个输入x，则可以很容易的通过正向传播（feedforward）的方法计算到输出，即不断的执行\\(w \\cdot x + b\\)操作，然后用sigmoid激活，再把上一层的输出传递给下一层作为输入，直到最后一层。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e5\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003efeedforward\u003c/span\u003e(self, a):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;Return the output of the network if ``a`` is input.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e b, w \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e zip(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebiases, self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eweights):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\ta \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sigmoid(np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edot(w, a)\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003eb)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e a\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e同时，网络的误差可以用均方误差（mean squared error, MSE）表示，即网络在最后一层的激活值（即网络的输出值）\\(a\\)和对应训练集输入\\(x\\)的正确答案\\(y(x)\\)的差的平方。有\\(n\\)个输入则误差取平均，\\(\\dfrac{1}{2}\\)是为了后续求导方便。\u003c/p\u003e","title":"Neural Networks and Deep Learning（二）BP网络"},{"content":"最近开始学习神经网络和深度学习，使用的是网上教程：http://neuralnetworksanddeeplearning.com/，这是学习心得第一讲，介绍经典的MNIST手写数字图片数据集。\nMNIST（Modified National Institute of Standards and Technology database）数据集改编自美国国家标准与技术研究所收集的更大的NIST数据集，该数据集来自250个不同人手写的数字图片，一半是人口普查局的工作人员，一半是高中生。该数据集包括60000张训练集图片和10000张测试集图片，训练集和测试集都提供了正确答案。每张图片都是28×28=784大小的灰度图片，也就是一个28×28的矩阵，里面每个值是一个像素点，值在[0,1]之间，0表示白色，1表示黑色，(0,1)之间表示不同的灰度。下面是该数据集中的一些手写数字图片，可以有一个感性的认识。\nMNIST数据集可以在Yann LeCun的网站上下载到：http://yann.lecun.com/exdb/mnist/，但是他提供的MNIST数据集格式比较复杂，需要自己写代码进行解析。目前很多深度学习框架都自带了MNIST数据集，比较流行的是转换为pkl格式的版本：http://deeplearning.net/data/mnist/mnist.pkl.gz，该版本把原始的60000张训练集进一步划分成了50000张小训练集和10000张验证集，下面以这个版本为例进行介绍。\npkl是python内置的一种格式，可以将python的各种数据结构序列化存储到磁盘中，需要时又可以读取并反序列化到内存中。mnist.pkl.gz做了两次操作，先pkl序列化，再gz压缩存储，所以要读取该文件，需要先解压再反序列化，在python3中，读取mnist.pkl.gz的方式如下：\n1 2 3 4 5 import pickle import gzip f = gzip.open(‘../data/mnist.pkl.gz’, ‘rb’) training_data, validation_data, test_data = pickle.load(f, encoding=’bytes’) f.close() 这样就得到了训练集、验证集和测试集。将数据集序列化到文件中的方法也很简单，需要注意的是pickle在序列化和反序列化时有不同的协议，可以用protocol参数进行设置。\n1 2 3 4 dataset=[training_data, validation_data, test_data] f=gzip.open(‘../data/mnist3.pkl.gz’,’wb’) pickle.dump(dataset,f,protocol=3) f.close() 我们从mnist.pkl.gz读取到的training_data, validation_data, test_data这三个数据的结构是一样的，每个都是一个二维的tuple。以training_data为例，training_data[0]是训练样本，是一个50000×784的矩阵，表示有50000个训练样本，每个训练样本是一个784的一维数组，784就是把一张28×28的图片展开reshape成的一维数组；training_data[1]是训练样本对应的类标号，大小为50000的一维数组，每个值为0~9中的某个数，表示对应样本的数字标号。\n对于第一次接触MNIST数据集的同学来说，以压缩的pkl格式存储的手写数字图片往往不利于他们感性直观的认识这个问题，下面介绍怎样将MNIST数据集打印成常见的png格式图片，也就是博客开头的真正的“图片”。\n就像上面介绍的一样，training_data, validation_data, test_data这三个数据的结构都是一个二维的tuple，下标0存储了n张图片数据，下标1存储了这n张图片对应的答案。现在我们把validation_data的第一张图片打印出来看看，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 def plot_digit(X, y): np.savetxt(\u0026#39;../fig/%d.csv\u0026#39;%y, X, delimiter=\u0026#39;,\u0026#39;) import matplotlib.pyplot as plt plt.imshow(X, cmap=\u0026#39;Greys\u0026#39;) # or \u0026#39;Greys_r\u0026#39; plt.savefig(\u0026#39;../fig/%d.png\u0026#39;%y) plt.show() training_data, validation_data, test_data = load_data() X=np.reshape(validation_data[0][0], (28, 28)) y=validation_data[1][0] plot_digit(X, y) 首先我们取出validation_data数据集的第一张图片，reshape成原始图片的28×28的矩阵形式，保存在X中，然后我们取出这张图片对应的正确答案，保存在y中，最后调用plot_digit函数打印这个数字。\n在plot_digit函数中，我们首先把矩阵X保存到一个csv表格中，如果我们打开这个表格，会发现传说中的图片真的就是一个28×28的矩阵，单元格的值在[0,1]之间，如果我们使用LibreOffice默认的条件格式进行着色的话，能明显看到一个数字3。\n另外，我们还可以matplotlib打印出这张手写数字图片，使用imshow并指定着色规则是灰度Greys即可，得到的就是本文开头看到的那种白底黑字了。\n好啦，有关MNIST数据集的介绍就到这里，完整代码可以查看我的Github：https://github.com/01joy/neural-networks-and-deep-learning/blob/master/src/mnist_loader.py，下一步开始学习使用BP网络进行图片分类。\n","permalink":"http://localhost:1313/posts/2018-11-25-neutral-networks-and-deep-learning-1-mnist-dataset/","summary":"\u003cp\u003e最近开始学习神经网络和深度学习，使用的是网上教程：\u003ca href=\"http://neuralnetworksanddeeplearning.com/\"\u003ehttp://neuralnetworksanddeeplearning.com/\u003c/a\u003e，这是学习心得第一讲，介绍经典的MNIST手写数字图片数据集。\u003c/p\u003e\n\u003cp\u003eMNIST（Modified National Institute of Standards and Technology database）数据集改编自美国国家标准与技术研究所收集的更大的NIST数据集，该数据集来自250个不同人手写的数字图片，一半是人口普查局的工作人员，一半是高中生。该数据集包括60000张训练集图片和10000张测试集图片，训练集和测试集都提供了正确答案。每张图片都是28×28=784大小的灰度图片，也就是一个28×28的矩阵，里面每个值是一个像素点，值在[0,1]之间，0表示白色，1表示黑色，(0,1)之间表示不同的灰度。下面是该数据集中的一些手写数字图片，可以有一个感性的认识。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/digits_separate.png\"\u003e\u003c/p\u003e\n\u003cp\u003eMNIST数据集可以在Yann LeCun的网站上下载到：\u003ca href=\"http://yann.lecun.com/exdb/mnist/\"\u003ehttp://yann.lecun.com/exdb/mnist/\u003c/a\u003e，但是他提供的MNIST数据集格式比较复杂，需要自己写代码进行解析。目前很多深度学习框架都自带了MNIST数据集，比较流行的是转换为pkl格式的版本：\u003ca href=\"http://deeplearning.net/data/mnist/mnist.pkl.gz\"\u003ehttp://deeplearning.net/data/mnist/mnist.pkl.gz\u003c/a\u003e，该版本把原始的60000张训练集进一步划分成了50000张小训练集和10000张验证集，下面以这个版本为例进行介绍。\u003c/p\u003e\n\u003cp\u003epkl是python内置的一种格式，可以将python的各种数据结构序列化存储到磁盘中，需要时又可以读取并反序列化到内存中。mnist.pkl.gz做了两次操作，先pkl序列化，再gz压缩存储，所以要读取该文件，需要先解压再反序列化，在python3中，读取mnist.pkl.gz的方式如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e5\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pickle\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e gzip\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ef \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e gzip\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e../\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003emnist\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epkl\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egz\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e, \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003erb\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etraining_data, validation_data, test_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pickle\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eload(f, encoding\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003ebytes\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ef\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e这样就得到了训练集、验证集和测试集。将数据集序列化到文件中的方法也很简单，需要注意的是pickle在序列化和反序列化时有不同的协议，可以用protocol参数进行设置。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edataset\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e[training_data, validation_data, test_data]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ef\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003egzip\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e../\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003emnist3\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epkl\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egz\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e,\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003ewb\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epickle\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edump(dataset,f,protocol\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ef\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e我们从mnist.pkl.gz读取到的training_data, validation_data, test_data这三个数据的结构是一样的，每个都是一个二维的tuple。以training_data为例，training_data[0]是训练样本，是一个50000×784的矩阵，表示有50000个训练样本，每个训练样本是一个784的一维数组，784就是把一张28×28的图片展开reshape成的一维数组；training_data[1]是训练样本对应的类标号，大小为50000的一维数组，每个值为0~9中的某个数，表示对应样本的数字标号。\u003c/p\u003e","title":"Neural Networks and Deep Learning（一）MNIST数据集介绍"},{"content":"VSCode是微软开源的一个很强大的IDE，可以支持几乎所有编程语言，而且是跨平台的，Linux用户终于可以用上宇宙最强IDE了。我最近在使用VSCode编写调试Python项目，其调试功能很强大，和VS上调试C++的感觉是一样的，强烈推荐。\nVSCode还可以连接Github，进行版本控制。下面以我最近学习的深度学习项目为例，介绍下怎样在Ubuntu下使用VSCode连接Github。以我fork的repo为例：https://github.com/01joy/neural-networks-and-deep-learning。\n连接Github有两种方式，一种是HTTPS，另一种是SSH，在每个repo页面的右边，有一个Clone or download按钮，可以获取到这两种连接方式的地址。HTTPS方式和网址类似，以HTTPS开头；SSH方式以git@githu.com开头。使用HTTPS连接比较简单，但是每次push的时候需要输入用户名和密码，比较麻烦，如果想记住密码，需要把用户名和密码以明文的形式保存到一个文件中，个人感觉不方便且不安全。下面以SSH连接为例进行介绍。\n首先设置Github提交时的用户名和密码，一般设置成全局的：https://help.github.com/articles/setting-your-username-in-git/、https://help.github.com/articles/setting-your-commit-email-address-in-git/ 生成一对新的SSH公钥和私钥，并添加到ssh-agent中。注意生成的时候需要输入passphrase，这个passphrase不是Github的密码，自己随便取一个记住就好。https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/ 把SSH的公钥添加到Github账号中：https://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account/ 测试SSH连接是否成功：https://help.github.com/articles/testing-your-ssh-connection/ （可选）修改SSH密码，即第1步设置的passphrase：https://help.github.com/articles/working-with-ssh-key-passphrases/ 到这里，本级就能通过SSH连接Github了。 如果没有安装VSCode，可以直接通过Ubuntu的终端连接Github，步骤如下：\n在本地创建一个和远程repo名称一样的空文件夹 终端cd到该文件夹内 git init # 在该文件夹内初始化 git remote add origin git@github.com:01joy/notes-on-writing.git # 使用repo的SSH地址 git pull origin master # 把远程代码拉到本地 修改代码 git add . # 在根目录执行，添加所有修改 git commit -m ‘comments’ # commit第7步添加的修改 git push origin master # 把第8步发布到远程 如果安装了VSCode，其实和直接用终端是一样的，在菜单栏的Terminal下新建一个终端，在这个终端内执行上述代码，如果在第4步出现”Enter password to unlock the private key”时，输入创建SSH时第2步的密码即可，只需一次，下次就不用再输入密码了。点击File的Open Folder打开本地repo文件夹。点击VSCode左边栏的Explorer可以在编辑器下修改代码。切换到左边栏的Source Control可以进行Git相关操作，修改的文件右边会出现一个M，点击这个M会出现diff视图；Source Control左边的右上角有三个点，点击这个按钮会出现很多Git操作，包括commit、push等，其实相当于调用上述代码，效果是一样的。\nVSCode快捷键Ctrl+Shift+P会出现命令窗口，在里面输入commit、push等会出现相关操作的，能起到一定的加速效果，当然也可以自定义快捷键。\nHave Fun!\n","permalink":"http://localhost:1313/posts/2018-11-13-access-github-from-vscode-in-ubuntu/","summary":"\u003cp\u003eVSCode是微软开源的一个很强大的IDE，可以支持几乎所有编程语言，而且是跨平台的，Linux用户终于可以用上宇宙最强IDE了。我最近在使用VSCode编写调试Python项目，其调试功能很强大，和VS上调试C++的感觉是一样的，强烈推荐。\u003c/p\u003e\n\u003cp\u003eVSCode还可以连接Github，进行版本控制。下面以我最近学习的深度学习项目为例，介绍下怎样在Ubuntu下使用VSCode连接Github。以我fork的repo为例：\u003ca href=\"https://github.com/01joy/neural-networks-and-deep-learning\"\u003ehttps://github.com/01joy/neural-networks-and-deep-learning\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e连接Github有两种方式，一种是HTTPS，另一种是SSH，在每个repo页面的右边，有一个Clone or download按钮，可以获取到这两种连接方式的地址。HTTPS方式和网址类似，以HTTPS开头；SSH方式以git@githu.com开头。使用HTTPS连接比较简单，但是每次push的时候需要输入用户名和密码，比较麻烦，如果想记住密码，需要把用户名和密码以明文的形式保存到一个文件中，个人感觉不方便且不安全。下面以SSH连接为例进行介绍。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e首先设置Github提交时的用户名和密码，一般设置成全局的：\u003ca href=\"https://help.github.com/articles/setting-your-username-in-git/\"\u003ehttps://help.github.com/articles/setting-your-username-in-git/\u003c/a\u003e、\u003ca href=\"https://help.github.com/articles/setting-your-commit-email-address-in-git/\"\u003ehttps://help.github.com/articles/setting-your-commit-email-address-in-git/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e生成一对新的SSH公钥和私钥，并添加到ssh-agent中。注意生成的时候需要输入passphrase，这个passphrase不是Github的密码，自己随便取一个记住就好。\u003ca href=\"https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/\"\u003ehttps://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e把SSH的公钥添加到Github账号中：\u003ca href=\"https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/\"\u003ehttps://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e测试SSH连接是否成功：\u003ca href=\"https://help.github.com/articles/testing-your-ssh-connection/\"\u003ehttps://help.github.com/articles/testing-your-ssh-connection/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e（可选）修改SSH密码，即第1步设置的passphrase：\u003ca href=\"https://help.github.com/articles/working-with-ssh-key-passphrases/\"\u003ehttps://help.github.com/articles/working-with-ssh-key-passphrases/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e到这里，本级就能通过SSH连接Github了。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e如果没有安装VSCode，可以直接通过Ubuntu的终端连接Github，步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e在本地创建一个和远程repo名称一样的空文件夹\u003c/li\u003e\n\u003cli\u003e终端cd到该文件夹内\u003c/li\u003e\n\u003cli\u003egit init # 在该文件夹内初始化\u003c/li\u003e\n\u003cli\u003egit remote add origin \u003ca href=\"mailto:git@github.com\"\u003egit@github.com\u003c/a\u003e:01joy/notes-on-writing.git # 使用repo的SSH地址\u003c/li\u003e\n\u003cli\u003egit pull origin master # 把远程代码拉到本地\u003c/li\u003e\n\u003cli\u003e修改代码\u003c/li\u003e\n\u003cli\u003egit add . # 在根目录执行，添加所有修改\u003c/li\u003e\n\u003cli\u003egit commit -m ‘comments’ # commit第7步添加的修改\u003c/li\u003e\n\u003cli\u003egit push origin master # 把第8步发布到远程\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e如果安装了VSCode，其实和直接用终端是一样的，在菜单栏的Terminal下新建一个终端，在这个终端内执行上述代码，如果在第4步出现”Enter password to unlock the private key”时，输入创建SSH时第2步的密码即可，只需一次，下次就不用再输入密码了。点击File的Open Folder打开本地repo文件夹。点击VSCode左边栏的Explorer可以在编辑器下修改代码。切换到左边栏的Source Control可以进行Git相关操作，修改的文件右边会出现一个M，点击这个M会出现diff视图；Source Control左边的右上角有三个点，点击这个按钮会出现很多Git操作，包括commit、push等，其实相当于调用上述代码，效果是一样的。\u003c/p\u003e\n\u003cp\u003eVSCode快捷键Ctrl+Shift+P会出现命令窗口，在里面输入commit、push等会出现相关操作的，能起到一定的加速效果，当然也可以自定义快捷键。\u003c/p\u003e\n\u003cp\u003eHave Fun!\u003c/p\u003e","title":"Ubuntu下使用VSCode连接Github"},{"content":"最近导师给我批注文章，说我的Word文档的批注行距极大，从入学到现在一直都是如此，对他造成了极大的困扰，希望我能解决这个问题。\n但是我自己用Word2016查看导师的批注，看不出行距极大的问题，显示完全是正常的。后来猜测导师用的是旧版的Word2010，于是在虚拟机中安装了Word2010，进行测试。\n经过长时间的Debug，终于发现问题所在。Word针对批注有一个默认的样式，为“批注文字（使用前隐藏）”，可以点击样式右下角的箭头，或者直接按快捷键Ctrl+Shift+Alt+S调出样式窗口。然后点击底部的管理样式就可以看到所有的样式了。有意思的是，批注文字样式默认是隐藏的，所以在下图的样式列表中是找不到这个样式的。\n找到批注文字样式，点击修改，在弹出的窗口中点击左下角的格式，选择段落，就可以看到批注的默认格式了。段落格式看不出什么异常，Word2016和Word2010的批注段落格式都是一样的，其中“如果定义了文档网格，则对齐到网格”都是默认选中的。\n有意思的是，Word2016和Word2010对网格的默认设置却不一样。在布局、页面设置中点击右下角的箭头，打开页面设置对话框。切换到文档网格选项卡。\nWord2016默认指定了行网格，而Word2010默认却是无网格。因为批注文字样式中选中了“如果定义了文档网格，则对齐到网格”这个选项，Word2016默认指定了行网格，所以批注文字会对齐到行网格，导致行间距太大，Word2010默认没有指定任何网格，所以其批注文字的行间距是正常。\nWord2016文档网格，默认指定了行网格\nWord2010文档网格，默认无网格\n解决办法就是，修改Word2016的设置，选中“无网格”，并设置为默认值，这样以后新建的Word文档默认都是无网格，批注的行间距也就正常了。\n","permalink":"http://localhost:1313/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/","summary":"\u003cp\u003e最近导师给我批注文章，说我的Word文档的批注行距极大，从入学到现在一直都是如此，对他造成了极大的困扰，希望我能解决这个问题。\u003c/p\u003e\n\u003cp\u003e但是我自己用Word2016查看导师的批注，看不出行距极大的问题，显示完全是正常的。后来猜测导师用的是旧版的Word2010，于是在虚拟机中安装了Word2010，进行测试。\u003c/p\u003e\n\u003cp\u003e经过长时间的Debug，终于发现问题所在。Word针对批注有一个默认的样式，为“\u003cstrong\u003e批注文字（使用前隐藏）\u003c/strong\u003e”，可以点击样式右下角的箭头，或者直接按快捷键Ctrl+Shift+Alt+S调出样式窗口。然后点击底部的管理样式就可以看到所有的样式了。有意思的是，批注文字样式默认是隐藏的，所以在下图的样式列表中是找不到这个样式的。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/word2016-open-style.webp\"\u003e\u003c/p\u003e\n\u003cp\u003e找到批注文字样式，点击修改，在弹出的窗口中点击左下角的格式，选择段落，就可以看到批注的默认格式了。段落格式看不出什么异常，Word2016和Word2010的批注段落格式都是一样的，其中“如果定义了文档网格，则对齐到网格”都是默认选中的。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/word2016-comment-style.webp\"\u003e\u003c/p\u003e\n\u003cp\u003e有意思的是，Word2016和Word2010对网格的默认设置却不一样。在布局、页面设置中点击右下角的箭头，打开页面设置对话框。切换到文档网格选项卡。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/word2016-open-page-setting.webp\"\u003e\u003c/p\u003e\n\u003cp\u003eWord2016默认指定了行网格，而Word2010默认却是无网格。因为批注文字样式中选中了“\u003cstrong\u003e如果定义了文档网格，则对齐到网格\u003c/strong\u003e”这个选项，Word2016默认指定了行网格，所以批注文字会对齐到行网格，导致行间距太大，Word2010默认没有指定任何网格，所以其批注文字的行间距是正常。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/word2016-doc-grid.webp\"\u003e\nWord2016文档网格，默认指定了行网格\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/word2010-doc-grid.webp\"\u003e\nWord2010文档网格，默认无网格\u003c/p\u003e\n\u003cp\u003e解决办法就是，修改Word2016的设置，选中“无网格”，并设置为默认值，这样以后新建的Word文档默认都是无网格，批注的行间距也就正常了。\u003c/p\u003e","title":"Word2016批注行距太大的问题"},{"content":"2017年是目前为止最折腾的一年。\n科研工作 今年的科研任务包括两个方面，一方面是pLink2软件的完善和发布，另一方面是完成pLink2文章初稿。软件方面，上半年忙着修改完善算法，下半年重写界面代码，修改各种bug并反复测试。最终在12月31日晚通过邮件正式发布。文章方面，元旦软件发布之后，一月份抽两周时间完成了初稿，算是自己的第一篇全英文初稿，正文加附录接近一万词。不过因为有一个实验结果不太好，还需要接着完善算法，文字也很稚嫩，需要反复修改。希望能在2018年上半年投出去。\n个人提高 上半年忙着找工作，一直在刷题看书攒面经，经过自己的不懈努力，收获了微软、百度、头条、Face++等心仪的Offer。在确定自己找工作没问题之后，被李沐的博客“忽悠”，下半年华丽丽的转博了，赶在了2015级最后一次转博前夕。\n因为找工作，看了7本专业书；因为有Kindle，以及忙里偷闲，竟也看了9本非专业书。元旦完成软件发布任务之后，奖励自己去电影院连看了一整天的电影，这种休假方式也是蛮奇葩的，今年累计去电影院看的电影数已经达到16了。上半年和欣欣看了一场“OFO轻睐演唱会”，第一次参加演唱会，现场的感觉和看视频不一样，气氛很热烈，大家都很兴奋，会情不自禁跟着一起唱。国庆第一次一个人远行，去了郑州、登封和杭州，加上12月份参加厦门质谱会议，今年去的第三个城市已经达到了4个。\n运动方面。在两个师兄的帮助以及室友的陪练下，真的学会了蛙泳，今年8月份还拿到了深水证，为此贺老师每个月奖励我100块钱，简单粗暴又有效的奖励机制:-)。临近年底的时候，心血来潮，准备提高乒乓球技术，混入了所里的乒乓球圈子，拜师王老师门下，经过训练以及看视频学习，竟偶尔能赢浩哥了，今年再接再厉。\n今年约好了和哥一起回家过年，给家里买了一台55寸的乐视超级电视，给父母的红包也涨了不少。总体来说，家里在一年一年变好。\n对照年初定的目标，\n发表pLink 2文章。只完成了初稿。 至少完成毕业工作的80%。转博了。 刷完LeetCode所有简单题和中等题，找工作之前最好刷完两遍。完成。 找到一个满意的工作。完成，具体请看https://bitjoy.net/posts/2018-02-04-2018-campus-recruiting/。 读10本书。完成16：《编程珠玑》、《C++ Primer》、《程序员面试笔试宝典》、《STL源码剖析》、《剑指Offer》、《深度探索C++对象模型》、《编程之美》、《人间失格》、《枪炮、病菌与钢铁》、《杀死一只知更鸟》、《别闹了，费曼先生》、《月亮与六便士》、《突破极限》、《解忧杂货店》、《北京折叠》、《以色列，一个国家的诞生123》。 去电影院看10场电影。完成16：《爱乐之城》、《摔跤吧！爸爸》、《银河护卫队2》、《战狼2》、《敦刻尔克》、《羞羞的铁拳》、《看不见的客人》、《东方快车谋杀案》、《寻梦环游记》、《帕丁顿熊2》、《芳华》、《解忧杂货店》、《前任3：再见前任》、《无问西东》、《南极之恋》、《太空救援》。 看一场话剧（音乐会、歌剧等都可以）。完成。2017年7月2日，北京工人体育场，OFO轻睐演唱会。 学会游泳。完成，学会蛙泳和踩水，年中拿到深水证。 去第三个城市。完成，国庆去了郑州、登封、杭州，12月份第一次坐飞机去了厦门。 总体来说，2017年的目标都完成了，而且好几项是超额完成。2018年目标如下：\n发表pLink2文章，科研的重中之重。 开展新课题，或SUMO或深度学习。 完成博士课程的学习。 读10本书。 去电影院看10场电影。 去北京公园年票范围中的19家公园。 看一场话剧（音乐会、歌剧等都可以）。 学会自由泳。 乒乓球稳赢。 去第三个城市。 机动目标，高温假带父母来北京玩。 突然发现每年的目标都差不多，读万卷书和行万里路是每年都有的保留项目。\n找工作，分手，转博，软件发布，文章写作构成了我2017年的365天，喜忧参半，在跌跌撞撞中前行。2018年要保持一如既往的冲劲，打赢转博之后的第一仗！\n","permalink":"http://localhost:1313/posts/2018-02-18-summary-of-2017/","summary":"\u003cp\u003e2017年是目前为止最折腾的一年。\u003c/p\u003e\n\u003ch1 id=\"科研工作\"\u003e科研工作\u003c/h1\u003e\n\u003cp\u003e今年的科研任务包括两个方面，一方面是pLink2软件的完善和发布，另一方面是完成pLink2文章初稿。软件方面，上半年忙着修改完善算法，下半年重写界面代码，修改各种bug并反复测试。最终在12月31日晚通过邮件正式发布。文章方面，元旦软件发布之后，一月份抽两周时间完成了初稿，算是自己的第一篇全英文初稿，正文加附录接近一万词。不过因为有一个实验结果不太好，还需要接着完善算法，文字也很稚嫩，需要反复修改。希望能在2018年上半年投出去。\u003c/p\u003e\n\u003ch1 id=\"个人提高\"\u003e个人提高\u003c/h1\u003e\n\u003cp\u003e上半年忙着找工作，一直在刷题看书攒面经，经过自己的不懈努力，收获了微软、百度、头条、Face++等心仪的Offer。在确定自己找工作没问题之后，被李沐的博客“忽悠”，下半年华丽丽的转博了，赶在了2015级最后一次转博前夕。\u003c/p\u003e\n\u003cp\u003e因为找工作，看了7本专业书；因为有Kindle，以及忙里偷闲，竟也看了9本非专业书。元旦完成软件发布任务之后，奖励自己去电影院连看了一整天的电影，这种休假方式也是蛮奇葩的，今年累计去电影院看的电影数已经达到16了。上半年和欣欣看了一场“OFO轻睐演唱会”，第一次参加演唱会，现场的感觉和看视频不一样，气氛很热烈，大家都很兴奋，会情不自禁跟着一起唱。国庆第一次一个人远行，去了郑州、登封和杭州，加上12月份参加厦门质谱会议，今年去的第三个城市已经达到了4个。\u003c/p\u003e\n\u003cp\u003e运动方面。在两个师兄的帮助以及室友的陪练下，真的学会了蛙泳，今年8月份还拿到了深水证，为此贺老师每个月奖励我100块钱，简单粗暴又有效的奖励机制:-)。临近年底的时候，心血来潮，准备提高乒乓球技术，混入了所里的乒乓球圈子，拜师王老师门下，经过训练以及看视频学习，竟偶尔能赢浩哥了，今年再接再厉。\u003c/p\u003e\n\u003cp\u003e今年约好了和哥一起回家过年，给家里买了一台55寸的乐视超级电视，给父母的红包也涨了不少。总体来说，家里在一年一年变好。\u003c/p\u003e\n\u003cp\u003e对照年初定的目标，\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e发表pLink 2文章。只完成了初稿。\u003c/li\u003e\n\u003cli\u003e至少完成毕业工作的80%。转博了。\u003c/li\u003e\n\u003cli\u003e刷完LeetCode所有简单题和中等题，找工作之前最好刷完两遍。完成。\u003c/li\u003e\n\u003cli\u003e找到一个满意的工作。完成，具体请看\u003ca href=\"https://bitjoy.net/posts/2018-02-04-2018-campus-recruiting/\"\u003ehttps://bitjoy.net/posts/2018-02-04-2018-campus-recruiting/\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e读10本书。完成16：《编程珠玑》、《C++ Primer》、《程序员面试笔试宝典》、《STL源码剖析》、《剑指Offer》、《深度探索C++对象模型》、《编程之美》、《人间失格》、《枪炮、病菌与钢铁》、《杀死一只知更鸟》、《别闹了，费曼先生》、《月亮与六便士》、《突破极限》、《解忧杂货店》、《北京折叠》、《以色列，一个国家的诞生123》。\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影。完成16：《爱乐之城》、《摔跤吧！爸爸》、《银河护卫队2》、《战狼2》、《敦刻尔克》、《羞羞的铁拳》、《看不见的客人》、《东方快车谋杀案》、《寻梦环游记》、《帕丁顿熊2》、《芳华》、《解忧杂货店》、《前任3：再见前任》、《无问西东》、《南极之恋》、《太空救援》。\u003c/li\u003e\n\u003cli\u003e看一场话剧（音乐会、歌剧等都可以）。完成。2017年7月2日，北京工人体育场，OFO轻睐演唱会。\u003c/li\u003e\n\u003cli\u003e学会游泳。完成，学会蛙泳和踩水，年中拿到深水证。\u003c/li\u003e\n\u003cli\u003e去第三个城市。完成，国庆去了郑州、登封、杭州，12月份第一次坐飞机去了厦门。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e总体来说，2017年的目标都完成了，而且好几项是超额完成。2018年目标如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e发表pLink2文章，科研的重中之重。\u003c/li\u003e\n\u003cli\u003e开展新课题，或SUMO或深度学习。\u003c/li\u003e\n\u003cli\u003e完成博士课程的学习。\u003c/li\u003e\n\u003cli\u003e读10本书。\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影。\u003c/li\u003e\n\u003cli\u003e去北京公园年票范围中的19家公园。\u003c/li\u003e\n\u003cli\u003e看一场话剧（音乐会、歌剧等都可以）。\u003c/li\u003e\n\u003cli\u003e学会自由泳。\u003c/li\u003e\n\u003cli\u003e乒乓球稳赢。\u003c/li\u003e\n\u003cli\u003e去第三个城市。\u003c/li\u003e\n\u003cli\u003e机动目标，高温假带父母来北京玩。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e突然发现每年的目标都差不多，读万卷书和行万里路是每年都有的保留项目。\u003c/p\u003e\n\u003cp\u003e找工作，分手，转博，软件发布，文章写作构成了我2017年的365天，喜忧参半，在跌跌撞撞中前行。2018年要保持一如既往的冲劲，打赢转博之后的第一仗！\u003c/p\u003e","title":"2017年终总结"},{"content":"证明你能做一件事的最好方法就是做成这件事！\n从保研开始，我根本就没打算读博士，心里想的是，好好学习，认真刷题，顺利毕业，高薪就业。\n2016年底，也就是进实验室半年之后，贺老师开始“怂恿”我读博：“贫寒人家子弟，有个高学历，在这个拼爹的时代，更容易出人头地。年轻时多读点儿难读的书，也会更好地在未来人工智能时代生存。”我不为所动。\n2017年春节在家，疯狂刷题看书，为开学后的实习面试以及半年之后的校招面试准备着。\n2017年2月份，开年工作计划会，老师说只要我愿意读博，博士的三个课题都帮我规划好了，要知道我们实验室没有哪个博士生是在三年级之前就确定方向的，大家都是摸着石头过河。\n我还是不为所动，疯狂刷题看书，攒实习面经。\n到了8月，老师最后来信希望我能认真考虑一下读博的事情：“pFind+NIBS是难得的良性成长环境，换一个新环境未见得能成长像现在这么快”。甚至把我的博士女朋友都搬出来了。我跟欣欣聊了聊，思想开始有点动摇了。但是那段时间忙于找工作，没空想太多。\n9月10日教师节，我和欣欣分手，与工作无关，与硕士博士无关。\n后来有一天，当我在对比百度凤巢和微软的Offer时，偶然看到凤巢前辈李沐博士写的一篇博客：《博士这五年》。李沐是上海交大ACM班的，毕业之后去了百度凤巢，但是后来毅然辞职去CMU攻读博士学位。博士五年期间，他不但发表了多篇很牛的paper，而且亲手写了一个类似TensorFlow的深度学习平台MXNet，MXNet现已加入Apache家族，并被Amazon选为官方深度学习框架。他博士答辩的评委有来自Google, Amazon, Apple的AI负责人，阵容非常强大。最后，李沐光荣毕业，加入Amazon。\n这篇博客的结尾在谈到如何选择工作和读博时有一段话，令我印象深刻：“不过我觉得还是会选择读博。赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次……更重要的是理想和情怀。人一生要工作五十年，为什么不花五年来追求下理想和情怀呢？”。\n看完这篇博客之后，那一整天，我都没心思上班。校招至今，拿了一堆Offer，不能说轻而易举，但是我现在知道拿Offer就跟考试一样，只要准备好，不会差到哪里去。甚至可以说，校招面试比回答贺老师的问题要简单多了。未来可以工作的时间还很长，何不花几年时间来挑战自己呢。当你在面对两难选择时，选择更难的那个，日后你会感谢当初自己的选择。\n人拼了命的工作，到底是为了什么，除了更早的成为工厂里的螺丝钉，成为房奴、孩奴，你还能得到什么。是的，提前三年工作，你也许能赚到100万，能积累更丰富的工作经验和更广阔的人脉。但是，这又怎样，这些东西该来的肯定会来，博士毕业之后，我同样能得到，只是比大多数人晚了一点。\n与其早早毕业，成为一个nobody，还不如再潜心修炼几年，成为somebody。博士这几年，我能得到更加系统全面的训练，pFind+NIBS的良性科研环境，也不可多得。曾经在知乎上看到有个人回答为什么选择读博：“在一个很安全的环境里，父母健在，自己不用操心赚钱养家；有老板给你提供指导、资金；你可以安心研究自己感兴趣的问题；发表的文章也将署上自己的名字，流传后世；习得的技能也将转化为自己的能力；获得的博士学位也将是自己的荣誉…”，这么好的事情，为什么不去做呢？\n那一天之后，我内心几乎就决定要转博了。\n然而，话虽如此，想到要放弃到手的大Offer，继续在实验室里待至少三年；想到免不了要经历大多数博士师兄师姐们经历过的痛苦日子；想到彼时同学们都已经年入x万，有房有车，说不定我日后的面试官就是现在的同学；想到我最亲密的女朋友对我的不信任；想到自己的苦衷无处倾诉…\n那段日子过得很艰难，也许是我到目前为止最低谷的时期。左手是各大互联网公司的Offer，立即可以实现我很多的愿望；右手是若干年未知的博士磨砺。虽然内心知道往右走是正确的，但还是下不了这个狠心。脑海中的两个小人，吵个不停。\n期间和很多在读的、已毕业的博士师兄师姐们聊过，也和很多公司的面试官聊过，当然也和老师父母聊过。得到的回答无外乎三种：读、不读，根据自己的情况决定。这些谈话更像是换了种方式的倾述，我已经记不清具体的内容了，只知道，我做选择的决心越来越坚定了。\n之后恰逢十一长假，给自己放了一个长长的假。规划去了郑州、登封、杭州。了却夙愿，重新开始。\n10月9日，长假结束。我给老师发了一封邮件：“贺老师，您好。非常感谢您的信任和等待，我决定读博了！这将是我人生二十多年来所作的第一个重大决定，我接受挑战！”\n人这一生，说长也长，说短也短，去做你认为对的事情吧，去追求你想要的生活。愿我们都能绽放美丽，不负芳华！\n","permalink":"http://localhost:1313/posts/2018-02-14-why-phd/","summary":"\u003cp\u003e证明你能做一件事的最好方法就是做成这件事！\u003c/p\u003e\n\u003cp\u003e从保研开始，我根本就没打算读博士，心里想的是，好好学习，认真刷题，顺利毕业，高薪就业。\u003c/p\u003e\n\u003cp\u003e2016年底，也就是进实验室半年之后，贺老师开始“怂恿”我读博：“贫寒人家子弟，有个高学历，在这个拼爹的时代，更容易出人头地。年轻时多读点儿难读的书，也会更好地在未来人工智能时代生存。”我不为所动。\u003c/p\u003e\n\u003cp\u003e2017年春节在家，\u003ca href=\"http://code.bitjoy.net/\"\u003e疯狂刷题看书\u003c/a\u003e，为开学后的实习面试以及半年之后的校招面试准备着。\u003c/p\u003e\n\u003cp\u003e2017年2月份，开年工作计划会，老师说只要我愿意读博，博士的三个课题都帮我规划好了，要知道我们实验室没有哪个博士生是在三年级之前就确定方向的，大家都是摸着石头过河。\u003c/p\u003e\n\u003cp\u003e我还是不为所动，疯狂刷题看书，攒实习面经。\u003c/p\u003e\n\u003cp\u003e到了8月，老师最后来信希望我能认真考虑一下读博的事情：“pFind+NIBS是难得的良性成长环境，换一个新环境未见得能成长像现在这么快”。甚至把我的博士女朋友都搬出来了。我跟欣欣聊了聊，思想开始有点动摇了。但是那段时间忙于找工作，没空想太多。\u003c/p\u003e\n\u003cp\u003e9月10日教师节，我和欣欣分手，与工作无关，与硕士博士无关。\u003c/p\u003e\n\u003cp\u003e后来有一天，当我在对比百度凤巢和微软的Offer时，偶然看到凤巢前辈李沐博士写的一篇博客：\u003ca href=\"https://zhuanlan.zhihu.com/p/25099638\"\u003e《博士这五年》\u003c/a\u003e。李沐是上海交大ACM班的，毕业之后去了百度凤巢，但是后来毅然辞职去CMU攻读博士学位。博士五年期间，他不但发表了多篇很牛的paper，而且亲手写了一个类似TensorFlow的深度学习平台MXNet，MXNet现已加入Apache家族，并被Amazon选为官方深度学习框架。他博士答辩的评委有来自Google, Amazon, Apple的AI负责人，阵容非常强大。最后，李沐光荣毕业，加入Amazon。\u003c/p\u003e\n\u003cp\u003e这篇博客的结尾在谈到如何选择工作和读博时有一段话，令我印象深刻：“不过我觉得还是会选择读博。赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次……更重要的是理想和情怀。人一生要工作五十年，为什么不花五年来追求下理想和情怀呢？”。\u003c/p\u003e\n\u003cp\u003e看完这篇博客之后，那一整天，我都没心思上班。校招至今，拿了一堆Offer，不能说轻而易举，但是我现在知道拿Offer就跟考试一样，只要准备好，不会差到哪里去。甚至可以说，校招面试比回答贺老师的问题要简单多了。未来可以工作的时间还很长，何不花几年时间来挑战自己呢。当你在面对两难选择时，选择更难的那个，日后你会感谢当初自己的选择。\u003c/p\u003e\n\u003cp\u003e人拼了命的工作，到底是为了什么，除了更早的成为工厂里的螺丝钉，成为房奴、孩奴，你还能得到什么。是的，提前三年工作，你也许能赚到100万，能积累更丰富的工作经验和更广阔的人脉。但是，这又怎样，这些东西该来的肯定会来，博士毕业之后，我同样能得到，只是比大多数人晚了一点。\u003c/p\u003e\n\u003cp\u003e与其早早毕业，成为一个nobody，还不如再潜心修炼几年，成为somebody。博士这几年，我能得到更加系统全面的训练，pFind+NIBS的良性科研环境，也不可多得。曾经在知乎上看到有个人回答为什么选择读博：“在一个很安全的环境里，父母健在，自己不用操心赚钱养家；有老板给你提供指导、资金；你可以安心研究自己感兴趣的问题；发表的文章也将署上自己的名字，流传后世；习得的技能也将转化为自己的能力；获得的博士学位也将是自己的荣誉…”，这么好的事情，为什么不去做呢？\u003c/p\u003e\n\u003cp\u003e那一天之后，我内心几乎就决定要转博了。\u003c/p\u003e\n\u003cp\u003e然而，话虽如此，想到要放弃到手的大Offer，继续在实验室里待至少三年；想到免不了要经历大多数博士师兄师姐们经历过的痛苦日子；想到彼时同学们都已经年入x万，有房有车，说不定我日后的面试官就是现在的同学；想到我最亲密的女朋友对我的不信任；想到自己的苦衷无处倾诉…\u003c/p\u003e\n\u003cp\u003e那段日子过得很艰难，也许是我到目前为止最低谷的时期。左手是各大互联网公司的Offer，立即可以实现我很多的愿望；右手是若干年未知的博士磨砺。虽然内心知道往右走是正确的，但还是下不了这个狠心。脑海中的两个小人，吵个不停。\u003c/p\u003e\n\u003cp\u003e期间和很多在读的、已毕业的博士师兄师姐们聊过，也和很多公司的面试官聊过，当然也和老师父母聊过。得到的回答无外乎三种：读、不读，根据自己的情况决定。这些谈话更像是换了种方式的倾述，我已经记不清具体的内容了，只知道，我做选择的决心越来越坚定了。\u003c/p\u003e\n\u003cp\u003e之后恰逢十一长假，给自己放了一个长长的假。规划去了郑州、登封、杭州。了却夙愿，重新开始。\u003c/p\u003e\n\u003cp\u003e10月9日，长假结束。我给老师发了一封邮件：“贺老师，您好。非常感谢您的信任和等待，我决定读博了！这将是我人生二十多年来所作的第一个重大决定，我接受挑战！”\u003c/p\u003e\n\u003cp\u003e人这一生，说长也长，说短也短，去做你认为对的事情吧，去追求你想要的生活。愿我们都能绽放美丽，不负芳华！\u003c/p\u003e","title":"一念成博"},{"content":"作为一名曾经的2018届硕士毕业生，为找工作忙活了大半年，最终收获了微软、百度、头条、Face++等十多个Offer。校招季对我来说，在9月份就差不多结束了。本来很早就酝酿了这篇博客，但是由于之后一系列事情，耽搁至今，趁着提交完年终技术报告，回家之前，把这段经历记录一下。\n首先介绍一些计算机专业校招的基本情况。由于移动互联网、人工智能等浪潮的兴起，计算机专业的毕业生就业前景可谓一片大好，尤其是对于名校毕业基础扎实的同学，应届生薪资倒挂老员工的事情几乎每年都在上演。所以首先祝贺所有CSer，这是属于你们的时代，各行各业都有属于你的舞台，尽情去施展才华吧。\n本专业的毕业生就业去向主要有这么几类：国内互联网公司、国外互联网公司（外企）、国企。其中国内互联网公司又分大厂和新兴创业公司，大厂如BAT、网易、360、京东、华为等，创业公司主要集中在人工智能这块，如商汤科技、Face++、头条、滴滴等。外企大概也分为两类，一类是来自日本的企业，比如Indeed、WAP；另一类是来自美国的企业，比如Microsoft、Google、Hulu、FreeWheel、Amazon等。国企是指传统的国有企业里面的IT部门，比如各大银行、证监会等。这几类公司的校招时间刚好都错开了，一般来说，日企来华校招是最早的，大概每年5月份就来了；接着是国内互联网公司的内推季，大概在7~8月份；进入9月份之后，就是国内互联网公司的正式校招了；美国的企业大概会在9~10月份启动校招，有可能一直持续到11月份；国企就比较晚了，听说最晚能持续到第二年3、4月份的。这种安排，对我们来说，既是好事，也是坏事，好处就是对于纠结的同学，可以每种类型的公司都试一试，多拿几个offer，最后根据自己的情况决定去哪里；坏处就是持续时间真的很长，面到最后，身心俱疲，需要做好心理准备。\n我经历过的面试主要是国内互联网和部分外企的研发岗，下面也将主要介绍这两类企业，按时间先后顺序。\nIndeed（FAILED） Indeed是全球最大的招聘信息搜索引擎公司，总部位于美国德州的首府奥斯汀，2012年被日本的Recruit收购，然后成立了Indeed Tokyo办公室。本文提到的Indeed都是指Indeed Tokyo，即拿到offer的话，要求去东京工作，不过可以轮转去奥斯汀总部。\nIndeed是最早开始校招的，当国内公司还在实习招聘的时候，它就跑来进行校招了。我参加了2017年4月17日在北大举办的校园宣讲会，介绍了Indeed的基本情况和招聘流程，以及抽奖机械硬盘等。Indeed的办公室很有科技范，其工位设置尤为吸引人，是六边形的环形设计，每个人既可以专注于自己的工作，又便于和组内同事讨论。宣讲的人包括HR和从该校毕业的学长，这个HR是中国人，后面有一轮HR面也是他，大家可以多多留意。\nIndeed最大的吸引力是，700万~800万日元的年薪，折合人民币大概四五十万吧，这样诱人的薪资，让每个路过其宣传海报的同学都驻足观看。当然其面试难度也不小，首先有一轮在线笔试，这个在线笔试有三次机会，只要有一次全部AC，就算通过。在线笔试题一共4道，难度比LeetCode稍大，但是一定提醒大家，他们家的题都有数据范围，而且范围很小，前3题用暴力解法几乎都可以过，所以一定要先试试暴力求解，不行再想DP。\n通过在线笔试之后，会有一个大约30分钟的HR面，就是上面提到的来华宣讲的中国人。这个面试严格来说是Case interview，通过Skype进行，主要考察逻辑逻辑思维能力和英文口语能力。由于是中国人，所以刚开始会用中文介绍下题意，然后让你思考一下，最后用英文给出解答。我当时的题目是，如何把微信支付的流水从xxx提高到yyy。由于提前非常认真的看了http://www.caseinterview.com/的视频教学，学到很多，这次HR面顺利通过。\n通过HR面之后，还有一轮Skype技术面，是从Indeed Tokyo那边打过来的，需要解算法题，通常是一题+好几个follow up。不过很多是往年的原题，在一亩三分地上都有，大家可以仔细在上面看看。我当时被问到的题是之前准备过的，但是没答好，比较突兀的给出了最优解，面试官可能觉得我是背答案了吧。。。\n如果这轮Skype技术面也通过的话，就可以免费飞到东京参加on-site面了，听说on-site面是3轮面试，一整个上午或一整个下午，几乎也是原题，可以在一亩三分地上找到。\n说来也奇怪，Indeed每年的面试题都差不多，但通过面试的人总是寥寥无几，这才是高级的面试官，考察的是应聘者的解题思路，而不是答案。\nIndeed Tokyo很不错，如果能拿到Offer，说明你很优秀，离人生巅峰也不远了。\nWorks Applications（FAILED） Works Applications简称WAP，是一家日本的ERP软件开发公司，ERP全称是Enterprise Resource Planning，简单理解就是面向企业用户的各种管理系统。WAP是正宗的日本企业，其风格和Indeed Tokyo截然不同，上班要求穿正装，估计各种行为规范也不少，但是钱也不少，折合人民币估计也有四十多万吧。WAP虽然总部在东京，但它在上海有办公室，国内校招生基本上都在上海办公。\nWAP的招聘流程和Indeed很像，首先会有一个宣讲会，建议大家都参加，类似于报名考试。宣讲会之后会收到一个在线笔试的链接，要求3天之内做完2道编程题，题目比较简单。在线笔试通过之后，有一轮在线技术面试，使用的是牛客网平台，要求视频面时不能离开面试页面查资料。视频面也比较简单，大概Leetcode的easy~medium题。\n对于WAP，前期的在线面试只是开胃小菜，好戏还在后头。通过两轮在线面试之后，会邀请去某个酒店现场面试。现场面试有三轮，全程英文，一般是先来段英文自我介绍，然后开始做题。比较搞笑的是，见到一面面试官时，被问到感觉如何，我说good，然后面试官说别人都是很nervous，我居然说good，感觉要被自己坑了，还好出的题都会做。前两面都不难，大概LeetCode中等题，第三面感觉是一个boss，已经不考LeetCode算法题了，考类似智力题的东西，比如有人被考到囚犯和帽子颜色的问题，我被问到的是怎样实现求两数的平均值，常规的(a+b)/2有可能导致a+b溢出，我想了很多方法，面试官都不满意，后来发现《程序员面试笔试宝典》上有。求平均值的问题可以先转换为求和，用位运算是a+b=((a\u0026amp;b)\u0026laquo;1)+(a^b)，a+b就是按位加，对应二进制也是按位加，要进位的情况就是对应位都为1，所以先用a\u0026amp;b找出需要进位的位，然后左移1位表示进位；还有些位可能只有一个1或者没有1，这部分加和的结果可以用异或表示，即a^b，所以a+b=((a\u0026amp;b)\u0026laquo;1)+(a^b)。那么，求平均值就是(a+b)/2=(a\u0026amp;b)+((a^b)\u0026raquo;1)。要是早点看了《程序员面试笔试宝典》，我估计也能拿到WAP的Offer了。\n三轮技术面之后，会有一个HR面，听说如果前面的技术面过关的话，HR面会遇到日本boss，直接发放Offer；否则是一个中国人，寒暄几句之后，被告知技术面没有通过，但是可以参加暑期为期一周的实习活动，实习通过的话，也可以获得Offer。每年的实习主题都差不多，比如做一个酒店管理系统、电影院管理系统之类的，由于我觉得时间代价太高了，没有参加暑期实习。\n虽然WAP的工资很好，但是要想拿Offer，比Indeed简单，LeetCode中等题足够，好好准备一下现场第三面。另外，即使拿到Offer，也要考虑一下工作内容是否符合自己的兴趣，毕竟ERP和当前火热的AI相比还是太古老了，而且穿正装上班估计也只此一家了。\n深信服（OFFER） 深信服公司是面向企业的安全与云计算解决方案供应商，可以理解为企业版360。听说创始人是从华为跳出来的，公司整体风格和华为很像，从宣讲会上还听说这家薪资不错，尤其是博士，宣称比BAT华为都高。\n深信服的提前批招聘也很早，7月初就来所里宣讲了。首先有两轮电话技术面试，面试官都会提前短信约时间，给人感觉不错。电话面试的内容比较广，网络、操作系统、C++、算法等都会问到。面试官手里应该有一个问题清单，挨个问下去，不会的跳过，节奏比较快。所以面试深信服之前，要好好复习计算机基础，尤其是网络相关的，因为其主营业务和网络密切相关。\n能通过前两轮电话面试的，基础都很扎实，接下来会邀请去参加他们的星云计划暑期夏令营。原本夏令营是要去深圳总部的，但是北京的很多同学都没时间，于是临时把夏令营分成了南北两波，北京的同学被安排在九华山庄度假村。在这里会听好多深信服的介绍和讲座，其中有一个清华的博士，在校期间发过很牛的Paper，自称是那一届的全国博士Top5，谈了很多选择去深信服的理由，核心思想是博士在深信服有很大的自主权，可以试错，主导一些项目，而且薪资估计真的很高。最后会有一个Boss面，主要是问项目经历，Boss是连夜赶来北京的，面试的时候哈欠连天，也没问什么实质性的问题。去的人应该都过了。其实这个夏令营主要是去体验生活的:)\n最后的Offer，中规中矩，薪资并没有想象的高，也不是自己喜欢做的事情，拒。\n华为（OFFER） 华为就不用介绍了，早年凭借电信网络产品赚得盆满钵满，近几年的智能手机业务更是冲出国门走向世界，真的是我国民族企业的榜样。华为严格来说是一个制造商企业，不算互联网企业，而且其招聘比较看重学历，给人感觉有点像国企。但是毕竟其产品都是计算机相关设备，对计算机专业人才的需求还是很旺盛的。华为的另一大特点是有钱，并且舍得给员工砸钱，我上一届的硕士师兄去了华为，工资碾压BAT，成功倒挂一大批老员工。仔细看看近几年各大重点高校的毕业生去向，去华为的占了很大比例，如果你想快速积累财富，又能吃苦，去华为能很好的满足你的要求。\n因为师兄去了华为，3月份收到内部通知说可以提前批内推了，于是把简历给了师兄进行内推。7月初的时候要求做一个性格测试，华为特色，其他公司都没有这一环节，据说是在筛选符合华为价值观的同学。7月22日参加华为提前批优招，真的是优招，去的大部分是清北中科院的，猜测还要求本科是985高校。 优招面试很简单，因为是业务面试，主要问问项目，面试官是那种成功人士风格的Boss。二面就不问技术了，会问周围同学老师是怎样评价自己的，科研压力大吗，想去哪工作之类的，类似的问题也是在衡量应聘者和华为公司的match程度。我应该是非常match的，面试结束的时候，Boss还跟我握手了！\n优招面试结束后没几天，会有一个在线笔试，编程题，三道题，最好全AC，我是前两题AC，第三题过了80%。至此，华为所有的笔试面试都结束了。但是直到9月初，才被再次邀请去华为北研参加Offer沟通会，这个会和大一刚入学参加各大社团的招新差不多，华为的各大部门开始抢人，我去了2012实验室中央软件院。\n四维图新（OFFER） 华为虽然是最早面完的，但是Offer迟迟没有下来，国内其他互联网公司又还没开始面试，心急之下，看到四维图新在招聘C++研发工程师，做地图搜索的，和自己有点关系。网上查了一下，发现还是腾讯地图的数据供应商，而且还是母校武大测绘学院有很紧密的合作，应该是个靠谱的公司。\n跑去面试，可能是公司比较小，面试流程还很原始，直接在接待室问了我几个问题，有些题目有一定难度，连红黑树都被问到了。然后被直接拉去工位，打开VS，开始编程，所幸全部AC。等了一会，直接HR面，拿到普通OFFER。我说想申请SP，HR说下周再来一轮Boss面吧。于是下周又跑去Boss面，Boss果然是Boss，气场就不一样，问题也很灵活，都是他们地图搜索开发过程中的实际问题，比如给定中国地图和一个GPS坐标，怎样快速定位这个坐标。类似的题目很有意思，虽然有一个题目回答得不是很好，但总体上聊得还比较开心。Boss面完之后，又一轮HR面，被告知拿到SP，而且如果能来实习，实习表现好，且能申请到户口指标，则有可能有户口。\n这个Offer是我校招季拿到的最早的Offer，薪资还不错，也算是稳住了阵脚。但是公司规模和名气都不算大，暂时拿来保底吧。\n百度（OFFER） 百度公司和我的专业是最匹配的了，国内做搜索技术最强的，非百度莫属。百度很人性化的一点是，公司不同部门的招聘分开进行，互不冲突，所以可以同时向不同部门投递简历。我就一口气投递了网页搜索部、商务搜索部和基础架构部。很幸运，同时拿到了这三个部门的提前批Offer。\n百度各部门的面试流程都很像，前两轮技术面，第三轮是Boss面或者HR面，越往后面试官的级别越高，第三面的面试官很可能就是你未来的Leader。第一轮面试比较基础，问一些网络、操作系统、C++的基础知识，然后写两道算法题。第二面先写两道算法题，然后问项目，项目问得很细，我的几个搜索引擎的项目，不但问了项目的实现细节，还问了很多follow up，比如，在实战场景中，千亿级别的数据量，怎样建索引使得查询更高效，如何实现怎个搜索过程等。因为面的是搜索部门，他们对相关的技术非常了解，不要抱任何侥幸心理，不会就说不会，切莫班门弄虎。第三面Boss面比较宏观，问问职业规划，如果面试官对你比较感兴趣，会主动介绍本部门的工作，凤巢的三面面试官甚至直接加了我的微信，受宠若惊啊。\n提前批面试完毕之后，9月初会有一个在线笔试，这个笔试也会刷人，所以不要掉以轻心，一定要认真准备。我当时是因为宿舍网络问题，被坑死了，那个在线笔试的系统也很变态，是个国外的系统，动不动就掉线，还只能登陆3次，超过自动退出。于是，很悲剧的3题只AC了2题。之后的几天，一直寝食难安，担心会栽在最后的笔试上。\n所幸，没过多久，收到了电话通知，笔试通过，需要确定部门，让我从三个部门中选一个。我当时那个纠结啊，网页搜索部、商务搜索部和基础架构部都是百度非常核心的部门，基架的低层技术很强，网搜是典型的文本检索，商搜是广告检索，网搜的三面面试官对我很好，时不时在微信上联系我；我和商搜的三面面试官也聊得很开心，商搜是百度最赚钱的部门，各种大牛非常之多。几番权衡之后，选择了商搜（凤巢），同时也拿到了SP。\nMicrosoft（OFFER） 微软是我面的唯一一个美国外企，面试流程数它最多了，前后经历了：1轮在线笔试+2轮skype面试+3轮on-site面试。\n首先，要拿到微软的skype面试机会就很难，需要通过Hihocoder的在线笔试。Hihocoder的题型和难度都相比于LeetCode复杂得多，我有一次很幸运的做到了前100名好像，拿到了skype面试机会。两轮skype面试难度也不小，比如search range，不但要求bug free，还要求你写测试用例；还比如对快排进行优化；手写堆排序；概率题等。微软的在线编程和skype面试和国内互联网不太一样，建议大家看看一亩三分地上的面经。\n过了两轮skype面之后，会被邀请去参加他们的探星夏令营，大概是在8月中旬，地点就在丹棱街的微软大厦。探星夏令营第一天是参观，我因为实验室忙就没去，第二天是三轮面试。我因为研究的方向是搜索引擎，所以被安排到bing组面试了。微软的现场面试难度也不小，不是像LeetCode那样直接叫你写个DP、排序什么的，而是给出一个实际问题，需要将其抽象成一个计算机问题，然后才是代码实现。前两面顺利通过。此时已经是下午4点多了，HR说三面安排不过来，让回去等。这一等直接从8月中旬等到9月初，期间还以为是二面挂了，“让回去等”是委婉的拒绝 ，看来微软还是说话算话的。三面是Boss面，和国内互联网比较像，面项目，问了很多细节，然后根据项目衍生出一个字符串压缩的题目，让写压缩和解压缩的代码。虽然写完了，但是没保证bug free，和面试官聊了聊可能的bug以及解决方案。\n过了大概一周，面试结果出来了，没有直接说给Offer，但是说面试反馈非常Positive，让加一个微信群。国庆节之前，收到微软HR电话，让我们稍安勿躁，国庆后会给正式Offer。后来直到10月31日，才收到HR的电话，正式通知Offer详情。接起电话，HR就说准备好纸笔，因为Offer内容比较多，然后就说了Package里面的各种福利，各种美金。总的来说，Package加起来在硕士里面应该是Top级别的，外企各种Balance，不加班，做的是自己喜欢的方向，而且还有可能拿户口，甚至人肉翻墙，可以说这个Offer是非常诱人的。\n京东（OFFER） 京东和百度类似，也是部门自己招聘，所以可以面多个部门。我面了AI和大数据部门以及商业推荐部门。印象比较深的是，原本面了一个做分布式的组，一面发现我更适合做搜索和架构，然后就被推荐到一个做京东智能音箱的组，这个组的三面面试官是从雅虎北研过来的，听口音感觉是广东人。因为我是做搜索，智能音箱里面也需要搜索，两个人聊得很不错，面试官当场就说帮我争取SP。\n面完技术面之后，过了大概一周，还要进行HR面。面试通知邮件也没说是哪个部门的。其中有个部门的HR面居然是群面，太奇葩了，也是我经历过的唯一一个群面。一屋子3个面试官，6个学生，就菜鸟网络和京东物流的对比展开讨论。首先自我介绍，有清华北大的，也有中科院各所的，还有北邮的。每次讨论我都是倒数几个发言的，对于这种压力测试，真是不适用。不过还好，HR后来跟我说我的表现不错。\nHR跟我谈薪资的时候，我客套说差不多就行，后来这两个部门都拿到了Offer，薪资还真的就是差不多，白菜价。因为已经有其他选择，也没有再争取SP。听别人说争取一下能有28左右？感觉京东的定价真是因人而异啊。后来有一天还收到三面面试官的电话，问我去向定哪了，真觉得有点愧对他。\n360（OFFER） 本来不打算面360，但是该公司在8月8号组织了一场中科院专场招聘会，在所有OFFER都还没有最终确定的情况下，去360逛一逛也没坏处。360的办公楼在酒仙桥，和MTK在一起，周围在施工，几乎没有吃饭的地方，给人的第一印象不是很好。10点钟到现场之后，已经人山人海了，和菜市场没什么区别，中间等待的时间都超过了面试时间。\n面试分为三轮，前两轮是技术面，第三轮是HR面。一面问了一些基础知识，写了一两个算法题。二面遇到了负责360地图开发的程序员，因为地图中也涉及POI搜索，聊得很欢。HR面被问到知道360的哪些产品，虽然我现在一个360的产品都不用了，但是知道的还是不少。\n面完之后，觉得Offer稳了，然后开心的回所里。第二天收到邮件通知，面试通过，还需参加一个在线笔试，类似于行测。做完之后，查看状态，被告知所有面试笔试都通过了，个人信息已经在Offer池中，但是没有正式Offer。Offer池是什么鬼，也就是没人要被扔到池子里等人捞呗。问了下其他人，大部分也是被扔到池子里了，只听说有一个人收到书面Offer。从此对360无感，无论是你们组织面试，还是我们参加面试，费了一天劲，硬是不发OFFER，坑爹。后来在10月16日，收到一封360的邮件，正式书面Offer，难道是被人相中捞起来了，真是无语。拒。\n阿里巴巴（FAILED） 阿里内推只能选一个部门，内推失败之后也只有一次校招机会，所以大家选部门一定要慎重，根据自己的实力和兴趣进行选择。当时群里给出了蚂蚁金服的内推消息之后，我第一时间就选择内推蚂蚁金服了。结果面了两轮之后查状态已经挂了，也没感觉面得差。可能是因为内推蚂蚁金服的人太多了，实力要求也很高，而且自己做搜索引擎的，和蚂蚁金服不太match。\n因为内推挂了之后，无法再面其他部门了。只能参加校招流程，校招在线笔试之后一直就没消息，状态也没更新，难度笔试挂了？\n总之，阿里的校招比较严格，选部门和岗位的时候一定要慎重。好的部门大家都想进，研发岗竞争也相对更激烈，算法岗还好一些，期间还有一个阿里面试官问我愿不愿意转算法，还加了我微信，说转算法希望挺大的，无奈自己没准备算法，婉拒。从校招体验来说，阿里是最差的，可能是仗着店大欺人，每次电话面试从来不提前预约时间，晚上十点钟还打电话过来面试。我室友拿到了淘宝的口头Offer，等了将近一个月也没发正式Offer，由于是公司座机，也联系不上。阿里的HC可能也比较少，我周围很少有听说拿到阿里Offer的。如果有同学拿到了，去杭州享受生活还是很不错的。\n腾讯（FAILED） 腾讯和阿里类似，一次内推机会，一次校招机会，而且只能选一个部门。开始两轮电话面试，和面试官解释了半天我的搜索引擎和索引，感觉面试官没理解，这也是电话面试的弊端。后来换了一个部门面试，说我的基础很扎实，过两天北京有一个专场面试，可以去参加现场面。于是就跑去一个酒店参加现场面了，遇到一个很奇葩的面试官，认为我不会网络编程就是基础差，直接把我轰走了，由于我手头已经有很多心仪的Offer，而且还另有打算，就淡定的和面试官辩论了起来，最后他居然失态了，想想他要是我的Leader，真是可怕。\n总的来说，BAT里面，阿里面试是最难的，其次是百度，然后是腾讯。阿里和腾讯不在北京，电话面试效果大打折扣，而且蚂蚁金服对系统要求很高，腾讯偏爱网络，不是这个方向的还真拿不定。我本科一同学，读了本校网络方向的研究生，拿到了腾讯的Offer，听说薪资不低，在深圳，也很不错。\n谷歌（FAILED） 嗯，作为一个CSer，明知道肯定进不了谷歌，但试是一定要试的。外企的招聘一视同仁，像微软谷歌之类的好像都没有内推。谷歌校招首先要参加它的在线笔试，即Kickstart。好像是三道题，每道题有小、大两个数据集，如果完整通过一道题的小、大数据集，即可进入到电话面试环节。无奈Kickstart对于没参加过ACM的同学来说，难度不小，很多DP题，我两次都只做出来两题的小数据集，过不了大数据集，没有拿到面试机会。想冲击谷歌的同学，需要多加练习编程题，而且最好是LeetCode Hard或者Hihocoder，尤其是DP题。\n谷歌虽然很难进，但是每年计算所都有几个人能拿到谷歌的OFFER，这种人真的是大牛。\n今日头条（OFFER） 头条的内推分两种，一种是白金码，需要笔试，但是比校招提前；另一种是铂金码，不用笔试，直接面试。头条的每个员工只有一个铂金码，比较珍贵。我当时就向上一届的师兄要了一个铂金码。无奈第一次内推面试的时间和深信服的夏令营时间冲突，就延后到9月初参加内推面试。\n很巧的是，内推一面的面试官居然是内推我的师兄，不过我们两个都觉得这样不太好，就临时换了其他面试官。头条面试包括3轮技术面+一轮HR面。技术面比其他公司稍难，LeetCode中等偏难的题+一些实际应用的场景题，还会问一些网络、操作系统的知识。\n面试完大概一周，收到邮件通知，面试通过。10月中旬又收到正式的书面Offer，不得不说，头条的Package真的好大，月薪加各种补贴，稳稳的Top3了。公司就在中航广场，离青年公寓非常之近。\n搜狗（OFFER） 有个本科同学在隔壁的搜狐上班，暑假请他帮忙内推了搜狐和搜狗，结果等了一个多月，搜狐都没给我面试通知，搜狗倒是在9月初叫我去五道口的搜狐网络大厦面试了。可能是因为搜狗要上市了，面试难度不小，和头条一个水平，面试官看起来也比较严肃。我面的是复合搜索部，除了基础知识，算法题外，还会问到和搜索引擎实际业务有关的问题。\n面完两面之后，被告知三面面试官不在，让我回去等三面通知。还以为这是婉言拒绝呢。等了一周，果然收到电话，再去进行第三面。面试自我感觉良好，可是迟迟没有收到Offer，直到11月3号的下午1:50，正当我骑着车去所里上班的时候，收到搜狗HR电话，正式通知Offer。那时候我早已另有打算了，不过了解一下搜狗的行情也是不错的。HR说头条的薪资已经是他们的SP了，问开出什么条件能够挽回我，SSP需要case by case的和Leader谈之类的。我谢过她之后，婉言拒绝了。\nFace++（OFFER） Face++是人脸识别的创业公司，支付宝上的人脸识别技术用的就是这家的。原本不打算投这家以人工智能为主的公司，担心研发岗在里面不受重视。某天有个Face++的师兄在群里发了一个内推信息，抱着试一试的心态，还是投了一份简历。9月11日收到面试邀请，当时因为别的事情心情很低落，就跟他们说不打算再面试了。过了两周，9月25日，又收到面试邀请，感觉如果不去面试的话，实在对不起内推我的师兄以及这个HR，加之公司就在隔壁，索性就答应去面试了。\n面试的过程很意外，三位面试官都很年轻，而且表现出了非常高的专业素质，其中一面面试官针对我的海量浮点数排序算法，详细的问了浮点数在内存中的表示方法，以及规格化浮点数和非规格化浮点数的区别，最后还给我留了一个家庭作业。我在无数次的面试中都会讲这个项目，只有这一个面试官问到了核心，顿时让我很佩服。二面面试官就两个有序数组求中位数的问题，要求我不用传统方法，只从中位数的数学含义进行求解，我在面试官一步步的引导下，也想出了正确解答，当时还觉得好开心。面试形式很自由，一会坐着聊，一会站着在白板上写代码，整个过程我和面试官有非常多的互动，感觉就像是两个同事在互相探讨一个问题。这次面试是我校招季的最后一次面试，也是我认为面试质量最高的一次，我非常享受这样的面试过程。\n面试结束之前，面试官详细的跟我介绍了他们的工作，研发在AI公司里也占有很重要的角色，他们内部都有自己的深度学习平台，Face++内部就自己搭建了一套类似TensorFlow的平台，完成海量数据的深度学习模型训练和优化。研发工程师在里面也要学习机器学习深度学习等算法，只有这样才能写出更加高效的系统代码。所以这类公司很欢迎研发能力强，又懂算法的同学。\n面试结束没几天，收到电话通知OFFER。\n除了上面列到的公司，我其实还面了很多家公司，每家公司的面试都大同小异，我就不再赘述了。\n汇总一下结果吧： 拿到Offer的公司：微软、百度、京东、今日头条、搜狗、Face++、华为、美团、360、四维图新、深信服、蘑菇街、PingCap、好未来 参加校招被拒的公司：谷歌、亚马逊、阿里巴巴、腾讯、拼多多 Offer年薪箱线图： 最后我想说的是，IT行业虽然就业前景很好，但是要想打赢校招这场职场第一仗，必须要下足功夫。越早准备越好，一般来说，理想的情况是：研一参加各种比赛或者实习；研二上认真刷题看书；研二下试一试实习面试，攒经验；暑假内推国内互联网公司，9月前拿下一波OFFER；9~11月面外企，拿下另一波OFFER；12月挑挑选选，签三方。\n针对研发岗，一些有用的资料：\n信息来源：\n校内就业群，很多师兄师姐会发内推消息，一定要抓住内推，可以省去很多麻烦 各大高校BBS就业板块，比如水木清华、北大、北邮人等 刷题网站：\nLeetcode，把所有Easy和Medium题刷两遍，坚持参加他们每周的比赛 Hihocoder，也有每周一题，难度较大，坚持参加他们的定期比赛 书籍：\n《程序员面试笔试宝典》：推荐，内容很多很详细，分门别类了，不过有一些小错误，是这本https://item.jd.com/11612615.html 《剑指OFFER》：和上一本书以及LeetCode很多类似的内容 《大话设计模式》：了解一些常用的设计模式，而且要会写比如工厂模式、单例模式 《STL源码剖析》：通俗易懂的源码讲解书 《深度探索C++对象模型》：对C++面向对象有很深入的探讨，读过之后对C++的低层有更多的了解 《编程珠玑》：很薄的一本编程技巧书，真的是字字珠玑，里面对堆排序的介绍很受用 《数学之美》：浅入了解一些机器学习、自然语言处理的知识 《编程之美》：难度较大，如果面微软，请坚持看完 《深入理解计算机系统》：很厚一本书，我买了没时间看，有富余时间的可以看 最最后，9月份经历了一些”我从哪里来，我要到哪里去“的事情，甚至开始拷问人生了。拒掉了所有的Offer，决定继续攻读博士学位，这也就是为什么标题中加了一个”伪“字。硕士3年或者博士6年，相对于人生来说，真的很短，不要花这么宝贵的时间仅仅为了找一份好的工作，去追求一下理想和情怀吧。不要担心，三年之后，我还可以再写一篇这样的面经。\n祝大家都能找到心仪的工作！\n","permalink":"http://localhost:1313/posts/2018-02-04-2018-campus-recruiting/","summary":"\u003cp\u003e作为一名曾经的2018届硕士毕业生，为找工作忙活了大半年，最终收获了微软、百度、头条、Face++等十多个Offer。校招季对我来说，在9月份就差不多结束了。本来很早就酝酿了这篇博客，但是由于之后一系列事情，耽搁至今，趁着提交完年终技术报告，回家之前，把这段经历记录一下。\u003c/p\u003e\n\u003cp\u003e首先介绍一些计算机专业校招的基本情况。由于移动互联网、人工智能等浪潮的兴起，计算机专业的毕业生就业前景可谓一片大好，尤其是对于名校毕业基础扎实的同学，应届生薪资倒挂老员工的事情几乎每年都在上演。所以首先祝贺所有CSer，这是属于你们的时代，各行各业都有属于你的舞台，尽情去施展才华吧。\u003c/p\u003e\n\u003cp\u003e本专业的毕业生就业去向主要有这么几类：国内互联网公司、国外互联网公司（外企）、国企。其中国内互联网公司又分大厂和新兴创业公司，大厂如BAT、网易、360、京东、华为等，创业公司主要集中在人工智能这块，如商汤科技、Face++、头条、滴滴等。外企大概也分为两类，一类是来自日本的企业，比如Indeed、WAP；另一类是来自美国的企业，比如Microsoft、Google、Hulu、FreeWheel、Amazon等。国企是指传统的国有企业里面的IT部门，比如各大银行、证监会等。这几类公司的校招时间刚好都错开了，一般来说，日企来华校招是最早的，大概每年5月份就来了；接着是国内互联网公司的内推季，大概在7~8月份；进入9月份之后，就是国内互联网公司的正式校招了；美国的企业大概会在9~10月份启动校招，有可能一直持续到11月份；国企就比较晚了，听说最晚能持续到第二年3、4月份的。这种安排，对我们来说，既是好事，也是坏事，好处就是对于纠结的同学，可以每种类型的公司都试一试，多拿几个offer，最后根据自己的情况决定去哪里；坏处就是持续时间真的很长，面到最后，身心俱疲，需要做好心理准备。\u003c/p\u003e\n\u003cp\u003e我经历过的面试主要是国内互联网和部分外企的研发岗，下面也将主要介绍这两类企业，按时间先后顺序。\u003c/p\u003e\n\u003ch1 id=\"indeedfailed\"\u003eIndeed（FAILED）\u003c/h1\u003e\n\u003cp\u003eIndeed是全球最大的招聘信息搜索引擎公司，总部位于美国德州的首府奥斯汀，2012年被日本的Recruit收购，然后成立了Indeed Tokyo办公室。本文提到的Indeed都是指Indeed Tokyo，即拿到offer的话，要求去东京工作，不过可以轮转去奥斯汀总部。\u003c/p\u003e\n\u003cp\u003eIndeed是最早开始校招的，当国内公司还在实习招聘的时候，它就跑来进行校招了。我参加了2017年4月17日在北大举办的校园宣讲会，介绍了Indeed的基本情况和招聘流程，以及抽奖机械硬盘等。Indeed的办公室很有科技范，其工位设置尤为吸引人，是六边形的环形设计，每个人既可以专注于自己的工作，又便于和组内同事讨论。宣讲的人包括HR和从该校毕业的学长，这个HR是中国人，后面有一轮HR面也是他，大家可以多多留意。\u003c/p\u003e\n\u003cp\u003eIndeed最大的吸引力是，700万~800万日元的年薪，折合人民币大概四五十万吧，这样诱人的薪资，让每个路过其宣传海报的同学都驻足观看。当然其面试难度也不小，首先有一轮在线笔试，这个在线笔试有三次机会，只要有一次全部AC，就算通过。在线笔试题一共4道，难度比LeetCode稍大，但是一定提醒大家，他们家的题都有数据范围，而且范围很小，前3题用暴力解法几乎都可以过，所以一定要先试试暴力求解，不行再想DP。\u003c/p\u003e\n\u003cp\u003e通过在线笔试之后，会有一个大约30分钟的HR面，就是上面提到的来华宣讲的中国人。这个面试严格来说是Case interview，通过Skype进行，主要考察逻辑逻辑思维能力和英文口语能力。由于是中国人，所以刚开始会用中文介绍下题意，然后让你思考一下，最后用英文给出解答。我当时的题目是，如何把微信支付的流水从xxx提高到yyy。由于提前非常认真的看了\u003ca href=\"http://www.caseinterview.com/\"\u003ehttp://www.caseinterview.com/\u003c/a\u003e的视频教学，学到很多，这次HR面顺利通过。\u003c/p\u003e\n\u003cp\u003e通过HR面之后，还有一轮Skype技术面，是从Indeed Tokyo那边打过来的，需要解算法题，通常是一题+好几个follow up。不过很多是往年的原题，在一亩三分地上都有，大家可以仔细在上面看看。我当时被问到的题是之前准备过的，但是没答好，比较突兀的给出了最优解，面试官可能觉得我是背答案了吧。。。\u003c/p\u003e\n\u003cp\u003e如果这轮Skype技术面也通过的话，就可以免费飞到东京参加on-site面了，听说on-site面是3轮面试，一整个上午或一整个下午，几乎也是原题，可以在一亩三分地上找到。\u003c/p\u003e\n\u003cp\u003e说来也奇怪，Indeed每年的面试题都差不多，但通过面试的人总是寥寥无几，这才是高级的面试官，考察的是应聘者的解题思路，而不是答案。\u003c/p\u003e\n\u003cp\u003eIndeed Tokyo很不错，如果能拿到Offer，说明你很优秀，离人生巅峰也不远了。\u003c/p\u003e\n\u003ch1 id=\"works-applicationsfailed\"\u003eWorks Applications（FAILED）\u003c/h1\u003e\n\u003cp\u003eWorks Applications简称WAP，是一家日本的ERP软件开发公司，ERP全称是Enterprise Resource Planning，简单理解就是面向企业用户的各种管理系统。WAP是正宗的日本企业，其风格和Indeed Tokyo截然不同，上班要求穿正装，估计各种行为规范也不少，但是钱也不少，折合人民币估计也有四十多万吧。WAP虽然总部在东京，但它在上海有办公室，国内校招生基本上都在上海办公。\u003c/p\u003e\n\u003cp\u003eWAP的招聘流程和Indeed很像，首先会有一个宣讲会，建议大家都参加，类似于报名考试。宣讲会之后会收到一个在线笔试的链接，要求3天之内做完2道编程题，题目比较简单。在线笔试通过之后，有一轮在线技术面试，使用的是牛客网平台，要求视频面时不能离开面试页面查资料。视频面也比较简单，大概Leetcode的easy~medium题。\u003c/p\u003e\n\u003cp\u003e对于WAP，前期的在线面试只是开胃小菜，好戏还在后头。通过两轮在线面试之后，会邀请去某个酒店现场面试。现场面试有三轮，全程英文，一般是先来段英文自我介绍，然后开始做题。比较搞笑的是，见到一面面试官时，被问到感觉如何，我说good，然后面试官说别人都是很nervous，我居然说good，感觉要被自己坑了，还好出的题都会做。前两面都不难，大概LeetCode中等题，第三面感觉是一个boss，已经不考LeetCode算法题了，考类似智力题的东西，比如有人被考到囚犯和帽子颜色的问题，我被问到的是怎样实现求两数的平均值，常规的(a+b)/2有可能导致a+b溢出，我想了很多方法，面试官都不满意，后来发现《程序员面试笔试宝典》上有。求平均值的问题可以先转换为求和，用位运算是a+b=((a\u0026amp;b)\u0026laquo;1)+(a^b)，a+b就是按位加，对应二进制也是按位加，要进位的情况就是对应位都为1，所以先用a\u0026amp;b找出需要进位的位，然后左移1位表示进位；还有些位可能只有一个1或者没有1，这部分加和的结果可以用异或表示，即a^b，所以a+b=((a\u0026amp;b)\u0026laquo;1)+(a^b)。那么，求平均值就是(a+b)/2=(a\u0026amp;b)+((a^b)\u0026raquo;1)。要是早点看了《程序员面试笔试宝典》，我估计也能拿到WAP的Offer了。\u003c/p\u003e\n\u003cp\u003e三轮技术面之后，会有一个HR面，听说如果前面的技术面过关的话，HR面会遇到日本boss，直接发放Offer；否则是一个中国人，寒暄几句之后，被告知技术面没有通过，但是可以参加暑期为期一周的实习活动，实习通过的话，也可以获得Offer。每年的实习主题都差不多，比如做一个酒店管理系统、电影院管理系统之类的，由于我觉得时间代价太高了，没有参加暑期实习。\u003c/p\u003e\n\u003cp\u003e虽然WAP的工资很好，但是要想拿Offer，比Indeed简单，LeetCode中等题足够，好好准备一下现场第三面。另外，即使拿到Offer，也要考虑一下工作内容是否符合自己的兴趣，毕竟ERP和当前火热的AI相比还是太古老了，而且穿正装上班估计也只此一家了。\u003c/p\u003e\n\u003ch1 id=\"深信服offer\"\u003e深信服（OFFER）\u003c/h1\u003e\n\u003cp\u003e深信服公司是面向企业的安全与云计算解决方案供应商，可以理解为企业版360。听说创始人是从华为跳出来的，公司整体风格和华为很像，从宣讲会上还听说这家薪资不错，尤其是博士，宣称比BAT华为都高。\u003c/p\u003e\n\u003cp\u003e深信服的提前批招聘也很早，7月初就来所里宣讲了。首先有两轮电话技术面试，面试官都会提前短信约时间，给人感觉不错。电话面试的内容比较广，网络、操作系统、C++、算法等都会问到。面试官手里应该有一个问题清单，挨个问下去，不会的跳过，节奏比较快。所以面试深信服之前，要好好复习计算机基础，尤其是网络相关的，因为其主营业务和网络密切相关。\u003c/p\u003e\n\u003cp\u003e能通过前两轮电话面试的，基础都很扎实，接下来会邀请去参加他们的星云计划暑期夏令营。原本夏令营是要去深圳总部的，但是北京的很多同学都没时间，于是临时把夏令营分成了南北两波，北京的同学被安排在九华山庄度假村。在这里会听好多深信服的介绍和讲座，其中有一个清华的博士，在校期间发过很牛的Paper，自称是那一届的全国博士Top5，谈了很多选择去深信服的理由，核心思想是博士在深信服有很大的自主权，可以试错，主导一些项目，而且薪资估计真的很高。最后会有一个Boss面，主要是问项目经历，Boss是连夜赶来北京的，面试的时候哈欠连天，也没问什么实质性的问题。去的人应该都过了。其实这个夏令营主要是去体验生活的:)\u003c/p\u003e\n\u003cp\u003e最后的Offer，中规中矩，薪资并没有想象的高，也不是自己喜欢做的事情，拒。\u003c/p\u003e\n\u003ch1 id=\"华为offer\"\u003e华为（OFFER）\u003c/h1\u003e\n\u003cp\u003e华为就不用介绍了，早年凭借电信网络产品赚得盆满钵满，近几年的智能手机业务更是冲出国门走向世界，真的是我国民族企业的榜样。华为严格来说是一个制造商企业，不算互联网企业，而且其招聘比较看重学历，给人感觉有点像国企。但是毕竟其产品都是计算机相关设备，对计算机专业人才的需求还是很旺盛的。华为的另一大特点是有钱，并且舍得给员工砸钱，我上一届的硕士师兄去了华为，工资碾压BAT，成功倒挂一大批老员工。仔细看看近几年各大重点高校的毕业生去向，去华为的占了很大比例，如果你想快速积累财富，又能吃苦，去华为能很好的满足你的要求。\u003c/p\u003e\n\u003cp\u003e因为师兄去了华为，3月份收到内部通知说可以提前批内推了，于是把简历给了师兄进行内推。7月初的时候要求做一个性格测试，华为特色，其他公司都没有这一环节，据说是在筛选符合华为价值观的同学。7月22日参加华为提前批优招，真的是优招，去的大部分是清北中科院的，猜测还要求本科是985高校。 优招面试很简单，因为是业务面试，主要问问项目，面试官是那种成功人士风格的Boss。二面就不问技术了，会问周围同学老师是怎样评价自己的，科研压力大吗，想去哪工作之类的，类似的问题也是在衡量应聘者和华为公司的match程度。我应该是非常match的，面试结束的时候，Boss还跟我握手了！\u003c/p\u003e\n\u003cp\u003e优招面试结束后没几天，会有一个在线笔试，编程题，三道题，最好全AC，我是前两题AC，第三题过了80%。至此，华为所有的笔试面试都结束了。但是直到9月初，才被再次邀请去华为北研参加Offer沟通会，这个会和大一刚入学参加各大社团的招新差不多，华为的各大部门开始抢人，我去了2012实验室中央软件院。\u003c/p\u003e\n\u003ch1 id=\"四维图新offer\"\u003e四维图新（OFFER）\u003c/h1\u003e\n\u003cp\u003e华为虽然是最早面完的，但是Offer迟迟没有下来，国内其他互联网公司又还没开始面试，心急之下，看到四维图新在招聘C++研发工程师，做地图搜索的，和自己有点关系。网上查了一下，发现还是腾讯地图的数据供应商，而且还是母校武大测绘学院有很紧密的合作，应该是个靠谱的公司。\u003c/p\u003e\n\u003cp\u003e跑去面试，可能是公司比较小，面试流程还很原始，直接在接待室问了我几个问题，有些题目有一定难度，连红黑树都被问到了。然后被直接拉去工位，打开VS，开始编程，所幸全部AC。等了一会，直接HR面，拿到普通OFFER。我说想申请SP，HR说下周再来一轮Boss面吧。于是下周又跑去Boss面，Boss果然是Boss，气场就不一样，问题也很灵活，都是他们地图搜索开发过程中的实际问题，比如给定中国地图和一个GPS坐标，怎样快速定位这个坐标。类似的题目很有意思，虽然有一个题目回答得不是很好，但总体上聊得还比较开心。Boss面完之后，又一轮HR面，被告知拿到SP，而且如果能来实习，实习表现好，且能申请到户口指标，则有可能有户口。\u003c/p\u003e\n\u003cp\u003e这个Offer是我校招季拿到的最早的Offer，薪资还不错，也算是稳住了阵脚。但是公司规模和名气都不算大，暂时拿来保底吧。\u003c/p\u003e\n\u003ch1 id=\"百度offer\"\u003e百度（OFFER）\u003c/h1\u003e\n\u003cp\u003e百度公司和我的专业是最匹配的了，国内做搜索技术最强的，非百度莫属。百度很人性化的一点是，公司不同部门的招聘分开进行，互不冲突，所以可以同时向不同部门投递简历。我就一口气投递了网页搜索部、商务搜索部和基础架构部。很幸运，同时拿到了这三个部门的提前批Offer。\u003c/p\u003e\n\u003cp\u003e百度各部门的面试流程都很像，前两轮技术面，第三轮是Boss面或者HR面，越往后面试官的级别越高，第三面的面试官很可能就是你未来的Leader。第一轮面试比较基础，问一些网络、操作系统、C++的基础知识，然后写两道算法题。第二面先写两道算法题，然后问项目，项目问得很细，我的几个搜索引擎的项目，不但问了项目的实现细节，还问了很多follow up，比如，在实战场景中，千亿级别的数据量，怎样建索引使得查询更高效，如何实现怎个搜索过程等。因为面的是搜索部门，他们对相关的技术非常了解，不要抱任何侥幸心理，不会就说不会，切莫班门弄虎。第三面Boss面比较宏观，问问职业规划，如果面试官对你比较感兴趣，会主动介绍本部门的工作，凤巢的三面面试官甚至直接加了我的微信，受宠若惊啊。\u003c/p\u003e\n\u003cp\u003e提前批面试完毕之后，9月初会有一个在线笔试，这个笔试也会刷人，所以不要掉以轻心，一定要认真准备。我当时是因为宿舍网络问题，被坑死了，那个在线笔试的系统也很变态，是个国外的系统，动不动就掉线，还只能登陆3次，超过自动退出。于是，很悲剧的3题只AC了2题。之后的几天，一直寝食难安，担心会栽在最后的笔试上。\u003c/p\u003e\n\u003cp\u003e所幸，没过多久，收到了电话通知，笔试通过，需要确定部门，让我从三个部门中选一个。我当时那个纠结啊，网页搜索部、商务搜索部和基础架构部都是百度非常核心的部门，基架的低层技术很强，网搜是典型的文本检索，商搜是广告检索，网搜的三面面试官对我很好，时不时在微信上联系我；我和商搜的三面面试官也聊得很开心，商搜是百度最赚钱的部门，各种大牛非常之多。几番权衡之后，选择了商搜（凤巢），同时也拿到了SP。\u003c/p\u003e\n\u003ch1 id=\"microsoftoffer\"\u003eMicrosoft（OFFER）\u003c/h1\u003e\n\u003cp\u003e微软是我面的唯一一个美国外企，面试流程数它最多了，前后经历了：1轮在线笔试+2轮skype面试+3轮on-site面试。\u003c/p\u003e\n\u003cp\u003e首先，要拿到微软的skype面试机会就很难，需要通过Hihocoder的在线笔试。Hihocoder的题型和难度都相比于LeetCode复杂得多，我有一次很幸运的做到了前100名好像，拿到了skype面试机会。两轮skype面试难度也不小，比如search range，不但要求bug free，还要求你写测试用例；还比如对快排进行优化；手写堆排序；概率题等。微软的在线编程和skype面试和国内互联网不太一样，建议大家看看一亩三分地上的面经。\u003c/p\u003e\n\u003cp\u003e过了两轮skype面之后，会被邀请去参加他们的探星夏令营，大概是在8月中旬，地点就在丹棱街的微软大厦。探星夏令营第一天是参观，我因为实验室忙就没去，第二天是三轮面试。我因为研究的方向是搜索引擎，所以被安排到bing组面试了。微软的现场面试难度也不小，不是像LeetCode那样直接叫你写个DP、排序什么的，而是给出一个实际问题，需要将其抽象成一个计算机问题，然后才是代码实现。前两面顺利通过。此时已经是下午4点多了，HR说三面安排不过来，让回去等。这一等直接从8月中旬等到9月初，期间还以为是二面挂了，“让回去等”是委婉的拒绝 ，看来微软还是说话算话的。三面是Boss面，和国内互联网比较像，面项目，问了很多细节，然后根据项目衍生出一个字符串压缩的题目，让写压缩和解压缩的代码。虽然写完了，但是没保证bug free，和面试官聊了聊可能的bug以及解决方案。\u003c/p\u003e\n\u003cp\u003e过了大概一周，面试结果出来了，没有直接说给Offer，但是说面试反馈非常Positive，让加一个微信群。国庆节之前，收到微软HR电话，让我们稍安勿躁，国庆后会给正式Offer。后来直到10月31日，才收到HR的电话，正式通知Offer详情。接起电话，HR就说准备好纸笔，因为Offer内容比较多，然后就说了Package里面的各种福利，各种美金。总的来说，Package加起来在硕士里面应该是Top级别的，外企各种Balance，不加班，做的是自己喜欢的方向，而且还有可能拿户口，甚至人肉翻墙，可以说这个Offer是非常诱人的。\u003c/p\u003e\n\u003ch1 id=\"京东offer\"\u003e京东（OFFER）\u003c/h1\u003e\n\u003cp\u003e京东和百度类似，也是部门自己招聘，所以可以面多个部门。我面了AI和大数据部门以及商业推荐部门。印象比较深的是，原本面了一个做分布式的组，一面发现我更适合做搜索和架构，然后就被推荐到一个做京东智能音箱的组，这个组的三面面试官是从雅虎北研过来的，听口音感觉是广东人。因为我是做搜索，智能音箱里面也需要搜索，两个人聊得很不错，面试官当场就说帮我争取SP。\u003c/p\u003e\n\u003cp\u003e面完技术面之后，过了大概一周，还要进行HR面。面试通知邮件也没说是哪个部门的。其中有个部门的HR面居然是群面，太奇葩了，也是我经历过的唯一一个群面。一屋子3个面试官，6个学生，就菜鸟网络和京东物流的对比展开讨论。首先自我介绍，有清华北大的，也有中科院各所的，还有北邮的。每次讨论我都是倒数几个发言的，对于这种压力测试，真是不适用。不过还好，HR后来跟我说我的表现不错。\u003c/p\u003e\n\u003cp\u003eHR跟我谈薪资的时候，我客套说差不多就行，后来这两个部门都拿到了Offer，薪资还真的就是差不多，白菜价。因为已经有其他选择，也没有再争取SP。听别人说争取一下能有28左右？感觉京东的定价真是因人而异啊。后来有一天还收到三面面试官的电话，问我去向定哪了，真觉得有点愧对他。\u003c/p\u003e\n\u003ch1 id=\"360offer\"\u003e360（OFFER）\u003c/h1\u003e\n\u003cp\u003e本来不打算面360，但是该公司在8月8号组织了一场中科院专场招聘会，在所有OFFER都还没有最终确定的情况下，去360逛一逛也没坏处。360的办公楼在酒仙桥，和MTK在一起，周围在施工，几乎没有吃饭的地方，给人的第一印象不是很好。10点钟到现场之后，已经人山人海了，和菜市场没什么区别，中间等待的时间都超过了面试时间。\u003c/p\u003e\n\u003cp\u003e面试分为三轮，前两轮是技术面，第三轮是HR面。一面问了一些基础知识，写了一两个算法题。二面遇到了负责360地图开发的程序员，因为地图中也涉及POI搜索，聊得很欢。HR面被问到知道360的哪些产品，虽然我现在一个360的产品都不用了，但是知道的还是不少。\u003c/p\u003e\n\u003cp\u003e面完之后，觉得Offer稳了，然后开心的回所里。第二天收到邮件通知，面试通过，还需参加一个在线笔试，类似于行测。做完之后，查看状态，被告知所有面试笔试都通过了，个人信息已经在Offer池中，但是没有正式Offer。Offer池是什么鬼，也就是没人要被扔到池子里等人捞呗。问了下其他人，大部分也是被扔到池子里了，只听说有一个人收到书面Offer。从此对360无感，无论是你们组织面试，还是我们参加面试，费了一天劲，硬是不发OFFER，坑爹。后来在10月16日，收到一封360的邮件，正式书面Offer，难道是被人相中捞起来了，真是无语。拒。\u003c/p\u003e\n\u003ch1 id=\"阿里巴巴failed\"\u003e阿里巴巴（FAILED）\u003c/h1\u003e\n\u003cp\u003e阿里内推只能选一个部门，内推失败之后也只有一次校招机会，所以大家选部门一定要慎重，根据自己的实力和兴趣进行选择。当时群里给出了蚂蚁金服的内推消息之后，我第一时间就选择内推蚂蚁金服了。结果面了两轮之后查状态已经挂了，也没感觉面得差。可能是因为内推蚂蚁金服的人太多了，实力要求也很高，而且自己做搜索引擎的，和蚂蚁金服不太match。\u003c/p\u003e\n\u003cp\u003e因为内推挂了之后，无法再面其他部门了。只能参加校招流程，校招在线笔试之后一直就没消息，状态也没更新，难度笔试挂了？\u003c/p\u003e","title":"伪·2018届校招面经"},{"content":"2017年12月7日~13日，打着参加“第三届全国质谱分析学术报告会”的旗号，实验室一行11人开启了为期一周的厦门之旅。有关学术交流的总结报告，已经在实验室内部分享了，这篇博客还是来聊聊吃喝玩乐的事吧:)\n第一次乘坐飞机 本次出行分为飞机组和火车组，大部队说飞机不安全要坐火车，我因为想体验一下乘坐飞机的感受，于是选择了飞机组。第一次坐飞机，体验有三点：1）快。从北京到厦门，横跨整个中国，只花了3个小时，真的好快，除去吃午餐等时间，连一部电影都没看完！2）噪。起飞和降落的时候噪声特别大，平飞的时候噪声也不小，而且快要到目的地时，会有短暂的耳胀，孙老师说第一次坐飞机的人好像都会出现耳胀的情况。还有就是有时候飞机比较颠簸，放在桌子上的水都快晃出来了。3）美。我因为第一次坐飞机，特地选了一个靠窗的位置，想体验一把俯瞰神州大地的感觉。从窗户往下看时，地形地貌和谷歌卫星地图一摸一样，窗外的云就像一朵朵棉花糖，很漂亮，很像各种神话剧中的天庭。\n飞机上不让开手机，这是在北京首都国际机场起飞前抓拍的照片，以后坐飞机记得带相机\n南国风光 下飞机之后，立即体验到了厦门这座城市的“热情”，20℃左右的温度，卸下厚重的羽绒服，看着路边的红花绿叶正艳，来到沙滩上，吹吹海风，一身清爽。南方的城市由于经常下雨，街道看起来很干净，柏油马路显露着其原本的黝黑色，路边的叶子绿的发亮，不像在北京被蒙上厚厚的一层灰。路上车辆和红绿灯不多，也很少听到鸣笛，听说厦门全岛禁止鸣笛，斑马线处虽然没有红绿灯，但是有摄像头，强制车辆遇到行人时必须礼让。这种规定在北京是不可能实行的吧。总体而言，厦门彰显了南方城市应有的魅力，和杭州类似，但又更加清新靓丽，是一个休闲生活的好地方。\n厦门大学——也许是中国最美丽的大学 本次参会，我们特地提前一天到达，留出时间来参观游览，地点之一就是厦门大学。厦大很美，这是来自一个在武大待了4年的人的由衷赞美。得益于其依山傍水的地理位置，校园内有锦绣的芙蓉湖、美丽的情人谷水库等景点，气氛十分静谧，这点相比于武大小小的鉴湖就好很多。由于地处热带，校园内随处可见高大挺拔的棕榈树，配上古朴的清水墙、多彩的琉璃顶，给人一种别样的美感。如果你要问厦大和武大哪个更美，我只能说她们都美！武大是幽静娴雅的大家闺秀，厦大则像开放热情的新时代女子。武大更加符合中国传统的园林审美，厦大更具风情和现代气息。两校都有山有水，都无愧于中国最美丽的大学的称号！\n看见有人在厦大的情人谷水库泛舟，真是“亦可赛艇”呀:)\n厦大还有一个特别的地方——芙蓉隧道，长达一公里，隧道内壁两侧，画满了风格各异，色彩缤纷的涂鸦，号称是中国最文艺的隧道，中国最长的涂鸦隧道。致青春，怀念那种开放、包容的大学氛围。\n环游鼓浪屿 会议结束的那天下午，我们前往国家5A级旅游景区鼓浪屿风景名胜区游玩。鼓浪屿，面积不到2平方千米，人口约2万，有“海上花园”、“万国建筑博览会”、“钢琴之岛”之美称。除环岛电动车外不允许机动车辆上岛，因此气氛幽静。2005年《中国国家地理》杂志将鼓浪屿评为“中国最美的城区”第一名。2017年7月在波兰克拉科夫举行的第41届世界遗产大会上被正式列入《世界遗产名录》。鼓浪屿、厦门岛和大陆的关系，就像月球、地球和太阳的关系。\n我们首先从厦门国际邮轮中心乘坐邮轮前往鼓浪屿的三丘田码头，邮轮北边能看到远处横跨鹭江海沧大桥，非常的简洁漂亮，据说是亚洲第一、世界第二（仅次于丹麦）的三跨连续全漂浮钢箱梁悬索桥，代表着20世纪中国建桥水平最高成就。由于是国际邮轮中心，所以也能看到不少外国邮轮，我当时就看到了欧洲和韩国的邮轮经过。\n20分钟之后，就到达了鼓浪屿的三丘田码头。我们沿着鼓浪屿的海岸线，优哉游哉的走着，嬉笑打闹，听着浪涛声，赏着落日余晖，好不惬意。\n我们原本是打算步行环岛游览一周的，但是走到卢戆章雕塑的地方，好多人都走不动了，打算穿过岛屿，提前返回。我个人其实挺想继续前进到鼓声洞的，鼓浪屿名称的由来就是因为在鼓声洞能听到阵阵浪涛拍击岩洞而发出轰隆巨响。\n横穿鼓浪屿的路不是很好走，蜿蜒曲折、上下颠簸，走到龙头路小吃街的时候，大家都饿得不行。小吃街排队的食客特别多，为了早点吃饱，大家分头行动，每两三个人排一家店，所以最终还吃了不少美食，比如沈家闽南肠粉、小马哥起司马铃薯、汤满贯等。有意思的是，在沈家闽南肠粉和小马哥起司马铃薯的中间，有一家土耳其冰淇淋店，两边的食客都排起了长龙，唯独这家店门前冷落鞍马稀。大家吃着美食就开始聊起来了，虽说厦门冬天不冷，但也没有热到对冰淇淋有那么大的渴望。更重要的是，走到此处的游客肯定饿了，首先想到的是填饱肚子，冰淇淋属于饭后甜点，应该算“奢侈品”了，如果这个店换成“土耳其烤肉”，估计能火，哈哈。\n岛上其实还有很多景点，比如风琴博物馆、菽庄花园、日光岩、海底世界等，但是我们都没进去，可能是因为沿途的风景太美了，这些收费的室内景点还不足以吸引我们吧。鼓浪屿的风景太美，可玩、可看的景点太多，半天的时间绝对不够用。下次再去，一定要细细品味。\nhttp://j.map.baidu.com/M2cPN\n厦门植物园 回北京之前，我们去了火车站附近的厦门植物园游玩。厦门植物园也称为万石植物园，背靠五老峰南普陀，集植物景观、自然景观、人文景观于一体，景色相当不错。我们沿路经过了百花厅、奇趣植物区、新碑林、摩崖石刻、长寿峡、半山观景台、多肉植物区、雨林世界、药用植物区，看到了很多奇花异草，整个行程在多肉植物区看到高大的仙人掌时达到高潮，非常值得游览的一个景点。\n厦门植物园石碑 类似食人花的猪笼草，开启之后像一个笼子 奇丑无比的白花异木棉 万石丛中 半山观景台，背景是厦门世茂双子塔 疯狂生长的仙人掌和仙人球 类似毛毛虫的仙人掌。。。 放了一个大招，生出许多仙人掌:) 胡吃海喝 在厦门的这一周，大家都说不是在吃就是在吃的路上，不论是酒店的自助餐还是在外面吃饭，都吃得很不错，我基本上每餐都是十分饱。在酒店的伙食，相对来说，早餐是最好的，午餐和晚餐比较一般，可能是因为早餐酒店提供，午餐和晚餐是会议主办方提供吧，毕竟会议注册费很便宜，伙食也不会太好。不过每餐都有海鲜、厦门特色沙茶面、各种肉、水果、甜点、饮料，相比于北京的食堂是好太多了。\n丰盛的早餐\n到厦门的第一天晚上，我和师弟去中山路小吃街逛了逛，结果并没有任何惊喜，感觉全国的小吃街都差不多：烧烤、臭豆腐、各种煎饼包子、粥、饼。中山路的新华书店倒是值得一逛，他们家第三层的书不少，国内外文学、小说、工具书一应俱全，一改我对新华书店只有教科书的旧思想。\n在外面吃的话，每餐必点被称为花蛤的“虫子”，还有另一种称为蛏（cheng）子的“虫子”，和花蛤很像，但是是长方形，且有两条美腿。知乎上有一个回答仔细的辨析了花蛤、蛏子、蚬子、蚶子、蛤蜊、海瓜子、贝壳的区别，很有意思。不过话说这些海底动物长得都很奇怪，有些还有很多软体触角，看着让人恶心。席间，有个师兄讲了个笑话，问为啥海底动物长得都这么丑，答案是因为海底没光，反正都看不见，大家就随便长长了:)。\n说到海产品，必不可少的是鱼了，厦门的清蒸金昌鱼非常好吃，味道鲜美，没有小刺，和上次在杭州吃到的西湖醋鱼简直是一个天上，一个地下，强烈推荐。\n离开厦门的最后一餐，我们去当地的特色饭店小眼镜大排档吃饭，虽然叫大排档，但其实是一个正经的餐厅。点了不少大菜，比如鲍*，大龙*，真是开眼了。。。\n咳咳，以上就是本次厦门之行的次要内容，主要内容当然是开会听各种院士大牛的报告啦，不过作为一个码农，跑去参加理化生同学的质谱会议，第一天就被一个大姐姐笑话“你们学计算机的为什么也跑来参加质谱会议呀？”，我表示我也很无奈。第一次外出参会，虽然我们是配角，也听不懂那些生化的报告，但还是精心制作了属于自己的第一份墙报，也算是入了研究生的门了。\n最后，贴出我在鼓浪屿买的“课业成功”的冰箱贴，希望自己读博顺利吧~\n","permalink":"http://localhost:1313/posts/2017-12-16-a-trip-to-xiamen/","summary":"\u003cp\u003e2017年12月7日~13日，打着参加“第三届全国质谱分析学术报告会”的旗号，实验室一行11人开启了为期一周的厦门之旅。有关学术交流的总结报告，已经在实验室内部分享了，这篇博客还是来聊聊吃喝玩乐的事吧:)\u003c/p\u003e\n\u003ch1 id=\"第一次乘坐飞机\"\u003e第一次乘坐飞机\u003c/h1\u003e\n\u003cp\u003e本次出行分为飞机组和火车组，大部队说飞机不安全要坐火车，我因为想体验一下乘坐飞机的感受，于是选择了飞机组。第一次坐飞机，体验有三点：1）快。从北京到厦门，横跨整个中国，只花了3个小时，真的好快，除去吃午餐等时间，连一部电影都没看完！2）噪。起飞和降落的时候噪声特别大，平飞的时候噪声也不小，而且快要到目的地时，会有短暂的耳胀，孙老师说第一次坐飞机的人好像都会出现耳胀的情况。还有就是有时候飞机比较颠簸，放在桌子上的水都快晃出来了。3）美。我因为第一次坐飞机，特地选了一个靠窗的位置，想体验一把俯瞰神州大地的感觉。从窗户往下看时，地形地貌和谷歌卫星地图一摸一样，窗外的云就像一朵朵棉花糖，很漂亮，很像各种神话剧中的天庭。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/airport.webp\"\u003e\n飞机上不让开手机，这是在北京首都国际机场起飞前抓拍的照片，以后坐飞机记得带相机\u003c/p\u003e\n\u003ch1 id=\"南国风光\"\u003e南国风光\u003c/h1\u003e\n\u003cp\u003e下飞机之后，立即体验到了厦门这座城市的“热情”，20℃左右的温度，卸下厚重的羽绒服，看着路边的红花绿叶正艳，来到沙滩上，吹吹海风，一身清爽。南方的城市由于经常下雨，街道看起来很干净，柏油马路显露着其原本的黝黑色，路边的叶子绿的发亮，不像在北京被蒙上厚厚的一层灰。路上车辆和红绿灯不多，也很少听到鸣笛，听说厦门全岛禁止鸣笛，斑马线处虽然没有红绿灯，但是有摄像头，强制车辆遇到行人时必须礼让。这种规定在北京是不可能实行的吧。总体而言，厦门彰显了南方城市应有的魅力，和杭州类似，但又更加清新靓丽，是一个休闲生活的好地方。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xiamen1.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xiamen2.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xiamen3.webp\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"厦门大学也许是中国最美丽的大学\"\u003e厦门大学——也许是中国最美丽的大学\u003c/h1\u003e\n\u003cp\u003e本次参会，我们特地提前一天到达，留出时间来参观游览，地点之一就是厦门大学。厦大很美，这是来自一个在武大待了4年的人的由衷赞美。得益于其依山傍水的地理位置，校园内有锦绣的芙蓉湖、美丽的情人谷水库等景点，气氛十分静谧，这点相比于武大小小的鉴湖就好很多。由于地处热带，校园内随处可见高大挺拔的棕榈树，配上古朴的清水墙、多彩的琉璃顶，给人一种别样的美感。如果你要问厦大和武大哪个更美，我只能说她们都美！武大是幽静娴雅的大家闺秀，厦大则像开放热情的新时代女子。武大更加符合中国传统的园林审美，厦大更具风情和现代气息。两校都有山有水，都无愧于中国最美丽的大学的称号！\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu1.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu2.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu3.webp\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu4.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu5.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu6.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu7.webp\"\u003e\n看见有人在厦大的情人谷水库泛舟，真是“亦可赛艇”呀:)\u003c/p\u003e\n\u003cp\u003e厦大还有一个特别的地方——芙蓉隧道，长达一公里，隧道内壁两侧，画满了风格各异，色彩缤纷的涂鸦，号称是中国最文艺的隧道，中国最长的涂鸦隧道。致青春，怀念那种开放、包容的大学氛围。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle1.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle2.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle3.webp\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle4.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle5.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle6.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle7.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle8.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle9.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"环游鼓浪屿\"\u003e环游鼓浪屿\u003c/h1\u003e\n\u003cp\u003e会议结束的那天下午，我们前往国家5A级旅游景区鼓浪屿风景名胜区游玩。鼓浪屿，面积不到2平方千米，人口约2万，有“海上花园”、“万国建筑博览会”、“钢琴之岛”之美称。除环岛电动车外不允许机动车辆上岛，因此气氛幽静。2005年《中国国家地理》杂志将鼓浪屿评为“中国最美的城区”第一名。2017年7月在波兰克拉科夫举行的第41届世界遗产大会上被正式列入《世界遗产名录》。鼓浪屿、厦门岛和大陆的关系，就像月球、地球和太阳的关系。\u003c/p\u003e\n\u003cp\u003e我们首先从厦门国际邮轮中心乘坐邮轮前往鼓浪屿的三丘田码头，邮轮北边能看到远处横跨鹭江海沧大桥，非常的简洁漂亮，据说是亚洲第一、世界第二（仅次于丹麦）的三跨连续全漂浮钢箱梁悬索桥，代表着20世纪中国建桥水平最高成就。由于是国际邮轮中心，所以也能看到不少外国邮轮，我当时就看到了欧洲和韩国的邮轮经过。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu1.webp\"\u003e\u003c/p\u003e\n\u003cp\u003e20分钟之后，就到达了鼓浪屿的三丘田码头。我们沿着鼓浪屿的海岸线，优哉游哉的走着，嬉笑打闹，听着浪涛声，赏着落日余晖，好不惬意。\u003c/p\u003e\n\u003cp\u003e我们原本是打算步行环岛游览一周的，但是走到卢戆章雕塑的地方，好多人都走不动了，打算穿过岛屿，提前返回。我个人其实挺想继续前进到鼓声洞的，鼓浪屿名称的由来就是因为在鼓声洞能听到阵阵浪涛拍击岩洞而发出轰隆巨响。\u003c/p\u003e\n\u003cp\u003e横穿鼓浪屿的路不是很好走，蜿蜒曲折、上下颠簸，走到龙头路小吃街的时候，大家都饿得不行。小吃街排队的食客特别多，为了早点吃饱，大家分头行动，每两三个人排一家店，所以最终还吃了不少美食，比如沈家闽南肠粉、小马哥起司马铃薯、汤满贯等。有意思的是，在沈家闽南肠粉和小马哥起司马铃薯的中间，有一家土耳其冰淇淋店，两边的食客都排起了长龙，唯独这家店门前冷落鞍马稀。大家吃着美食就开始聊起来了，虽说厦门冬天不冷，但也没有热到对冰淇淋有那么大的渴望。更重要的是，走到此处的游客肯定饿了，首先想到的是填饱肚子，冰淇淋属于饭后甜点，应该算“奢侈品”了，如果这个店换成“土耳其烤肉”，估计能火，哈哈。\u003c/p\u003e\n\u003cp\u003e岛上其实还有很多景点，比如风琴博物馆、菽庄花园、日光岩、海底世界等，但是我们都没进去，可能是因为沿途的风景太美了，这些收费的室内景点还不足以吸引我们吧。鼓浪屿的风景太美，可玩、可看的景点太多，半天的时间绝对不够用。下次再去，一定要细细品味。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu2.webp\"\u003e\n\u003ca href=\"http://j.map.baidu.com/M2cPN\"\u003ehttp://j.map.baidu.com/M2cPN\u003c/a\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu3.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu4.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu5.webp\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu6.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu7.webp\"\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"厦门植物园\"\u003e厦门植物园\u003c/h1\u003e\n\u003cp\u003e回北京之前，我们去了火车站附近的厦门植物园游玩。厦门植物园也称为万石植物园，背靠五老峰南普陀，集植物景观、自然景观、人文景观于一体，景色相当不错。我们沿路经过了百花厅、奇趣植物区、新碑林、摩崖石刻、长寿峡、半山观景台、多肉植物区、雨林世界、药用植物区，看到了很多奇花异草，整个行程在多肉植物区看到高大的仙人掌时达到高潮，非常值得游览的一个景点。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e厦门植物园石碑\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e类似食人花的猪笼草，开启之后像一个笼子\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg1.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg2.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e奇丑无比的白花异木棉\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e万石丛中\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg3.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg4.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e半山观景台，背景是厦门世茂双子塔\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e疯狂生长的仙人掌和仙人球\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg5.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg6.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e类似毛毛虫的仙人掌。。。\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e放了一个大招，生出许多仙人掌:)\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg7.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg8.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"胡吃海喝\"\u003e胡吃海喝\u003c/h1\u003e\n\u003cp\u003e在厦门的这一周，大家都说不是在吃就是在吃的路上，不论是酒店的自助餐还是在外面吃饭，都吃得很不错，我基本上每餐都是十分饱。在酒店的伙食，相对来说，早餐是最好的，午餐和晚餐比较一般，可能是因为早餐酒店提供，午餐和晚餐是会议主办方提供吧，毕竟会议注册费很便宜，伙食也不会太好。不过每餐都有海鲜、厦门特色沙茶面、各种肉、水果、甜点、饮料，相比于北京的食堂是好太多了。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/breakfast.webp\"\u003e\n丰盛的早餐\u003c/p\u003e\n\u003cp\u003e到厦门的第一天晚上，我和师弟去中山路小吃街逛了逛，结果并没有任何惊喜，感觉全国的小吃街都差不多：烧烤、臭豆腐、各种煎饼包子、粥、饼。中山路的新华书店倒是值得一逛，他们家第三层的书不少，国内外文学、小说、工具书一应俱全，一改我对新华书店只有教科书的旧思想。\u003c/p\u003e\n\u003cp\u003e在外面吃的话，每餐必点被称为花蛤的“虫子”，还有另一种称为蛏（cheng）子的“虫子”，和花蛤很像，但是是长方形，且有两条美腿。知乎上有一个回答仔细的辨析了\u003ca href=\"https://www.zhihu.com/question/25165185/answer/30233547\"\u003e花蛤、蛏子、蚬子、蚶子、蛤蜊、海瓜子、贝壳的区别\u003c/a\u003e，很有意思。不过话说这些海底动物长得都很奇怪，有些还有很多软体触角，看着让人恶心。席间，有个师兄讲了个笑话，问为啥海底动物长得都这么丑，答案是因为海底没光，反正都看不见，大家就随便长长了:)。\u003c/p\u003e\n\u003cp\u003e说到海产品，必不可少的是鱼了，厦门的清蒸金昌鱼非常好吃，味道鲜美，没有小刺，和上次在杭州吃到的西湖醋鱼简直是一个天上，一个地下，强烈推荐。\u003c/p\u003e","title":"厦门之行"},{"content":"上一篇博客主要介绍了逻辑回归的理论知识，这篇博客咱们用Python机器学习包sklearn中的LogisticRegression做一个分类的实例。\n数据还是学生样本，只有两个特征，分别是两门课的分数score1和score2，类标号y表示这个学生是否能被录取。先上分类效果图：\n完整的Python代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Wed Nov 08 17:49:41 2017 @author: zhenlin \u0026#34;\u0026#34;\u0026#34; import numpy as np import pandas as pd from sklearn.cross_validation import train_test_split from sklearn.linear_model import LogisticRegression import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve from sklearn.metrics import classification_report # 1. 构造数据 sample_number = 200 # 第一个高斯分布参数 mean1 = [0, 4] # 两个维度上的均值 cov1 = [[5, 3], [3, 10]] # 两个维度的协方差矩阵，必须满足对称半正定 # 第二个高斯分布参数 mean2 = [7, 5] cov2 = [[7, 2], [2, 15]] # 从两个二元高斯分布中随机采样数据点 class1_x1, class1_x2 = np.random.multivariate_normal(mean1, cov1, sample_number).T # .T表示转置 class2_x1, class2_x2 = np.random.multivariate_normal(mean2, cov2, sample_number).T # 两个高斯分布对应两个类标号 data = [[class1_x1[i],class1_x2[i],0] for i in range(sample_number)]+[[class2_x1[i],class2_x2[i],1] for i in range(sample_number)] # 填充到pandas中 data = pd.DataFrame(data,columns=[\u0026#39;score1\u0026#39;,\u0026#39;score2\u0026#39;,\u0026#39;result\u0026#39;]) score_data = data[[\u0026#39;score1\u0026#39;,\u0026#39;score2\u0026#39;]] result_data = data[\u0026#39;result\u0026#39;] # 2. 训练模型 average_precision = 0 # 平均准确度 iters = 10 # 交叉验证次数 for i in xrange(iters): # 数据划分，80%用于训练，20%用于预测 x_train, x_test, y_train, y_test = train_test_split(score_data, result_data, test_size = 0.2) # 构造默认逻辑回归模型 model = LogisticRegression() # 训练 model.fit(x_train, y_train) # 预测 predict_y = model.predict(x_test) # 计算测试集上的准确度 average_precision += np.mean(predict_y == y_test) average_precision /= iters # 3. 绘制分类面 - 法1 x1_min, x1_max = score_data[\u0026#39;score1\u0026#39;].min() - .5, score_data[\u0026#39;score1\u0026#39;].max() + .5 def generate_face(prob): y = -np.log(1.0 / prob - 1.0) n = 500 x1 = np.linspace(x1_min, x1_max, n) # w1x1+w2x2+b=y x2 = (-model.coef_[0][0] / float(model.coef_[0][1])) * x1 + (y - model.intercept_) / float(model.coef_[0][1]) return x1, x2 pos_data = data[data[\u0026#39;result\u0026#39;] == 1] neg_data = data[data[\u0026#39;result\u0026#39;] == 0] plt.scatter(x = pos_data[\u0026#39;score1\u0026#39;], y = pos_data[\u0026#39;score2\u0026#39;], color = \u0026#39;black\u0026#39;, marker = \u0026#39;o\u0026#39;) plt.scatter(x = neg_data[\u0026#39;score1\u0026#39;], y = neg_data[\u0026#39;score2\u0026#39;], color = \u0026#39;red\u0026#39;, marker = \u0026#39;*\u0026#39;) face_04_x1, face_04_x2 = generate_face(0.4) face_05_x1, face_05_x2 = generate_face(0.5) face_06_x1, face_06_x2 = generate_face(0.6) plt.plot(face_04_x1, face_04_x2) plt.plot(face_05_x1, face_05_x2) plt.plot(face_06_x1, face_06_x2) plt.xlim(score_data[\u0026#39;score1\u0026#39;].min(), score_data[\u0026#39;score1\u0026#39;].max()) plt.ylim(score_data[\u0026#39;score2\u0026#39;].min(), score_data[\u0026#39;score2\u0026#39;].max()) plt.xlabel(\u0026#39;score1\u0026#39;) plt.ylabel(\u0026#39;score2\u0026#39;) plt.legend([\u0026#39;prob_threshold = 0.4\u0026#39;, \u0026#39;prob_threshold = 0.5\u0026#39;, \u0026#39;prob_threshold = 0.6\u0026#39;], loc=\u0026#39;center left\u0026#39;, bbox_to_anchor=(1, 0.865)) plt.show() # 4. 绘制分类面 - 法2 pos_data = data[data[\u0026#39;result\u0026#39;] == 1] neg_data = data[data[\u0026#39;result\u0026#39;] == 0] h = 0.02 s1_min, s1_max = score_data[\u0026#39;score1\u0026#39;].min() - .5, score_data[\u0026#39;score1\u0026#39;].max() + .5 s2_min, s2_max = score_data[\u0026#39;score2\u0026#39;].min() - .5, score_data[\u0026#39;score2\u0026#39;].max() + .5 # 生成s1在[s1_min, s1_max]，且s2在[s2_min, s2_max]的网格数据点 # meshgrid含义参见：http://blog.sciencenet.cn/blog-791749-675394.html s1, s2 = np.meshgrid(np.arange(s1_min, s1_max, h), np.arange(s2_min, s2_max, h)) # 把两个坐标的值按列拼在一起构成二维数据点 Z = model.predict(np.c_[s1.ravel(), s2.ravel()]) # 绘制边界和散点 Z = Z.reshape(s1.shape) # 坐标点是(s1[i], s2[i])，对应颜色是Z[i]，颜色主题使用plt.cm.Paired plt.pcolormesh(s1, s2, Z, cmap = plt.cm.Paired) plt.scatter(x = pos_data[\u0026#39;score1\u0026#39;], y = pos_data[\u0026#39;score2\u0026#39;], color = \u0026#39;black\u0026#39;, marker = \u0026#39;o\u0026#39;) plt.scatter(x = neg_data[\u0026#39;score1\u0026#39;], y = neg_data[\u0026#39;score2\u0026#39;], color = \u0026#39;red\u0026#39;, marker = \u0026#39;*\u0026#39;) plt.xlim(s1.min(), s1.max()) plt.ylim(s2.min(), s2.max()) plt.xlabel(\u0026#39;score1\u0026#39;) plt.ylabel(\u0026#39;score2\u0026#39;) plt.show() # 5. 评估模型 # 对于测试数据，模型输出1的概率 answer = model.predict_proba(x_test)[:,1] # 计算不同概率阈值下的P和R precision, recall, thresholds = precision_recall_curve(y_test, answer) # prob \u0026gt; 0.5的报告为1 report = answer \u0026gt; 0.5 print(classification_report(y_test, report, target_names = [\u0026#39;neg\u0026#39;, \u0026#39;pos\u0026#39;])) print(\u0026#39;average precision: %f\u0026#39;%average_precision) # 6. 绘制PRC曲线 # step阶跃图，在点(recall[i],precision[i])进行跳变 plt.step(recall, precision, color=\u0026#39;b\u0026#39;, alpha=0.2, where=\u0026#39;post\u0026#39;) # 对PRC下方填充颜色 plt.fill_between(recall, precision, step=\u0026#39;post\u0026#39;, alpha=0.2, color=\u0026#39;b\u0026#39;) plt.xlabel(\u0026#39;Recall\u0026#39;) plt.ylabel(\u0026#39;Precision\u0026#39;) plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1.0]) plt.title(\u0026#39;2-class Precision-Recall curve\u0026#39;) plt.show() 下面将逐模块介绍代码细节，大神可以略过。\n一、构造数据 数据的来源可以有很多种方式，sklearn包中自带7个Toy datasets，比如我们耳熟能详的鸢尾花卉数据集iris和手写数字数据集digits。为了学一学numpy，我们尝试自行构造数据集，这样也方便读者重复。我们可以从两个不同参数的高斯分布中采样相同数量的数据点，由于数据包含两个维度(score1, score2)，所以必须是二维高斯分布。二维高斯分布概率密度函数为：\n参数包括：\n\\(\\mu\\)中的\\(\\mu_X\\)和\\(\\mu_Y\\)分别表示两个维度上的均值，\\(\\Sigma\\)是这两个维度上的协方差矩阵，需要满足对称半正定。\nnumpy中的random.multivariate_normal可以从多元正态分布中进行采样，传入的参数包括均值mean, 协方差矩阵cov和采样数据点个数。比如本文中第二类数据点的均值为[7,5]，表示在score1维上的均值为7，在score2维上的均值为5，从图上黑点的横纵坐标分布也可以看出来；协方差矩阵为\\(\\begin{bmatrix}7 \u0026 2\\\\2 \u0026 15\\end{bmatrix}\\)，表示score1和score2上的方差分别为7和15，然后标准差的积再乘以相关系数等于2。用这些参数调用multivariate_normal就能得到对应二维平面\\(x_1Ox_2\\)上的数据点了。\n最后，给两类数据点贴上不同的类标号，填充到Pandas中就大功告成了。Pandas中每一行表示一个样本，共有三列，分别表示score1、score2和result，其中result就是类标号。\n二、训练模型 由于是直接调用sklearn中的逻辑回归函数，所以这一步非常简单。为了评估模型的准确度，我们做了一个交叉验证，即随机把数据分成80%用于训练，20%用于预测，重复10次，求预测准确度的平均值。这可以用模型选择中的train_test_split快速完成。\n调用sklearn.linear_model.LogisticRegression()就能得到一个逻辑回归模型，该函数有很多参数，但是作为入门，所有参数都使用默认值。训练直接调用model.fit(x,y)，预测直接调用model.predict(x)。需要注意的是，model中带了很多成员变量，比如训练得到的分类面参数coef_和intercept_等，后面会用到。\n三、绘制分类面（法1） 上一篇博客提到，虽然\\(f(\\mathbf{x})\\)被Sigmoid函数映射到了概率空间，但逻辑回归的分类面还是\\(f(\\mathbf{x})=\\mathbf{w^T x}+b=0\\)。model通过fit函数训练之后，就能得到分类面的法向量\\(\\mathbf{w}\\)（coef_）和截距\\(b\\)（intercept_），也就是说对于分类面\\(w_1x_1+w_2x_2+b=0\\)，我们已经知道了\\(w_1\\)、\\(w_2\\)和\\(b\\)，所以就可以在平面\\(x_1Ox_2\\)上画出这条直线了。具体的方法就是使用np.linspace先在\\(x_1\\)上选取足够多的点，代入\\(w_1x_1+w_2x_2+b=0\\)得到对应的\\(x_2\\)。该分类面如下图的prob_threshold=0.5。\n事实上，逻辑回归的分类面可以不止一个，我们上面得到的分类面是\\(f(\\mathbf{x})=0\\)，代入到\\(g(\\mathbf{x})\\)就是\\(g(\\mathbf{x})=0.5\\)，也就是说当逻辑回归计算到的概率\u0026gt;=0.5分类为1，\u0026lt;0.5分类为0。但是我们也可以提高这个阈值，比如要求概率\u0026gt;=0.6分类为1，\u0026lt;0.6分类为0，这时，相当于我们对于分类为1的阈值提高了，要求更严格了，所以分类面应该向右边黑点方向移动。\n求解prob_threshold=0.6的分类面也不难，令\\(g(\\mathbf{x})=0.6\\)，得到\\(f(\\mathbf{x})=-lg(\\frac{1}{0.6}-1)\\)，剩下的过程和\\(f(\\mathbf{x})=0\\)是一样的。由此得到的分类面如上图的prob_threshold=0.6那条线，确实在prob_threshold=0.5的右边。类似的，可以画出prob_threshold=0.4的分类面。\n四、绘制分类面（法2） 还有一种简单粗暴的方法可以快速绘制出分类面。分类面的本质是在该分类面左右两侧的类标签不一样，如果我们把平面上所有点都预测一遍，对预测值为1的标上一种颜色，对预测值为0的标上另一种颜色，那么这两种颜色的交界处自然就是分类面了。\n首先，我们使用np.meshgrid生成网格数据点，关于np.meshgrid的用法，这篇博客的介绍很好理解。然后对所有网格数据点调用model.predict进行预测。最后使用plt.pcolormesh的plt.cm.Paired颜色主题进行着色，即类标签为1的一种颜色，类标签为0的另一种颜色。最后把原始训练数据点画上去，就得到博客开篇的那张分类面图：\n五、评估模型 model.predict是直接预测出类标号1或者0，而model.predict_proba是给出类标号分别为1和0的概率，用户可以自行根据prob_threshold进行分类。\n1 precision, recall, thresholds = precision_recall_curve(y_test, answer) precision_recall_curve的参数是正确答案y_test和model.predict_proba预测出来的概率，返回值分别表示不同threshold阈值下的precision和recall。sklearn官方的例子如下：\n1 2 3 y_true = np.array([0, 0, 1, 1]) y_scores = np.array([0.1, 0.4, 0.35, 0.8]) precision, recall, thresholds = precision_recall_curve(y_true, y_scores) 得到的结果如下：\n1 2 3 4 5 6 precision array([ 0.66..., 0.5 , 1. , 1. ]) recall array([ 1. , 0.5, 0.5, 0. ]) thresholds array([ 0.35, 0.4 , 0.8 ]) 比如当threshold=0.4时，表示\u0026gt;=0.4分类为1，\u0026lt;0.4分类为0，则预测结果为[0,1,0,1]。正确率为预测为1的结果中对了几个1/2=0.5，召回率为召回了多少个正确答案为1的结果1/2=0.5。其他阈值的计算类似。\n最后调用classification_report会算出不同类别的precision、recall和F1，以及对应的支持数据个数。\n1 2 3 4 5 6 7 8 precision recall f1-score support neg 0.89 1.00 0.94 42 pos 1.00 0.87 0.93 38 avg / total 0.94 0.94 0.94 80 average precision: 0.925000 六、绘制PRC曲线 PRC曲线就是precision recall curve，由于上一步已经调用precision_recall_curve得到了不同阈值下的precision和recall，这一步直接拿来用就好了。为了防止画出来的曲线抖动形成毛刺，我们使用plt.step阶跃函数来绘制，起到平滑的作用。最后使用plt.fill_between填充曲线下方的面积。得到下图：\n至此，整个示例讲解完毕。\n如果要使用逻辑回归处理多分类问题，只需要构造好多标签的训练数据就好了，剩下的就交给模型自己处理。LogisticRegression的multi_class参数可以设置使用何种策略求解多分类问题，one-vs-rest (OvR)即构建k个二元分类器，multinomial即使用Softmax回归，默认使用OvR。\n参考：\nhttp://blog.csdn.net/u011721501/article/details/49661585 ","permalink":"http://localhost:1313/posts/2017-12-05-logistic-regression-in-python/","summary":"\u003cp\u003e上一篇博客主要介绍了逻辑回归的理论知识，这篇博客咱们用Python机器学习包sklearn中的LogisticRegression做一个分类的实例。\u003c/p\u003e\n\u003cp\u003e数据还是学生样本，只有两个特征，分别是两门课的分数score1和score2，类标号y表示这个学生是否能被录取。先上分类效果图：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-05-logistic-regression-in-python/lr_face_3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e完整的Python代码如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 46\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 47\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 48\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 49\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 50\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 51\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 52\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 53\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 54\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 55\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 56\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 57\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 58\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 59\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 60\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 61\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 62\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 63\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 64\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 65\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 66\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 67\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 68\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 69\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 70\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 71\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 72\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 73\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 74\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 75\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 76\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 77\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 78\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 79\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 80\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 81\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 82\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 83\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 84\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 85\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 86\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 87\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 88\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 89\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 90\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 91\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 92\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 93\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 94\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 95\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 96\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 97\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 98\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 99\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e100\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e101\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e102\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e103\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e104\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e105\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e106\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e107\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e108\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e109\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e110\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e111\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e112\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e113\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e114\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e115\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e116\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e117\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e118\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e119\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e120\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e121\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e122\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e123\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e124\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e125\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e126\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e127\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e128\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e129\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e130\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e131\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e132\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e133\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e134\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e135\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e136\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e137\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e138\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e139\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e140\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e141\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e142\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e143\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e144\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e145\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -*- coding: utf-8 -*-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eCreated on Wed Nov 08 17:49:41 2017\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e@author: zhenlin\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np  \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pandas \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e pd\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.cross_validation \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e train_test_split\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.linear_model \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e LogisticRegression\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e plt\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.metrics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e precision_recall_curve\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.metrics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e classification_report\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 1. 构造数据\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esample_number \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e200\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 第一个高斯分布参数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emean1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e] \u003cspan style=\"color:#75715e\"\u003e# 两个维度上的均值\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecov1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [[\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e], [\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e]] \u003cspan style=\"color:#75715e\"\u003e# 两个维度的协方差矩阵，必须满足对称半正定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 第二个高斯分布参数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emean2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecov2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [[\u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e], [\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e]]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 从两个二元高斯分布中随机采样数据点\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eclass1_x1, class1_x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emultivariate_normal(mean1, cov1, sample_number)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eT \u003cspan style=\"color:#75715e\"\u003e# .T表示转置\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eclass2_x1, class2_x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emultivariate_normal(mean2, cov2, sample_number)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eT\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 两个高斯分布对应两个类标号\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edata \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [[class1_x1[i],class1_x2[i],\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(sample_number)]\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e[[class2_x1[i],class2_x2[i],\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e] \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(sample_number)]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 填充到pandas中\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edata \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataFrame(data,columns\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003escore_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e]]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresult_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 2. 训练模型\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eaverage_precision \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# 平均准确度\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eiters \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# 交叉验证次数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e xrange(iters):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 数据划分，80%用于训练，20%用于预测\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    x_train, x_test, y_train, y_test \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e train_test_split(score_data, result_data, test_size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 构造默认逻辑回归模型\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e LogisticRegression()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 训练\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efit(x_train, y_train)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 预测\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    predict_y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epredict(x_test)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 计算测试集上的准确度\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    average_precision \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emean(predict_y \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e y_test)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eaverage_precision \u003cspan style=\"color:#f92672\"\u003e/=\u003c/span\u003e iters\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 3. 绘制分类面 - 法1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ex1_min, x1_max \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin() \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e, score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egenerate_face\u003c/span\u003e(prob):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003enp\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elog(\u003cspan style=\"color:#ae81ff\"\u003e1.0\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e prob \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1.0\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    n \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e500\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    x1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elinspace(x1_min, x1_max, n)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# w1x1+w2x2+b=y\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003emodel\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecoef_[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e][\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e float(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecoef_[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e][\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])) \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e x1 \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e (y \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eintercept_) \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e float(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecoef_[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e][\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e x1, x2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epos_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eneg_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escatter(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pos_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e], y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pos_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e], color \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;black\u0026#39;\u003c/span\u003e, marker \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;o\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escatter(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e neg_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e], y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e neg_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e], color \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;red\u0026#39;\u003c/span\u003e, marker \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;*\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eface_04_x1, face_04_x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e generate_face(\u003cspan style=\"color:#ae81ff\"\u003e0.4\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eface_05_x1, face_05_x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e generate_face(\u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eface_06_x1, face_06_x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e generate_face(\u003cspan style=\"color:#ae81ff\"\u003e0.6\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eplot(face_04_x1, face_04_x2)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eplot(face_05_x1, face_05_x2)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eplot(face_06_x1, face_06_x2)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlim(score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin(), score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylim(score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin(), score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elegend([\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;prob_threshold = 0.4\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;prob_threshold = 0.5\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;prob_threshold = 0.6\u0026#39;\u003c/span\u003e], loc\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;center left\u0026#39;\u003c/span\u003e, bbox_to_anchor\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0.865\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 4. 绘制分类面 - 法2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epos_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eneg_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eh \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.02\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003es1_min, s1_max \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin() \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e, score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003es2_min, s2_max \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin() \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e, score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 生成s1在[s1_min, s1_max]，且s2在[s2_min, s2_max]的网格数据点\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# meshgrid含义参见：http://blog.sciencenet.cn/blog-791749-675394.html\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003es1, s2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emeshgrid(np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003earange(s1_min, s1_max, h), np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003earange(s2_min, s2_max, h))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 把两个坐标的值按列拼在一起构成二维数据点\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eZ \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epredict(np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ec_[s1\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eravel(), s2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eravel()])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 绘制边界和散点\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eZ \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Z\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ereshape(s1\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 坐标点是(s1[i], s2[i])，对应颜色是Z[i]，颜色主题使用plt.cm.Paired\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epcolormesh(s1, s2, Z, cmap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e plt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ePaired)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escatter(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pos_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e], y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pos_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e], color \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;black\u0026#39;\u003c/span\u003e, marker \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;o\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escatter(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e neg_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e], y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e neg_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e], color \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;red\u0026#39;\u003c/span\u003e, marker \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;*\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlim(s1\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin(), s1\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylim(s2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin(), s2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 5. 评估模型\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 对于测试数据，模型输出1的概率\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eanswer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epredict_proba(x_test)[:,\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 计算不同概率阈值下的P和R\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprecision, recall, thresholds \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e precision_recall_curve(y_test, answer)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# prob \u0026gt; 0.5的报告为1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ereport \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e answer \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(classification_report(y_test, report, target_names \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;neg\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;pos\u0026#39;\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;average precision: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%f\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003eaverage_precision)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 6. 绘制PRC曲线\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# step阶跃图，在点(recall[i],precision[i])进行跳变\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estep(recall, precision, color\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;b\u0026#39;\u003c/span\u003e, alpha\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e, where\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;post\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 对PRC下方填充颜色\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efill_between(recall, precision, step\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;post\u0026#39;\u003c/span\u003e, alpha\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e, color\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;b\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Recall\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Precision\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylim([\u003cspan style=\"color:#ae81ff\"\u003e0.0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e1.05\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlim([\u003cspan style=\"color:#ae81ff\"\u003e0.0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e1.0\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etitle(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;2-class Precision-Recall curve\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e下面将逐模块介绍代码细节，大神可以略过。\u003c/p\u003e","title":"逻辑回归之Python应用实例"},{"content":"最近实验室在组织学习NG的机器学习视频，我也跟进了一下。讲到逻辑回归那一课，一直想不明白，逻辑回归到底是怎样分类的？逻辑回归的分类面在哪里？逻辑回归有像SVM的max margin那样清晰的推导过程吗？为什么需要Sigmoid函数？今天就让我们来一窥逻辑回归的始末。\n假设有一堆学生样本，为了简单起见，只有两个特征，分别是两门课的分数score1和score2，类标号y表示这个学生是否能被录取。可视化如下图，黑点表示y=1即被录取，红点表示y=0即未被录取，可以看到黑点处在score1和score2都比较高的区域。我们的任务就是给定这些训练样本，需要确定一个分类面来划分这两类数据。\n分类面的实质就是\\(y=\\mathbf{w^T x}+b\\)，其中\\(\\mathbf{w}\\)和\\(\\mathbf{x}\\)都是向量，对应到本例中，展开为\\(y=w_1x_1+w_2x_2+b\\)。所以，寻找分类面的过程就是寻找倾斜度\\(\\mathbf{w}\\)和截距\\(b\\)。\n因为分类的结果是离散的，只有0或者1，可以用感知机来分类。即\n但是怎样找这里的\\(\\mathbf{w}\\)和\\(b\\)使得分类结果最好呢，我们需要定义一个优化的目标函数或者说损失函数。这里只能定义为分类错误的个数，只能一点点挪动超平面来找分类错误最少的超平面了，即只能用暴力枚举的方法来找\\(\\mathbf{w}\\)和\\(b\\)。\n另一种方法是令我们的分类面算出来的值和真实标号越接近越好，即最小化误差\\((\\mathbf{w^T}x^{(i)}+b-y^{(i)})^2\\)，然后通过梯度下降求解\\(\\mathbf{w}\\)和\\(b\\)。\n但是这会有一个问题，对于上图数据，可以求到一个比较好的分类面，如下左图。但是如果数据中出现了如下右图右边的一个离群点或者噪声，为了最小化这个点的误差，即\\((\\mathbf{w^T}x^{(i)}+b-1)^2\\)，导致分类面往右偏了，这一偏直接导致很多y=1的样本分类错误。所以这种最小化误差的方法对离群点很敏感。\n假设分类超平面还是\\(f(\\mathbf{x})=\\mathbf{w^T x}+b\\)，我们希望的分类效果是这样的：\\(f(\\mathbf{x})=0\\)是分类面；\\(f(\\mathbf{x})\u003e0\\)分类为1，且不管\\(f(\\mathbf{x})\\)多大，都分为1；\\(f(\\mathbf{x})\u003c0\\)分类为0，且不管\\(f(\\mathbf{x})\\)多小，都分为0。\n因为类标号是离散的{0,1}，所以想到把\\(f(\\mathbf{x})\\)映射到[0,1]之间，即\\(g(f(\\mathbf{x}))\\)。为了满足上述条件，\\(g(f(\\mathbf{x}))\\)需要满足：\n\\(g(0)=0.5\\)，即在分类面上无法判断类标号是0还是1 当\\(f(\\mathbf{x})\u003e0\\)时，\\(g(f(\\mathbf{x}))\u003e0.5\\) 当\\(f(\\mathbf{x})\\rightarrow+\\infty\\)，\\(g(f(\\mathbf{x}))\\rightarrow 1\\)，且\\(g'(f(\\mathbf{x}))\\rightarrow 0\\)，即对于上右图右边的离群点，分类为1，且导数趋近于0，表示对其不敏感 当\\(f(\\mathbf{x})\u003c0\\)时，\\(g(f(\\mathbf{x}))\u003c0.5\\) 当\\(f(\\mathbf{x})\\rightarrow-\\infty\\)，\\(g(f(\\mathbf{x}))\\rightarrow 0\\)，且\\(g'(f(\\mathbf{x}))\\rightarrow 0\\)，和第3点类似 满足上述性质的函数之一就是Sigmoid函数，其定义域为\\([-\\infty,+\\infty]\\)，值域为[0,1]，正好把原始的函数结果\\(f(\\mathbf{x})\\)映射到了[0,1]，而概率的取值范围正好是[0,1]，所以Sigmoid函数正好可以作为分类为1的概率。\n所以逻辑回归最终的形式就是：\n$$g(\\mathbf{x})=\\frac{1}{1+e^{-(\\mathbf{w^T x}+b)}}$$分类面依然还是\\(f(\\mathbf{x})=\\mathbf{w^T x}+b=0\\)，因为\\(f(\\mathbf{x})=0\\)时，\\(g(\\mathbf{x})=0.5\\)，正好满足上述条件1。\nSigmoid函数的另一个优点是，它把原始函数值映射到了概率空间，这样就可以用极大似然的方法求解参数\\(\\mathbf{w}\\)和\\(b\\)。下面用参数\\(\\mathbf{\\theta}\\)代表参数\\(\\mathbf{w}\\)和\\(b\\)，用\\(h_{\\mathbf{\\theta}}(\\mathbf{x})\\)代表\\(g(f(\\mathbf{x}))\\)。则有：\n$$P(y=1|\\mathbf{x};\\mathbf{\\theta})=h_{\\mathbf{\\theta}}(\\mathbf{x})$$$$P(y=0|\\mathbf{x};\\mathbf{\\theta})=1-h_{\\mathbf{\\theta}}(\\mathbf{x})$$合并成一个式子就是：\n$$P(y|\\mathbf{x};\\mathbf{\\theta})=h_{\\mathbf{\\theta}}(\\mathbf{x})^y(1-h_{\\mathbf{\\theta}}(\\mathbf{x}))^{1-y}$$由于所有样本独立同分布（I.I.D.），似然函数就是\n$$L(\\mathbf{\\theta})=P(\\mathbf{y}|X;\\mathbf{\\theta})=\\prod\\limits_{i}P(y^{(i)}|\\mathbf{x}^{(i)};\\mathbf{\\theta})=\\prod\\limits_{i}h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})^{y^{(i)}}(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))^{1-y^{(i)}}$$最大化似然的含义就是，在给定样本\\(X\\)的情况下，我们想找一个参数\\(\\mathbf{\\theta}\\)，使得观测到类标号\\(\\mathbf{y}\\)的概率最大。\n最大化似然等价于最大化log似然，log展开之后就是：\n$$l(\\mathbf{\\theta})=logL(\\mathbf{\\theta})=\\sum\\limits_{i}y^{(i)}logh_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})+(1-y^{(i)})log(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))$$而很巧的是，最大化log似然，其实等效于最小化log对数损失。对于单个样本，损失为：\n$$cost(h_{\\theta}(\\mathbf{x}),y) = \\begin{cases}-log(h_{\\theta}(\\mathbf{x})) \u0026 \\text {if y=1} \\\\ -log(1-h_{\\theta}(\\mathbf{x})) \u0026 \\text{if y=0} \\end{cases}$$即如果正确类标号是1，但算出来的\\(h_{\\theta}(\\mathbf{x})\\)很接近0的话，则损失\\(-log(h_{\\theta}(\\mathbf{x}))\\)就会很大。类标号为0的情况类似。把这两种情况合成一个式子就是：\n$$cost(h_{\\theta}(\\mathbf{x}),y) = -ylog(h_{\\theta}(\\mathbf{x})) – (1-y)log(1-h_{\\theta}(\\mathbf{x}))$$所有样本的损失之和就是：\n$$J(\\mathbf{\\theta})=cost(h_{\\theta}(X),\\mathbf{y}) = \\sum\\limits_{i}-y^{(i)}logh_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-(1-y^{(i)})log(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))$$所以最大化对数似然\\(\\max l(\\mathbf{\\theta})\\)和最小化对数损失\\(\\min J(\\mathbf{\\theta})\\)是等效的！最优化求解的方法用梯度下降和牛顿法都可以。\n和Sigmoid很像的函数还有很多，比如y=arctan(x)也可以，不过Sigmoid有一个很好的特点，即它的导数可以由自身算出来，\\(f'(x)=f(x)(1-f(x))\\)。\n传统的逻辑回归只能处理二分类问题，那么怎样将其扩展到解决多分类问题呢？有两种方法，一种方法是建立k个二元分类器，比如类标号有A,B,C，则建立3个二元分类器，分别是1）A和非A；2）B和非B；3）C和非C。训练每个2元分类器时，都对所有的数据重新标号为0或1，这样共需要训练k个二元分类器。\n还有一种方法是直接将二元逻辑回归推广到多元回归，即Softmax回归，有关Softmax回归的内容，请参考此博客，非常详细。简单来说，二元逻辑回归的假设函数是：多元Softmax回归的假设函数在形式上有所不同：其中是模型的参数。请注意这一项对概率分布进行归一化，使得所有概率之和为1。\nSoftmax回归的损失函数如下，其实就是logistic回归损失函数的推广：\n二元逻辑回归是多元Softmax回归在k=2时的特例，令k=2并利用Softmax回归参数冗余的特点，可以得到一般形式的二元逻辑回归假设函数，具体可以看原博客。\n面对一个多元分类问题，是选择Softmax回归还是k个二元分类器呢，这取决于你的类别之间是否互斥，如果互斥，可以用Softmax回归，否则，请用k个二元分类器。\n这就是逻辑回归的理论知识，下一篇博客将介绍逻辑回归在Python中的应用。\n参考：\nhttp://www.cnblogs.com/sparkwen/p/3441197.html https://tech.meituan.com/intro_to_logistic_regression.html http://blog.csdn.net/bitcarmanlee/article/details/51165444 http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92 ","permalink":"http://localhost:1313/posts/2017-11-26-introduction-to-logistic-regression/","summary":"\u003cp\u003e最近实验室在组织学习\u003ca href=\"http://open.163.com/special/opencourse/machinelearning.html\"\u003eNG的机器学习视频\u003c/a\u003e，我也跟进了一下。讲到逻辑回归那一课，一直想不明白，逻辑回归到底是怎样分类的？逻辑回归的分类面在哪里？逻辑回归有像SVM的max margin那样清晰的推导过程吗？为什么需要Sigmoid函数？今天就让我们来一窥逻辑回归的始末。\u003c/p\u003e\n\u003cp\u003e假设有一堆学生样本，为了简单起见，只有两个特征，分别是两门课的分数score1和score2，类标号y表示这个学生是否能被录取。可视化如下图，黑点表示y=1即被录取，红点表示y=0即未被录取，可以看到黑点处在score1和score2都比较高的区域。我们的任务就是给定这些训练样本，需要确定一个分类面来划分这两类数据。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-11-26-introduction-to-logistic-regression/lr_samples.png\"\u003e\u003c/p\u003e\n\u003cp\u003e分类面的实质就是\\(y=\\mathbf{w^T x}+b\\)，其中\\(\\mathbf{w}\\)和\\(\\mathbf{x}\\)都是向量，对应到本例中，展开为\\(y=w_1x_1+w_2x_2+b\\)。所以，寻找分类面的过程就是寻找倾斜度\\(\\mathbf{w}\\)和截距\\(b\\)。\u003c/p\u003e\n\u003cp\u003e因为分类的结果是离散的，只有0或者1，可以用感知机来分类。即\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-11-26-introduction-to-logistic-regression/perceptron.svg\"\u003e\u003c/p\u003e\n\u003cp\u003e但是怎样找这里的\\(\\mathbf{w}\\)和\\(b\\)使得分类结果最好呢，我们需要定义一个优化的目标函数或者说损失函数。这里只能定义为分类错误的个数，只能一点点挪动超平面来找分类错误最少的超平面了，即只能用暴力枚举的方法来找\\(\\mathbf{w}\\)和\\(b\\)。\u003c/p\u003e\n\u003cp\u003e另一种方法是令我们的分类面算出来的值和真实标号越接近越好，即最小化误差\\((\\mathbf{w^T}x^{(i)}+b-y^{(i)})^2\\)，然后通过梯度下降求解\\(\\mathbf{w}\\)和\\(b\\)。\u003c/p\u003e\n\u003cp\u003e但是这会有一个问题，对于上图数据，可以求到一个比较好的分类面，如下左图。但是如果数据中出现了如下右图右边的一个离群点或者噪声，为了最小化这个点的误差，即\\((\\mathbf{w^T}x^{(i)}+b-1)^2\\)，导致分类面往右偏了，这一偏直接导致很多y=1的样本分类错误。所以这种最小化误差的方法对离群点很敏感。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-11-26-introduction-to-logistic-regression/lr_face_1.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-11-26-introduction-to-logistic-regression/lr_face_2.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e假设分类超平面还是\\(f(\\mathbf{x})=\\mathbf{w^T x}+b\\)，我们希望的分类效果是这样的：\\(f(\\mathbf{x})=0\\)是分类面；\\(f(\\mathbf{x})\u003e0\\)分类为1，且不管\\(f(\\mathbf{x})\\)多大，都分为1；\\(f(\\mathbf{x})\u003c0\\)分类为0，且不管\\(f(\\mathbf{x})\\)多小，都分为0。\u003c/p\u003e\n\u003cp\u003e因为类标号是离散的{0,1}，所以想到把\\(f(\\mathbf{x})\\)映射到[0,1]之间，即\\(g(f(\\mathbf{x}))\\)。为了满足上述条件，\\(g(f(\\mathbf{x}))\\)需要满足：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\\(g(0)=0.5\\)，即在分类面上无法判断类标号是0还是1\u003c/li\u003e\n\u003cli\u003e当\\(f(\\mathbf{x})\u003e0\\)时，\\(g(f(\\mathbf{x}))\u003e0.5\\)\u003c/li\u003e\n\u003cli\u003e当\\(f(\\mathbf{x})\\rightarrow+\\infty\\)，\\(g(f(\\mathbf{x}))\\rightarrow 1\\)，且\\(g'(f(\\mathbf{x}))\\rightarrow 0\\)，即对于上右图右边的离群点，分类为1，且导数趋近于0，表示对其不敏感\u003c/li\u003e\n\u003cli\u003e当\\(f(\\mathbf{x})\u003c0\\)时，\\(g(f(\\mathbf{x}))\u003c0.5\\)\u003c/li\u003e\n\u003cli\u003e当\\(f(\\mathbf{x})\\rightarrow-\\infty\\)，\\(g(f(\\mathbf{x}))\\rightarrow 0\\)，且\\(g'(f(\\mathbf{x}))\\rightarrow 0\\)，和第3点类似\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e满足上述性质的函数之一就是Sigmoid函数，其定义域为\\([-\\infty,+\\infty]\\)，值域为[0,1]，正好把原始的函数结果\\(f(\\mathbf{x})\\)映射到了[0,1]，而概率的取值范围正好是[0,1]，所以Sigmoid函数正好可以作为分类为1的概率。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\"\u003e\u003c/p\u003e\n\u003cp\u003e所以逻辑回归最终的形式就是：\u003c/p\u003e\n$$g(\\mathbf{x})=\\frac{1}{1+e^{-(\\mathbf{w^T x}+b)}}$$\u003cp\u003e分类面依然还是\\(f(\\mathbf{x})=\\mathbf{w^T x}+b=0\\)，因为\\(f(\\mathbf{x})=0\\)时，\\(g(\\mathbf{x})=0.5\\)，正好满足上述条件1。\u003c/p\u003e\n\u003cp\u003eSigmoid函数的另一个优点是，它把原始函数值映射到了概率空间，这样就可以用极大似然的方法求解参数\\(\\mathbf{w}\\)和\\(b\\)。下面用参数\\(\\mathbf{\\theta}\\)代表参数\\(\\mathbf{w}\\)和\\(b\\)，用\\(h_{\\mathbf{\\theta}}(\\mathbf{x})\\)代表\\(g(f(\\mathbf{x}))\\)。则有：\u003c/p\u003e\n$$P(y=1|\\mathbf{x};\\mathbf{\\theta})=h_{\\mathbf{\\theta}}(\\mathbf{x})$$$$P(y=0|\\mathbf{x};\\mathbf{\\theta})=1-h_{\\mathbf{\\theta}}(\\mathbf{x})$$\u003cp\u003e合并成一个式子就是：\u003c/p\u003e\n$$P(y|\\mathbf{x};\\mathbf{\\theta})=h_{\\mathbf{\\theta}}(\\mathbf{x})^y(1-h_{\\mathbf{\\theta}}(\\mathbf{x}))^{1-y}$$\u003cp\u003e由于所有样本独立同分布（I.I.D.），似然函数就是\u003c/p\u003e\n$$L(\\mathbf{\\theta})=P(\\mathbf{y}|X;\\mathbf{\\theta})=\\prod\\limits_{i}P(y^{(i)}|\\mathbf{x}^{(i)};\\mathbf{\\theta})=\\prod\\limits_{i}h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})^{y^{(i)}}(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))^{1-y^{(i)}}$$\u003cp\u003e最大化似然的含义就是，在给定样本\\(X\\)的情况下，我们想找一个参数\\(\\mathbf{\\theta}\\)，使得观测到类标号\\(\\mathbf{y}\\)的概率最大。\u003c/p\u003e\n\u003cp\u003e最大化似然等价于最大化log似然，log展开之后就是：\u003c/p\u003e\n$$l(\\mathbf{\\theta})=logL(\\mathbf{\\theta})=\\sum\\limits_{i}y^{(i)}logh_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})+(1-y^{(i)})log(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))$$\u003cp\u003e而很巧的是，最大化log似然，其实等效于最小化log对数损失。对于单个样本，损失为：\u003c/p\u003e\n$$cost(h_{\\theta}(\\mathbf{x}),y) = \\begin{cases}-log(h_{\\theta}(\\mathbf{x})) \u0026 \\text {if y=1} \\\\ -log(1-h_{\\theta}(\\mathbf{x})) \u0026 \\text{if y=0} \\end{cases}$$\u003cp\u003e即如果正确类标号是1，但算出来的\\(h_{\\theta}(\\mathbf{x})\\)很接近0的话，则损失\\(-log(h_{\\theta}(\\mathbf{x}))\\)就会很大。类标号为0的情况类似。把这两种情况合成一个式子就是：\u003c/p\u003e\n$$cost(h_{\\theta}(\\mathbf{x}),y) = -ylog(h_{\\theta}(\\mathbf{x})) – (1-y)log(1-h_{\\theta}(\\mathbf{x}))$$\u003cp\u003e所有样本的损失之和就是：\u003c/p\u003e\n$$J(\\mathbf{\\theta})=cost(h_{\\theta}(X),\\mathbf{y}) = \\sum\\limits_{i}-y^{(i)}logh_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-(1-y^{(i)})log(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))$$\u003cp\u003e所以最大化对数似然\\(\\max l(\\mathbf{\\theta})\\)和最小化对数损失\\(\\min J(\\mathbf{\\theta})\\)是等效的！最优化求解的方法用梯度下降和牛顿法都可以。\u003c/p\u003e\n\u003cp\u003e和Sigmoid很像的函数还有很多，比如y=arctan(x)也可以，不过Sigmoid有一个很好的特点，即它的导数可以由自身算出来，\\(f'(x)=f(x)(1-f(x))\\)。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e传统的逻辑回归只能处理二分类问题，那么怎样将其扩展到解决多分类问题呢？有两种方法，一种方法是建立k个二元分类器，比如类标号有A,B,C，则建立3个二元分类器，分别是1）A和非A；2）B和非B；3）C和非C。训练每个2元分类器时，都对所有的数据重新标号为0或1，这样共需要训练k个二元分类器。\u003c/p\u003e\n\u003cp\u003e还有一种方法是直接将二元逻辑回归推广到多元回归，即Softmax回归，\u003ca href=\"http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92\"\u003e有关Softmax回归的内容，请参考此博客，非常详细\u003c/a\u003e。简单来说，二元逻辑回归的假设函数是：多元Softmax回归的假设函数在形式上有所不同：其中是模型的参数。请注意这一项对概率分布进行归一化，使得所有概率之和为1。\u003c/p\u003e\n\u003cp\u003eSoftmax回归的损失函数如下，其实就是logistic回归损失函数的推广：\u003c/p\u003e\n\u003cp\u003e二元逻辑回归是多元Softmax回归在k=2时的特例，令k=2并利用Softmax回归参数冗余的特点，可以得到一般形式的二元逻辑回归假设函数，具体可以看原博客。\u003c/p\u003e\n\u003cp\u003e面对一个多元分类问题，是选择Softmax回归还是k个二元分类器呢，这取决于你的类别之间是否互斥，如果互斥，可以用Softmax回归，否则，请用k个二元分类器。\u003c/p\u003e\n\u003cp\u003e这就是逻辑回归的理论知识，下一篇博客将介绍逻辑回归在Python中的应用。\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://www.cnblogs.com/sparkwen/p/3441197.html\"\u003ehttp://www.cnblogs.com/sparkwen/p/3441197.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://tech.meituan.com/intro_to_logistic_regression.html\"\u003ehttps://tech.meituan.com/intro_to_logistic_regression.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://blog.csdn.net/bitcarmanlee/article/details/51165444\"\u003ehttp://blog.csdn.net/bitcarmanlee/article/details/51165444\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92\"\u003ehttp://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"初探逻辑回归"},{"content":"今年国庆假期连着中秋，加起来有8天时间，考虑到上半年忙着找工作，太累了，打算利用这个假期好好放松一下。\n一、行程规划 9月份的某个周末，规划好行程，制定了人生第一份独自出游的计划。整个过程只花了一天的时间，包括景点规划、住宿、车票购买。其实9月初就在脑海中规划这个事情，当时想的还是2个人同行，但是中间遇到了一些事情，最终只能一个人出游。一个人的话，不用考虑太多，为了节省路上的时间，全程购买高铁票，唯独从杭州回北京的高铁票没有了，幸运的是利用分流抢票软件抢到了一张硬座。住宿方面，一个人住青旅是再好不过了，既便宜、又可以遇到有意思的驴友。国内正规青旅可以在YHAChina上预定，我就是在这个网站上预定了杭州荷方青年旅社。没有在YHA上注册的青旅，可以通过booking.com预定，booking的优势是预定不收费，入住当天前取消也不收费，很不错，我就是在这个网站上预定了郑州的畅旅太空舱宾馆，不过到店之后老板建议我取消网站上的预定，进行线下支付，可以便宜几块钱。。。\n二、郑州半日游 3号到达郑州之后开始下雨，booking上的青旅地址有误，浪费了不少时间才找到宾馆并办理了入住。这个太空舱宾馆是在某个高层小区的6层，由居住房改造而成的，开始我还担心是黑店，要求查证件，毕竟第一次住青旅，还是有点担心。后来陆续来了一些驴友，有俄罗斯帅哥、韩国萌妹，又考察了一下房间和网上的照片是一样的，也就不太担心了。太空舱宾馆在医学院地铁站旁边，楼下有一个永辉超市，地理位置还是很不错的。\n办完入住之后已经4点多了，行程单上的郑州大学、河南博物馆肯定是没时间去了。于是去了农科路的一家巴奴火锅店吃火锅，一个人吃火锅心里真不是滋味。\n回青旅的路上，需要经过二七广场站，虽然有点晚了，但想着别错过郑州为数不多的景点，就跑出来看了看。二七广场周边都是一些商场，没啥意思，二七纪念塔也已经闭馆了。看着联体双塔造型的二七纪念塔，觉得好奇怪，为什么要建成联体的呢，网上查阅资料才得知，此塔为纪念二七大罢工（也称为京汉铁路工人大罢工）中牺牲的汪胜友、司文德两位烈士，我猜大多数郑州人也不知道吧。\n三、少林寺一日游 4号早上6:30起床，7点出门去郑州长途汽车中心站，准备坐大巴前往登封市。无奈天公不作美，还在下雨，汽车中心站在郑州火车站旁边，地铁只能到郑州火车站，前前后后还要步行。所以虽然青旅离中心站不远，但到中心站的时候已经8:15了。\n中心站大厅有很多自动售票机，而且可以在线支付，所以快速购买了8:40前往登封的汽车票，票价28元。进站的时候，安检居然不让带水进去，太可恶了，只能把水扔掉。上车之后，磨蹭到9点才发车，到达登封汽车站的时间已经是中午11点了。这个时候，有很多黑车拉客去少林寺景区，需要四五十块钱，在我犹豫的时候，直达少林寺景区的8路公交来了，只要5块钱，所以这里提醒大家，从登封汽车站去少林寺景区，等8路公交是最实惠的。\n前往少林寺景区的路上拥堵不堪。将近12点才到达景区门口。买了少林寺景区通票100元（学生票半价，后悔本科的时候没多出去玩玩）。\n进入景区之后，跟着人群，先后经过了演武厅、少林寺常住院、塔林。演武厅要2点才上演武术表演，所以略过。少林寺常住院里面就是各种殿，比如大雄宝殿、藏经阁。基本上每个殿里面都会有一个佛像，供游客参拜。说实话，这些殿堂，没有解说，看完也就看完了，并没有留下深刻的印象。\n3点钟的时候，做了个明智的选择，乘少林索道去看自然风光了（注意嵩杨索道是去看二祖庵的，还是人文景观）。下了索道，瞬间被眼前的景色惊艳住，由于下雨，山上烟雾缭绕，如临仙境；山涧瀑布飞流直下，哗哗作响；三皇栈道奇峰怪石，好不惊险。本来还想去感受下垂悬雾中的吊桥，由于时间关系只得作罢。\n回来的路上，雨过天晴，一扫连日来的阴霾，真是豁然开朗，心情极其舒畅！\n返程时没必要先坐8路公交到登封汽车站了，在景区出口右侧有从少林寺直达郑州火车站的大巴，票价30，很方便。\n四、杭州半日游 5号早上6点起床，赶8点去杭州的高铁。下午1点多到达杭州，根据导航前往荷方青年旅社。走出定安路地铁口，扑鼻而来的桂花香，甜到心里了，看着周围的白墙黛瓦、花格窗棂，我知道我来到江南了！\n入住的杭州荷方青年旅社在清河坊步行街的尽头，所以导航的时候，顺带逛了逛清河坊小吃街。沿街的酒幡和各种幌子瞬间有种回到南宋的感觉。旅舍由二十世纪五六十年代的传统江南风格的四合院民居改建而成，傍山而居。白墙黛瓦，花格窗棂，青石板，柚子树，清爽舒适。\n办理好入住之后，根据计划前往浙大紫金港校区。浙大今年120周年校庆，校门口站了两个志愿者，看起来一脸青涩，我问他们浙大的外语学院和计算机学院在哪里，他们说他们是大一新生，不知道这两个学院在哪…真不知道志愿者是干啥的。\n浙大紫金港校区里面有个人工湖，叫启真湖，还挺大的，有点类似于北大未名湖，湖里还养殖了黑天鹅，比武大的鉴湖有意思。除此之外，其他的建筑并没有太多的特点，也就是一个正常的学校了。走在校园里，也是满园的桂花飘香，此时好想回到武大。\n吃过晚饭之后，听说南山路的夜景不错，于是在南山路上溜达了一圈，权当饭后消食。\n五、西湖一日游 西湖很大，为了不错过每一个景点，我在网上查了很多攻略，其中知乎上一个用户的骑行线路深得我心，我是完完全全的根据他给的线路图一个景点一个景点遍历的。虽然清河坊离柳浪闻莺最近，但我还是硬生生先骑车到断桥残雪作为起点，固执可见一斑:-)\n另外，吸取了游少林寺看不懂人文景观的教训，这次我提前下载了一个口袋导游APP，该APP可以根据定位自动播放所在景点的介绍，很不错。\n漫步在西湖边，给我最大的感受就是舒适、悠闲。依然是熟悉的桂花香，随处可见的荷叶，清风徐来，杨柳依依。若是走累了，坐在湖边的石头上，观鱼逗鱼也别有一番乐趣。\n这次环游西湖很有意思的一件事是自己和自己玩定向越野。当我来到第一个景点断桥残雪时，我发现有一个御碑亭和对应的标志碑，上面都写着“断桥残雪”，只不过御碑亭上是康熙、乾隆等皇帝题写的，标志碑是中华人民共和国国务院立的。所以西湖十景的每个景点肯定都有对应的御碑亭和标志碑，我当时就决定，每到一个景点，请路人帮我拍一张和标志碑的合影，一来观景有了目标，二来强迫自己开口说话，和路人搭讪。\n由于我是严格按照上面的路线图游完的，没有经过双峰插云和三潭印月，所以只有8张合影。\n看着这些照片，我只想说路人和路人的拍照水平差别好大:-) 其实重点不是照片，而是寻找标志碑和找路人拍照的过程。比如雷锋夕照的标志碑并不在雷峰塔脚下，而是要走出雷峰塔，去到旁边的小山丘上才能找到。而柳浪闻莺的标志碑就更难找了，这个公园很大，标志碑的位置很隐蔽，我用百度地图导航来回走了3遍都没找到，最后问了景点的一个售货员才知道，这个标志碑在钱王祠前面，路的内测，需要绕一个很长的弯才能找到。最后当我找到柳浪闻莺的标志碑的时候，天已经黑了，由于位置比较偏僻，几乎没有游客，我等了十分钟，终于等来一个游客，帮我拍了一张合影，我猜80%的游客都未曾找到过这个标志碑。\n关于拍照。以前自己不敢也不想麻烦路人帮忙拍照，现在算是勇敢的做出尝试，其实路人是很愿意帮助别人的，当然我找的都是年轻人。有意思的是，在雷峰塔脚下，我看到一个和我一样独自游览雷峰塔的年轻人，想自拍又苦于无人帮助，于是我主动提出帮忙，正好也借机让他帮我拍照。互拍完之后我们就散了，没想到登上雷峰塔塔顶之后，我们又相遇了，一阵欣喜之后，又决定互拍。真是猿粪呀。\n环游西湖结束之后，准备找一个特色餐厅吃饭。阿溜说她之前在西湖边的外婆家吃过，挺不错的。外婆家的slogan是“我家就在西湖边”，那就去尝尝地道的外婆菜咯。取完号之后发现前面还有40+桌在等，火爆程度可见一斑。幸好带了kindle，就坐在湖边的长凳上看起了《月亮与六便士》。等到大概8点的时候，终于叫上号了。点了西湖有名的西湖醋鱼和东坡肉，还点了一杯稻花米乳。这大概是我点菜最失败的一次了，西湖醋鱼是草鱼，刺太多，而且有点腥；东坡肉一整块也没完全切开，拿下去重新切了之后，一块块七零八落，观感极差。至于稻花米乳，就是真真正正的稻米磨成的汁，无味。虽然菜不好吃，但没尝试过怎么知道呢，所以秉承“勇于尝试”的精神，我并没有感到太沮丧。\n回到清河坊特色街已经晚上10点了，为了挑选纪念品，挨个店进去考察。到最后发现能体现杭州西湖特色的也就是印有西湖十景的冰箱贴了，以后决定每去一个地方都买当地的冰箱贴，争取博士毕业前把工位上的墙贴满。\n六、西溪一日游 制定计划的时候，考虑到6号骑行环游西湖肯定会很累，所以决定7号去西溪，乘坐游船转一圈，正好歇歇脚。\n西溪湿地很大，分为东、中、西三个部分，中部是生活街区，免费开放的，东西部要收费。门票全价80，船票全价60，电瓶车5元。\n我在西溪天堂买好所有的票之后，乘坐电瓶车到天目山路周家村出入口，沿着游船线路，先后游玩了周家村→渔村烟雨→深潭口。\n周家村在办火柿节，没什么好玩的。渔村烟雨的花样比较多，主要有三个展厅，第一个展厅演示了西溪当地人养蚕织布的情景；第二个展厅西溪人家展示了西溪当地人家的家具、饮食等风格，诸如灶台、风车、打谷机等都和我老家的很像；第三个展厅表演西溪当地人婚嫁的情景，因为此处陆路不通，迎娶新娘只能乘船，挺有意思的。深潭口有一片养殖珍珠的水域，电影《非诚勿扰》曾在这里取景，除此之外，好像没有什么特别的地方。\n西溪游船如下左图所示，十几分钟就能从一个景点到达下一个景点，每经过一个景点，就会在船票上戳一个孔表示你不能再在这个点上船了。下次有机会要试一下右图的摇橹船，哈哈。\n西溪湿地公园给我的感觉和西湖是完全不一样的，西湖是美景+人文情怀，是阳春白雪似的美，声名远扬，游人如织；而西溪湿地更多的是江南水乡的体现，是下里巴人的美，更加贴近普通老百姓的生活，游人可能只有西湖的20%？\n转完一圈之后，在高庄出口骑小黄车回到西溪天堂，乘坐公交赶往杭州火车站。至此，结束了整个假期的旅行。\n七、总结 此次旅行，是我第一次独自规划、独自出行，前后游玩了郑州、杭州两个城市，规划的景点基本都去了，预期的目标也基本达到了，完成度80%。自己还算比较满意。\n总的来说，郑州和杭州这两座城市的差别还是很明显的。河南很好的把不同中心划分在了不同的城市，郑州作为行政中心和教育中心，而洛阳、开封、登封等城市则作为旅游文化中心，所以郑州其实没什么好玩的。而杭州是一个非常鲜明的旅游城市，西湖、西溪、西泠，三西旅游胜地，市民和游客的素质也比较高。\n另外有一点让我印象深刻的是，无论是郑州还是杭州，地铁和公交的购票系统都非常先进，全部可以互联网购票并支付。郑州的地铁可以用微信购票，然后到取票机上换取纸质票，这极大的方便了那些忘带或不愿带零钱的朋友。另外，郑州的安检是我遇到过的最严格的安检，无论是汽车站、火车站还是地铁站，进站安检时恨不得把你全身上下都摸一遍。说到购票的便利程度，杭州更胜一筹，杭州全城只要涉及交易的地方，都可以用支付宝，雷峰塔景区甚至有两个支付宝付款专用窗口，另外连公交车上都专门配备了支付宝扫码机，用户只需要在支付宝→城市生活上添加一张公交卡，就可以在线购票刷卡了，超级方便。我严重怀疑，这肯定是因为马云爸爸在杭州:-)至于交通的拥堵程度，虽然是旅游旺季，但是郑州和杭州的地铁、公交拥堵程度远低于平日的北京，这点作为帝都的北京还是扳回一局。\n旅途中稍微有一点让我遗憾的是，青旅并没有我预想的那么有趣。我原本以为青旅里住着一群来自天南海北的驴友，一到晚上，大家就聚在一起侃大山，分享各自的故事。事实证明，住青旅的人很大一部分是穷学生或者上班族，他们从白天到晚上都在旅游，结束了一天的旅行之后，回到青旅洗个澡睡觉，第二天6点钟爬起来赶火车，所以大家其实是没有时间促膝长谈的。\n最后，费用方面，花费2500-3000左右，算是穷游了，主要费用花在了交通（40%）、食宿（40%）、门票（20%），这个费用对我来说可以接受。\n好啦，今年的“第三座城”旅行计划超额完成任务！\n","permalink":"http://localhost:1313/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/","summary":"\u003cp\u003e今年国庆假期连着中秋，加起来有8天时间，考虑到上半年忙着找工作，太累了，打算利用这个假期好好放松一下。\u003c/p\u003e\n\u003ch1 id=\"一行程规划\"\u003e一、行程规划\u003c/h1\u003e\n\u003cp\u003e9月份的某个周末，规划好行程，制定了人生第一份独自出游的计划。整个过程只花了一天的时间，包括景点规划、住宿、车票购买。其实9月初就在脑海中规划这个事情，当时想的还是2个人同行，但是中间遇到了一些事情，最终只能一个人出游。一个人的话，不用考虑太多，为了节省路上的时间，全程购买高铁票，唯独从杭州回北京的高铁票没有了，幸运的是利用\u003ca href=\"http://www.12306bypass.com/\"\u003e分流抢票软件\u003c/a\u003e抢到了一张硬座。住宿方面，一个人住青旅是再好不过了，既便宜、又可以遇到有意思的驴友。国内正规青旅可以在\u003ca href=\"http://www.yhachina.com/index.php?hostID=1\"\u003eYHAChina\u003c/a\u003e上预定，我就是在这个网站上预定了杭州荷方青年旅社。没有在YHA上注册的青旅，可以通过\u003ca href=\"https://www.booking.com/s/34_6/11148825\"\u003ebooking.com\u003c/a\u003e预定，booking的优势是预定不收费，入住当天前取消也不收费，很不错，我就是在这个网站上预定了郑州的畅旅太空舱宾馆，不过到店之后老板建议我取消网站上的预定，进行线下支付，可以便宜几块钱。。。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/plan_2017_10.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/trip_2017_10.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"二郑州半日游\"\u003e二、郑州半日游\u003c/h1\u003e\n\u003cp\u003e3号到达郑州之后开始下雨，booking上的青旅地址有误，浪费了不少时间才找到宾馆并办理了入住。这个太空舱宾馆是在某个高层小区的6层，由居住房改造而成的，开始我还担心是黑店，要求查证件，毕竟第一次住青旅，还是有点担心。后来陆续来了一些驴友，有俄罗斯帅哥、韩国萌妹，又考察了一下房间和网上的照片是一样的，也就不太担心了。太空舱宾馆在医学院地铁站旁边，楼下有一个永辉超市，地理位置还是很不错的。\u003c/p\u003e\n\u003cp\u003e办完入住之后已经4点多了，行程单上的郑州大学、河南博物馆肯定是没时间去了。于是去了农科路的一家巴奴火锅店吃火锅，一个人吃火锅心里真不是滋味。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/zhengzhou_2.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/zhengzhou_4.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e回青旅的路上，需要经过二七广场站，虽然有点晚了，但想着别错过郑州为数不多的景点，就跑出来看了看。二七广场周边都是一些商场，没啥意思，二七纪念塔也已经闭馆了。看着联体双塔造型的二七纪念塔，觉得好奇怪，为什么要建成联体的呢，网上查阅资料才得知，\u003ca href=\"http://blog.sina.com.cn/s/blog_63558ae80101aaxd.html\"\u003e此塔为纪念二七大罢工（也称为京汉铁路工人大罢工）中牺牲的汪胜友、司文德两位烈士\u003c/a\u003e，我猜大多数郑州人也不知道吧。\u003c/p\u003e\n\u003ch1 id=\"三少林寺一日游\"\u003e三、少林寺一日游\u003c/h1\u003e\n\u003cp\u003e4号早上6:30起床，7点出门去郑州长途汽车中心站，准备坐大巴前往登封市。无奈天公不作美，还在下雨，汽车中心站在郑州火车站旁边，地铁只能到郑州火车站，前前后后还要步行。所以虽然青旅离中心站不远，但到中心站的时候已经8:15了。\u003c/p\u003e\n\u003cp\u003e中心站大厅有很多自动售票机，而且可以在线支付，所以快速购买了8:40前往登封的汽车票，票价28元。进站的时候，安检居然不让带水进去，太可恶了，只能把水扔掉。上车之后，磨蹭到9点才发车，到达登封汽车站的时间已经是中午11点了。这个时候，有很多黑车拉客去少林寺景区，需要四五十块钱，在我犹豫的时候，直达少林寺景区的8路公交来了，只要5块钱，所以这里提醒大家，从登封汽车站去少林寺景区，等8路公交是最实惠的。\u003c/p\u003e\n\u003cp\u003e前往少林寺景区的路上拥堵不堪。将近12点才到达景区门口。买了少林寺景区通票100元（学生票半价，后悔本科的时候没多出去玩玩）。\u003c/p\u003e\n\u003cp\u003e进入景区之后，跟着人群，先后经过了演武厅、少林寺常住院、塔林。演武厅要2点才上演武术表演，所以略过。少林寺常住院里面就是各种殿，比如大雄宝殿、藏经阁。基本上每个殿里面都会有一个佛像，供游客参拜。说实话，这些殿堂，没有解说，看完也就看完了，并没有留下深刻的印象。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_1.png\"\u003e\n\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e3点钟的时候，做了个明智的选择，乘少林索道去看自然风光了（注意嵩杨索道是去看二祖庵的，还是人文景观）。下了索道，瞬间被眼前的景色惊艳住，由于下雨，山上烟雾缭绕，如临仙境；山涧瀑布飞流直下，哗哗作响；三皇栈道奇峰怪石，好不惊险。本来还想去感受下垂悬雾中的吊桥，由于时间关系只得作罢。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_3.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_4.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_5.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_6.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e回来的路上，雨过天晴，一扫连日来的阴霾，真是豁然开朗，心情极其舒畅！\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_7.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_8.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e返程时没必要先坐8路公交到登封汽车站了，在景区出口右侧有从少林寺直达郑州火车站的大巴，票价30，很方便。\u003c/p\u003e\n\u003ch1 id=\"四杭州半日游\"\u003e四、杭州半日游\u003c/h1\u003e\n\u003cp\u003e5号早上6点起床，赶8点去杭州的高铁。下午1点多到达杭州，根据导航前往荷方青年旅社。走出定安路地铁口，扑鼻而来的桂花香，甜到心里了，看着周围的白墙黛瓦、花格窗棂，我知道我来到江南了！\u003c/p\u003e\n\u003cp\u003e入住的杭州荷方青年旅社在清河坊步行街的尽头，所以导航的时候，顺带逛了逛清河坊小吃街。沿街的酒幡和各种幌子瞬间有种回到南宋的感觉。旅舍由二十世纪五六十年代的传统江南风格的四合院民居改建而成，傍山而居。白墙黛瓦，花格窗棂，青石板，柚子树，清爽舒适。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/hangzhou_1.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/hangzhou_2.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e办理好入住之后，根据计划前往浙大紫金港校区。浙大今年120周年校庆，校门口站了两个志愿者，看起来一脸青涩，我问他们浙大的外语学院和计算机学院在哪里，他们说他们是大一新生，不知道这两个学院在哪…真不知道志愿者是干啥的。\u003c/p\u003e\n\u003cp\u003e浙大紫金港校区里面有个人工湖，叫启真湖，还挺大的，有点类似于北大未名湖，湖里还养殖了黑天鹅，比武大的鉴湖有意思。除此之外，其他的建筑并没有太多的特点，也就是一个正常的学校了。走在校园里，也是满园的桂花飘香，此时好想回到武大。\u003c/p\u003e\n\u003cp\u003e吃过晚饭之后，听说南山路的夜景不错，于是在南山路上溜达了一圈，权当饭后消食。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/hangzhou_3.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/hangzhou_4.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"五西湖一日游\"\u003e五、西湖一日游\u003c/h1\u003e\n\u003cp\u003e西湖很大，为了不错过每一个景点，我在网上查了很多攻略，其中\u003ca href=\"https://www.zhihu.com/question/19834881/answer/211720257\"\u003e知乎上一个用户的骑行线路\u003c/a\u003e深得我心，我是完完全全的根据他给的线路图一个景点一个景点遍历的。虽然清河坊离柳浪闻莺最近，但我还是硬生生先骑车到断桥残雪作为起点，固执可见一斑:-)\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e另外，吸取了游少林寺看不懂人文景观的教训，这次我提前下载了一个口袋导游APP，该APP可以根据定位自动播放所在景点的介绍，很不错。\u003c/p\u003e\n\u003cp\u003e漫步在西湖边，给我最大的感受就是舒适、悠闲。依然是熟悉的桂花香，随处可见的荷叶，清风徐来，杨柳依依。若是走累了，坐在湖边的石头上，观鱼逗鱼也别有一番乐趣。\u003c/p\u003e\n\u003cp\u003e这次环游西湖很有意思的一件事是自己和自己玩定向越野。当我来到第一个景点断桥残雪时，我发现有一个御碑亭和对应的标志碑，上面都写着“断桥残雪”，只不过御碑亭上是康熙、乾隆等皇帝题写的，标志碑是中华人民共和国国务院立的。所以西湖十景的每个景点肯定都有对应的御碑亭和标志碑，我当时就决定，每到一个景点，请路人帮我拍一张和标志碑的合影，一来观景有了目标，二来强迫自己开口说话，和路人搭讪。\u003c/p\u003e\n\u003cp\u003e由于我是严格按照上面的路线图游完的，没有经过双峰插云和三潭印月，所以只有8张合影。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_2.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_3.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_5.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_6.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_8.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_9.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_7.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_4.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e看着这些照片，我只想说路人和路人的拍照水平差别好大:-) 其实重点不是照片，而是寻找标志碑和找路人拍照的过程。比如雷锋夕照的标志碑并不在雷峰塔脚下，而是要走出雷峰塔，去到旁边的小山丘上才能找到。而柳浪闻莺的标志碑就更难找了，这个公园很大，标志碑的位置很隐蔽，我用百度地图导航来回走了3遍都没找到，最后问了景点的一个售货员才知道，这个标志碑在钱王祠前面，路的内测，需要绕一个很长的弯才能找到。最后当我找到柳浪闻莺的标志碑的时候，天已经黑了，由于位置比较偏僻，几乎没有游客，我等了十分钟，终于等来一个游客，帮我拍了一张合影，我猜80%的游客都未曾找到过这个标志碑。\u003c/p\u003e\n\u003cp\u003e关于拍照。以前自己不敢也不想麻烦路人帮忙拍照，现在算是勇敢的做出尝试，其实路人是很愿意帮助别人的，当然我找的都是年轻人。有意思的是，在雷峰塔脚下，我看到一个和我一样独自游览雷峰塔的年轻人，想自拍又苦于无人帮助，于是我主动提出帮忙，正好也借机让他帮我拍照。互拍完之后我们就散了，没想到登上雷峰塔塔顶之后，我们又相遇了，一阵欣喜之后，又决定互拍。真是猿粪呀。\u003c/p\u003e\n\u003cp\u003e环游西湖结束之后，准备找一个特色餐厅吃饭。阿溜说她之前在西湖边的外婆家吃过，挺不错的。外婆家的slogan是“我家就在西湖边”，那就去尝尝地道的外婆菜咯。取完号之后发现前面还有40+桌在等，火爆程度可见一斑。幸好带了kindle，就坐在湖边的长凳上看起了《月亮与六便士》。等到大概8点的时候，终于叫上号了。点了西湖有名的西湖醋鱼和东坡肉，还点了一杯稻花米乳。这大概是我点菜最失败的一次了，西湖醋鱼是草鱼，刺太多，而且有点腥；东坡肉一整块也没完全切开，拿下去重新切了之后，一块块七零八落，观感极差。至于稻花米乳，就是真真正正的稻米磨成的汁，无味。虽然菜不好吃，但没尝试过怎么知道呢，所以秉承“勇于尝试”的精神，我并没有感到太沮丧。\u003c/p\u003e\n\u003cp\u003e回到清河坊特色街已经晚上10点了，为了挑选纪念品，挨个店进去考察。到最后发现能体现杭州西湖特色的也就是印有西湖十景的冰箱贴了，以后决定每去一个地方都买当地的冰箱贴，争取博士毕业前把工位上的墙贴满。\u003c/p\u003e\n\u003ch1 id=\"六西溪一日游\"\u003e六、西溪一日游\u003c/h1\u003e\n\u003cp\u003e制定计划的时候，考虑到6号骑行环游西湖肯定会很累，所以决定7号去西溪，乘坐游船转一圈，正好歇歇脚。\u003c/p\u003e\n\u003cp\u003e西溪湿地很大，分为东、中、西三个部分，中部是生活街区，免费开放的，东西部要收费。门票全价80，船票全价60，电瓶车5元。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e我在西溪天堂买好所有的票之后，乘坐电瓶车到天目山路周家村出入口，沿着游船线路，先后游玩了周家村→渔村烟雨→深潭口。\u003c/p\u003e\n\u003cp\u003e周家村在办火柿节，没什么好玩的。渔村烟雨的花样比较多，主要有三个展厅，第一个展厅演示了西溪当地人养蚕织布的情景；第二个展厅西溪人家展示了西溪当地人家的家具、饮食等风格，诸如灶台、风车、打谷机等都和我老家的很像；第三个展厅表演西溪当地人婚嫁的情景，因为此处陆路不通，迎娶新娘只能乘船，挺有意思的。深潭口有一片养殖珍珠的水域，电影《非诚勿扰》曾在这里取景，除此之外，好像没有什么特别的地方。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_2.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_3.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_4.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_6.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_5.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_7.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e西溪游船如下左图所示，十几分钟就能从一个景点到达下一个景点，每经过一个景点，就会在船票上戳一个孔表示你不能再在这个点上船了。下次有机会要试一下右图的摇橹船，哈哈。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_8.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_9.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e西溪湿地公园给我的感觉和西湖是完全不一样的，西湖是美景+人文情怀，是阳春白雪似的美，声名远扬，游人如织；而西溪湿地更多的是江南水乡的体现，是下里巴人的美，更加贴近普通老百姓的生活，游人可能只有西湖的20%？\u003c/p\u003e","title":"2017年国庆旅行——郑州、杭州"},{"content":"上一篇博客简单介绍了Linux平台下不同性能分析工具的特点，最后是Google出品的gperftools胜出。今天我们将举一个用gperftools对链接库进行性能分析的例子。\n假设我们创建了一个TestGperftools项目，其中包括一个动态链接库模块complex和主程序main，build文件夹用于存放动态链接库和可执行程序，文件结构如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 czl@ubuntu:~/czl/TestGperftools$ tree . ├── build ├── complex │ ├── complex.cpp │ ├── complex.h │ └── Makefile ├── main │ ├── main.cpp │ └── Makefile └── Makefile 3 directories, 6 files 其中complex借用了这篇博客的第三个公式计算圆周率π，主要用来模拟一个很耗时的操作。该模块会生成一个libcomplex.so动态链接库供外部调用。其complex.cpp函数如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #include \u0026#34;complex.h\u0026#34; Complex::Complex() {} void Complex::recordPI(double x) { pis.push_back(x); if (pis.size() \u0026gt; 10000) pis.clear(); } double Complex::calculatePI(int n) { double x = 2, z = 2; int a = 1, b = 3; while (n--) { z = z * a / b; x += z; a++; b += 2; recordPI(x); } return x; } Complex::~Complex() {} calculatePI为计算π的函数，传入参数n表示循环计算的次数，也就是第三个公式中累加的次数，当然次数越多，精度越高，时间越长。recordPI是刻意加上去的一个函数，作为calculatePI的被调函数，同时包含STL的一些操作。该模块的Makefile如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SRC = complex.cpp OBJS = $(SRC:.cpp=.o) BUILD_DIR = ../build TARGET = libcomplex.so all: $(TARGET) clean: -rm -f $(OBJS) $(TARGET) $(OBJS): $(SRC) $(CXX) -fPIC -c $(SRC) $(TARGET): $(OBJS) $(CXX) -shared -o $(TARGET) $(OBJS) cp $(TARGET) $(BUILD_DIR) 注意生成动态链接库的Makefile格式，编译时需要-fPIC生成位置无关目标文件，链接时需要-shared选项。由Makefile文件可知，该模块会生成libcomplex.so动态链接库。\n下面我们来看一下main.cpp文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #include\u0026#34;../complex/complex.h\u0026#34; #include\u0026lt;iostream\u0026gt; #include\u0026lt;iomanip\u0026gt; using namespace std; vector \u0026lt; long long \u0026gt; vll; void recordSum(long long s) { vll.push_back(s); if (vll.size() \u0026gt; 10000) vll.clear(); } long long calculateSum(long long n) { long long s = 0; for (int i = 1; i \u0026lt;= n; ++i) { s += i; recordSum(s); } return s; } int main() { long long n = 1000000000; Complex c; double p = c.calculatePI(n); long long s = calculateSum(n); cout \u0026lt;\u0026lt; \u0026#34;pi=\u0026#34; \u0026lt;\u0026lt; fixed \u0026lt;\u0026lt; setprecision(13) \u0026lt;\u0026lt; p \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;1+2+…+\u0026#34; \u0026lt;\u0026lt; n \u0026lt;\u0026lt; \u0026#34;=\u0026#34; \u0026lt;\u0026lt; s \u0026lt;\u0026lt; endl; return 0; } main函数除了调用Complex类计算π之外，还会调用自身的calculateSum计算1+2+…+n的和，并且也会调用一个刻意的recordSum函数。该模块的Makefile如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SRC = main.cpp OBJS = $(SRC:.cpp=.o) BUILD_DIR = ../build LIBS = -lcomplex TARGET = main all: $(TARGET) clean: -rm -f $(OBJS) $(TARGET) $(OBJS): $(SRC) $(CXX) -c $(SRC) $(TARGET): $(OBJS) $(CXX) -L$(BUILD_DIR) -o $(TARGET) $(OBJS) $(LIBS) cp $(TARGET) $(BUILD_DIR) 可以看到该模块链接了-lcomplex动态链接库，注意第16行链接的顺序不能乱，-L要在-o前面，$(OBJS)要在$(LIBS)的前面。\n编译的方法为：直接在TestGperftools目录中执行make，主make会分别进入complex和main文件夹执行各自的子make，最后动态链接库libcomplex.so和可执行程序main都复制到了build文件夹中。具体代码请看根目录中的Makefile文件，这里不再列出。\n接下来我们要安装gperftools性能分析工具，在Ubuntu下安装非常简单，直接执行以下命令，该命令安装了两个软件，google-perftools和graphviz，前者就是gperftools，后者是用于可视化性能分析结果的。\n1 sudo apt-get install google-perftools graphviz 下面我们开始对main进行性能分析，我们预期libcomplex.so中的calculatePI会是主要的性能瓶颈，看看gperftools给出的结果是否符合预期。\n进入到build文件夹，执行以下命令： 1 $ export LD_LIBRARY_PATH=. 该命令表示查找动态链接库的首选位置为当前路径，如果没有设置，在执行main程序时，会找不到libcomplex.so而报错：./main: error while loading shared libraries: libcomplex.so: cannot open shared object file: No such file or directory\n执行以下命令： 1 $ LD_PRELOAD=/usr/lib/libprofiler.so.0 CPUPROFILE=./main.prof ./main LD_PRELOAD用于预加载gperftools的动态链接库，有些发行版可能是/usr/lib/libprofiler.so，我的是后面有.0，也有可能是其他数字；CPUPPROFILE用于指定生成的性能分析结果文件路径，我设置的是在当前文件夹下生成main.prof；最后接上需要分析的可执行程序./main，如果你的可执行程序由命令行参数，也可以一并跟在./main后面。\n程序执行结束之后，输出如下： 1 2 3 pi=3.1415926535898 1+2+…+1000000000=500000000500000000 PROFILE: interrupts/evictions/bytes = 4680/1585/112344 前两行是程序输出，最后一行是gperftools输出。\n最后进行性能分析，执行以下命令： 1 $ google-pprof –web ./main ./main.prof –web表示性能分析结果以网页的形式展示，后面依次跟可执行程序和上一步生成的*.prof文件。程序自动打开浏览器，性能分析结果如下图所示。\n详情页中，左上角给出了汇总结果，总共采样了4680次，扔掉了采样次数小于23和4次的节点和边。gperftools 性能分析图中，每个节点代表一个函数，节点内包含三个信息，从上往下依次为：函数名、该函数自身被采样到的次数（及占总采样数的比例）、该函数和被调函数被采样到的次数（及占总采样数的比例），节点的大小正比于该函数自身被采样到的次数；每条边代表调用者到被调用者被采样到的次数。简而言之，图中节点越大，该函数自身耗时占比越大，越可能是性能瓶颈。\n由图可知，Complex::calculatePI确实是性能瓶颈，占用了23.0%的时间，而calculateSum只占用了6.3%的时间。另外，我们刻意增加的recordPI和recordSum也占用了不少的时间，因为需要不断的push_back以及重新分配内存，所以占用的时间也很多。\n总的来说，这样的分析结果是符合预期的，我们能够根据这样一个热点分布图，找到性能瓶颈，然后有针对的进行性能优化。比如针对上面的例子，我们可以优化calculatePI，如果对π的精度要求不是很高，可以减少while循环的次数；对于vector的push_back，因为我们是大于10000时会clear一下，为了避免多次的clear和reallocate，可以预先设置vector的大小为10000，然后重复对0~9999下标进行填充，这样就只需一次分配内存了。 本博客完整的TestGperftools项目可以查看我的Github：TestGperftools。\ngperftools套装中还包括a memory leak detgector和a heap profiler，用于检查程序中是否有内存泄漏的风险以及对内存堆栈的分析，具体介绍可以查看其Github上的WIKI。\n一些有用的链接：\n3步学会使用gperftools的超简易教程：https://wiki.geany.org/howtos/profiling/gperftools gperftools官方文档：https://gperftools.github.io/gperftools/cpuprofile.html ","permalink":"http://localhost:1313/posts/2017-02-07-gperftools-tutorial/","summary":"\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2017-02-07-introduction-to-performance-analysis-tools-in-linux/\"\u003e上一篇博客简单介绍了Linux平台下不同性能分析工具的特点\u003c/a\u003e，最后是Google出品的gperftools胜出。今天我们将举一个用gperftools对链接库进行性能分析的例子。\u003c/p\u003e\n\u003cp\u003e假设我们创建了一个TestGperftools项目，其中包括一个动态链接库模块complex和主程序main，build文件夹用于存放动态链接库和可执行程序，文件结构如下所示。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eczl@ubuntu:~/czl/TestGperftools$ tree\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e.\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e├── build\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e├── complex\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e│   ├── complex.cpp\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e│   ├── complex.h\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e│   └── Makefile\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e├── main\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e│   ├── main.cpp\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e│   └── Makefile\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e└── Makefile\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e directories, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e files\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e其中complex借用了\u003ca href=\"http://www.cppfans.com/articles/basecalc/c_pi_10000.asp\"\u003e这篇博客\u003c/a\u003e的第三个公式计算圆周率π，主要用来模拟一个很耗时的操作。该模块会生成一个libcomplex.so动态链接库供外部调用。其complex.cpp函数如下：\u003c/p\u003e","title":"Gperftools性能分析工具使用教程"},{"content":"这半年一直在研究pLink 2的加速策略，了解了相关的性能分析工具，现记录如下。\n对软件进行加速的技术路线一般为：首先对代码进行性能分析（ performance analysis，也称为 profiling），然后针对性能瓶颈提出优化方案，最后在大数据集上评测加速效果。所以进行性能优化的前提就是准确地测量代码中各个模块的时间消耗。听起来很简单，不就是测量每行代码的运行时间吗，直接用time_t t=clock();不就好了，但是事情并没有那么简单。如果只进行粗粒度的性能分析，比如测量几个大的模块的运行时间，clock()还比较准确，但是如果测量的是运算量比较小的函数调用，而且有大量的小函数调用，clock()就不太准确了。\n比如下面的一段代码，我最开始的性能分析方法是在fun1()~fun3()前后添加time_t t=clock()，然后作差求和的。但是3个fun()加起来的时间居然不等于整个while循环的时间，有将近50%的时间不翼而飞了！\n1 2 3 4 5 6 7 8 9 10 11 while (true) { if (fun1()) { for (int i = 0; i \u0026lt; k; ++k) { if (flag1) { fun2(); } } } else { fun3(); } } 一种可能是while循环以及内部的for循环本身占用了较多的时间，但是好像不太可能呀。还有一种可能是clock()测量有误，time_t只能精确到秒，clock_t只能精确到毫秒，如果一次fun*()的时间太短，导致一次测量几乎为0，那么多次的while和for循环调用累加的时间也几乎为0，导致实际测量到的fun*()时间远小于真实时间。所以自己用代码进行性能分析可能会有较大的误差，最好借助已有的性能分析工具。\n性能分析和操作系统有较大的关系。因为C++11以前的多线程在不同操作系统中有不同的实现，比如在Windows中使用的是Win32 threads，需要包含windows.h头文件，在Linux中使用的是POSIX Threads，需要包含pthread.h头文件，所以选择性能分析工具首先需要看代码使用的多线程是哪个版本。如果使用的是Win32 threads，则需要在Windows平台选择热点分析工具；如果使用的是POSIX Threads，则需要在Linux平台选择热点分析工具；当然，如果代码中没有多线程或者采用的多线程是C++11标准统一的多线程，原则上可以忽略操作系统的限制。\nWindows平台上，之前用过微软的Visual Studio工具进行Profiling，效果很不错，网上的介绍也很多，这里就不详细介绍了。\n通过网络搜索我发现了三款Linux平台下主流的热点分析工具，分别是GNU gprof、Valgrind和Google perftools，三款工具的主要特点如下表：\n工具 使用命令 是否需要重新编译 Profiling速度 是否支持多线程热点分析 是否支持链接库热点分析 GNU gprof ./test; gprof ./test ./gmon.out 是 慢 否 否 Valgrind Valgrind –tool=callgrind ./test 否 非常慢 是 是 Google perftools LD_PRELOAD=/usr/lib/libprofiler.so CPUPROFILE=./test.prof ./test 否 快 是 是 GNU gprof是GNU G++自带的热点分析工具，使用方法是：1. 使用-pg选项重新编译代码；2. 执行程序./test，生成热点分析结果gmont.out；3.使用gprof查看结果gprof ./test ./gmon.out。因为gprof要求用-pg重新编译代码，需要在Debug模式下进行Profiling，所以速度较慢。另外gprof不支持多线程的热点分析。这个工具另一个大问题是，不支持链接库的热点分析。很多大型项目为了模块化管理会生成很多动态链接库供其他程序调用，如果要分析每个模块的热点，这个工具就不适用了。\nValgrind是一系列工具的套装，包括内存分析、热点分析等。它的使用非常简单，安装好之后，直接调用Vallgrind中的callgrind工具即可，命令为Valgrind –tool=callgrind ./test。使用该工具Profiling无需重新编译代码，也支持多线程和链接库的热点分析，但是由于Profiling原理的特殊性，其Profiling速度非常之慢，比直接运行程序慢了将近50倍，所以并不适合稍大型程序的热点分析。本人试用之后发现其结果展示做得也不是很好。\nGoogle perftools原是Google内部的性能分析工具，后来在Github上开源了，地址是https://github.com/gperftools/gperftools。gperftools 的工作原理为通过定期采样当前正在执行的指令进行性能分析，如果某个函数被采样到的次数越多，则该函数在执行时占用的时间比例越大，很可能就是性能瓶颈。 gperftools 可以在被测软件处于 Release 模式下进行性能分析，所以能最大程度的模拟软件的实际使用情况。这个工具使用起来也非常简单，只需Preload其.so文件并指定生成的Profiling文件路径即可，命令为LD_PRELOAD=/usr/lib/libprofiler.so CPUPROFILE=./test.prof ./test。程序结束之后，使用命令google-pprof –web ./test ./test.prof即可查看热点分析结果。使用该工具Profiling无需重新编译代码，也支持多线程和链接库的热点分析，同时由于其是通过定期采样正在执行的指令进行热点分析，所以Profiling速度非常快，和正常release下执行程序的速度几乎相当。\n通过试用，发现gperftools的易用性、可视化效果等都是最好的，所以推荐大家使用gperftools。我最看重gperftools的特点是支持链接库和多线程的热点分析。下一篇博客将简单举一个用gperftools对链接库进行性能分析的例子。\n参考：\nLinux平台性能分析工具“综述”：http://gernotklingler.com/blog/gprof-valgrind-gperftools-evaluation-tools-application-level-cpu-profiling-linux/\n","permalink":"http://localhost:1313/posts/2017-02-07-introduction-to-performance-analysis-tools-in-linux/","summary":"\u003cp\u003e这半年一直在研究pLink 2的加速策略，了解了相关的性能分析工具，现记录如下。\u003c/p\u003e\n\u003cp\u003e对软件进行加速的技术路线一般为：首先对代码进行性能分析（ performance analysis，也称为 profiling），然后针对性能瓶颈提出优化方案，最后在大数据集上评测加速效果。所以进行性能优化的前提就是准确地测量代码中各个模块的时间消耗。听起来很简单，不就是测量每行代码的运行时间吗，直接用time_t t=clock();不就好了，但是事情并没有那么简单。如果只进行粗粒度的性能分析，比如测量几个大的模块的运行时间，clock()还比较准确，但是如果测量的是运算量比较小的函数调用，而且有大量的小函数调用，clock()就不太准确了。\u003c/p\u003e\n\u003cp\u003e比如下面的一段代码，我最开始的性能分析方法是在fun1()~fun3()前后添加time_t t=clock()，然后作差求和的。但是3个fun()加起来的时间居然不等于整个while循环的时间，有将近50%的时间不翼而飞了！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e (true) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (fun1()) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e (\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e; i \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e k; \u003cspan style=\"color:#f92672\"\u003e++\u003c/span\u003ek) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (flag1) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        fun2();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  } \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    fun3();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e一种可能是while循环以及内部的for循环本身占用了较多的时间，但是好像不太可能呀。还有一种可能是clock()测量有误，time_t只能精确到秒，clock_t只能精确到毫秒，如果一次fun*()的时间太短，导致一次测量几乎为0，那么多次的while和for循环调用累加的时间也几乎为0，导致实际测量到的fun*()时间远小于真实时间。所以自己用代码进行性能分析可能会有较大的误差，最好借助已有的性能分析工具。\u003c/p\u003e","title":"Linux性能分析工具简介"},{"content":"2017农历新年的钟声都已经敲响了，我这2016的年终总结才开始动笔。\n2016年，经历了很多，也成长了很多，遇到了很多曾经以为只会出现在电视剧中的场景，令人开始怀疑这个世界。前几天在朋友圈看到一个同学发的状态，觉得很适合作为这篇年终总结的开端。（同学你要是觉得被侵权了，告诉我，我立马删掉:-)）\n每个家庭的故事都会是一部长篇史诗。曾经总以为很多情节只会出现在电视剧中，现实的生活很是平淡无味，没有任何波澜，偶尔甚至还会抱怨一下自己不是故事的女主角，其不知现实的生活相比于电视剧，往往是有过之而无不及。 今天哥哥来电话了，从“天津”，一个美丽的谎言，一直在继续，还会坚持很久… 奶奶很开心… ——by angel\n关于学习和科研。上半年在雁栖湖完成了研一的下半个学期，完完全全的结束了自己的学生时代。下半年开始进入实验室，直面惨淡的科研。原本以为会由组里的大师兄超哥指导我，没想到中秋前3天，直接接到H Boss的指令，要在中秋前完成一项我从来没做过的评测。不知道怎样设计实验，不知道怎样计算评价指标，甚至连需要评测的软件都不熟。不过好在加班加点完成了。\n凌乱的工位\n从9月到10月中旬，一直在各种数据集上做各种对比评测，基本上一周做完一个评测数据集，完成一份报告，直接提交给H Boss，下一周在做另一个评测的同时，要根据导师的反馈建议修改完上一个评测报告。这一个多月的时间，共完成了5份报告共计14个版本，真的是要吐了。\n后来发现pLink要比对手慢，于是就尝试各种加速策略。开始从外围查找原因，尝试了各种策略，虽然多多少少能加速，但是效果都不完美，有可能对精度有影响。直到12月份，借助谷歌的开源性能分析工具gperftools才找到了软件的性能瓶颈，开始优化并取得了好几倍的加速效果。12月份对软件本身的性能优化大概是我这半年做的唯一一个和计算机本身有点关系的工作吧，可能也是为数不多的我比较享受的一件事。\n整个这半年的工作，都是H Boss御驾亲征，亲自指导，当然这样有利有弊。好处是能直接和H Boss对话，机会难得呀，H那种严于律己、追求完美的品质，H的为人处世、口才都是非常值得我学习的。坦白说，虽然我只是无名小卒一个，但从小到大，真正让我从心里佩服的人没几个，H Boss绝对算是其中之一。当然不便之处也非常明显，首先会感觉特别累和压抑，除了每周一次的面谈，每天的邮件，还经常在晚上11点多收到老师的工作微信。所以这半年确实不轻松，工作日晚上基本都在加班，而且加班到晚上11点也是常事，周末也至少工作一天。几乎没有时间运动，身体素质应该下降了不少。与人的沟通也非常少，好几个师兄师姐都问我为啥看我整天都在工位上坐着，好像从来没有动过。夸张点说就是每天晚上下班要走的时候，才发现自己这一天还没有说话。\n当然半年高强度的工作，也有不少的收获，基本摸清了pLink代码的来龙去脉，也加速了两三倍，自己的表现也稍微得到了老师和同学的肯定。不过我自己还是不太满意的，加速并没有达到理想的效果。\n关于亲情。随着我们两兄弟的大学毕业，家里的情况也在稍稍好转，但是只能算是曲折前进吧。以前小的时候，都是爸妈两个人闹，现在哥哥出场了，真是可笑。谈了快两年的女朋友，女方父母又是要查户口本，又是催着要付定金，说什么不给定金就要拉回老家相亲。这TM比电视剧还荒唐，真把自己当商品了，是不是给的钱多就跟谁呀，混蛋。哥哥也不是个省油的灯，分手之后没过多久说什么被公司派去新加坡学习了，去了之后，连个固定的联系方式都没有，三天两头失联，都老大不小的人了，还让父母担心。工作了两年，一分钱都没攒到，连大学的助学贷款都要我这个还在上学的弟弟替他还。女朋友分手也就算了，没赚到钱也不要紧，关键是你不能让家人这样担心你呀，你定期给家人打个电话，说说你到底在哪里干什么，既然到了新加坡，发几张国外的照片回来分享一下，不可以吗？已经两年过年没回家了，而且两年除夕居然连个电话都没有，这不是不孝是什么，混蛋。\n今年妈妈也终于愿意外出挣钱了，虽然不多，但是起码在和爸爸一起努力。家里装修好了一层房子，但是也就是把墙什么的弄好了，家具还没制备。本来想着过年回家给家里买个小米电视，但是爸妈死活说不要买，现在买了也就过年看几天，不划算。后来只好作罢。放假给老爸买的红米手机，终于在除夕这一天拿到手了。给爸妈包的红包，也在按计划逐年的递增着。\n自己有时候也埋怨家里，为什么家境这么的不堪，为什么父母没有达到我理想的高度，为什么哥哥这么不争气，为什么没有人关心我。但是埋怨有用吗，肯定是没用的，还是要看各自的造化。\n关于爱情。我还是太幼稚，看看我的家境吧，有哪个女生愿意摊上我家这些破事呢，我就不应该奢望有什么爱情。不过今年上半年，爱情确实来过，抛开所有的一切，纯粹的校园爱情。可是10月份的一件事，彻底打醒了我，爱情没有那么简单，需要考虑的问题太多了。关于那段时间的记忆，写过很多文字，也流过很多泪。回顾整个下半年，欣欣和我的状态都不太好，除了那件事的原因，和工作变化也有很大的关系。我们都从雁栖湖到中关村，需要经历一个由学生到科研工作者的角色转变，面对科研的未知，都显得有些手足无措。科研的不顺，生活的压力以及家里的一些烦心事，一股脑的涌向了我们，矛盾也时有发生。经历过不少的磕磕绊绊，总算顺利度过了2016年，没有了热恋时期的疯狂，生活终要归于平淡。正所谓陪伴是最长情的告白，爱情的意义是否就在于两个人一起经历，一起成长呢。让我们共同守护。\n关于友情。是的，我还欠很多人一顿饭。很多同学，如果长时间不见面，恐怕真的要忘掉了。\n关于个人提高。上半年忙于课程学习，下半年忙于科研，花在个人提高上的时间真的是太少了。不过感谢有欣欣一直做我的榜样，我现在的小目标就是希望比欣欣看更多的书、刷更多的题。\n另外下半年回到市区之后，也去了一些之前没去过的地方，比如：清华艺术博物馆、繁星戏剧村、中国美术馆、三联书店、香山等地。其中前两个地方都是和欣欣一起去的，感觉超棒~第一次看达芬奇的特展，开始了解这样一个天才，后来还看过他的传记；第一次去剧院看话剧，感觉和看电影完全不一样，小剧场的效果也是棒棒的。2016年12月31日也是一个值得纪念的日子：\n2017跨年活动~\n今年借着CNCP2016会议的机会，去了一次大连，见识了一下海滨城市的风貌，后来还跑去渤海学游泳，海水很脏，而且咸得发苦。希望今后每年都去一个除了上班地点和家里之外的第三个城市。\n大连滨海国家地质公园\n大连滨海国家地质公园\n最后看看年初计划的完成情况：\n完成国科大下学期的课程任务：完成 接手pLink软件：完成 刷完LeetCode所有题目：进度147/461，没有完成 读10本书：目前读了《数学之美》、《大话设计模式》、《我不知道该说什么，关于死亡还是爱情》、《男人来自火星、女人来自金星，卷I》、《只有医生知道，卷I》、《文学的种子》、《讲理》、《暗时间》、《达·芬奇传：放飞的心灵》、《人间失格》，刚好10本，圆满完成任务:-) 去电影院看10场电影：目前看了《美人鱼》、《北京遇上西雅图之不二情书》、《忍者神龟2：破影而出》、《七月与安生》、《湄公河行动》、《比利·林恩的中场战事》、《你的名字》、《血战钢锯岭》，只有8场，其中7场是和欣欣一起看的~ 改正坐姿：有一段时间刻意改正了，但是这东西貌似改不过来？ 除了LeetCode完成度太差之外，其他计划完成度还是蛮高的。下面定一下2017年的年度计划：\n发表pLink 2文章 至少完成毕业工作的80% 刷完LeetCode所有简单题和中等题，找工作之前最好刷完两遍 找到一个满意的工作 读10本书 去电影院看10场电影 看一场话剧（音乐会、歌剧等都可以） 学会游泳 去第三个城市 简短总结一下我的2016：完成了由上课到科研的转变；开始有能力感恩家人；遇到了欣欣，由一个人变成了两个人；第一次去剧场看话剧。展望2017，找工作和准备毕业迫在眉睫，注定又是繁忙的一年！\n最后用汪老师的年终总结PPT封面的一句话来结束吧：\n","permalink":"http://localhost:1313/posts/2017-01-28-2016-summary/","summary":"\u003cp\u003e2017农历新年的钟声都已经敲响了，我这2016的年终总结才开始动笔。\u003c/p\u003e\n\u003cp\u003e2016年，经历了很多，也成长了很多，遇到了很多曾经以为只会出现在电视剧中的场景，令人开始怀疑这个世界。前几天在朋友圈看到一个同学发的状态，觉得很适合作为这篇年终总结的开端。（同学你要是觉得被侵权了，告诉我，我立马删掉:-)）\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e每个家庭的故事都会是一部长篇史诗。曾经总以为很多情节只会出现在电视剧中，现实的生活很是平淡无味，没有任何波澜，偶尔甚至还会抱怨一下自己不是故事的女主角，其不知现实的生活相比于电视剧，往往是有过之而无不及。\n今天哥哥来电话了，从“天津”，一个美丽的谎言，一直在继续，还会坚持很久…\n奶奶很开心…\n——by angel\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e关于学习和科研\u003c/strong\u003e。\u003ca href=\"https://bitjoy.net/posts/2016-08-20-2016-mid-year-summary/\"\u003e上半年在雁栖湖完成了研一的下半个学期，完完全全的结束了自己的学生时代。\u003c/a\u003e下半年开始进入实验室，直面惨淡的科研。原本以为会由组里的大师兄超哥指导我，没想到中秋前3天，直接接到H Boss的指令，要在中秋前完成一项我从来没做过的评测。不知道怎样设计实验，不知道怎样计算评价指标，甚至连需要评测的软件都不熟。不过好在加班加点完成了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"my-office-cubicle\" loading=\"lazy\" src=\"/posts/2017-01-28-2016-summary/my-office-cubicle.jpg\"\u003e\n凌乱的工位\u003c/p\u003e\n\u003cp\u003e从9月到10月中旬，一直在各种数据集上做各种对比评测，基本上一周做完一个评测数据集，完成一份报告，直接提交给H Boss，下一周在做另一个评测的同时，要根据导师的反馈建议修改完上一个评测报告。这一个多月的时间，共完成了5份报告共计14个版本，真的是要吐了。\u003c/p\u003e\n\u003cp\u003e后来发现pLink要比对手慢，于是就尝试各种加速策略。开始从外围查找原因，尝试了各种策略，虽然多多少少能加速，但是效果都不完美，有可能对精度有影响。直到12月份，借助谷歌的开源性能分析工具gperftools才找到了软件的性能瓶颈，开始优化并取得了好几倍的加速效果。12月份对软件本身的性能优化大概是我这半年做的唯一一个和计算机本身有点关系的工作吧，可能也是为数不多的我比较享受的一件事。\u003c/p\u003e\n\u003cp\u003e整个这半年的工作，都是H Boss御驾亲征，亲自指导，当然这样有利有弊。好处是能直接和H Boss对话，机会难得呀，H那种严于律己、追求完美的品质，H的为人处世、口才都是非常值得我学习的。坦白说，虽然我只是无名小卒一个，但从小到大，真正让我从心里佩服的人没几个，H Boss绝对算是其中之一。当然不便之处也非常明显，首先会感觉特别累和压抑，除了每周一次的面谈，每天的邮件，还经常在晚上11点多收到老师的工作微信。所以这半年确实不轻松，工作日晚上基本都在加班，而且加班到晚上11点也是常事，周末也至少工作一天。几乎没有时间运动，身体素质应该下降了不少。与人的沟通也非常少，好几个师兄师姐都问我为啥看我整天都在工位上坐着，好像从来没有动过。夸张点说就是每天晚上下班要走的时候，才发现自己这一天还没有说话。\u003c/p\u003e\n\u003cp\u003e当然半年高强度的工作，也有不少的收获，基本摸清了pLink代码的来龙去脉，也加速了两三倍，自己的表现也稍微得到了老师和同学的肯定。不过我自己还是不太满意的，加速并没有达到理想的效果。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于亲情\u003c/strong\u003e。随着我们两兄弟的大学毕业，家里的情况也在稍稍好转，但是只能算是曲折前进吧。以前小的时候，都是爸妈两个人闹，现在哥哥出场了，真是可笑。谈了快两年的女朋友，女方父母又是要查户口本，又是催着要付定金，说什么不给定金就要拉回老家相亲。这TM比电视剧还荒唐，真把自己当商品了，是不是给的钱多就跟谁呀，混蛋。哥哥也不是个省油的灯，分手之后没过多久说什么被公司派去新加坡学习了，去了之后，连个固定的联系方式都没有，三天两头失联，都老大不小的人了，还让父母担心。工作了两年，一分钱都没攒到，连大学的助学贷款都要我这个还在上学的弟弟替他还。女朋友分手也就算了，没赚到钱也不要紧，关键是你不能让家人这样担心你呀，你定期给家人打个电话，说说你到底在哪里干什么，既然到了新加坡，发几张国外的照片回来分享一下，不可以吗？已经两年过年没回家了，而且两年除夕居然连个电话都没有，这不是不孝是什么，混蛋。\u003c/p\u003e\n\u003cp\u003e今年妈妈也终于愿意外出挣钱了，虽然不多，但是起码在和爸爸一起努力。家里装修好了一层房子，但是也就是把墙什么的弄好了，家具还没制备。本来想着过年回家给家里买个小米电视，但是爸妈死活说不要买，现在买了也就过年看几天，不划算。后来只好作罢。放假给老爸买的红米手机，终于在除夕这一天拿到手了。给爸妈包的红包，也在按计划逐年的递增着。\u003c/p\u003e\n\u003cp\u003e自己有时候也埋怨家里，为什么家境这么的不堪，为什么父母没有达到我理想的高度，为什么哥哥这么不争气，为什么没有人关心我。但是埋怨有用吗，肯定是没用的，还是要看各自的造化。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于爱情\u003c/strong\u003e。我还是太幼稚，看看我的家境吧，有哪个女生愿意摊上我家这些破事呢，我就不应该奢望有什么爱情。不过今年上半年，爱情确实来过，抛开所有的一切，纯粹的校园爱情。可是10月份的一件事，彻底打醒了我，爱情没有那么简单，需要考虑的问题太多了。关于那段时间的记忆，写过很多文字，也流过很多泪。回顾整个下半年，欣欣和我的状态都不太好，除了那件事的原因，和工作变化也有很大的关系。我们都从雁栖湖到中关村，需要经历一个由学生到科研工作者的角色转变，面对科研的未知，都显得有些手足无措。科研的不顺，生活的压力以及家里的一些烦心事，一股脑的涌向了我们，矛盾也时有发生。经历过不少的磕磕绊绊，总算顺利度过了2016年，没有了热恋时期的疯狂，生活终要归于平淡。正所谓陪伴是最长情的告白，爱情的意义是否就在于两个人一起经历，一起成长呢。让我们共同守护。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于友情\u003c/strong\u003e。是的，我还欠很多人一顿饭。很多同学，如果长时间不见面，恐怕真的要忘掉了。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于个人提高\u003c/strong\u003e。上半年忙于课程学习，下半年忙于科研，花在个人提高上的时间真的是太少了。不过感谢有欣欣一直做我的榜样，我现在的小目标就是希望比欣欣看更多的书、刷更多的题。\u003c/p\u003e\n\u003cp\u003e另外下半年回到市区之后，也去了一些之前没去过的地方，比如：清华艺术博物馆、繁星戏剧村、中国美术馆、三联书店、香山等地。其中前两个地方都是和欣欣一起去的，感觉超棒~第一次看达芬奇的特展，开始了解这样一个天才，后来还看过他的传记；第一次去剧院看话剧，感觉和看电影完全不一样，小剧场的效果也是棒棒的。2016年12月31日也是一个值得纪念的日子：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"from-2016-to-2017\" loading=\"lazy\" src=\"/posts/2017-01-28-2016-summary/from-2016-to-2017.jpg\"\u003e\n2017跨年活动~\u003c/p\u003e\n\u003cp\u003e今年借着CNCP2016会议的机会，去了一次大连，见识了一下海滨城市的风貌，后来还跑去渤海学游泳，海水很脏，而且咸得发苦。希望今后每年都去一个除了上班地点和家里之外的第三个城市。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-01-28-2016-summary/dalian1-201608.jpg\"\u003e\n大连滨海国家地质公园\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-01-28-2016-summary/dalian2-201608.jpg\"\u003e\n大连滨海国家地质公园\u003c/p\u003e\n\u003cp\u003e最后看看\u003ca href=\"https://bitjoy.net/posts/2016-01-03-2016-happy-new-year/\"\u003e年初计划\u003c/a\u003e的完成情况：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cdel\u003e完成国科大下学期的课程任务：完成\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e\u003cdel\u003e接手pLink软件：完成\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e刷完LeetCode所有题目：进度147/461，没有完成\u003c/li\u003e\n\u003cli\u003e\u003cdel\u003e读10本书：目前读了《数学之美》、《大话设计模式》、《我不知道该说什么，关于死亡还是爱情》、《男人来自火星、女人来自金星，卷I》、《只有医生知道，卷I》、《文学的种子》、《讲理》、《暗时间》、《达·芬奇传：放飞的心灵》、《人间失格》，刚好10本，圆满完成任务:-)\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影：目前看了《美人鱼》、《北京遇上西雅图之不二情书》、《忍者神龟2：破影而出》、《七月与安生》、《湄公河行动》、《比利·林恩的中场战事》、《你的名字》、《血战钢锯岭》，只有8场，其中7场是和欣欣一起看的~\u003c/li\u003e\n\u003cli\u003e改正坐姿：有一段时间刻意改正了，但是这东西貌似改不过来？\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e除了LeetCode完成度太差之外，其他计划完成度还是蛮高的。下面定一下2017年的年度计划：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e发表pLink 2文章\u003c/li\u003e\n\u003cli\u003e至少完成毕业工作的80%\u003c/li\u003e\n\u003cli\u003e刷完LeetCode所有简单题和中等题，找工作之前最好刷完两遍\u003c/li\u003e\n\u003cli\u003e找到一个满意的工作\u003c/li\u003e\n\u003cli\u003e读10本书\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影\u003c/li\u003e\n\u003cli\u003e看一场话剧（音乐会、歌剧等都可以）\u003c/li\u003e\n\u003cli\u003e学会游泳\u003c/li\u003e\n\u003cli\u003e去第三个城市\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e简短总结一下我的2016：完成了由上课到科研的转变；开始有能力感恩家人；遇到了欣欣，由一个人变成了两个人；第一次去剧场看话剧。展望2017，找工作和准备毕业迫在眉睫，注定又是繁忙的一年！\u003c/p\u003e\n\u003cp\u003e最后用汪老师的年终总结PPT封面的一句话来结束吧：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"from-2016-to-2017-2\" loading=\"lazy\" src=\"/posts/2017-01-28-2016-summary/from-2016-to-2017-2.png\"\u003e\u003c/p\u003e","title":"2016年终总结"},{"content":"今天中午和超哥在食堂吃饭的时候聊起了电动汽车加电站的问题，很有意思。\n超哥现在开的是一辆电动汽车，他说目前也挺满意的，只要在北京市内开，几乎没问题，充电桩到处都有，马力也足，开起来没有任何噪声。唯一的问题是需要每天充电，去到稍微远一点的京郊可能会电量不足。\n然后就讨论到目前电动汽车的瓶颈，主要还是在电池上，一个是续航时间短，另一个是充电时间慢。\n然后我就想现在的加油车为什么没有上面的两个问题呢，第一个问题，如果加的油少的话，是不是也会出现续航时间短的问题呢，所以把电动汽车的电池做大一点，密度高一点是不是就可以了呢，虽然技术上可能会有难度，但是我觉得并没有第二个问题严重，所以我觉得电池续航短不是太大的问题。。。\n对于第二个问题，在加油站给汽车加油只需要几分钟的时间，但是电动车在充电桩充电可能需要几十分钟甚至一个多小时，所以充电时间慢确实是一个很严重的问题。我当时就说一辆越野车如果要跑沙漠的话，会在车上预存好几桶油备用，类似的，电动车能不能在车上备上几个电池，没电了就换呢，就像手机备用电池一样。然后超哥进一步说不如干脆在现有加油站的基础上，建一个加电站，每辆电动车进站之后，卸下车上的电池，换上提前充满电的电池，整个过程和加油完全一样，还比加油干净。\n我突然觉得，哇塞，这个idea不错呀，统一所有电动车的电池，电池没电之后，到站换电池，一个电池就像一个小型的集装箱（或者docker）一样，被卸下来，然后插上满电电池，so easy~但是为啥没公司这么做呢。此时旁边坐着的一位老师也加入了对话，他说电池寿命有长有短，如果自己刚买的新车，没电了拿去换了一个旧电池，肯定不爽；再说了，要统一全国的电池标准，几乎是不可能的，在全国建这样的加电站，没有哪个公司能承担得起这样的成本，最终羊毛出在羊身上，电动车的价格肯定会上涨的….\n这位老师说的都对，但是我依然觉得这个idea是可行的，关键是要看有关部门有没有这个魄力来做这件事。比如国家或行业层面可以强制统一电池的标准，XX汽车协会规定今后的汽车电池必须做成0.5m*0.5m*0.5m的方块，正负极距离5cm，便于拆卸等；同时要求电动汽车的电池安放位置必须在汽车的后右侧等一些列规定。即使国家层面没人愿意做这件事，哪家有魄力的电动汽车公司，是不是可以尝试一下呢，比如特斯拉，统一旗下所有电车的电池规格，并在全球建造加电站，统一更换特斯拉的电池。如果特斯拉这样做了，我相信买特斯拉的车主是愿意承担一部分费用的，毕竟这样的加电站最终还是方便了自己。至于说新车换到旧电池，其实大可不必担心，说不定你的旧车会换到别人的新电池呢，而且特斯拉可以建立一个标准，只有电池能量转换效率大于80%的电池才能进入加电站循环，这样保证了每个人换到的电池续航有保障，至于新旧，我才不管呢，反正下一次加电又要换了。\n我个人还是挺喜欢电动汽车的，节能、环保、静音，还看起来酷酷的:-)好希望这个idea能在未来实现呀~\n知乎：电动汽车为什么不统一电池，充电站更换相同档次满电电池？\n","permalink":"http://localhost:1313/posts/2017-01-03-electric-vehicle-charging-station/","summary":"\u003cp\u003e今天中午和超哥在食堂吃饭的时候聊起了电动汽车加电站的问题，很有意思。\u003c/p\u003e\n\u003cp\u003e超哥现在开的是一辆电动汽车，他说目前也挺满意的，只要在北京市内开，几乎没问题，充电桩到处都有，马力也足，开起来没有任何噪声。唯一的问题是需要每天充电，去到稍微远一点的京郊可能会电量不足。\u003c/p\u003e\n\u003cp\u003e然后就讨论到目前电动汽车的瓶颈，主要还是在电池上，一个是续航时间短，另一个是充电时间慢。\u003c/p\u003e\n\u003cp\u003e然后我就想现在的加油车为什么没有上面的两个问题呢，第一个问题，如果加的油少的话，是不是也会出现续航时间短的问题呢，所以把电动汽车的电池做大一点，密度高一点是不是就可以了呢，虽然技术上可能会有难度，但是我觉得并没有第二个问题严重，所以我觉得电池续航短不是太大的问题。。。\u003c/p\u003e\n\u003cp\u003e对于第二个问题，在加油站给汽车加油只需要几分钟的时间，但是电动车在充电桩充电可能需要几十分钟甚至一个多小时，所以充电时间慢确实是一个很严重的问题。我当时就说一辆越野车如果要跑沙漠的话，会在车上预存好几桶油备用，类似的，电动车能不能在车上备上几个电池，没电了就换呢，就像手机备用电池一样。然后超哥进一步说不如干脆在现有加油站的基础上，建一个加电站，每辆电动车进站之后，卸下车上的电池，换上提前充满电的电池，整个过程和加油完全一样，还比加油干净。\u003c/p\u003e\n\u003cp\u003e我突然觉得，哇塞，这个idea不错呀，统一所有电动车的电池，电池没电之后，到站换电池，一个电池就像一个小型的集装箱（或者docker）一样，被卸下来，然后插上满电电池，so easy~但是为啥没公司这么做呢。此时旁边坐着的一位老师也加入了对话，他说电池寿命有长有短，如果自己刚买的新车，没电了拿去换了一个旧电池，肯定不爽；再说了，要统一全国的电池标准，几乎是不可能的，在全国建这样的加电站，没有哪个公司能承担得起这样的成本，最终羊毛出在羊身上，电动车的价格肯定会上涨的….\u003c/p\u003e\n\u003cp\u003e这位老师说的都对，但是我依然觉得这个idea是可行的，关键是要看有关部门有没有这个魄力来做这件事。比如国家或行业层面可以强制统一电池的标准，XX汽车协会规定今后的汽车电池必须做成0.5m*0.5m*0.5m的方块，正负极距离5cm，便于拆卸等；同时要求电动汽车的电池安放位置必须在汽车的后右侧等一些列规定。即使国家层面没人愿意做这件事，哪家有魄力的电动汽车公司，是不是可以尝试一下呢，比如特斯拉，统一旗下所有电车的电池规格，并在全球建造加电站，统一更换特斯拉的电池。如果特斯拉这样做了，我相信买特斯拉的车主是愿意承担一部分费用的，毕竟这样的加电站最终还是方便了自己。至于说新车换到旧电池，其实大可不必担心，说不定你的旧车会换到别人的新电池呢，而且特斯拉可以建立一个标准，只有电池能量转换效率大于80%的电池才能进入加电站循环，这样保证了每个人换到的电池续航有保障，至于新旧，我才不管呢，反正下一次加电又要换了。\u003c/p\u003e\n\u003cp\u003e我个人还是挺喜欢电动汽车的，节能、环保、静音，还看起来酷酷的:-)好希望这个idea能在未来实现呀~\u003c/p\u003e\n\u003cp\u003e知乎：\u003ca href=\"https://www.zhihu.com/question/28793345\"\u003e电动汽车为什么不统一电池，充电站更换相同档次满电电池？\u003c/a\u003e\u003c/p\u003e","title":"电动汽车加电站"},{"content":"最怕空气突然安静 最怕朋友突然的关心 最怕回忆突然翻滚绞痛着不平息 最怕突然听到你的消息 想念如果会有声音 不愿那是悲伤的哭泣 事到如今终於让自已属於我自已 只剩眼泪还骗不过自己 突然好想你你会在哪里 过的快乐或委屈 突然好想你突然锋利的回忆 突然模糊的眼睛 我们像一首最美丽的歌曲 变成两部悲伤的电影 为什麽你带我走过最难忘的旅行 然後留下最痛的纪念品 我们那麽甜那麽美那麽相信 那麽疯那麽热烈的曾经 为何我们还是要奔向 各自的幸福和遗憾中老去 突然好想你你会在哪里 过的快乐或委屈 突然好想你突然锋利的回忆 突然模糊的眼睛 最怕空气突然安静 最怕朋友突然的关心 最怕回忆突然翻滚绞痛着不平息 最怕突然听到你的消息 最怕此生已经决定自己过 没有你却又突然听到你的消息 就好像是突然之间，整个世界都失去了你的声音，以前每天都会收到你的喜怒哀乐、衣食住行、午安晚安，突然之间，空气都安静了，没有了你的消息。翻遍你的空间、朋友圈、微博、博客，都没有消息，不知道你在干什么，好伤心。\n想你，想知道你在哪里，想知道你在干什么，想要发消息给你，又害怕不能收到你的回信，然后郁闷一整天。想要引起你的注意，绞尽脑汁故意发一些不着边际的微博，等了一整天，没有收到你的点赞或评论。\n约你出来吃饭，你一句简单得不能再简单的“可以”，冷冰冰。见到你，裹着厚厚的棉衣，戴着帽子和手套，没有一丝的眼神交流，两个人就这样默默的吃着不知道什么味道的饭菜。\n想要打破这宁静的空气，之前想到无数要和你说的人和事，现在却一句话也说不出。两个最熟悉的人，突然之间，像多年未见的朋友，因完全不同的人生轨迹而没有任何共同语言，成了最熟悉的陌生人。\n你说最近科研压力很大，周围的同学又是发论文，又是发专利，你却一无所有。挑战赛答辩和开题答辩在即，手足无措。\n你说未来太渺茫，没钱买房买车，就算月入两万，要在北京买房也得攒20年。即使买了房，在北京还要买车还要摇号，看病也很贵，还要担心孩子上学各种问题。\n我向来是个不会安慰别人的人，但是我尽力想要告诉你，你已经很优秀了，从小到大的尖子生，多才多艺，研一在雁栖湖那么多高手，你照样轻松拿下第一。你的编程能力也远超你们实验室的人，只不过你目前处于一种有力没处使的状态，你的导师让你干一些杂活，如果你的导师也让专心指导你的挑战赛，你现在肯定也写论文了。\n我尽力安慰你，希望你对对未来乐观一点。面包会有的，不要太在意这些东西，生活快乐最重要，不要太在意别人的看法，自己纵向比较有进步就行了。突然觉得自己词穷，完全不会安慰一个人。\n世界上比你悲惨的人多得太多了，为什么不想想自己有的，至少你有一个健康的身体。\n是的，我猜到了，你不理我和我们上周的体检结果有关。我有病，是的，我有病，而且是不可能治好的病，是随时都可能发病的病。我知道这对于你来说很为难，你有矛盾，我理解，只是我希望你能把你的所有顾虑都告诉我，至少让我和你一起分担，如果你说因为我的病，因为我那卑微的背景，想要离开我，我无话可说，这也无可厚非，我接受。\n刚开始交往时，我没有告诉你，是我的错。上周的检查结果至少说明你是健康的，我很庆幸，没有伤害到你。如果因为我而让你不开心，让你纠结，请一定让我知道，我会默默走开，我是一个讲理的人。希望你永远健康快乐。\n你说这东西还有窗口期，我认，再过几个月，我们再去检查一次，我默默祈祷你是健康的。我知道你和我一样，对一件微小的事也要纠结半天，我不想让你那样难过。\n我原本以为我是一个耐得住安静和寂寞的人，我一度还觉得你像一个叽叽喳喳的小鸟不停的在我耳边发出噪声。直到有一天，我发现你的声音不见了，我开始慌了，不安、烦躁、忧郁充斥我的大脑，想要马上见到你问个一清二楚。也许这就是日久生情，这就是感情吧。\n熟悉了你随性而又纠结的脾气；习惯了每次和你一起吃饭时纠结菜里到底有没有混猪肉；习惯了每次吃饭时要双份碗筷和三份汤的感觉；渐渐喜欢上了和你一起漫无目的的逛街试衣服的感觉；习惯了买两本一样的书，然后每次你看得都比我快，逼迫我不得不也快快的看；喜欢和你一起看电影出去玩的感觉。\n也不知从什么时候开始，微信里的情侣表情都积了厚厚的一层灰；好久好久都没有掰你了，你的手指应该纤细如初了吧。带上耳机，听了一整晚的音乐，已经很久很久没有听歌了。\n以前常常对电视剧里的爱情嗤之以鼻，完全不能理解他们为什么要为爱情哭得死去活来，现在，我大概理解了。\n你说每个正常人遇到我这种情况，都会矛盾，会需要思考、抉择和权衡，是的，爱情完全不是小说里的义无反顾，说到底是各种利益的权衡。\n10月真是一个让人忧伤的月份，亲人的离世，自己也前前后后进了三次医院，揭开了深藏心底N年的伤疤，曾经一直陪伴在身边的人也想要离开。科研上的压力就更不说了，老板一直不停的催促，每天活得像个陀螺，没有方向的转，没有一丝的停顿。\n这病态的社会。\n亲爱的，我愿意等你，我尊重你的决定，如果你离开，我会祝福你，如果你留下，我希望你不是在怜悯我。\n","permalink":"http://localhost:1313/posts/2016-10-28-waiting-for-you/","summary":"\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e最怕空气突然安静\n最怕朋友突然的关心\n最怕回忆突然翻滚绞痛着不平息\n最怕突然听到你的消息\n\n想念如果会有声音\n不愿那是悲伤的哭泣\n事到如今终於让自已属於我自已\n只剩眼泪还骗不过自己\n\n突然好想你你会在哪里\n过的快乐或委屈\n突然好想你突然锋利的回忆\n突然模糊的眼睛\n\n我们像一首最美丽的歌曲\n变成两部悲伤的电影\n为什麽你带我走过最难忘的旅行\n然後留下最痛的纪念品\n\n我们那麽甜那麽美那麽相信\n那麽疯那麽热烈的曾经\n为何我们还是要奔向\n各自的幸福和遗憾中老去\n\n突然好想你你会在哪里\n过的快乐或委屈\n突然好想你突然锋利的回忆\n突然模糊的眼睛\n\n最怕空气突然安静\n最怕朋友突然的关心\n最怕回忆突然翻滚绞痛着不平息\n最怕突然听到你的消息\n最怕此生已经决定自己过\n没有你却又突然听到你的消息\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e就好像是突然之间，整个世界都失去了你的声音，以前每天都会收到你的喜怒哀乐、衣食住行、午安晚安，突然之间，空气都安静了，没有了你的消息。翻遍你的空间、朋友圈、微博、博客，都没有消息，不知道你在干什么，好伤心。\u003c/p\u003e\n\u003cp\u003e想你，想知道你在哪里，想知道你在干什么，想要发消息给你，又害怕不能收到你的回信，然后郁闷一整天。想要引起你的注意，绞尽脑汁故意发一些不着边际的微博，等了一整天，没有收到你的点赞或评论。\u003c/p\u003e\n\u003cp\u003e约你出来吃饭，你一句简单得不能再简单的“可以”，冷冰冰。见到你，裹着厚厚的棉衣，戴着帽子和手套，没有一丝的眼神交流，两个人就这样默默的吃着不知道什么味道的饭菜。\u003c/p\u003e\n\u003cp\u003e想要打破这宁静的空气，之前想到无数要和你说的人和事，现在却一句话也说不出。两个最熟悉的人，突然之间，像多年未见的朋友，因完全不同的人生轨迹而没有任何共同语言，成了最熟悉的陌生人。\u003c/p\u003e\n\u003cp\u003e你说最近科研压力很大，周围的同学又是发论文，又是发专利，你却一无所有。挑战赛答辩和开题答辩在即，手足无措。\u003c/p\u003e\n\u003cp\u003e你说未来太渺茫，没钱买房买车，就算月入两万，要在北京买房也得攒20年。即使买了房，在北京还要买车还要摇号，看病也很贵，还要担心孩子上学各种问题。\u003c/p\u003e\n\u003cp\u003e我向来是个不会安慰别人的人，但是我尽力想要告诉你，你已经很优秀了，从小到大的尖子生，多才多艺，研一在雁栖湖那么多高手，你照样轻松拿下第一。你的编程能力也远超你们实验室的人，只不过你目前处于一种有力没处使的状态，你的导师让你干一些杂活，如果你的导师也让专心指导你的挑战赛，你现在肯定也写论文了。\u003c/p\u003e\n\u003cp\u003e我尽力安慰你，希望你对对未来乐观一点。面包会有的，不要太在意这些东西，生活快乐最重要，不要太在意别人的看法，自己纵向比较有进步就行了。突然觉得自己词穷，完全不会安慰一个人。\u003c/p\u003e\n\u003cp\u003e世界上比你悲惨的人多得太多了，为什么不想想自己有的，至少你有一个健康的身体。\u003c/p\u003e\n\u003cp\u003e是的，我猜到了，你不理我和我们上周的体检结果有关。我有病，是的，我有病，而且是不可能治好的病，是随时都可能发病的病。我知道这对于你来说很为难，你有矛盾，我理解，只是我希望你能把你的所有顾虑都告诉我，至少让我和你一起分担，如果你说因为我的病，因为我那卑微的背景，想要离开我，我无话可说，这也无可厚非，我接受。\u003c/p\u003e\n\u003cp\u003e刚开始交往时，我没有告诉你，是我的错。上周的检查结果至少说明你是健康的，我很庆幸，没有伤害到你。如果因为我而让你不开心，让你纠结，请一定让我知道，我会默默走开，我是一个讲理的人。希望你永远健康快乐。\u003c/p\u003e\n\u003cp\u003e你说这东西还有窗口期，我认，再过几个月，我们再去检查一次，我默默祈祷你是健康的。我知道你和我一样，对一件微小的事也要纠结半天，我不想让你那样难过。\u003c/p\u003e\n\u003cp\u003e我原本以为我是一个耐得住安静和寂寞的人，我一度还觉得你像一个叽叽喳喳的小鸟不停的在我耳边发出噪声。直到有一天，我发现你的声音不见了，我开始慌了，不安、烦躁、忧郁充斥我的大脑，想要马上见到你问个一清二楚。也许这就是\u003ca href=\"http://zhihu.com/question/26049681/answer/32014770\"\u003e日久生情\u003c/a\u003e，这就是感情吧。\u003c/p\u003e\n\u003cp\u003e熟悉了你随性而又纠结的脾气；习惯了每次和你一起吃饭时纠结菜里到底有没有混猪肉；习惯了每次吃饭时要双份碗筷和三份汤的感觉；渐渐喜欢上了和你一起漫无目的的逛街试衣服的感觉；习惯了买两本一样的书，然后每次你看得都比我快，逼迫我不得不也快快的看；喜欢和你一起看电影出去玩的感觉。\u003c/p\u003e\n\u003cp\u003e也不知从什么时候开始，微信里的情侣表情都积了厚厚的一层灰；好久好久都没有掰你了，你的手指应该纤细如初了吧。带上耳机，听了一整晚的音乐，已经很久很久没有听歌了。\u003c/p\u003e\n\u003cp\u003e以前常常对电视剧里的爱情嗤之以鼻，完全不能理解他们为什么要为爱情哭得死去活来，现在，我大概理解了。\u003c/p\u003e\n\u003cp\u003e你说每个正常人遇到我这种情况，都会矛盾，会需要思考、抉择和权衡，是的，爱情完全不是小说里的义无反顾，说到底是各种利益的权衡。\u003c/p\u003e\n\u003cp\u003e10月真是一个让人忧伤的月份，亲人的离世，自己也前前后后进了三次医院，揭开了深藏心底N年的伤疤，曾经一直陪伴在身边的人也想要离开。科研上的压力就更不说了，老板一直不停的催促，每天活得像个陀螺，没有方向的转，没有一丝的停顿。\u003c/p\u003e\n\u003cp\u003e这病态的社会。\u003c/p\u003e\n\u003cp\u003e亲爱的，我愿意等你，我尊重你的决定，如果你离开，我会祝福你，如果你留下，我希望你不是在怜悯我。\u003c/p\u003e","title":"最怕空气突然安静"},{"content":"今天阅读《C++ Primer, 5e》的第二章，介绍C++的基本内置类型，觉得有一些平时工作容易出错的知识点，现摘录如下：\n1 unsigned char c = -1; // 假设char占8比特，c的值为255 当我们赋给无符号类型一个超出它表示范围的值时，结果是初始值对无符号类型表示数值总数取模后的余数。例如，8比特大小的unsigned char可以表示0至255区间内的值，如果我们赋了一个区间以外的值，则实际的结果是该值对256取模后所得的余数。因此，把-1赋给8比特大小的unsigned char所得的结果是255。\n1 signed char c2 = 256; // 假设char占8比特，c2的值是未定义的 当我们赋给带符号类型一个超出它表示范围的值时，结果是未定义的（undefined）。此时，程序可能继续工作、可能崩溃，也可能生成垃圾数据。\n1 2 3 4 unsigned u = 10; int i = -42; std::cout \u0026lt;\u0026lt; i + i \u0026lt;\u0026lt; std::endl; // 输出-84 std::cout \u0026lt;\u0026lt; u + i \u0026lt;\u0026lt; std::endl; // 如果int占32位，输出4294967264 在第一个输出表达式里，两个（负）整数相加并得到了期望的结果。在第二个输出表达式里，相加前首先把整数-42转换成无符号数。把负数转换成无符号数类似于直接给无符号数赋一个负数，结果等于这个负数加上无符号数的模。unsigned (int)的取值范围是0~\\(2^{32}-1\\)，所以总数有\\(2^{32}\\)个数，-42%\\(2^{32}\\)=-42+\\(2^{32}\\)，u+i=10+(-42+\\(2^{32}\\))=4294967264。\n1 2 3 unsigned u1 = 42, u2 = 10; std::cout \u0026lt;\u0026lt; u1 – u2 \u0026lt;\u0026lt; std::endl; // 正确：输出32 std::cout \u0026lt;\u0026lt; u2 – u1 \u0026lt;\u0026lt; std::endl; // 正确：不过，结果是取模后的值 当从无符号数中减去一个值时，不管这个值是不是无符号数，我们都必须确保结果不能是一个负值。\n无符号数不会小于0这一事实同样关系到循环的写法。例如我们常用的循环如下：\n1 2 for (int i = 10; i \u0026gt;= 0; --i) std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; 可能你会觉得反正也不打算输出负数，可以用无符号数来重写这个循环。然而，这个不经意的改变却意味着死循环；\n1 2 3 // 错误：变量u永远也不会小于0，循环条件一直成立 for (unsigned u = 10; u \u0026gt;= 0; --u) std::cout \u0026lt;\u0026lt; u \u0026lt;\u0026lt; std::endl; 来看看当u等于0时发生了什么，这次迭代输出0，然后继续执行for语句里的表达式。表达式\u0026ndash;u从u当中减去1，得到的结果-1并不满足无符号数的要求，此时像所有表示范围之外的其他数字一样，-1被自动地转换成一个合法的无符号数。假设int类型占32位，则当u等于0时，–u的结果将会是-1%\\(2^{32}\\)=4294967295。\n一种解决的办法是，用while语句来代替for语句，因为前者让我们能够在输出变量之前（而非之后）先减1：\n1 2 3 4 5 unsigned u = 11; // 确定要输出的最大数，从比它大1的数开始 while (u \u0026gt; 0){ --u; std::cout \u0026lt;\u0026lt; u \u0026lt;\u0026lt; std::endl; } ","permalink":"http://localhost:1313/posts/2016-09-24-memo-about-cpp-built-in-types/","summary":"\u003cp\u003e今天阅读《C++ Primer, 5e》的第二章，介绍C++的基本内置类型，觉得有一些平时工作容易出错的知识点，现摘录如下：\u003c/p\u003e\n\u003chr\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eunsigned\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e c \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e; \u003cspan style=\"color:#75715e\"\u003e// 假设char占8比特，c的值为255\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e当我们赋给无符号类型一个超出它表示范围的值时，结果是初始值对无符号类型表示数值总数取模后的余数。例如，8比特大小的unsigned char可以表示0至255区间内的值，如果我们赋了一个区间以外的值，则实际的结果是该值对256取模后所得的余数。因此，把-1赋给8比特大小的unsigned char所得的结果是255。\u003c/p\u003e\n\u003chr\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003esigned\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e c2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e256\u003c/span\u003e; \u003cspan style=\"color:#75715e\"\u003e// 假设char占8比特，c2的值是未定义的\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e当我们赋给带符号类型一个超出它表示范围的值时，结果是未定义的（undefined）。此时，程序可能继续工作、可能崩溃，也可能生成垃圾数据。\u003c/p\u003e\n\u003chr\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eunsigned\u003c/span\u003e u \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e42\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003ecout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eendl; \u003cspan style=\"color:#75715e\"\u003e// 输出-84\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003ecout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e u \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eendl; \u003cspan style=\"color:#75715e\"\u003e// 如果int占32位，输出4294967264\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e在第一个输出表达式里，两个（负）整数相加并得到了期望的结果。在第二个输出表达式里，相加前首先把整数-42转换成无符号数。把负数转换成无符号数类似于直接给无符号数赋一个负数，结果等于这个负数加上无符号数的模。unsigned (int)的取值范围是0~\\(2^{32}-1\\)，所以总数有\\(2^{32}\\)个数，-42%\\(2^{32}\\)=-42+\\(2^{32}\\)，u+i=10+(-42+\\(2^{32}\\))=4294967264。\u003c/p\u003e\n\u003chr\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eunsigned\u003c/span\u003e u1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e42\u003c/span\u003e, u2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003ecout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e u1 \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e u2 \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eendl; \u003cspan style=\"color:#75715e\"\u003e// 正确：输出32\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003ecout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e u2 \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e u1 \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eendl; \u003cspan style=\"color:#75715e\"\u003e// 正确：不过，结果是取模后的值\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e当从无符号数中减去一个值时，不管这个值是不是无符号数，我们都必须确保结果不能是一个负值。\u003c/p\u003e","title":"C++基本数据类型备忘"},{"content":"随机矩阵是这样一类方阵，其元素为非负实数，且行和或列和为1。如果行和为1，则称为行随机矩阵；如果列和为1，则称为列随机矩阵；如果行和和列和都为1，则称为双随机矩阵。\n前面我们介绍的谷歌矩阵和HMM中的转移矩阵都属于随机矩阵，所以随机矩阵也称为概率矩阵、转移矩阵、或马尔可夫矩阵。\n随机矩阵有一个性质，就是其所有特征值的绝对值小于等于1，且其最大特征值为1。下面通过两种方法证明这个结论。\n首先，随机矩阵A肯定有特征值1，即\n$$\\begin{equation}A\\vec 1=1\\times\\vec 1\\end{equation}$$其中的单位向量\\(\\vec 1=(\\frac{1}{n},…,\\frac{1}{n})^T\\)，因为A的行和为1，所以上述等式成立。即1是A的特征值。\n反证法 假设存在大于1的特征值\\(\\lambda\\)，则有\\(A\\vec x=\\lambda\\vec x\\)。令\\(x_k\\)是\\(\\vec x\\)中最大的元素。又因为A的元素非负，且行和为1，所以\\(\\lambda\\vec x\\)中的每个元素都是\\(\\vec x\\)中元素的凸组合，所以\\(\\lambda\\vec x\\)中的每个元素都小于等于\\(x_k\\)。\n$$\\begin{equation}a_{i1}x_1+a_{i2}x_2+…+a_{in}x_n=\\lambda x_i\\leq x_k\\end{equation}$$但是如果\\(\\lambda\u003e1\\)，则\\(\\lambda x_k\u003ex_k\\)，和(2)式矛盾，所以\\(\\lambda\\leq 1\\)。又因为(1)式，所以A的最大特征值为1。\n常规证法 设对称随机矩阵A的特征值\\(\\lambda\\)对应的特征向量为\\(x\\)（为了简便，以下省略向量符号），则有\\(Ax=\\lambda x\\)，即\\(x^TAx=\\lambda x^Tx\\)，欲证明\\(|\\lambda|\\leq 1\\)，只需证明\n$$\\begin{equation}\\lambda=\\frac{\u003c x, Ax \u003e}{\u003c x, x \u003e}\\leq 1\\end{equation}$$根据定义有：\n$$\\begin{equation}\u003c x, Ax \u003e=\\sum_{i=1}^na_{ii}x_i^2+2\\sum_{i \u003c j, i\\sim j}a_{ij}x_ix_j\\end{equation}$$对于\\(i \u003c j, i\\sim j\\)，有：\n$$\\begin{equation}a_{ij}(x_i-x_j)^2=a_{ij}x_i^2-2a_{ij}x_ix_j+a_{ij}x_j^2\\end{equation}$$两边求和并移项得到：\n$$ \\begin{equation} \\begin{array} \\displaystyle{2\\sum_{i \u003c j}}a_{ij}x_ix_j \u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003c j}a_{ij}x_j^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003c j}a_{ji}x_j^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003e j}a_{ij}x_i^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_i(\\sum_{j\\neq i}a_{ij}x_i^2)-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_i(x_i^2(1-a_{ii}))-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2} \\end{array} \\end{equation} $$第2、3个等号都是因为A是对称矩阵，所以可以把\\(a_{ij}\\)替换为\\(a_{ji}\\)，然后互换\\(i,j\\)下标。最后一个等号是因为A的行和为1。\n将(6)代入(4)式得到：\n$$ \\begin{equation} \\begin{array} \\displaystyle{\u003c x, Ax \u003e} \u0026 = \u0026 \\displaystyle{\\sum_ia_{ii}x_i^2+\\sum_i(x_i^2(1-a_{ii}))-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_ix_i^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2} \\end{array} \\end{equation} $$所以：\n$$ \\begin{equation} \\lambda=\\frac{\u003c x, Ax \u003e}{\u003c x, x\u003e} = \\frac{\\sum_ix_i^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}{\\sum_ix_i^2} \\leq 1 \\end{equation} $$又因为(1)式，所以A的最大特征值为1。\n随机矩阵的第二大特征值\\(\\lambda(A)\\)也很有用，\\(1-\\lambda(A)\\)被称为矩阵A的谱间隔（spectral gap），它衡量的是最大特征值和第二大特征值之间的差值。\\(\\lambda(A)\\)在马尔可夫随机游走领域有重要作用。\n$$\\begin{equation}||A^lp-1||_2\\leq \\lambda^l(A)\\end{equation}$$上式是扩张图（Expander）领域很重要的一个引理，A为扩张图的邻接矩阵，\\(p\\)为在所有节点上的初始概率分布，\\(\\lambda(A)\\)为矩阵A的第二大的特征值。因为\\(\\lambda(A)\u003c1\\)，所以\\(\\lambda^l(A)\\)会快速的降到0。也就是说，在初始概率\\(p\\)上，随机游走\\(l\\)步，很快就能达到均匀分布\\(1\\)。\n参考：《Computational Complexity: A Modern Approach》书上7.A.RANDOM WALKS AND EIGENVALUES介绍了这个引理，该书地址：http://theory.cs.princeton.edu/complexity/，相关内容在第153页。\n","permalink":"http://localhost:1313/posts/2016-08-23-the-eigenvalue-of-stochastic-matrix/","summary":"\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Stochastic_matrix\"\u003e随机矩阵\u003c/a\u003e是这样一类方阵，其元素为非负实数，且行和或列和为1。如果行和为1，则称为行随机矩阵；如果列和为1，则称为列随机矩阵；如果行和和列和都为1，则称为双随机矩阵。\u003c/p\u003e\n\u003cp\u003e前面我们介绍的\u003ca href=\"https://bitjoy.net/posts/2016-08-04-googles-pagerank-and-beyond/\"\u003e谷歌矩阵\u003c/a\u003e和\u003ca href=\"https://bitjoy.net/posts/2016-08-20-introduction-to-hmm-1/\"\u003eHMM中的转移矩阵\u003c/a\u003e都属于随机矩阵，所以随机矩阵也称为概率矩阵、转移矩阵、或马尔可夫矩阵。\u003c/p\u003e\n\u003cp\u003e随机矩阵有一个性质，就是其所有特征值的绝对值小于等于1，且其最大特征值为1。下面通过两种方法证明这个结论。\u003c/p\u003e\n\u003cp\u003e首先，随机矩阵A肯定有特征值1，即\u003c/p\u003e\n$$\\begin{equation}A\\vec 1=1\\times\\vec 1\\end{equation}$$\u003cp\u003e其中的单位向量\\(\\vec 1=(\\frac{1}{n},…,\\frac{1}{n})^T\\)，因为A的行和为1，所以上述等式成立。即1是A的特征值。\u003c/p\u003e\n\u003ch1 id=\"反证法\"\u003e\u003ca href=\"https://mikespivey.wordpress.com/2013/01/17/eigenvalue-stochasti/\"\u003e反证法\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e假设存在大于1的特征值\\(\\lambda\\)，则有\\(A\\vec x=\\lambda\\vec x\\)。令\\(x_k\\)是\\(\\vec x\\)中最大的元素。又因为A的元素非负，且行和为1，所以\\(\\lambda\\vec x\\)中的每个元素都是\\(\\vec x\\)中元素的凸组合，所以\\(\\lambda\\vec x\\)中的每个元素都小于等于\\(x_k\\)。\u003c/p\u003e\n$$\\begin{equation}a_{i1}x_1+a_{i2}x_2+…+a_{in}x_n=\\lambda x_i\\leq x_k\\end{equation}$$\u003cp\u003e但是如果\\(\\lambda\u003e1\\)，则\\(\\lambda x_k\u003ex_k\\)，和(2)式矛盾，所以\\(\\lambda\\leq 1\\)。又因为(1)式，所以A的最大特征值为1。\u003c/p\u003e\n\u003ch1 id=\"常规证法\"\u003e常规证法\u003c/h1\u003e\n\u003cp\u003e设\u003cstrong\u003e对称随机矩阵A\u003c/strong\u003e的特征值\\(\\lambda\\)对应的特征向量为\\(x\\)（为了简便，以下省略向量符号），则有\\(Ax=\\lambda x\\)，即\\(x^TAx=\\lambda x^Tx\\)，欲证明\\(|\\lambda|\\leq 1\\)，只需证明\u003c/p\u003e\n$$\\begin{equation}\\lambda=\\frac{\u003c x, Ax \u003e}{\u003c x, x \u003e}\\leq 1\\end{equation}$$\u003cp\u003e根据定义有：\u003c/p\u003e\n$$\\begin{equation}\u003c x, Ax \u003e=\\sum_{i=1}^na_{ii}x_i^2+2\\sum_{i \u003c j, i\\sim j}a_{ij}x_ix_j\\end{equation}$$\u003cp\u003e对于\\(i \u003c j, i\\sim j\\)，有：\u003c/p\u003e\n$$\\begin{equation}a_{ij}(x_i-x_j)^2=a_{ij}x_i^2-2a_{ij}x_ix_j+a_{ij}x_j^2\\end{equation}$$\u003cp\u003e两边求和并移项得到：\u003c/p\u003e\n$$\n\\begin{equation}\n\\begin{array}\n\\displaystyle{2\\sum_{i \u003c j}}a_{ij}x_ix_j \u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003c j}a_{ij}x_j^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\\n\u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003c j}a_{ji}x_j^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003e j}a_{ij}x_i^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\\n\u0026 = \u0026 \\displaystyle{\\sum_i(\\sum_{j\\neq i}a_{ij}x_i^2)-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\\n\u0026 = \u0026 \\displaystyle{\\sum_i(x_i^2(1-a_{ii}))-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\n\\end{array}\n\\end{equation}\n$$\u003cp\u003e第2、3个等号都是因为A是对称矩阵，所以可以把\\(a_{ij}\\)替换为\\(a_{ji}\\)，然后互换\\(i,j\\)下标。最后一个等号是因为A的行和为1。\u003c/p\u003e","title":"随机矩阵及其特征值"},{"content":"马尔可夫聚类算法（The Markov Cluster Algorithm, MCL）是一种快速可扩展的基于图的聚类算法。它的基本思想为：在一个稀疏图G中，如果某个区域A是稠密的（是一个聚类），则在A中随机游走k步，还在A内的概率很大，也就是说，A内的k步路径（k-length path）很多。所以我们可以在图中随机游走k步，如果某个区域连通的概率很大，则该区域是一个聚类。随机游走的下一步只和当前所处节点有关，也就是说这是一个马尔可夫的随机游走过程。\n我们用一个例子来演示马尔可夫聚类算法的过程。\n上图是一个很小的网络，我们用肉眼大概能看出有三个聚类，分别是左边的{1,6,7,10}，中间的{2,3,5}和右边的{4,8,9,11,12}。我们用MCL看看结果如何。\n为了随机游走，我们常用邻接矩阵来表示图，如果i,j有边，则N[i][j]=1，否则N[i][j]=0。又随机游走可能有自回路，所以加上单位矩阵I，得到矩阵N+I。\nMCL有两个关键的步骤，分别是Expansion和Inflation。\nExpansion就是不断对矩阵进行幂次运算，相当于随机游走。假设随机游走了2步，则得到如下图的关联矩阵\\((N+I)^2\\)，第1行第10列为4，说明1到10的2-length path有4条：1→6→10，1→7→10，1→1→10，1→10→10。随机游走k步之后，\\((N+I)^k[i][j]\\)越大，说明\\(i\\)和\\(j\\)之间的连通性越强。\n$$\\begin{equation}Expand(M)=M^k\\end{equation}$$\nInflation是为了增强更强的连接，减弱更弱的连接，只有这样才能得到边界比较明确的聚类。MCL的做法是对元素做幂次运算，然后按列归一化，公式为：\n$$\\begin{equation}(\\Gamma_rM)_{pq}=\\frac{(M_{pq})^r}{\\sum_{i=1}^k(M_{iq})^r}\\end{equation}$$参数经验值是\\(k=r=2\\)。不断做Expansion和Inflation操作，直到算法收敛，得到若干个聚类。中间过程请点此查看，下图为最终结果。\n从图中可以看出，和1有边的只剩下6,7,10了，所以得到聚类{1,6,7,10}，同理能得到聚类{2,3,5}和{4,8,9,11,12} ，和我们肉眼得到的结果是一致的。\nMCL算法的原理很简单，得到的聚类效果也不错。下面总结一下MCL的算法过程：\n给定无向图G，Expansion和Inflation的参数\\(k\\)和\\(r\\) 生成G的邻接矩阵\\(N\\) 添加自回路，得到矩阵\\(N+I\\) 循环对\\(N+I\\)做Expansion和Inflation操作，即计算公式(1)和(2)，直到收敛 根据最终得到的矩阵，进行划分聚类 此算法是我在上《生物信息学中的算法设计》课上是学到的，当时觉得这个算法真是神奇，如此简单，但又如此有效，实在高明。查阅文献得知，此为Stijn van Dongen的博士论文，本博客的图片均来自其博士论文，想深入了解图聚类算法，请下载他的论文。\n","permalink":"http://localhost:1313/posts/2016-08-22-the-markov-cluster-algorithm/","summary":"\u003cp\u003e马尔可夫聚类算法（The Markov Cluster Algorithm, MCL）是一种快速可扩展的基于图的聚类算法。它的基本思想为：在一个稀疏图G中，如果某个区域A是稠密的（是一个聚类），则在A中随机游走k步，还在A内的概率很大，也就是说，A内的k步路径（k-length path）很多。所以我们可以在图中随机游走k步，如果某个区域连通的概率很大，则该区域是一个聚类。随机游走的下一步只和当前所处节点有关，也就是说这是一个马尔可夫的随机游走过程。\u003c/p\u003e\n\u003cp\u003e我们用一个例子来演示马尔可夫聚类算法的过程。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"mcl-1\" loading=\"lazy\" src=\"/posts/2016-08-22-the-markov-cluster-algorithm/mcl-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上图是一个很小的网络，我们用肉眼大概能看出有三个聚类，分别是左边的{1,6,7,10}，中间的{2,3,5}和右边的{4,8,9,11,12}。我们用MCL看看结果如何。\u003c/p\u003e\n\u003cp\u003e为了随机游走，我们常用邻接矩阵来表示图，如果i,j有边，则N[i][j]=1，否则N[i][j]=0。又随机游走可能有自回路，所以加上单位矩阵I，得到矩阵N+I。\u003c/p\u003e\n\u003cp\u003eMCL有两个关键的步骤，分别是Expansion和Inflation。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExpansion\u003c/strong\u003e就是不断对矩阵进行幂次运算，相当于随机游走。假设随机游走了2步，则得到如下图的关联矩阵\\((N+I)^2\\)，第1行第10列为4，说明1到10的2-length path有4条：1→6→10，1→7→10，1→1→10，1→10→10。随机游走k步之后，\\((N+I)^k[i][j]\\)越大，说明\\(i\\)和\\(j\\)之间的连通性越强。\u003c/p\u003e\n$$\\begin{equation}Expand(M)=M^k\\end{equation}$$\u003cp\u003e\u003cimg alt=\"mcl-2\" loading=\"lazy\" src=\"/posts/2016-08-22-the-markov-cluster-algorithm/mcl-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInflation\u003c/strong\u003e是为了增强更强的连接，减弱更弱的连接，只有这样才能得到边界比较明确的聚类。MCL的做法是对元素做幂次运算，然后按列归一化，公式为：\u003c/p\u003e\n$$\\begin{equation}(\\Gamma_rM)_{pq}=\\frac{(M_{pq})^r}{\\sum_{i=1}^k(M_{iq})^r}\\end{equation}$$\u003cp\u003e参数经验值是\\(k=r=2\\)。不断做Expansion和Inflation操作，直到算法收敛，得到若干个聚类。\u003ca href=\"/posts/2016-08-22-the-markov-cluster-algorithm/mcl-3.pdf\"\u003e中间过程请点此查看\u003c/a\u003e，下图为最终结果。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"mcl-4\" loading=\"lazy\" src=\"/posts/2016-08-22-the-markov-cluster-algorithm/mcl-4.png\"\u003e\u003c/p\u003e\n\u003cp\u003e从图中可以看出，和1有边的只剩下6,7,10了，所以得到聚类{1,6,7,10}，同理能得到聚类{2,3,5}和{4,8,9,11,12} ，和我们肉眼得到的结果是一致的。\u003c/p\u003e\n\u003cp\u003eMCL算法的原理很简单，得到的聚类效果也不错。下面总结一下MCL的算法过程：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e给定无向图G，Expansion和Inflation的参数\\(k\\)和\\(r\\)\u003c/li\u003e\n\u003cli\u003e生成G的邻接矩阵\\(N\\)\u003c/li\u003e\n\u003cli\u003e添加自回路，得到矩阵\\(N+I\\)\u003c/li\u003e\n\u003cli\u003e循环对\\(N+I\\)做Expansion和Inflation操作，即计算公式(1)和(2)，直到收敛\u003c/li\u003e\n\u003cli\u003e根据最终得到的矩阵，进行划分聚类\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e此算法是我在上《生物信息学中的算法设计》课上是学到的，当时觉得这个算法真是神奇，如此简单，但又如此有效，实在高明。查阅文献得知，此为\u003ca href=\"http://www.micans.org/mcl/\"\u003eStijn van Dongen的博士论文\u003c/a\u003e，本博客的图片均来自其博士论文，想深入了解图聚类算法，请\u003ca href=\"http://micans.org/mcl/lit/svdthesis.pdf.gz\"\u003e下载他的论文\u003c/a\u003e。\u003c/p\u003e","title":"马尔可夫聚类算法"},{"content":"上一回介绍了HMM的解码问题，今天我们介绍HMM的学习问题和识别问题，先来看学习问题。\n正如上一回结束时所说，HMM的学习问题是：仅已知观测序列\\(\\vec y\\)，要估计出模型参数组\\(\\vec\\lambda=(\\mu,A,B)\\)，其中\\(\\mu\\)为初始概率分布向量，\\(A\\)为转移概率矩阵，\\(B\\)为发射概率矩阵。\n算法设计 求解HMM的参数学习问题，就是求解如下的最优化问题：\n$$\\begin{equation} P(\\vec Y = \\vec y|\\hat \\lambda)=\\max\\limits_{\\vec \\lambda} P(\\vec Y = \\vec y|\\vec \\lambda)\\end{equation}$$也就是找一个参数\\(\\vec \\lambda\\)，使得模型在该参数下最有可能产生当前的观测\\(\\vec y\\)。如果使用极大似然法求解，对于似然函数\\(P(\\vec Y=\\vec y|\\vec \\lambda)=\\sum\\limits_{i_1,…,i_T}\\mu_{i_1}b_{i_1y_1}a_{i_1i_2}…a_{i_{T-1}i_T}b_{i_Ty_T}\\)而言，这个最大值问题的计算量过大，在实际中是不可能被采用的。为此，人们构造了一个递推算法，使其能相当合理地给出模型参数\\(\\vec \\lambda\\)的粗略估计。其核心思想是：并不要求备选\\(\\vec\\lambda\\)使得\\(P(\\vec Y=\\vec y|\\vec \\lambda)\\)达到最大或局部极大，而只要求使\\(P(\\vec Y=\\vec y|\\vec \\lambda)\\)相当大，从而使计算变为实际可能。\nEM算法 为此，我们定义一个描述模型“趋势”的量\\(Q(\\vec\\lambda^*|\\vec\\lambda)\\)代替似然函数\\(P(\\vec Y=\\vec y|\\vec\\lambda)\\)，其定义为：\n$$\\begin{equation} Q(\\vec\\lambda^*|\\vec\\lambda)=\\sum\\limits_{\\vec x}P(\\vec x,\\vec y|\\vec\\lambda)\\ln P(\\vec x,\\vec y|\\vec\\lambda^*)\\end{equation}$$利用在\\(0 \u003c x \u003c 1\\)时，不等式\\(\\ln x\\leq x-1\\)成立，可以证明：\n$$\\begin{equation} Q(\\vec\\lambda^*|\\vec\\lambda)-Q(\\vec\\lambda|\\vec\\lambda)\\leq P(\\vec Y=\\vec y|\\vec\\lambda^*)-P(\\vec Y=\\vec y|\\vec\\lambda)\\end{equation}$$由此可见，对于固定的\\(\\vec\\lambda\\)，只要\\(Q(\\vec\\lambda^*|\\vec\\lambda)\u003eQ(\\vec\\lambda|\\vec\\lambda)\\)，就有\\(P(\\vec Y=\\vec y|\\vec\\lambda^*)\u003eP(\\vec Y=\\vec y|\\vec\\lambda)\\)。于是想把模型\\(\\vec\\lambda_m\\)修改为更好的模型\\(\\vec\\lambda_{m+1}\\)，只需找\\(\\vec\\lambda_{m+1}\\)使得：\n$$\\begin{equation}Q(\\vec\\lambda_{m+1}|\\vec\\lambda_m)=\\sup_{\\vec\\lambda}Q(\\vec\\lambda|\\vec\\lambda_m)\\end{equation}$$即只要把\\(Q(\\vec\\lambda|\\vec\\lambda_m)\\)关于\\(\\vec\\lambda\\)的最大值处取成\\(\\vec\\lambda_{m+1}\\)，就有\\(P(\\vec Y=\\vec y|\\vec\\lambda_{m+1})\u003eP(\\vec Y=\\vec y|\\vec\\lambda_m)\\)。\n这样得到的模型序列\\(\\{\\vec\\lambda_m\\}\\)能保证\\(P(\\vec Y=\\vec y|\\vec\\lambda_m)\\)关于\\(m\\)是严格递增的，虽然在这里还不能在理论上证明\\(P(\\vec Y=\\vec y|\\vec\\lambda_m)\\)收敛到\\(\\max_{\\vec\\lambda}P(\\vec Y=\\vec y|\\vec\\lambda)\\)，但是当\\(m\\)充分大时，\\(\\vec\\lambda_m\\)也还能提供在实际中较为满意的粗略近似。\n综上论述，我们把如上得到的近似模型列\\(\\vec\\lambda_m\\)的方法归结为两个步骤：\nE步骤（求期望）：计算$$\\begin{equation}Q(\\vec\\lambda^*|\\vec\\lambda)=\\sum\\limits_{\\vec x}P(\\vec x,\\vec y|\\vec\\lambda)\\ln P(\\vec x,\\vec y|\\vec\\lambda^*)\\end{equation}$$ M步骤（求最大）：求\\(\\vec\\lambda_{m+1}\\)使$$\\begin{equation}Q(\\vec\\lambda_{m+1}|\\vec\\lambda_m)=\\sup_{\\vec\\lambda}Q(\\vec\\lambda|\\vec\\lambda_m)\\end{equation}$$ 这两个步骤合起来构成的算法，称为期望最大化（Expectation-maximization, EM）算法。EM算法是针对在测量数据不完全时，求参数的一种近似于最大似然估计的统计方法。\nBaum-Welch算法 隐Markov模型中的M-步骤的解可以有显式表示，这就是一组把模型参数修改为新的模型参数的递推公式，这组公式正好是在隐Markov模型中普遍应用的著名的Baum-Welch公式。\n$$\\begin{equation}\\hat\\mu_i^{m+1}=\\frac{P(\\vec Y=\\vec y,X_1=i|\\vec\\lambda_m)}{P(\\vec Y=\\vec y|\\vec\\lambda_m)}=\\gamma_1(i)\\end{equation}$$$$\\begin{equation}\\hat a_{ij}^{m+1}=\\frac{\\sum\\limits_{t=1}^{T-1}P(X_t=i,X_{t+1}=j|\\vec Y=\\vec y,\\vec\\lambda_m)}{\\sum\\limits_{t=1}^{T-1}P(X_t=i|\\vec Y=\\vec y,\\vec\\lambda_m)}\\triangleq\\frac{\\sum\\limits_{t=1}^{T-1}\\xi_t(i,j)}{\\sum\\limits_{t=1}^{T-1}\\gamma_t(i)}\\end{equation}$$$$\\begin{equation}\\hat b_{il}^{m+1}=\\frac{\\sum\\limits_{t=1}^TP(\\vec Y=\\vec y,X_t=i|\\vec\\lambda_m)I_{\\{l\\}}(y_t)}{\\sum\\limits_{t=1}^TP(\\vec Y=\\vec y,X_t=i|\\vec\\lambda_m)}\\triangleq\\frac{\\sum\\limits_{t=1,y_t=l}^T\\gamma_t(i)}{\\sum\\limits_{t=1}^T\\gamma_t(i)}\\end{equation}$$Baum-Welch算法用到了如下几个公式：\n向前算法，\\(\\alpha_t(i)=P(Y_1=y_1,…,Y_t=y_t,X_t=i|\\lambda)\\)，满足前\\(t\\)个状态，推进到满足前\\(t+1\\)个状态（\\(t\\rightarrow t+1\\)）：\\(\\begin{equation}\\alpha_1(i)=\\mu_ib_{iy_1}\\quad \\alpha_{t+1}(i)=\\sum\\limits_j\\alpha_t(j)a_{ji}b_{iy_{t+1}}\\end{equation}\\) 向后算法，\\(\\beta_t(i)=P(Y_{t+1}=y_{t+1},…,Y_T=y_T|X_t=i,\\lambda)\\)，满足后\\(t-1\\)个状态，推进到满足后\\(t\\)个状态（\\(t+1\\rightarrow t\\)）：\\(\\begin{equation}\\beta_T(i)=1\\quad \\beta_t(i)=\\sum\\limits_j\\beta_{t+1}(j)a_{ij}b_{jy_{t+1}}\\end{equation}\\) 向前向后算法，满足所有观测状态，且\\(t\\)时刻的隐状态为\\(i\\)：\\(\\begin{equation}\\gamma_t(i)=P(X_t=i|\\vec Y=\\vec y,\\vec\\lambda)=\\frac{P(\\vec Y=\\vec y,X_t=i|\\vec\\lambda)}{\\sum\\limits_iP(\\vec Y=\\vec y,X_t=i|\\vec\\lambda)}=\\frac{\\alpha_t(i)\\beta_t(i)}{\\sum\\limits_i\\alpha_t(i)\\beta_t(i)}\\end{equation}\\) 以及记号\\(\\begin{equation}\\xi_t(i,j)\\triangleq P(X_t=i,X_{t+1}=j|\\vec Y=\\vec y,\\vec\\lambda)=\\frac{\\alpha_t(i)a_{ij}b_{jy_{t+1}}\\beta_{t+1}(j)}{\\sum\\limits_i\\alpha_t(i)\\beta_t(i)}\\end{equation}\\) 算法流程 最后，我们可以将Baum-Welch公式应用于EM算法中的M步骤，来逐步改进模型参数\\(\\vec\\lambda\\)。为了使训练结果更加可信，通常应该有多条观测序列。假设输入为所有\\(k\\)次观测序列集合\\(S\\)和收敛阈值\\(\\epsilon\\)，输出为训练得到的模型参数\\(\\hat{\\vec\\lambda}\\)，则基于Baum-Welch公式的EM算法求解HMM学习问题的伪代码如下：\n现在要求解另一个韦小宝的骰子的问题：韦小宝有两个有偏的骰子A,B，A,B掷出相同点数的概率不同，每次韦小宝随机拿一个骰子并投掷，记录下正面朝上的点数，重复100次，得到一条长度为100的点数序列，如此重复100次，得到100条类似的序列。现只给定这100条点数序列，要求解出韦小宝每次投掷的是哪个骰子，并分析这两个骰子有什么区别。\n这就是一个典型的HMM的参数学习问题，利用上述伪代码可以很快的求解出模型参数\\(\\vec\\lambda\\)，A,B的发射概率就是它们的不同点。\nHMM的识别问题是：对于一个特定的观测链\\(\\vec y\\)，已知它可能是由已经学习好的若干模型之一所得的观测，要决定此观测究竟是得自其中哪一个模型，这称为识别问题。\n判决步骤：\n根据参数求出在每一个模型中，出现给定样本的概率\\(P(\\vec Y=\\vec y|\\lambda_k)\\)，归一化就得到给定样本来自每个模型的概率\\(P(\\lambda_k|\\vec Y=\\vec y)\\)。 利用贝叶斯原理，就可以得到最好模型的猜测。 本博客开头提到，要求解\\(P(\\vec Y=\\vec y|\\lambda)\\)需要指数时间（\\(O(N^T)\\)）：\n$$\\begin{equation}P(\\vec Y=\\vec y|\\vec \\lambda)=\\sum\\limits_{i_1,…,i_T}\\mu_{i_1}b_{i_1y_1}a_{i_1i_2}…a_{i_{T-1}i_T}b_{i_Ty_T}\\end{equation}$$所以可以利用向前算法（式(10)）或者向后算法（式(11)），对应的结果分别为：\n$$\\begin{equation}P(\\vec Y=\\vec y|\\lambda)=\\sum_{i=1}^N\\alpha_T(i)\\end{equation}$$$$\\begin{equation}P(\\vec Y=\\vec y|\\lambda)=\\sum_{i=1}^N\\beta_1(i)\\mu_ib_{iy_1}\\end{equation}$$然后利用贝叶斯公式得到\\(P(\\lambda_k|\\vec Y=\\vec y)\\)，使结果最大的\\(k\\)即为所求模型。\n","permalink":"http://localhost:1313/posts/2016-08-21-introduction-to-hmm-2/","summary":"\u003cp\u003e上一回介绍了HMM的解码问题，今天我们介绍HMM的学习问题和识别问题，先来看学习问题。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e正如上一回结束时所说，\u003cstrong\u003eHMM的学习问题\u003c/strong\u003e是：仅已知观测序列\\(\\vec y\\)，要估计出模型参数组\\(\\vec\\lambda=(\\mu,A,B)\\)，其中\\(\\mu\\)为初始概率分布向量，\\(A\\)为转移概率矩阵，\\(B\\)为发射概率矩阵。\u003c/p\u003e\n\u003ch1 id=\"算法设计\"\u003e算法设计\u003c/h1\u003e\n\u003cp\u003e求解HMM的参数学习问题，就是求解如下的最优化问题：\u003c/p\u003e\n$$\\begin{equation} P(\\vec Y = \\vec y|\\hat \\lambda)=\\max\\limits_{\\vec \\lambda} P(\\vec Y = \\vec y|\\vec \\lambda)\\end{equation}$$\u003cp\u003e也就是找一个参数\\(\\vec \\lambda\\)，使得模型在该参数下最有可能产生当前的观测\\(\\vec y\\)。如果使用极大似然法求解，对于似然函数\\(P(\\vec Y=\\vec y|\\vec \\lambda)=\\sum\\limits_{i_1,…,i_T}\\mu_{i_1}b_{i_1y_1}a_{i_1i_2}…a_{i_{T-1}i_T}b_{i_Ty_T}\\)而言，这个最大值问题的计算量过大，在实际中是不可能被采用的。为此，人们构造了一个递推算法，使其能相当合理地给出模型参数\\(\\vec \\lambda\\)的粗略估计。其核心思想是：并不要求备选\\(\\vec\\lambda\\)使得\\(P(\\vec Y=\\vec y|\\vec \\lambda)\\)达到最大或局部极大，而只要求使\\(P(\\vec Y=\\vec y|\\vec \\lambda)\\)相当大，从而使计算变为实际可能。\u003c/p\u003e\n\u003ch1 id=\"em算法\"\u003eEM算法\u003c/h1\u003e\n\u003cp\u003e为此，我们定义一个描述模型“趋势”的量\\(Q(\\vec\\lambda^*|\\vec\\lambda)\\)代替似然函数\\(P(\\vec Y=\\vec y|\\vec\\lambda)\\)，其定义为：\u003c/p\u003e\n$$\\begin{equation} Q(\\vec\\lambda^*|\\vec\\lambda)=\\sum\\limits_{\\vec x}P(\\vec x,\\vec y|\\vec\\lambda)\\ln P(\\vec x,\\vec y|\\vec\\lambda^*)\\end{equation}$$\u003cp\u003e利用在\\(0 \u003c x \u003c 1\\)时，不等式\\(\\ln x\\leq x-1\\)成立，可以证明：\u003c/p\u003e\n$$\\begin{equation} Q(\\vec\\lambda^*|\\vec\\lambda)-Q(\\vec\\lambda|\\vec\\lambda)\\leq P(\\vec Y=\\vec y|\\vec\\lambda^*)-P(\\vec Y=\\vec y|\\vec\\lambda)\\end{equation}$$\u003cp\u003e由此可见，对于固定的\\(\\vec\\lambda\\)，只要\\(Q(\\vec\\lambda^*|\\vec\\lambda)\u003eQ(\\vec\\lambda|\\vec\\lambda)\\)，就有\\(P(\\vec Y=\\vec y|\\vec\\lambda^*)\u003eP(\\vec Y=\\vec y|\\vec\\lambda)\\)。于是想把模型\\(\\vec\\lambda_m\\)修改为更好的模型\\(\\vec\\lambda_{m+1}\\)，只需找\\(\\vec\\lambda_{m+1}\\)使得：\u003c/p\u003e\n$$\\begin{equation}Q(\\vec\\lambda_{m+1}|\\vec\\lambda_m)=\\sup_{\\vec\\lambda}Q(\\vec\\lambda|\\vec\\lambda_m)\\end{equation}$$\u003cp\u003e即只要把\\(Q(\\vec\\lambda|\\vec\\lambda_m)\\)关于\\(\\vec\\lambda\\)的最大值处取成\\(\\vec\\lambda_{m+1}\\)，就有\\(P(\\vec Y=\\vec y|\\vec\\lambda_{m+1})\u003eP(\\vec Y=\\vec y|\\vec\\lambda_m)\\)。\u003c/p\u003e\n\u003cp\u003e这样得到的模型序列\\(\\{\\vec\\lambda_m\\}\\)能保证\\(P(\\vec Y=\\vec y|\\vec\\lambda_m)\\)关于\\(m\\)是严格递增的，虽然在这里还不能在理论上证明\\(P(\\vec Y=\\vec y|\\vec\\lambda_m)\\)收敛到\\(\\max_{\\vec\\lambda}P(\\vec Y=\\vec y|\\vec\\lambda)\\)，但是当\\(m\\)充分大时，\\(\\vec\\lambda_m\\)也还能提供在实际中较为满意的粗略近似。\u003c/p\u003e","title":"隐马尔可夫模型及其应用（2）学习问题\u0026识别问题"},{"content":"隐马尔可夫模型（Hidden Markov Model, HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。\n先举一个简单的例子以直观地理解HMM的实质——韦小宝的骰子。\n假设韦小宝有两个骰子，一个正常的骰子A，A以1/6的概率均等的出现每个点；一个不正常的骰子B，B出现5,6点数的概率为0.3，出现其他点数的概率为0.1。显然投掷B更容易出现大的点数。每次试验第一次投掷时，韦小宝会以0.4的概率出千（即投掷B）。但是在一次试验中，韦小宝不太可能一直出千，所以骰子会在A、B之间转换，比如这次投了B，下次可能会以0.1的概率投A。A、B之间的转移概率如下图。\n某一次试验，我们观察到韦小宝掷出的骰子序列为\\(O=(1,3,4,5,5,6,6,3,2,6)\\)，请问韦小宝什么时候出千了。这个问题就可以通过HMM求解。\nHMM有2个状态：\n观测状态。我们观察到的骰子序列称为观测状态\\(\\mathbf{Y}=\\{y_1,y_2,…,y_T\\}\\) 隐状态。隐含在每个观测状态里面的是隐状态\\(\\mathbf{X}=\\{x_1,x_2,…,x_T\\}\\) T是时间，也可以认为是观测的次数。HMM有3个参数：\n初始分布\\(\\mathbf{\\mu}=(\\mu_i)\\)，\\(\\mu_i=Pr(x_1=i)\\)，即第一次观测时，每个隐状态出现的概率 转移概率矩阵\\(A=(a_{ij})\\)，\\(a_{ij}=Pr(x_{t+1}=j|x_t=i)\\)，即t时刻的隐状态为i，t+1时刻转移到隐状态j的概率 发射概率矩阵\\(B=(b_{il})\\)，\\(b_{il}=Pr(y_t=l|x_t=i)\\)，即t时候隐状态为i的情况下，观测到状态为l的概率 参数\\(\\mathbf{\\lambda=\\{\\mu,A,B\\}}\\)称为HMM的模型参数。具体到上面的例子，我们有初始分布和转移概率为：\n发射概率为：\n观测状态为\\(\\mathbf{Y}=(1,3,4,5,5,6,6,3,2,6)\\)，问题就是求解出隐状态\\(\\mathbf{X}\\)，此问题被称为HMM的解码问题，可以由著名的维特比算法（Viterbi algorithm）解决。\n解码问题是要求出使得观测状态\\(Y\\)出现概率最大的隐状态\\(X\\)，假设有N个隐状态（本例为2），共有T个时刻（本例为10），则每个时刻有N个取值可能，则共有\\(N^T\\)条可能的隐状态链（本例为\\(2^{10}\\)）。我们需要求出每一条隐状态链下T个发射概率的乘积，然后取最大值，这是指数时间复杂度的（\\(O(N^T)\\)）。\n但是Viterbi算法是一个动态规划算法，只需多项式时间即可解决该问题。该算法的原理很好理解，假设我们求得到\\(s_{i2}\\)的最大概率路径为下图中的红线\\(s_{11}\\rightarrow s_{22}\\rightarrow … s_{i2}\\)，则在求经过\\(s_{i2}\\)到\\(s_{(i+1)1}\\)的最大概率路径时，不需要再测试\\(s_{13}\\rightarrow s_{21}\\rightarrow s_{i2}\\rightarrow s_{(i+1)1}\\)这条路径（下图蓝线），因为显然已经知道红线概率大于蓝线概率了。图中还有很多类似蓝线的路径都可以不用计算了，大大提高了求解速度。\n因为计算第\\(i+1\\)时刻的累积概率只和第\\(i\\)时刻的概率有关，每次至多计算\\(N*N\\)个概率乘积（可以从\\(i\\)时刻的\\(N\\)个状态到达\\(i+1\\)时刻的某个状态，\\(i+1\\)时刻共有\\(N\\)个状态），最多计算T次（共T个时刻），所以时间复杂度降到了\\(O(N^2T)\\)。\n下面我们形式化的描述Viterbi算法。\n假设\\(\\delta_t(i)\\)为\\(t\\)时刻取到隐状态\\(i\\)，且1~t的观测状态都符合观测值\\(Y\\)的各个路径的最大概率，即\n$$ \\begin{equation}\\delta_t(i)=\\underset{i_1,…,i_{t-1}}{\\max}Pr(X_t=i,X_{t-1}=i_{t-1},…,X_1=i_1,Y_t=y_t,…,Y_1=y_1|\\mathbf{\\lambda})\\end{equation} $$联系上图，可认为\\(\\delta_t(i)\\)为红线。则递推公式为：\n$$ \\begin{equation}\\delta_{t+1}(i)=b_{iy_{t+1}}\\underset{j}{\\max}(\\delta_t(j)a_{ji})\\end{equation} $$由\\(j\\)到\\(i\\)的转移概率，再乘上\\(i\\)发射\\(y_{t+1}\\)的概率。\n在初始时刻\\(t=1\\)，有：\n$$ \\begin{equation}\\delta_1(i)=\\mu_ib_{iy_1}\\end{equation} $$最后的全局最大概率为\\(\\underset{j}{\\max}\\delta_T(j)\\)。为了得到完整路径，我们保留每一隐状态取得最大概率时的上一隐状态，即：\n$$ \\begin{equation}\\psi_{t+1}(i)=j^*\\end{equation} $$其中\\(j^*\\)要满足\n$$ \\begin{equation}\\delta_{t+1}(i)=b_{iy_{t+1}}\\delta_t(j^*)a_{j^*i}\\end{equation} $$最后使用如下回溯法得到所有最佳隐状态：\n$$\\begin{equation}X_T=i^*\\in\\{i:\\delta_T(i)=\\underset{j}{\\max}\\delta_T(j)\\}\\end{equation}$$$$\\begin{equation}X_t=\\psi_{t+1}(X_{t+1})\\end{equation}$$下面我们利用Viterbi算法来求解韦小宝的骰子这个例子。\n\\(t=1\\)时，\\(y_1=1\\)，有\\(\\delta_1(A)=0.6*1/6=0.1\\)，\\(\\delta_1(B)=0.4*0.1=0.04\\)。\n\\(t=2\\)时，\\(y_2=3\\)，有：\n隐状态为A：a）A-\u0026gt;A有\\(\\delta_2(A)=(1/6)*0.1*0.8=1.33*10^{-2}\\)；b）B-\u0026gt;A有\\(\\delta_2(A)=(1/6)*0.04*0.1=6.6*10^{-4}\\)。所以A-\u0026gt;A，\\(\\psi_2(A)=A\\)。 隐状态为B：a）A-\u0026gt;B有\\(\\delta_2(B)=0.1*0.1*0.2=2*10^{-3}\\)；b）B-\u0026gt;B有\\(\\delta_2(B)=0.1*0.04*0.9=3.6*10^{-3}\\)。所以B-\u0026gt;B，\\(\\psi_2(B)=B\\)。 如此计算下去，可以得到如下表： \\(t=10\\)时最大概率为\\(\\delta_{10}(B)\\)，经过回溯得到最佳隐状态为：\n所以HMM很神奇吧，可以抓住韦小宝从第5次开始就一直在出千，而且出千之后，掷出的点数大部分为5和6。\nViterbi算法还可用于解决语音识别或者拼音输入法。我们知道中文的一个拼音可以对应多个汉字，连续的一段拼音就能组成成千上万种可能的句子，哪一个句子才是最佳候选呢？我们可以把每个拼音当成观测状态，同音的汉字当成可能的隐状态。通过背景语料库统计得到每个汉字出现在词首的概率、汉字之间的转移概率和汉字与拼音之间的发射概率，这样我们就能得到模型参数，然后利用Viterbi算法求解出一个最佳的隐状态序列，这样就能完成一个简易的拼音输入法。\nHMM在实际中主要有3个方面的应用，分别是：\n从一段观测序列\\(\\mathbf{Y}\\)及已知模型\\(\\mathbf{\\lambda=(\\mu,A,B)}\\)出发，估计出隐状态\\(\\mathbf{X}\\)的最佳值，称为解码问题，这是状态估计问题。这篇博客讨论的就是这个问题。 从一段观测序列\\(\\mathbf{Y}\\)出发，估计模型参数组\\(\\mathbf{\\lambda=(\\mu,A,B)}\\)，称为学习问题，就是参数估计问题。 对于一个特定的观测链\\(\\mathbf{Y}\\)，已知它可能是由已经学习好的若干模型之一所得的观测，要决定此观测究竟是得自其中哪一个模型，这称为识别问题，就是分类问题。 关于HMM的学习问题和识别问题，请听下回分解。\n","permalink":"http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/","summary":"\u003cp\u003e\u003ca href=\"https://zh.wikipedia.org/wiki/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B\"\u003e隐马尔可夫模型\u003c/a\u003e（Hidden Markov Model, HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。\u003c/p\u003e\n\u003cp\u003e先举一个简单的例子以直观地理解HMM的实质——韦小宝的骰子。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-2\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e假设韦小宝有两个骰子，一个正常的骰子A，A以1/6的概率均等的出现每个点；一个不正常的骰子B，B出现5,6点数的概率为0.3，出现其他点数的概率为0.1。显然投掷B更容易出现大的点数。每次试验\u003cstrong\u003e第一次投掷时\u003c/strong\u003e，韦小宝会以0.4的概率出千（即投掷B）。但是在一次试验中，韦小宝不太可能一直出千，所以骰子会在A、B之间转换，比如这次投了B，下次可能会以0.1的概率投A。A、B之间的转移概率如下图。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-1\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e某一次试验，我们观察到韦小宝掷出的骰子序列为\\(O=(1,3,4,5,5,6,6,3,2,6)\\)，请问韦小宝什么时候出千了。这个问题就可以通过HMM求解。\u003c/p\u003e\n\u003cp\u003eHMM有2个状态：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e观测状态。我们观察到的骰子序列称为观测状态\\(\\mathbf{Y}=\\{y_1,y_2,…,y_T\\}\\)\u003c/li\u003e\n\u003cli\u003e隐状态。隐含在每个观测状态里面的是隐状态\\(\\mathbf{X}=\\{x_1,x_2,…,x_T\\}\\)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eT是时间，也可以认为是观测的次数。HMM有3个参数：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e初始分布\\(\\mathbf{\\mu}=(\\mu_i)\\)，\\(\\mu_i=Pr(x_1=i)\\)，即第一次观测时，每个隐状态出现的概率\u003c/li\u003e\n\u003cli\u003e转移概率矩阵\\(A=(a_{ij})\\)，\\(a_{ij}=Pr(x_{t+1}=j|x_t=i)\\)，即t时刻的隐状态为i，t+1时刻转移到隐状态j的概率\u003c/li\u003e\n\u003cli\u003e发射概率矩阵\\(B=(b_{il})\\)，\\(b_{il}=Pr(y_t=l|x_t=i)\\)，即t时候隐状态为i的情况下，观测到状态为l的概率\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e参数\\(\\mathbf{\\lambda=\\{\\mu,A,B\\}}\\)称为HMM的模型参数。具体到上面的例子，我们有初始分布和转移概率为：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-3\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e发射概率为：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-4\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-4.png\"\u003e\u003c/p\u003e\n\u003cp\u003e观测状态为\\(\\mathbf{Y}=(1,3,4,5,5,6,6,3,2,6)\\)，问题就是求解出隐状态\\(\\mathbf{X}\\)，此问题被称为HMM的解码问题，可以由著名的\u003ca href=\"https://zh.wikipedia.org/wiki/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95\"\u003e维特比算法（Viterbi algorithm）\u003c/a\u003e解决。\u003c/p\u003e\n\u003cp\u003e解码问题是要求出使得观测状态\\(Y\\)出现概率最大的隐状态\\(X\\)，假设有N个隐状态（本例为2），共有T个时刻（本例为10），则每个时刻有N个取值可能，则共有\\(N^T\\)条可能的隐状态链（本例为\\(2^{10}\\)）。我们需要求出每一条隐状态链下T个发射概率的乘积，然后取最大值，这是指数时间复杂度的（\\(O(N^T)\\)）。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-5\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-5.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是Viterbi算法是一个动态规划算法，只需多项式时间即可解决该问题。该算法的原理很好理解，假设我们求得到\\(s_{i2}\\)的最大概率路径为下图中的红线\\(s_{11}\\rightarrow s_{22}\\rightarrow … s_{i2}\\)，则在求经过\\(s_{i2}\\)到\\(s_{(i+1)1}\\)的最大概率路径时，不需要再测试\\(s_{13}\\rightarrow s_{21}\\rightarrow s_{i2}\\rightarrow s_{(i+1)1}\\)这条路径（下图蓝线），因为显然已经知道红线概率大于蓝线概率了。图中还有很多类似蓝线的路径都可以不用计算了，大大提高了求解速度。\u003c/p\u003e\n\u003cp\u003e因为计算第\\(i+1\\)时刻的累积概率只和第\\(i\\)时刻的概率有关，每次至多计算\\(N*N\\)个概率乘积（可以从\\(i\\)时刻的\\(N\\)个状态到达\\(i+1\\)时刻的某个状态，\\(i+1\\)时刻共有\\(N\\)个状态），最多计算T次（共T个时刻），所以时间复杂度降到了\\(O(N^2T)\\)。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-6\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-6.png\"\u003e\u003c/p\u003e\n\u003cp\u003e下面我们形式化的描述Viterbi算法。\u003c/p\u003e\n\u003cp\u003e假设\\(\\delta_t(i)\\)为\\(t\\)时刻取到隐状态\\(i\\)，且1~t的观测状态都符合观测值\\(Y\\)的各个路径的最大概率，即\u003c/p\u003e\n$$\n\\begin{equation}\\delta_t(i)=\\underset{i_1,…,i_{t-1}}{\\max}Pr(X_t=i,X_{t-1}=i_{t-1},…,X_1=i_1,Y_t=y_t,…,Y_1=y_1|\\mathbf{\\lambda})\\end{equation}\n$$\u003cp\u003e联系上图，可认为\\(\\delta_t(i)\\)为红线。则递推公式为：\u003c/p\u003e\n$$\n\\begin{equation}\\delta_{t+1}(i)=b_{iy_{t+1}}\\underset{j}{\\max}(\\delta_t(j)a_{ji})\\end{equation}\n$$\u003cp\u003e由\\(j\\)到\\(i\\)的转移概率，再乘上\\(i\\)发射\\(y_{t+1}\\)的概率。\u003c/p\u003e\n\u003cp\u003e在初始时刻\\(t=1\\)，有：\u003c/p\u003e\n$$\n\\begin{equation}\\delta_1(i)=\\mu_ib_{iy_1}\\end{equation}\n$$\u003cp\u003e最后的全局最大概率为\\(\\underset{j}{\\max}\\delta_T(j)\\)。为了得到完整路径，我们保留每一隐状态取得最大概率时的上一隐状态，即：\u003c/p\u003e\n$$\n\\begin{equation}\\psi_{t+1}(i)=j^*\\end{equation}\n$$\u003cp\u003e其中\\(j^*\\)要满足\u003c/p\u003e\n$$\n\\begin{equation}\\delta_{t+1}(i)=b_{iy_{t+1}}\\delta_t(j^*)a_{j^*i}\\end{equation}\n$$\u003cp\u003e最后使用如下回溯法得到所有最佳隐状态：\u003c/p\u003e\n$$\\begin{equation}X_T=i^*\\in\\{i:\\delta_T(i)=\\underset{j}{\\max}\\delta_T(j)\\}\\end{equation}$$$$\\begin{equation}X_t=\\psi_{t+1}(X_{t+1})\\end{equation}$$\u003cp\u003e下面我们利用Viterbi算法来求解韦小宝的骰子这个例子。\u003c/p\u003e\n\u003cp\u003e\\(t=1\\)时，\\(y_1=1\\)，有\\(\\delta_1(A)=0.6*1/6=0.1\\)，\\(\\delta_1(B)=0.4*0.1=0.04\\)。\u003c/p\u003e\n\u003cp\u003e\\(t=2\\)时，\\(y_2=3\\)，有：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e隐状态为A：a）A-\u0026gt;A有\\(\\delta_2(A)=(1/6)*0.1*0.8=1.33*10^{-2}\\)；b）B-\u0026gt;A有\\(\\delta_2(A)=(1/6)*0.04*0.1=6.6*10^{-4}\\)。所以A-\u0026gt;A，\\(\\psi_2(A)=A\\)。\u003c/li\u003e\n\u003cli\u003e隐状态为B：a）A-\u0026gt;B有\\(\\delta_2(B)=0.1*0.1*0.2=2*10^{-3}\\)；b）B-\u0026gt;B有\\(\\delta_2(B)=0.1*0.04*0.9=3.6*10^{-3}\\)。所以B-\u0026gt;B，\\(\\psi_2(B)=B\\)。\n如此计算下去，可以得到如下表：\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-7\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-7.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\\(t=10\\)时最大概率为\\(\\delta_{10}(B)\\)，经过回溯得到最佳隐状态为：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-8\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-8.png\"\u003e\u003c/p\u003e\n\u003cp\u003e所以HMM很神奇吧，可以抓住韦小宝从第5次开始就一直在出千，而且出千之后，掷出的点数大部分为5和6。\u003c/p\u003e\n\u003cp\u003eViterbi算法还可用于解决语音识别或者拼音输入法。我们知道中文的一个拼音可以对应多个汉字，连续的一段拼音就能组成成千上万种可能的句子，哪一个句子才是最佳候选呢？我们可以把每个拼音当成观测状态，同音的汉字当成可能的隐状态。通过背景语料库统计得到每个汉字出现在词首的概率、汉字之间的转移概率和汉字与拼音之间的发射概率，这样我们就能得到模型参数，然后利用Viterbi算法求解出一个最佳的隐状态序列，这样就能完成一个简易的拼音输入法。\u003c/p\u003e\n\u003cp\u003eHMM在实际中主要有3个方面的应用，分别是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e从一段观测序列\\(\\mathbf{Y}\\)及已知模型\\(\\mathbf{\\lambda=(\\mu,A,B)}\\)出发，估计出隐状态\\(\\mathbf{X}\\)的最佳值，称为解码问题，这是状态估计问题。这篇博客讨论的就是这个问题。\u003c/li\u003e\n\u003cli\u003e从一段观测序列\\(\\mathbf{Y}\\)出发，估计模型参数组\\(\\mathbf{\\lambda=(\\mu,A,B)}\\)，称为学习问题，就是参数估计问题。\u003c/li\u003e\n\u003cli\u003e对于一个特定的观测链\\(\\mathbf{Y}\\)，已知它可能是由已经学习好的若干模型之一所得的观测，要决定此观测究竟是得自其中哪一个模型，这称为识别问题，就是分类问题。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e关于HMM的学习问题和识别问题，请听下回分解。\u003c/p\u003e","title":"隐马尔可夫模型及其应用（1）简介\u0026解码问题"},{"content":"半年的时光又过去了，圆满结束了一年的集中教学任务，离开了美丽的雁栖湖，回到闹市中关村。\n这半年基本上延续了研一上学期的高强度学习，四门硬课。《高级算法》这门课由四位大师级的老师授课，内容囊括了近似算法、计算复杂性、随机算法、局部搜索、全息规约等，完全是神一样的课。最后复习的时候，大家都生不如死啊，不过经过一个月的挑灯夜战，我还是取得了97分的好成绩，值了。\n《大数据系统与大规模数据分析》这门课的老师是一个年轻的海归，要求很严格，有专门的算法检查平时作业是否抄袭，真的有好几个同学因为抄袭而得0分。这门课的大作业是在GraphLite上实现SVD，我带领队员经过一个月的努力比较圆满的完成了大作业，感谢组里的编程大神。\n《机器学习方法与应用》是面向电子学院的课程，讲得太简单，考试基本是概念题，不建议选修。\n《生物信息学中的算法设计》这门课其实应该叫统计机器学习在生物信息领域的应用，讲的内容比《机器学习方法与应用》的内容更深更广。不过内容太多也难以消化，好好做大作业应该会有不少收获。\n集中教学一年，研一上的GPA是87分，研一下的GPA是89.3分，平均是88.1分。\n除了完成若干个课程大作业，这学期还完成了两个组内大作业，分别是倒排索引和蛋白质搜索引擎，也多谢XN和我一起查Bug、对答案。（天啊，我半年是做了多少个大作业啊…）\n这半年每周二回所和师姐交接任务，真是要感谢天真呆萌的JL师姐，当初保研的时候就被师姐的热情所感染，现在又有幸接替师姐的接力棒，好幸运。\n要说上半年最大的收获，应该是收获了一枚女朋友吧~没错，就是我这篇博客里提到的欣欣~真的没想到这么聊得来，一起吃饭、看电影、聊代码、骑行、游山玩水。这半年拍的照片，比我前22年拍的照片还多。和她在一起很开心，不过有时候也会很累，身体累（羸弱），有时候也心累，毕竟课程压力和组内压力摆在那里，白天去玩了，晚上还是要加班补回来的。有时候冷落了她，也会感到愧疚不安，特别是我在复习《高级算法》期间，两人都很少见面，那一次是真的惹欣欣生气了:-(\n总结一下在雁栖湖一年的收获，大致有如下图的四个方面：\n看看年初计划的完成情况：\n完成国科大下学期的课程任务：完成 接手pLink软件：完成 刷完LeetCode所有题目：上半年基本没刷题，下半年一定完成 读10本书：目前读了《数学之美》、《大话设计模式》、《我不知道该说什么，关于死亡还是爱情》、《男人来自火星、女人来自金星，卷I》，还差6本，下半年加油！ 去电影院看10场电影：目前看了《美人鱼》、《北京遇上西雅图之不二情书》、《忍者神龟2：破影而出》，还差好多… 改正坐姿：有一段时间刻意改正了，但是这东西貌似改不过来？ 下半年就进入实验室，开始科研实战了，做交联的师兄师姐都毕业了，留下我一个人，感觉好艰难，希望我能顺利进入角色，协助师兄把文章发了，维护好pLink2的软件，并且开发集群版。\n","permalink":"http://localhost:1313/posts/2016-08-20-2016-mid-year-summary/","summary":"\u003cp\u003e半年的时光又过去了，圆满结束了一年的集中教学任务，离开了美丽的雁栖湖，回到闹市中关村。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ucas-schedule-2016-spring\" loading=\"lazy\" src=\"/posts/2016-08-20-2016-mid-year-summary/ucas-schedule-2016-spring.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这半年基本上延续了研一上学期的高强度学习，四门硬课。《高级算法》这门课由四位大师级的老师授课，内容囊括了近似算法、计算复杂性、随机算法、局部搜索、全息规约等，完全是神一样的课。最后复习的时候，大家都生不如死啊，不过经过一个月的挑灯夜战，我还是取得了97分的好成绩，值了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"advanced-algorithm\" loading=\"lazy\" src=\"/posts/2016-08-20-2016-mid-year-summary/advanced-algorithm.png\"\u003e\u003c/p\u003e\n\u003cp\u003e《大数据系统与大规模数据分析》这门课的老师是一个年轻的海归，要求很严格，有专门的算法检查平时作业是否抄袭，真的有好几个同学因为抄袭而得0分。这门课的大作业是在GraphLite上实现SVD，我带领队员经过一个月的努力比较圆满的完成了大作业，感谢组里的编程大神。\u003c/p\u003e\n\u003cp\u003e《机器学习方法与应用》是面向电子学院的课程，讲得太简单，考试基本是概念题，不建议选修。\u003c/p\u003e\n\u003cp\u003e《生物信息学中的算法设计》这门课其实应该叫统计机器学习在生物信息领域的应用，讲的内容比《机器学习方法与应用》的内容更深更广。不过内容太多也难以消化，好好做大作业应该会有不少收获。\u003c/p\u003e\n\u003cp\u003e集中教学一年，研一上的GPA是87分，研一下的GPA是89.3分，平均是88.1分。\u003c/p\u003e\n\u003cp\u003e除了完成若干个课程大作业，这学期还完成了两个组内大作业，分别是倒排索引和蛋白质搜索引擎，也多谢XN和我一起查Bug、对答案。（天啊，我半年是做了多少个大作业啊…）\u003c/p\u003e\n\u003cp\u003e这半年每周二回所和师姐交接任务，真是要感谢天真呆萌的JL师姐，当初保研的时候就被师姐的热情所感染，现在又有幸接替师姐的接力棒，好幸运。\u003c/p\u003e\n\u003cp\u003e要说上半年最大的收获，应该是收获了一枚女朋友吧~没错，就是我\u003cdel\u003e这篇博客\u003c/del\u003e里提到的欣欣~真的没想到这么聊得来，一起吃饭、看电影、聊代码、骑行、游山玩水。这半年拍的照片，比我前22年拍的照片还多。和她在一起很开心，不过有时候也会很累，身体累（羸弱），有时候也心累，毕竟课程压力和组内压力摆在那里，白天去玩了，晚上还是要加班补回来的。有时候冷落了她，也会感到愧疚不安，特别是我在复习《高级算法》期间，两人都很少见面，那一次是真的惹欣欣生气了:-(\u003c/p\u003e\n\u003cp\u003e总结一下在雁栖湖一年的收获，大致有如下图的四个方面：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"mid-year 2016 summary\" loading=\"lazy\" src=\"/posts/2016-08-20-2016-mid-year-summary/mid-year-2016-summary.png\"\u003e\u003c/p\u003e\n\u003cp\u003e看看\u003ca href=\"https://bitjoy.net/posts/2016-01-03-2016-happy-new-year/\"\u003e年初计划\u003c/a\u003e的完成情况：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cdel\u003e完成国科大下学期的课程任务：完成\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e\u003cdel\u003e接手pLink软件：完成\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e刷完LeetCode所有题目：上半年基本没刷题，下半年一定完成\u003c/li\u003e\n\u003cli\u003e读10本书：目前读了《数学之美》、《大话设计模式》、《我不知道该说什么，关于死亡还是爱情》、《男人来自火星、女人来自金星，卷I》，还差6本，下半年加油！\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影：目前看了《美人鱼》、《北京遇上西雅图之不二情书》、《忍者神龟2：破影而出》，还差好多…\u003c/li\u003e\n\u003cli\u003e改正坐姿：有一段时间刻意改正了，但是这东西貌似改不过来？\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e下半年就进入实验室，开始科研实战了，做交联的师兄师姐都毕业了，留下我一个人，感觉好艰难，希望我能顺利进入角色，协助师兄把文章发了，维护好pLink2的软件，并且开发集群版。\u003c/p\u003e","title":"2016年中总结"},{"content":"我们知道常规的快速排序算法是一个不稳定的算法，也就是两个相等的数排序之后的顺序可能和在原序列中的顺序不同。这是因为当选定一个枢轴（pivot），要把其他数分到小于pivot和大于pivot的两边的时候，不同实现的分法不一样。\n下面我实现了一种稳定版快速排序算法，在Partition函数中保持了原序列中所有元素的相对顺序，只把pivot放到了它的正确位置。具体方法是三遍扫描原序列：1）第一遍先把小于pivot的元素按先后顺序放到tmp里，然后把pivot放到它的正确位置tmp[k]；2）第二遍把大于pivot的元素按先后顺序追加在tmp里，这样除了pivot以前的其他元素，都保持了和原序列中一样的顺序；3）第三遍把tmp赋值回原数组A。\n当排序算法稳定之后，就可以借此统计逆序数了，文件Q5.txt中共包含100000个不同的整数，每行一个数。我们可以使用稳定版快速排序算法对其排序，并统计出其中的逆序数个数。\n具体的Python 3实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Tue Oct 6 00:21:37 2015 @author: bitjoy \u0026#34;\u0026#34;\u0026#34; import time inversions = 0 def Partition(A, p, r): global inversions tmp = [0] * (r-p+1) pivot = A[p] k = 0 for i in range(p+1, r+1): # first if A[i] \u0026lt; pivot: tmp[k] = A[i] inversions = inversions + i – k – p k = k + 1 tmp[k] = pivot ans = k + p k = k + 1 for i in range(p+1, r+1): # second if A[i] \u0026gt; pivot: tmp[k] = A[i] k = k + 1 k = 0 for i in range(p, r+1): # third A[i] = tmp[k] k = k + 1 return ans def QuickSortAndCount(A, p, r): if p \u0026lt; r: q = Partition(A, p, r) QuickSortAndCount(A, p, q-1) QuickSortAndCount(A, q + 1, r) if __name__ == \u0026#34;__main__\u0026#34;: Q5 = open(\u0026#39;Q5.txt\u0026#39;, encoding = \u0026#39;utf-8\u0026#39;) data = [ int(x) for x in Q5 ] Q5.close() start = time.clock() QuickSortAndCount(data, 0, len(data) -1 ) end = time.clock() print(\u0026#34;number of inversions:%d\\ntime:%f s\u0026#34;%(inversions,end-start)) 虽然这种快排的时间复杂度还是O(nlgn)，但是在Partition函数中扫描了3次数组，并且借用了辅助数组tmp，不再是in-place排序算法，所以排序用时会比常规快排或者归并排序要慢。\n","permalink":"http://localhost:1313/posts/2016-08-18-the-stable-quick-sort/","summary":"\u003cp\u003e我们知道常规的快速排序算法是一个不稳定的算法，也就是两个相等的数排序之后的顺序可能和在原序列中的顺序不同。这是因为当选定一个枢轴（pivot），要把其他数分到小于pivot和大于pivot的两边的时候，不同实现的分法不一样。\u003c/p\u003e\n\u003cp\u003e下面我实现了一种稳定版快速排序算法，在Partition函数中保持了原序列中所有元素的相对顺序，只把pivot放到了它的正确位置。具体方法是三遍扫描原序列：1）第一遍先把小于pivot的元素按先后顺序放到tmp里，然后把pivot放到它的正确位置tmp[k]；2）第二遍把大于pivot的元素按先后顺序追加在tmp里，这样除了pivot以前的其他元素，都保持了和原序列中一样的顺序；3）第三遍把tmp赋值回原数组A。\u003c/p\u003e\n\u003cp\u003e当排序算法稳定之后，就可以借此统计逆序数了，文件\u003ca href=\"/posts/2016-08-18-the-stable-quick-sort/Q5.zip\"\u003eQ5.txt\u003c/a\u003e中共包含100000个\u003cstrong\u003e不同\u003c/strong\u003e的整数，每行一个数。我们可以使用稳定版快速排序算法对其排序，并统计出其中的逆序数个数。\u003c/p\u003e\n\u003cp\u003e具体的Python 3实现如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e46\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -*- coding: utf-8 -*-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eCreated on Tue Oct 6 00:21:37 2015\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e@author: bitjoy\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e time\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003einversions \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ePartition\u003c/span\u003e(A, p, r):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eglobal\u003c/span\u003e inversions\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    tmp \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e (r\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ep\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    pivot \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e A[p]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(p\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, r\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e): \u003cspan style=\"color:#75715e\"\u003e# first\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e A[i] \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e pivot:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            tmp[k] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e A[i]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            inversions \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e inversions \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e k \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e p\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        tmp[k] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pivot\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ans \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e p\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(p\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, r\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e): \u003cspan style=\"color:#75715e\"\u003e# second\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e A[i] \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e pivot:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            tmp[k] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e A[i]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(p, r\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e): \u003cspan style=\"color:#75715e\"\u003e# third\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        A[i] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e tmp[k]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e ans\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eQuickSortAndCount\u003c/span\u003e(A, p, r):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e p \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e r:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    q \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Partition(A, p, r)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    QuickSortAndCount(A, p, q\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    QuickSortAndCount(A, q \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, r)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    Q5 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e open(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Q5.txt\u0026#39;\u003c/span\u003e, encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;utf-8\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [ int(x) \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e x \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e Q5 ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    Q5\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    start \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e time\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclock()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    QuickSortAndCount(data, \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, len(data) \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    end \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e time\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclock()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;number of inversions:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003etime:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%f\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e s\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(inversions,end\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003estart))\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e虽然这种快排的时间复杂度还是O(nlgn)，但是在Partition函数中扫描了3次数组，并且借用了辅助数组tmp，不再是in-place排序算法，所以排序用时会比常规快排或者归并排序要慢。\u003c/p\u003e","title":"稳定版快速排序算法"},{"content":"哈弗曼编码是一个很经典的压缩算法，压缩率能达到50%，甚至更低。它的基本原理包括四个步骤：\n统计文件中每个字符出现的频率。 构建一个哈弗曼树。建树的过程是不断的合并频率最小的两个节点，父亲节点的频率为两个孩子节点的频率之和。如此循环直到合并成一个根节点。叶子节点为不同的字符及其频率。 生成哈弗曼编码。从树根开始对树进行编码，比如进入左孩子的边标记为0，进入右孩子的边标记为1，这里的0和1都是二进制位。这样之后，每个叶子节点都有一个唯一的二进制编码，这就是哈弗曼编码。频率越低的字符哈弗曼编码越长，频率越高的字符哈弗曼编码越短，这样就能起到压缩的效果。 第二遍扫描文件，把字符转换为对应的哈弗曼编码，保存成压缩文件。 解压缩的过程就是解析二进制位，然后查找哈弗曼树，每找到一个叶子节点，就解析出一个字符，直到解析完所有二进制位。下面详细解释我的C++实现。\n首先定义一个哈弗曼编码类，对外只提供压缩Compress和解压缩Decompress两个接口。值得注意的是有一个Node结构体，用于构成哈弗曼树的节点。此外count_node的key是字符频率，value是所在节点，且是multimap类型的，所以count_node会自动按字符频率有小到大排序，在构建哈弗曼树时，每次只需要取count_node的前两个节点进行合并即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class HuffmanCode { public: HuffmanCode(); void Compress(string src, string dest); void Decompress(string src, string dest); virtual ~HuffmanCode(); private: void CountLetter(string src); void ConstructHuffmanTree(); void GenerateHuffmanCode(); void WriteHuffmanCode(ofstream \u0026amp;os); void Compressing(string src, string dest); void InsertIntoHuffmanTree(char letter, string \u0026amp;code, int \u0026amp;k); void ConstructHuffmanTreeFromFile(ifstream \u0026amp;is); void Decompressing(ifstream \u0026amp;is, ofstream \u0026amp;os); map\u0026lt;char, int\u0026gt; letter_count; typedef struct Node { int id; bool is_leaf; char letter; int parent, lchild, rchild; Node() { } Node(int i, bool il, char lt, int p, int lc, int rc) : id(i), is_leaf(il), letter(lt), parent(p), lchild(lc), rchild(rc) { } }; multimap\u0026lt;int, Node\u0026gt; count_node; vector\u0026lt;Node\u0026gt; huffman_tree; map\u0026lt;char, vector\u0026lt;char\u0026gt;\u0026gt; letter_hcode; // hufman code for each letter }; 压缩函数Compress串起压缩的整个流程，包括统计字符频率、构建哈弗曼树、生成哈弗曼编码以及最后将原始文件转换成哈弗曼编码的二进制文件。\n1 2 3 4 5 6 void HuffmanCode::Compress(string src, string dest) { CountLetter(src); ConstructHuffmanTree(); GenerateHuffmanCode(); Compressing(src, dest); } Compress中的前三个函数不难，值得注意的是Compressing函数，它是真正进行压缩的函数。函数首先调用WriteHuffmanCode把每个字符的哈弗曼编码写入文件，作为文件头信息，以备后续解压使用。然后循环读取文件，把字符转换为哈弗曼二进制编码。每8 bit哈弗曼二进制位构成一个char byte，多个byte构成os_buf，当os_buf满时写入文件。\n在最后边界位置，需要小心处理。因为可能所有二进制位并不刚好是8的整数倍，所以在压缩文件的末尾用 1 byte作为标记。如果flag为0x0，则所有二进制位刚好是8的整数倍，无需特别处理。如果flag为0x01，则还剩小于8个二进制位需要单独放在一个byte里面，所以还需要一个byte存储剩余多少个二进制位。假设最后3个bytes分别为x,y,z，则如果z==0x0，则x,y常规解析；如果z==0x01，则只解析x中的前y个bits。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 void HuffmanCode::Compressing(string src, string dest) { ifstream is(src, ios::binary); ofstream os(dest, ios::binary); WriteHuffmanCode(os); char *is_buf = new char[MAX_LEN], *os_buf = new char[MAX_LEN]; list\u0026lt;char\u0026gt; tmp_hcode; int start_pos = 0, i, j, k, len, t; char c, flag = 0x0; // flag for the last byte list\u0026lt;char\u0026gt;::iterator it; while (is.peek() != EOF) { is.read(is_buf, MAX_LEN); len = is.gcount(); for (i = 0; i \u0026lt; len; i++) tmp_hcode.insert(tmp_hcode.end(), letter_hcode[is_buf[i]].begin(), letter_hcode[is_buf[i]].end()); k = tmp_hcode.size() / 8; t = 0; i = 0; it = tmp_hcode.begin(); while (i \u0026lt; 8 * k) { c = 0x0; for (j = i; j \u0026lt;= i + 7; j++) { c = (*it == \u0026#39;1\u0026#39;) ? (c | (1 \u0026lt;\u0026lt; (i + 7 – j))) : c; // char -\u0026gt; bit it++; } os_buf[t++] = c; i += 8; } os.write(os_buf, t * sizeof(char)); tmp_hcode.erase(tmp_hcode.begin(), it); } c = 0x0; i = 7; bool done = true; while (it != tmp_hcode.end()) { done = false; c = (*it == \u0026#39;1\u0026#39;) ? (c | (1 \u0026lt;\u0026lt; i)) : c; // left bits i–; it++; } if (!done) { os.write(\u0026amp;c, sizeof(char)); c = 7 – i; // only c bits used in the last byte os.write(\u0026amp;c, sizeof(char)); flag = 0x1; // the last byte is incomplete } os.write(\u0026amp;flag, sizeof(char)); is.close(); os.close(); delete[] is_buf; delete[] os_buf; } 函数Decompress串起解压缩的整个流程。首先调用ConstructHuffmanTreeFromFile读取压缩文件的头信息，也就是字符和哈弗曼编码的对应关系，然后构建哈弗曼树。同样Decompressing是实际的解压缩过程，它不断读取哈弗曼二进制位，然后从哈弗曼树根节点开始往下走，直到到达一个叶子节点，则解析出一个字符，如此循环，直到解析完所有二进制位。\n1 2 3 4 5 6 7 8 void HuffmanCode::Decompress(string src, string dest) { ifstream is(src, ios::binary); ofstream os(dest, ios::binary); ConstructHuffmanTreeFromFile(is); Decompressing(is, os); is.close(); os.close(); } 完整项目可以查看我的Github项目HZip，Windows版可执行程序请点此下载。\n压缩命令为：\n1 HZip.exe -c original_file_path compressed_file_path 解压缩命令为：\n1 HZip.exe -x compressed_file_path decompressed_file_path 下面是一些测试结果。\n//还没有统计好。。。\n//看来还是7Z道高一尺。\n我后面发现HZip甚至可以压缩/解压缩中文txt、pdf、图片、视频等（其实只要是ASCII编码的应该都可以吧？）。但是中文压缩效率较低，图片视频等压缩之后的大小几乎和没压缩是一样的:-(其实这很好理解，因为哈弗曼编码是根据字符频率的差异来编码的，英文只有26个字母加上一些符号，压缩效率肯定很高，而中文是以字为单位存储的，所以当以char读取来编码的时候，不同char的数量肯定更多，导致压缩效率较低。图片和视频就不得而知了。\n在测试的时候我发现压缩和解压缩大文件的时候，速度极其的慢，简直到了不能忍的地步，下一步我将分析性能瓶颈，争取把速度提高到可以接受的范围。\n","permalink":"http://localhost:1313/posts/2016-08-18-the-implementation-of-huffman-code/","summary":"\u003cp\u003e哈弗曼编码是一个很经典的压缩算法，压缩率能达到50%，甚至更低。它的基本原理包括四个步骤：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e统计文件中每个字符出现的频率。\u003c/li\u003e\n\u003cli\u003e构建一个哈弗曼树。建树的过程是不断的合并频率最小的两个节点，父亲节点的频率为两个孩子节点的频率之和。如此循环直到合并成一个根节点。叶子节点为不同的字符及其频率。\u003c/li\u003e\n\u003cli\u003e生成哈弗曼编码。从树根开始对树进行编码，比如进入左孩子的边标记为0，进入右孩子的边标记为1，这里的0和1都是二进制位。这样之后，每个叶子节点都有一个唯一的二进制编码，这就是哈弗曼编码。频率越低的字符哈弗曼编码越长，频率越高的字符哈弗曼编码越短，这样就能起到压缩的效果。\u003c/li\u003e\n\u003cli\u003e第二遍扫描文件，把字符转换为对应的哈弗曼编码，保存成压缩文件。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e解压缩的过程就是解析二进制位，然后查找哈弗曼树，每找到一个叶子节点，就解析出一个字符，直到解析完所有二进制位。下面详细解释我的C++实现。\u003c/p\u003e\n\u003cp\u003e首先定义一个哈弗曼编码类，对外只提供压缩Compress和解压缩Decompress两个接口。值得注意的是有一个Node结构体，用于构成哈弗曼树的节点。此外count_node的key是字符频率，value是所在节点，且是multimap类型的，所以count_node会自动按字符频率有小到大排序，在构建哈弗曼树时，每次只需要取count_node的前两个节点进行合并即可。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e37\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eHuffmanCode\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    HuffmanCode();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eCompress\u003c/span\u003e(string src, string dest);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eDecompress\u003c/span\u003e(string src, string dest);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evirtual\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003eHuffmanCode();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eprivate\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e CountLetter(string src);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eConstructHuffmanTree\u003c/span\u003e();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eGenerateHuffmanCode\u003c/span\u003e();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eWriteHuffmanCode\u003c/span\u003e(ofstream \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003eos);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eCompressing\u003c/span\u003e(string src, string dest);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eInsertIntoHuffmanTree\u003c/span\u003e(\u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e letter, string \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003ecode, \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003ek);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eConstructHuffmanTreeFromFile\u003c/span\u003e(ifstream \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003eis);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eDecompressing\u003c/span\u003e(ifstream \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003eis, ofstream \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003eos);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    map\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e, \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e letter_count;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003etypedef\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estruct\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eNode\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e id;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ebool\u003c/span\u003e is_leaf;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e letter;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e parent, lchild, rchild;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Node() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Node(\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e i, \u003cspan style=\"color:#66d9ef\"\u003ebool\u003c/span\u003e il, \u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e lt, \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e p, \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e lc, \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e rc) \u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            id(i), is_leaf(il), letter(lt), parent(p), lchild(lc), rchild(rc) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    };\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    multimap\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e, Node\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e count_node;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    vector\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003eNode\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e huffman_tree;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    map\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e, vector\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e letter_hcode; \u003cspan style=\"color:#75715e\"\u003e// hufman code for each letter\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e};\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e压缩函数Compress串起压缩的整个流程，包括统计字符频率、构建哈弗曼树、生成哈弗曼编码以及最后将原始文件转换成哈弗曼编码的二进制文件。\u003c/p\u003e","title":"Huffman编码压缩算法及其实现"},{"content":"之前写了七篇博客详细介绍了搜索引擎的工作原理。彼时的搜索引擎主要讲查询和网页的相关性匹配，是动态的、在线的、实时的。相关性匹配有一个问题，网页很容易作弊，比如可以在一个网页中写满诸如“免费”、“美容”之类的垃圾关键词，进而提升查询相关性。但是用户在查询时，一定希望返回的网页比较权威可信，比如同样搜索“苹果电脑”，排名第一的应该是Apple的官网，而不应该是中关村在线之类的第三方网站。\n权威性是一个静态的（或者说变化较慢的）衡量网页重要性的指标。但是应该怎样度量权威性呢，HITS算法使用authority来度量，即指向自身的网页数量越多，则自身的authority值越大。谷歌的PageRank算法是用PageRank值来衡量权威性的。HITS和PageRank一个比较大的区别是HITS和查询有关，而PageRank和查询无关，所以PageRank可以离线计算。下面主要介绍PageRank算法。\nPageRank’s thesis is that a webpage is important if it is pointed to by other important pages.\n我先不加解释的给出PageRank的公式，然后带领大家一步步推导出这个公式。\n$$\\pi^T=\\pi^T(\\alpha S+(1-\\alpha)E)$$我们首先明确目标：PageRank计算的是网页的静态权威度（PR值），也就是如果给定了一个网络结构，则每个网页的PR值就可以通过PageRank算法计算出。假设网页\\(P_i\\)的PR值为\\(r(P_i)\\)，则\\(r(P_i)\\)等于所有指向\\(P_i\\)的网页的PR值之和，即\n$$\\begin{equation}r(P_i)=\\sum\\limits_{P_j\\in B_{P_i}}\\frac{r(P_j)}{|P_j|}\\end{equation}$$其中\\(B_{P_i}\\)为指向\\(P_i\\)的网页集合，\\(|P_j|\\)为\\(P_j\\)的出边的数量。这个式子很好理解，包括两方面内容：1）\\(\\sum\\limits_{P_j\\in B_{P_i}}\\)表示如果指向\\(P_i\\)的网页数量越多，说明网页\\(P_i\\)越重要；2）\\(\\frac{r(P_j)}{|P_j|}\\)表示如果\\(P_j\\)指向的页面数量越少，但有一个指向了\\(P_i\\)，说明网页\\(P_i\\)越重要（如果一个大牛写了很多推荐信（\\(|P_j|\\)大），则这些推荐信的效力就下降了，如果大牛只给你写了推荐信（\\(|P_j|=1\\)），则这封推荐信的效力一定很高）。\n(1)式有一个问题，初始给定一个网络结构时，并不知道\\(r(P_i), r(P_j)\\)，如何计算呢？Brin和Page利用递归的思想求解，初始假设所有网页的PR值相等，都为\\(\\frac{1}{n}\\)，其中\\(n\\)为网络中网页的数量。则第\\(k+1\\)轮的PR计算公式为：\n$$\\begin{equation}r_{k+1}(P_i)=\\sum\\limits_{P_j\\in B_{P_i}}\\frac{r_k(P_j)}{|P_j|}\\end{equation}$$初始对所有网页\\(P_i\\)有\\(r_0(P_i)=\\frac{1}{n}\\)，迭代\\(k\\)步之后，可以计算出所有网页的PR值，然后按PR值从大到小排序，就可以知道每个网页的重要性了。\n对于上图的小网络，我们可以计算出其每一步的PR值：\n可以看到经过2次迭代之后，节点4的PR值最大，从图中也可以看出，节点4的出入边较多，它可能比较重要。\n注意到对于(2)式，当\\(i,j\\)之间有边时，\\(\\frac{1}{|P_j|}\\)相当于对\\(P_j\\)出度的归一化，设矩阵\\(H\\)为图的邻接矩阵的行归一化矩阵，对于上图，为\n设行向量\\(\\pi^{(k)T}\\)为第\\(k\\)轮迭代时所有网页的PR值，则式(2)可以转换为如下的矩阵形式：\n$$\\begin{equation}\\pi^{(k+1)T}=\\pi^{(k)T}H\\end{equation}$$初始有\\(\\pi^{(0)T}=\\frac{1}{n}e^T\\)，\\(e^T\\)为全1的行向量。我们可以从(3)式观测出几点信息：\n(3)式的每一轮计算涉及到向量和矩阵的乘法，复杂度为\\(O(n^2)\\)，\\(n\\)为矩阵\\(H\\)的大小 \\(H\\)是一个稀疏矩阵，因为大部分网页只和很少的网页有链接关系，所以上述向量和矩阵的乘法复杂度还可以降低 \\(H\\)有点像马尔科夫链中的随机转移矩阵，但又不完全是，因为如果有dangling nodes，则这一行就是全0，所以\\(H\\)被称为substochastic matrix 上图中的节点3就是一个dangling node，它只有入边，没有出边，也就是说，每一轮迭代，PR值只会流入3号节点，不会从3号节点流出，久而久之，3就像一个水槽(sink)一样，吸走了大部分的PR，导致PR值虚高。\n所以问题随之而来，怎样保证(3)式一定能够收敛到一个平稳概率分布\\(\\pi^T\\)，\\(\\pi^T\\)和\\(\\pi^{(0)T}\\)有关吗，怎样解决dangling nodes问题，等等。此时需要引入一点马尔科夫链理论的知识。\n在马尔科夫理论呢中，如果一个矩阵\\(P\\)是随机的（stochastic）、不可约的（irreducible）和非周期的（aperiodic），则对于任意的起始向量，都能收敛到一个唯一的平稳正向量。所以如果PageRank矩阵\\(H\\)满足上述三个条件，则可以用幂法（Power Method）找到一个平稳概率分布\\(\\pi^T\\)。幂法是用来计算最大特征值的特征向量。因为\\(H\\)的最大特征值为1，所以可以用幂法找到稳态时（\\(\\pi^T=\\pi^TH\\)）的概率分布\\(\\pi^T\\)。\n下面我们就将矩阵\\(H\\)调整为随机的（stochastic）、不可约的（irreducible）和非周期的（aperiodic）。\n行随机矩阵是指行和为1的非负矩阵。如果图中含有dangling nodes，则\\(H\\)不是随机的，比如上面的例子，第二行为全0。所以第一个调整是对于所有dangling nodes，都加上一个随机跳转向量\\(e^T/n\\)，含义就是如果进入死胡同（dangling nodes），则随机跳转到网络中的任意一个网页。定义向量\\(a\\)：\n$$\\begin{equation}a_i=\\begin{cases}1\\quad\\text{if page}~i\\text{ is a dangling node}\\\\0\\quad\\text{otherwise}\\end{cases}\\end{equation}$$则新的Google矩阵为：\n$$\\begin{equation}S=H+a\\frac{1}{n}e^T\\end{equation}$$新矩阵\\(S\\)就是一个行随机矩阵了。对于上图的例子，有\n为了保证矩阵\\(S\\)满足不可约性（irreducible）和非周期性（aperiodic），必须使\\(S\\)对应的图是强连通的且每个节点有自回路。所以再次调整为：\n$$\\begin{equation}G=\\alpha S+(1-\\alpha)\\frac{1}{n}ee^T\\end{equation}$$令\n$$\\begin{equation}E=\\frac{1}{n}ee^T\\end{equation}$$则得到本博客开头的Google矩阵公式：\n$$\\begin{equation}G=\\alpha S+(1-\\alpha)E\\end{equation}$$\\(E\\)即为随机平均游走矩阵。矩阵\\(G\\)也很好解释，大家上网的时候以\\(\\alpha\\)的概率沿着某个网页里面的链接一步步深入进去（\\(S\\)），当沿着链接走累的时候，以\\(1-\\alpha\\)的概率在地址栏输入一个新地址，随机跳走了（\\(E\\)）。\n此时的矩阵\\(G\\)满足随机性（stochastic）、不可约性（irreducible）和非周期性（aperiodic），所以可以根据幂法（Power Method）找到一个平稳概率分布\\(\\pi^T\\)，\\(\\pi^T_i\\)就衡量了网页\\(P_i\\)的重要性或者权威性。\n此时只剩下参数\\(\\alpha\\)了，\\(\\alpha\\)平衡了网络结构和随机游走。如果\\(\\alpha\\)很小，则\\(1-\\alpha\\)大，\\(G\\)就退化成一个人造随机网络，不能很好的反应真实的网络结构。如果\\(\\alpha\\)很大，则有可能不能得到一个稳态分布，或者幂法会失效。当\\(\\alpha\\approx 1\\)时，幂法失效，且\\(\\pi^T(\\alpha)\\)对\\(H\\)的微小扰动很敏感。Google的选择是\\(\\alpha=0.85\\)。\n将(5)式带入(6)式，得到\n$$\\begin{equation}G=\\alpha H+(\\alpha a+(1-\\alpha)e)\\frac{1}{n}e^T\\end{equation}$$(9)式就非常好计算了，只涉及到向量和矩阵的乘法，而且矩阵\\(H\\)还是稀疏矩阵，复杂度还可以降低。\n幂法（Power Method）求解PageRank稳态分布就是不断计算下面的等式：\n$$\\begin{equation}\\pi^{(k+1)T}=\\pi^{(k)T}G=\\alpha \\pi^{(k)T}H+(\\alpha\\pi^{(k)T}a+1-\\alpha)e^T/n\\end{equation}$$当前后两次的\\(\\pi^{(k+1)T}\\)和\\(\\pi^{(k)T}\\)变化小于某个阈值时，算法收敛，所以算法实现是非常容易的。Brin and Page在他们1998年的论文中提到，只需要50-100次迭代运算就可以收敛了。\n对于上图的例子，令\\(\\alpha=0.9\\)，解得\n利用幂法解得稳态分布为\n所以这6个网页的排名为4\u0026gt;6\u0026gt;5\u0026gt;2\u0026gt;3\u0026gt;1。\n真正的搜索引擎应该综合了网页的静态权威性（如PageRank值）和查询的相关性，每个网站都有一个PR值，具体可以点此查询。\n本博客主要内容参考Google’s PageRank and Beyond: The Science of Search Engine Rankings[1]，插图即为该书封面；如果想快速了解PageRank，可以参考[2]；[3]的讲解也很详细。\nhttp://geza.kzoo.edu/~erdi/patent/langvillebook.pdf http://www.cs.cmu.edu/~elaw/pagerank.pdf http://www.ams.org/samplings/feature-column/fcarc-pagerank ","permalink":"http://localhost:1313/posts/2016-08-04-googles-pagerank-and-beyond/","summary":"\u003cp\u003e之前写了\u003ca href=\"https://bitjoy.net/categories/%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%E6%9E%84%E5%BB%BA%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/\"\u003e七篇博客\u003c/a\u003e详细介绍了搜索引擎的工作原理。彼时的搜索引擎主要讲查询和网页的\u003cstrong\u003e相关性\u003c/strong\u003e匹配，是动态的、在线的、实时的。相关性匹配有一个问题，网页很容易作弊，比如可以在一个网页中写满诸如“免费”、“美容”之类的垃圾关键词，进而提升查询相关性。但是用户在查询时，一定希望返回的网页比较\u003cstrong\u003e权威可信\u003c/strong\u003e，比如同样搜索“苹果电脑”，排名第一的应该是Apple的官网，而不应该是中关村在线之类的第三方网站。\u003c/p\u003e\n\u003cp\u003e权威性是一个静态的（或者说变化较慢的）衡量网页重要性的指标。但是应该怎样度量权威性呢，\u003ca href=\"https://en.wikipedia.org/wiki/HITS_algorithm\"\u003eHITS算法\u003c/a\u003e使用authority来度量，即指向自身的网页数量越多，则自身的authority值越大。谷歌的\u003ca href=\"https://en.wikipedia.org/wiki/PageRank\"\u003ePageRank算法\u003c/a\u003e是用PageRank值来衡量权威性的。\u003ca href=\"http://blog.sina.com.cn/s/blog_72995dcc01013bkb.html\"\u003eHITS和PageRank一个比较大的区别是HITS和查询有关，而PageRank和查询无关，所以PageRank可以离线计算。\u003c/a\u003e下面主要介绍PageRank算法。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/press.princeton.edu/images/k8216.gif\"\u003e\u003c/p\u003e\n\u003cp\u003ePageRank’s thesis is that a webpage is important if it is pointed to by other important pages.\u003c/p\u003e\n\u003cp\u003e我先不加解释的给出PageRank的公式，然后带领大家一步步推导出这个公式。\u003c/p\u003e\n$$\\pi^T=\\pi^T(\\alpha S+(1-\\alpha)E)$$\u003cp\u003e我们首先明确目标：PageRank计算的是网页的静态权威度（PR值），也就是如果给定了一个网络结构，则每个网页的PR值就可以通过PageRank算法计算出。假设网页\\(P_i\\)的PR值为\\(r(P_i)\\)，则\\(r(P_i)\\)等于所有指向\\(P_i\\)的网页的PR值之和，即\u003c/p\u003e\n$$\\begin{equation}r(P_i)=\\sum\\limits_{P_j\\in B_{P_i}}\\frac{r(P_j)}{|P_j|}\\end{equation}$$\u003cp\u003e其中\\(B_{P_i}\\)为指向\\(P_i\\)的网页集合，\\(|P_j|\\)为\\(P_j\\)的出边的数量。这个式子很好理解，包括两方面内容：1）\\(\\sum\\limits_{P_j\\in B_{P_i}}\\)表示如果指向\\(P_i\\)的网页数量越多，说明网页\\(P_i\\)越重要；2）\\(\\frac{r(P_j)}{|P_j|}\\)表示如果\\(P_j\\)指向的页面数量越少，但有一个指向了\\(P_i\\)，说明网页\\(P_i\\)越重要（如果一个大牛写了很多推荐信（\\(|P_j|\\)大），则这些推荐信的效力就下降了，如果大牛只给你写了推荐信（\\(|P_j|=1\\)），则这封推荐信的效力一定很高）。\u003c/p\u003e\n\u003cp\u003e(1)式有一个问题，初始给定一个网络结构时，并不知道\\(r(P_i), r(P_j)\\)，如何计算呢？Brin和Page利用递归的思想求解，初始假设所有网页的PR值相等，都为\\(\\frac{1}{n}\\)，其中\\(n\\)为网络中网页的数量。则第\\(k+1\\)轮的PR计算公式为：\u003c/p\u003e\n$$\\begin{equation}r_{k+1}(P_i)=\\sum\\limits_{P_j\\in B_{P_i}}\\frac{r_k(P_j)}{|P_j|}\\end{equation}$$\u003cp\u003e初始对所有网页\\(P_i\\)有\\(r_0(P_i)=\\frac{1}{n}\\)，迭代\\(k\\)步之后，可以计算出所有网页的PR值，然后按PR值从大到小排序，就可以知道每个网页的重要性了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"pr-1\" loading=\"lazy\" src=\"/posts/2016-08-04-googles-pagerank-and-beyond/pr-1.png\"\u003e\n对于上图的小网络，我们可以计算出其每一步的PR值：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"pr-2\" loading=\"lazy\" src=\"/posts/2016-08-04-googles-pagerank-and-beyond/pr-2.png\"\u003e\n可以看到经过2次迭代之后，节点4的PR值最大，从图中也可以看出，节点4的出入边较多，它可能比较重要。\u003c/p\u003e\n\u003cp\u003e注意到对于(2)式，当\\(i,j\\)之间有边时，\\(\\frac{1}{|P_j|}\\)相当于对\\(P_j\\)出度的归一化，设矩阵\\(H\\)为图的邻接矩阵的行归一化矩阵，对于上图，为\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"pr-3\" loading=\"lazy\" src=\"/posts/2016-08-04-googles-pagerank-and-beyond/pr-3.png\"\u003e\n设行向量\\(\\pi^{(k)T}\\)为第\\(k\\)轮迭代时所有网页的PR值，则式(2)可以转换为如下的矩阵形式：\u003c/p\u003e\n$$\\begin{equation}\\pi^{(k+1)T}=\\pi^{(k)T}H\\end{equation}$$\u003cp\u003e初始有\\(\\pi^{(0)T}=\\frac{1}{n}e^T\\)，\\(e^T\\)为全1的行向量。我们可以从(3)式观测出几点信息：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(3)式的每一轮计算涉及到向量和矩阵的乘法，复杂度为\\(O(n^2)\\)，\\(n\\)为矩阵\\(H\\)的大小\u003c/li\u003e\n\u003cli\u003e\\(H\\)是一个稀疏矩阵，因为大部分网页只和很少的网页有链接关系，所以上述向量和矩阵的乘法复杂度还可以降低\u003c/li\u003e\n\u003cli\u003e\\(H\\)有点像马尔科夫链中的随机转移矩阵，但又不完全是，因为如果有dangling nodes，则这一行就是全0，所以\\(H\\)被称为substochastic matrix\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"pr-4\" loading=\"lazy\" src=\"/posts/2016-08-04-googles-pagerank-and-beyond/pr-4.png\"\u003e\n上图中的节点3就是一个dangling node，它只有入边，没有出边，也就是说，每一轮迭代，PR值只会流入3号节点，不会从3号节点流出，久而久之，3就像一个水槽(sink)一样，吸走了大部分的PR，导致PR值虚高。\u003c/p\u003e\n\u003cp\u003e所以问题随之而来，怎样保证(3)式一定能够收敛到一个平稳概率分布\\(\\pi^T\\)，\\(\\pi^T\\)和\\(\\pi^{(0)T}\\)有关吗，怎样解决dangling nodes问题，等等。此时需要引入一点马尔科夫链理论的知识。\u003c/p\u003e\n\u003cp\u003e在马尔科夫理论呢中，如果一个矩阵\\(P\\)是随机的（stochastic）、不可约的（irreducible）和非周期的（aperiodic），则对于任意的起始向量，都能收敛到一个唯一的平稳正向量。所以如果PageRank矩阵\\(H\\)满足上述三个条件，则可以用幂法（Power Method）找到一个平稳概率分布\\(\\pi^T\\)。\u003ca href=\"https://en.wikipedia.org/wiki/Power_iteration\"\u003e幂法\u003c/a\u003e是用来计算最大特征值的特征向量。因为\\(H\\)的最大特征值为1，所以可以用幂法找到稳态时（\\(\\pi^T=\\pi^TH\\)）的概率分布\\(\\pi^T\\)。\u003c/p\u003e\n\u003cp\u003e下面我们就将矩阵\\(H\\)调整为随机的（stochastic）、不可约的（irreducible）和非周期的（aperiodic）。\u003c/p\u003e\n\u003cp\u003e行随机矩阵是指行和为1的非负矩阵。如果图中含有dangling nodes，则\\(H\\)不是随机的，比如上面的例子，第二行为全0。所以第一个调整是对于所有dangling nodes，都加上一个随机跳转向量\\(e^T/n\\)，含义就是如果进入死胡同（dangling nodes），则随机跳转到网络中的任意一个网页。定义向量\\(a\\)：\u003c/p\u003e\n$$\\begin{equation}a_i=\\begin{cases}1\\quad\\text{if page}~i\\text{ is a dangling node}\\\\0\\quad\\text{otherwise}\\end{cases}\\end{equation}$$\u003cp\u003e则新的Google矩阵为：\u003c/p\u003e\n$$\\begin{equation}S=H+a\\frac{1}{n}e^T\\end{equation}$$\u003cp\u003e新矩阵\\(S\\)就是一个行随机矩阵了。对于上图的例子，有\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"pr-5\" loading=\"lazy\" src=\"/posts/2016-08-04-googles-pagerank-and-beyond/pr-5.png\"\u003e\n为了保证矩阵\\(S\\)满足\u003ca href=\"http://mathworld.wolfram.com/ReducibleMatrix.html\"\u003e不可约性（irreducible）\u003c/a\u003e和\u003ca href=\"https://en.wikipedia.org/wiki/Aperiodic_graph\"\u003e非周期性（aperiodic）\u003c/a\u003e，必须使\\(S\\)对应的图是强连通的且每个节点有自回路。所以再次调整为：\u003c/p\u003e\n$$\\begin{equation}G=\\alpha S+(1-\\alpha)\\frac{1}{n}ee^T\\end{equation}$$\u003cp\u003e令\u003c/p\u003e\n$$\\begin{equation}E=\\frac{1}{n}ee^T\\end{equation}$$\u003cp\u003e则得到本博客开头的Google矩阵公式：\u003c/p\u003e\n$$\\begin{equation}G=\\alpha S+(1-\\alpha)E\\end{equation}$$\u003cp\u003e\\(E\\)即为随机平均游走矩阵。矩阵\\(G\\)也很好解释，大家上网的时候以\\(\\alpha\\)的概率沿着某个网页里面的链接一步步深入进去（\\(S\\)），当沿着链接走累的时候，以\\(1-\\alpha\\)的概率在地址栏输入一个新地址，随机跳走了（\\(E\\)）。\u003c/p\u003e\n\u003cp\u003e此时的矩阵\\(G\\)满足随机性（stochastic）、不可约性（irreducible）和非周期性（aperiodic），所以可以根据幂法（Power Method）找到一个平稳概率分布\\(\\pi^T\\)，\\(\\pi^T_i\\)就衡量了网页\\(P_i\\)的重要性或者权威性。\u003c/p\u003e","title":"还原谷歌PageRank算法真相"},{"content":"$$\\begin{equation}Pr(|\\hat{p}-p|\\geq 5\\%)\\leq 5\\%\\end{equation}$$上一回我们讲到当\\(p\\)本身很小的时候，容易被5%（绝对误差）给淹没掉，导致结果的不可信。我们可以引入相对误差，把(1)式转换为如下的不等式\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq\\delta p)\\leq\\epsilon\\end{equation}$$同理，我们可以用\n$$\\begin{equation}\\hat{p}=\\frac{x_1+x_2+…+x_n}{n}\\end{equation}$$代替\\(\\hat{p}\\)（建议先看上一篇博客），转换为\n$$\\begin{equation}Pr(|X-np|\\geq\\delta np)\\end{equation}$$类似的，\\(X=x_1+x_2+…+x_n\\)，\\(E(X)=\\mu=np\\)，所以(4)式等价为\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\end{equation}$$这个时候，因为不等号右边和均值\\(\\mu\\)有关，不能再用切比雪夫不等式了，我们需要另外一个武器：Chernoff bound。它有两种形式：\n$$\\begin{equation}Pr(X\\geq (1+\\delta)\\mu)\\leq[\\frac{e^\\delta}{(1+\\delta)^{1+\\delta}}]^\\mu\\leq e^{-\\frac{\\mu}{3}\\delta^2}\\quad\\forall\\delta\u003e0\\end{equation}$$$$\\begin{equation}Pr(X\\leq (1-\\delta)\\mu)\\leq[\\frac{e^{-\\delta}}{(1-\\delta)^{1-\\delta}}]^\\mu\\leq e^{-\\frac{\\mu}{2}\\delta^2}\\quad\\forall 0\u003c\\delta\u003c1\\end{equation}$$Chernoff bound的证明需要用到马尔可夫不等式，有一点技巧。以上两种形式可以统一成\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\leq 2e^{-\\frac{\\mu}{3}\\delta^2}\\end{equation}$$也是一个很漂亮的不等式。\n利用Chernoff bound求解(5)式：\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\leq 2e^{-\\frac{\\mu}{3}\\delta^2}\\\\=2e^{-\\frac{np}{3}\\delta^2}\\leq\\epsilon\\end{equation}$$解得\n$$\\begin{equation}n\\geq\\left\\lceil\\frac{3ln\\frac{2}{\\epsilon}}{p\\delta^2}\\right\\rceil\\end{equation}$$这个结果看起来就很复杂了。也就是说，如果要设计调查问卷使满足(2)式的精度，抽样的样本数必须满足(10)式。从(10)式可知，当要求的精度越高（即\\(\\delta\\)和\\(\\epsilon\\)越小），所需的样本数越大。并且结果还和真实值\\(p\\)有关。\n","permalink":"http://localhost:1313/posts/2016-07-23-the-validity-of-the-questionnaire-2/","summary":"$$\\begin{equation}Pr(|\\hat{p}-p|\\geq 5\\%)\\leq 5\\%\\end{equation}$$\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-07-23-the-validity-of-the-questionnaire-1/\"\u003e上一回\u003c/a\u003e我们讲到当\\(p\\)本身很小的时候，容易被5%（绝对误差）给淹没掉，导致结果的不可信。我们可以引入相对误差，把(1)式转换为如下的不等式\u003c/p\u003e\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq\\delta p)\\leq\\epsilon\\end{equation}$$\u003cp\u003e同理，我们可以用\u003c/p\u003e\n$$\\begin{equation}\\hat{p}=\\frac{x_1+x_2+…+x_n}{n}\\end{equation}$$\u003cp\u003e代替\\(\\hat{p}\\)（建议先看\u003ca href=\"https://bitjoy.net/posts/2016-07-23-the-validity-of-the-questionnaire-1/\"\u003e上一篇博客\u003c/a\u003e），转换为\u003c/p\u003e\n$$\\begin{equation}Pr(|X-np|\\geq\\delta np)\\end{equation}$$\u003cp\u003e类似的，\\(X=x_1+x_2+…+x_n\\)，\\(E(X)=\\mu=np\\)，所以(4)式等价为\u003c/p\u003e\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\end{equation}$$\u003cp\u003e这个时候，因为不等号右边和均值\\(\\mu\\)有关，不能再用切比雪夫不等式了，我们需要另外一个武器：\u003ca href=\"https://en.wikipedia.org/wiki/Chernoff_bound\"\u003eChernoff bound\u003c/a\u003e。它有两种形式：\u003c/p\u003e\n$$\\begin{equation}Pr(X\\geq (1+\\delta)\\mu)\\leq[\\frac{e^\\delta}{(1+\\delta)^{1+\\delta}}]^\\mu\\leq e^{-\\frac{\\mu}{3}\\delta^2}\\quad\\forall\\delta\u003e0\\end{equation}$$$$\\begin{equation}Pr(X\\leq (1-\\delta)\\mu)\\leq[\\frac{e^{-\\delta}}{(1-\\delta)^{1-\\delta}}]^\\mu\\leq e^{-\\frac{\\mu}{2}\\delta^2}\\quad\\forall 0\u003c\\delta\u003c1\\end{equation}$$\u003cp\u003eChernoff bound的证明需要用到马尔可夫不等式，有一点技巧。以上两种形式可以统一成\u003c/p\u003e\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\leq 2e^{-\\frac{\\mu}{3}\\delta^2}\\end{equation}$$\u003cp\u003e也是一个很漂亮的不等式。\u003c/p\u003e\n\u003cp\u003e利用Chernoff bound求解(5)式：\u003c/p\u003e\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\leq 2e^{-\\frac{\\mu}{3}\\delta^2}\\\\=2e^{-\\frac{np}{3}\\delta^2}\\leq\\epsilon\\end{equation}$$\u003cp\u003e解得\u003c/p\u003e\n$$\\begin{equation}n\\geq\\left\\lceil\\frac{3ln\\frac{2}{\\epsilon}}{p\\delta^2}\\right\\rceil\\end{equation}$$\u003cp\u003e这个结果看起来就很复杂了。也就是说，如果要设计调查问卷使满足(2)式的精度，抽样的样本数必须满足(10)式。从(10)式可知，当要求的精度越高（即\\(\\delta\\)和\\(\\epsilon\\)越小），所需的样本数越大。并且结果还和真实值\\(p\\)有关。\u003c/p\u003e","title":"调查问卷的有效性（2）相对误差"},{"content":"每年春晚过后，央视又要吹嘘说今年春晚收视率创新高了，但是我们总感觉央视在骗我们，因为我是越长大越不看春晚了[笑cry]，所以收视率到底是怎么统计出来的，央视的说法是否靠谱呢？\n最近的美国大选真是热闹，很多机构都会发放一些调查问卷，然后统计出希拉里或者唐纳德的民众支持率是多少，但是我并没有收到调查问卷，凭什么就得出了民众支持率了，意思是把我排除在民众之外咯？所以引出这样一个问题，调查问卷是否可信，即调查问卷的有效性。\n其实，央视统计收视率并不要问全中国14亿人口有多少人看了春晚，他只需要从14亿人口里面随机抽\\(n\\)个人，问一下这\\(n\\)个人里有多少人看了春晚，然后把看的人数除以总数就大概估计出全国的收视率了。同理调查民众支持率也是一样，只需要随机调查\\(n\\)个人的意向，把支持希拉里的人数除以总数就大概得到了希拉里的支持率。\n但是你要问了，通过抽样调查出来的收视率和支持率靠谱吗，需要随机抽样多少人才能得到一个比较好的全局近似解呢？今天我们就来解决这个问题。\n假设我们随机抽样了\\(n\\)个人，分别是\\(x_1,x_2,…,x_n\\)。如果第\\(i\\)个人看了春晚，则\\(x_i=1\\)，否则\\(x_i=0\\)。那么通过这\\(n\\)个人的收视情况，我们可以估计出一个收视率\n$$\\begin{equation}\\hat{p}=\\frac{x_1+x_2+…+x_n}{n}\\end{equation}$$假设全国的真实收视率是\\(p\\)，那么平均到每一个人，他看了春晚的概率就是\\(p\\)，也即\\(Pr(x_i=1)=p\\)，所以有\n$$\\begin{equation}E(x_i)=p\\quad E(x_i^2)=p\\quad Var(x_i)=p(1-p)\\end{equation}$$我们的目的就是希望通过\\(n\\)个人估计出来的\\(\\hat{p}\\)和\\(p\\)越接近越好。换句话说，我们希望\\(\\hat{p}\\)和\\(p\\)相差大于5%的概率要小于5%。再换句话说就是有至少95%的概率，\\(\\hat{p}\\)和\\(p\\)相差在5%以内，即\\(\\hat{p}\\)和\\(p\\)很接近。注意这里的两个5%都是可以换成任意你想要的精度。用数学语言表示就是，\\(n\\)至少为多少时，以下不等式可以被满足。\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq 5\\%)\\leq 5\\%\\end{equation}$$把(1)式代入(3)式，用\\(\\frac{1}{20}\\)代替5%，得到等价形式：\n$$\\begin{equation}Pr(|(\\frac{x_1+x_2+…+x_n}{n})-p|\\geq\\frac{1}{20})\\\\ \\Longleftrightarrow~Pr(|X-np|\\geq\\frac{n}{20})\\end{equation}$$其中\\(X=x_1+x_2+…+x_n\\)。根据期望的线性可加性，有\n$$\\begin{equation}E(X)=E(x_1+x_2+…+x_n)=E(x_1)+E(x_2)+…+E(x_n)=np\\end{equation}$$所以(4)又等价于\n$$\\begin{equation}Pr(|X-E(X)|\\geq\\frac{n}{20})\\end{equation}$$我们需要利用著名的切比雪夫不等式来求解上式，切比雪夫不等式如下：\n$$\\begin{equation}Pr(|X-E(X)|\\geq~c)\\leq\\frac{Var(X)}{c^2}\\end{equation}$$切比雪夫不等式可以直接由马尔可夫不等式得到，马尔可夫不等式的证明也不难，略过。\n利用切比雪夫不等式求解(6)式\n$$\\begin{equation}Pr(|X-E(X)|\\geq\\frac{n}{20})\\leq\\frac{Var(X)}{n^2}*400\\\\ =\\frac{n*Var(x_i)}{n^2}*400\\\\ =\\frac{p(1-p)}{n}*400\\\\ \\leq\\frac{1/4}{n}*400=\\frac{100}{n} \\end{equation}$$第一个等号是因为\\(n\\)个变量是独立同分布的，所以方差也有类似于(5)式的线性性质。最后一个不等号是因为\\(p(1-p)\\)是一个开口向下的抛物线，在\\(p=1/2\\)时取到极值\\(1/4\\)。\n回到最初的不等式(3)，则(8)式要满足\\(\\frac{100}{n}\\leq 5\\%\\)，解得\\(n\\geq 2000\\)。注意到求出的\\(n\\)和总体人数是无关的，也就是说，虽然全中国有十几亿人口，但是央视只要随机抽样调查2000个人的收视情况，就能以比较高的概率准确估计出全国的收视率。\n这个结论还是很漂亮的，但是这种方法有两个限制条件：\n采样满足独立同分布，即这\\(n\\)个人是独立同分布的，不能针对某一特定人群调查 (3)式的5%是一个绝对误差，当\\(p\\)本身很小的时候，容易被5%淹没 对于第1个问题，稍微好处理一点，抽样的时候尽量随机一点。对于第2个问题，比较好的解决办法是引入相对误差，即把(3)式转换为如下的不等式\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq\\delta p)\\leq\\epsilon\\end{equation}$$(9)式的求解就比较复杂了，得出的结论也没有上面那么简单，具体的求解方法请听下回分解。\n","permalink":"http://localhost:1313/posts/2016-07-23-the-validity-of-the-questionnaire-1/","summary":"\u003cp\u003e每年春晚过后，央视又要吹嘘说今年春晚收视率创新高了，但是我们总感觉央视在骗我们，因为我是越长大越不看春晚了[笑cry]，所以收视率到底是怎么统计出来的，央视的说法是否靠谱呢？\u003c/p\u003e\n\u003cp\u003e最近的美国大选真是热闹，很多机构都会发放一些调查问卷，然后统计出希拉里或者唐纳德的民众支持率是多少，但是我并没有收到调查问卷，凭什么就得出了民众支持率了，意思是把我排除在民众之外咯？所以引出这样一个问题，调查问卷是否可信，即调查问卷的有效性。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/www.carp.ca/wp-content/uploads/2012/08/questionnaire1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e其实，央视统计收视率并不要问全中国14亿人口有多少人看了春晚，他只需要从14亿人口里面随机抽\\(n\\)个人，问一下这\\(n\\)个人里有多少人看了春晚，然后把看的人数除以总数就大概估计出全国的收视率了。同理调查民众支持率也是一样，只需要随机调查\\(n\\)个人的意向，把支持希拉里的人数除以总数就大概得到了希拉里的支持率。\u003c/p\u003e\n\u003cp\u003e但是你要问了，通过抽样调查出来的收视率和支持率靠谱吗，需要随机抽样多少人才能得到一个比较好的全局近似解呢？今天我们就来解决这个问题。\u003c/p\u003e\n\u003cp\u003e假设我们随机抽样了\\(n\\)个人，分别是\\(x_1,x_2,…,x_n\\)。如果第\\(i\\)个人看了春晚，则\\(x_i=1\\)，否则\\(x_i=0\\)。那么通过这\\(n\\)个人的收视情况，我们可以估计出一个收视率\u003c/p\u003e\n$$\\begin{equation}\\hat{p}=\\frac{x_1+x_2+…+x_n}{n}\\end{equation}$$\u003cp\u003e假设全国的真实收视率是\\(p\\)，那么平均到每一个人，他看了春晚的概率就是\\(p\\)，也即\\(Pr(x_i=1)=p\\)，所以有\u003c/p\u003e\n$$\\begin{equation}E(x_i)=p\\quad E(x_i^2)=p\\quad Var(x_i)=p(1-p)\\end{equation}$$\u003cp\u003e我们的目的就是希望通过\\(n\\)个人估计出来的\\(\\hat{p}\\)和\\(p\\)越接近越好。换句话说，我们希望\\(\\hat{p}\\)和\\(p\\)相差大于5%的概率要小于5%。再换句话说就是有至少95%的概率，\\(\\hat{p}\\)和\\(p\\)相差在5%以内，即\\(\\hat{p}\\)和\\(p\\)很接近。注意这里的两个5%都是可以换成任意你想要的精度。用数学语言表示就是，\\(n\\)至少为多少时，以下不等式可以被满足。\u003c/p\u003e\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq 5\\%)\\leq 5\\%\\end{equation}$$\u003cp\u003e把(1)式代入(3)式，用\\(\\frac{1}{20}\\)代替5%，得到等价形式：\u003c/p\u003e\n$$\\begin{equation}Pr(|(\\frac{x_1+x_2+…+x_n}{n})-p|\\geq\\frac{1}{20})\\\\ \\Longleftrightarrow~Pr(|X-np|\\geq\\frac{n}{20})\\end{equation}$$\u003cp\u003e其中\\(X=x_1+x_2+…+x_n\\)。根据期望的线性可加性，有\u003c/p\u003e\n$$\\begin{equation}E(X)=E(x_1+x_2+…+x_n)=E(x_1)+E(x_2)+…+E(x_n)=np\\end{equation}$$\u003cp\u003e所以(4)又等价于\u003c/p\u003e\n$$\\begin{equation}Pr(|X-E(X)|\\geq\\frac{n}{20})\\end{equation}$$\u003cp\u003e我们需要利用著名的\u003ca href=\"https://en.wikipedia.org/wiki/Chebyshev%27s_inequality\"\u003e切比雪夫不等式\u003c/a\u003e来求解上式，切比雪夫不等式如下：\u003c/p\u003e\n$$\\begin{equation}Pr(|X-E(X)|\\geq~c)\\leq\\frac{Var(X)}{c^2}\\end{equation}$$\u003cp\u003e切比雪夫不等式可以直接由\u003ca href=\"https://en.wikipedia.org/wiki/Markov%27s_inequality\"\u003e马尔可夫不等式\u003c/a\u003e得到，马尔可夫不等式的证明也不难，略过。\u003c/p\u003e\n\u003cp\u003e利用切比雪夫不等式求解(6)式\u003c/p\u003e\n$$\\begin{equation}Pr(|X-E(X)|\\geq\\frac{n}{20})\\leq\\frac{Var(X)}{n^2}*400\\\\ =\\frac{n*Var(x_i)}{n^2}*400\\\\ =\\frac{p(1-p)}{n}*400\\\\ \\leq\\frac{1/4}{n}*400=\\frac{100}{n} \\end{equation}$$\u003cp\u003e第一个等号是因为\\(n\\)个变量是独立同分布的，所以方差也有类似于(5)式的线性性质。最后一个不等号是因为\\(p(1-p)\\)是一个开口向下的抛物线，在\\(p=1/2\\)时取到极值\\(1/4\\)。\u003c/p\u003e\n\u003cp\u003e回到最初的不等式(3)，则(8)式要满足\\(\\frac{100}{n}\\leq 5\\%\\)，解得\\(n\\geq 2000\\)。注意到求出的\\(n\\)和总体人数是无关的，也就是说，虽然全中国有十几亿人口，但是央视只要随机抽样调查2000个人的收视情况，就能以比较高的概率准确估计出全国的收视率。\u003c/p\u003e\n\u003cp\u003e这个结论还是很漂亮的，但是这种方法有两个限制条件：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e采样满足独立同分布，即这\\(n\\)个人是独立同分布的，不能针对某一特定人群调查\u003c/li\u003e\n\u003cli\u003e(3)式的5%是一个绝对误差，当\\(p\\)本身很小的时候，容易被5%淹没\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e对于第1个问题，稍微好处理一点，抽样的时候尽量随机一点。对于第2个问题，比较好的解决办法是引入相对误差，即把(3)式转换为如下的不等式\u003c/p\u003e\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq\\delta p)\\leq\\epsilon\\end{equation}$$\u003cp\u003e(9)式的求解就比较复杂了，得出的结论也没有上面那么简单，具体的求解方法请听下回分解。\u003c/p\u003e","title":"调查问卷的有效性（1）绝对误差"},{"content":"你是否想过如下问题：怎样向色盲证明两只袜子的颜色是不一样的？怎样证明两个图是不同构的？怎样证明一个数是二次非剩余的？\n咋听起来觉得很有意思吧，色盲是区分不了颜色的，怎么能让他相信两只袜子的颜色不一样呢。图同构问题目前既没有被证明属于P，也没有被证明属于NP-Complete。二次非剩余问题也没有被证明属于NP。\n这些听起来很“难”的问题，却可以通过交互式证明进行证明，下面先通过“向色盲证明两只袜子的颜色不同”这个有趣的例子一窥交互式证明的强大。\n向色盲证明两只袜子的颜色不同 P有一只红袜子和黄袜子，她的一个色盲朋友V不相信P的袜子颜色不同，P如何才能让V相信这是真的呢？一个简单的办法如下：\nP把两只袜子给V，V每只手拿了一只袜子 P转过身背对V V抛一枚硬币，如果头面朝上，则保持两只袜子不动，否则交换左右手的袜子 P转过身，V问P是否交换过袜子 如果P回答错误，则V不相信；否则，重复100次实验，如果P都回答正确，则V相信这两只袜子是不同颜色的 如果两只袜子的颜色确实不一样，则P通过区分两只袜子的颜色能正确回答V有没有交换过袜子。但是如果两只袜子颜色一样，则不管V有没有交换过，P都无法分辨这两只袜子，所以只好猜V有没有交换，而猜对的概率只有1/2，重复100次，都猜对的概率只有\\((1/2)^{100}\\)，这是一个非常小的数，可以认为几乎不会发生，即出错的概率极低。\n这就是交互式证明的一个例子，上述证明有三个特点：1）交互过程，整个证明需要P和V进行交互才能完成；2）具有随机性，即V需要抛一枚硬币，来决定是否交换袜子；3）零知识，虽然V最终相信了这两只袜子是不同颜色的，但V还是不知道这两只袜子是什么颜色的。\n下面我们给出交互式证明的形式化定义。\n交互式证明（Interactive Proofs, IP） 令\\(k\\)是\\(N\\rightarrow N\\)的一个函数，我们称语言\\(L\\)属于\\(IP[k]\\)，如果存在一个\\(k(|x|)\\)多项式时间概率图灵机TM \\(V\\)，使得：\n$$ \\begin{equation} \\begin{cases} x\\in L \\Longrightarrow\\exists P\\quad Pr[V \\text{ accepts }x, V(x)=1]\\geq 2/3 \\\\ x\\notin L \\Longrightarrow\\forall P\\quad Pr[V \\text{ accepts }x, V(x)=1]\\leq 1/3 \\end{cases} \\end{equation} $$定义\n$$IP=\\underset{c}{\\bigcup} IP[n^c]$$上述定义的第一条称为“完备性”（Completeness）：如果\\(x\\in L\\)，则存在一个证明者P（prover），使得验证者V（verfier）能以多项式时间接受\\(x\\)，且接受的概率大于2/3；第二条称为“可靠性”（Soundness），如果\\(x\\notin L\\)，则对于所有证明者P，V接受\\(x\\)的概率都不会超过1/3。\n对应到上面的例子，完备性：当两只袜子的颜色确实不同时，V接受的概率为1\u0026gt;2/3；可靠性：当两只袜子的颜色相同时，重复100次实验，V接受的概率只有\\((1/2)^{100}\u003c1/3\\)。\nIP这个复杂性类就是所有IP[k]的并集。在IP中，P的能力是无穷的，但它不一定是诚实的；V能力较弱，只能进行多项式时间的计算。\n下面我们给出另外两个交互式证明的协议。\n图不同构（Graph Non-isomorphism, GNI）的交互式证明 如果图\\(G_1\\)和\\(G_2\\)可以通过对顶点进行恰当标记来将它们转换为同一个图，则称\\(G_1\\)和\\(G_2\\)同构，记为\\(G_1\\cong G_2\\)。换句话说，如果\\(G_1\\)和\\(G_2\\)同构，则在\\(G_1\\)的所有顶点标签上存在一个置换\\(\\pi\\)使得\\(\\pi (G_1)=G_2\\)，其中\\(\\pi (G_1)\\)是将\\(\\pi\\)作用到\\(G_1\\)的各个顶点上之后得到的图。下图就是两个同构图，右边给出了置换\\(\\pi\\)。\n图同构的补集为图不同构（Graph Non-isomorphism, GNI），即判定给定的两个图是否不同构。下面是GNI的一个交互式证明过程。\n给定两个图\\(G_1\\)和\\(G_2\\)，证明者P想要向验证者V证明\\(G_1\\ncong G_2\\)。\nV：随机选一个\\(i\\in \\{1,2\\}\\)，对\\(G_i\\)做一个随机的置换，得到新图\\(H\\)，则有\\(H\\cong G_i\\)，将\\(H\\)发送给P P：发送\\(j\\)给V V：如果\\(i\\neq j\\)，则拒绝；否则重复100次实验，都有\\(i==j\\)，则相信\\(G_1\\ncong G_2\\) 完备性：如果\\(G_1\\ncong G_2\\)，则\\(H\\)只和\\(G_1, G_2\\)中的一个图同构，P因为能力无穷，一定能找出和\\(H\\)同构的图\\(G_j\\)，且满足\\(j==i\\)。\n可靠性：如果\\(G_1\\cong G_2\\)，则\\(H\\)和\\(G_1, G_2\\)都同构，所以P无法区分，只好猜一个\\(j\\)，所以\\(j==i\\)的概率只有1/2，重复100次实验，P都猜对的概率只有\\((1/2)^{100}\u003c1/3\\)。\n零知识：虽然V相信了\\(G_1\\ncong G_2\\)，但V对于P怎样证出来的一无所知。\nP.S.\n有趣的是，关于图同构问题，芝加哥大学的科学家László Babai最近给出了一个伪多项式时间的算法，被称为是计算机理论界近10年最重要的成果。\nNew algorithm cracks graph problem A Quasipolynomial Time Algorithm for Graph Isomorphism: The Details Graph Isomorphism in Quasipolynomial Time 图同构在P/NP问题上重大突破，计算机理论10年最重要成果 二次非剩余（Quadratic non-residuosity, QNR）的交互式证明 如果存在整数\\(b\\)使得\\(a\\equiv b^2(\\mod p)\\)，则称整数\\(a\\)是模\\(p\\)的二次剩余，并称\\(b\\)是\\(a\\)模\\(p\\)的平方根。显然，\\(-b\\)是\\(a\\)模\\(p\\)的另一个平方根，而且\\(a\\)模\\(p\\)不存在其他平方根，因为\\(x^2-a=0\\)在域\\(GF(p)\\)上至多有两个解。\n类似的，如果不存在整数\\(b\\)使得\\(a\\equiv b^2(\\mod p)\\)，则称整数\\(a\\)是模\\(p\\)的二次非剩余（Quadratic non-residuosity, QNR），记为\\(\u003c a, p\u003e\\in QNR\\)。下面是QNR的一个交互式证明过程。\n给定一个素数\\(p\\)和另一个整数\\(a\\)，P要向V证明\\(\u003c a, p\u003e\\in QNR\\)。\nV：取模\\(p\\)的随机数\\(r(\\mod p)\\)和随机数\\(b\\in\\{0,1\\}\\)，如果\\(b=0\\)，发送\\(r^2(\\mod p)\\)给P；否则发送\\(a\\cdot r^2(\\mod p)\\)给P P：发送\\(b’\\)给V V：如果\\(b’\\neq b\\)，则拒绝；否则重复100次实验，都有\\(b’=b\\)，则相信\\(\u003c a, p\u003e\\in QNR\\) 完备性：如果\\(\u003c a, p\u003e\\in QNR\\)，则\\(\u003c a \\cdot r^2,p\u003e\\in QNR\\)，但\\(\u003c r^2, p\u003e\\notin QNR\\)，所以P能区分\\(a\\cdot r^2\\)和\\(r^2\\)，即总能回答正确使得\\(b’=b\\)。\n可靠性：\\(\u003c a, p\u003e\\notin QNR\\)，则\\(\u003c a \\cdot r^2,p\u003e\\notin QNR\\)，且\\(\u003c r^2, p\u003e\\notin QNR\\)，即\\(a\\cdot r^2\\)和\\(r^2\\)都是二次剩余，所以P无法区分，只能瞎猜，正确的概率为1/2，重复100次，都回答正确的概率只有\\((1/2)^{100}\u003c1/3\\)。\n零知识：虽然V相信了\\(\u003c a, p\u003e\\in QNR\\)，但V对于P怎样证出来的一无所知。\n交互式证明的零知识真是有趣，它是密码学中大量研究工作的基础。很多场合都可能会用到零知识证明，比如要向别人证明你有密码，但又不透露密码；要向别人证明你会解某道题，但又不透露解题过程；要让别人相信你知道怎样从甲地到乙地，但又不告诉别人从甲到乙的路……\n交互式证明是这学期选修《高级算法》时接触的，主要参考书目Computational Complexity: A Modern Approach\n","permalink":"http://localhost:1313/posts/2016-07-14-the-interesting-interactive-proofs/","summary":"\u003cp\u003e你是否想过如下问题：怎样向色盲证明两只袜子的颜色是不一样的？怎样证明两个图是不同构的？怎样证明一个数是二次非剩余的？\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/writtenbyrel.com/wp-content/uploads/2014/06/socks.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e咋听起来觉得很有意思吧，色盲是区分不了颜色的，怎么能让他相信两只袜子的颜色不一样呢。图同构问题目前既没有被证明属于P，也没有被证明属于NP-Complete。二次非剩余问题也没有被证明属于NP。\u003c/p\u003e\n\u003cp\u003e这些听起来很“难”的问题，却可以通过交互式证明进行证明，下面先通过“向色盲证明两只袜子的颜色不同”这个有趣的例子一窥交互式证明的强大。\u003c/p\u003e\n\u003ch1 id=\"向色盲证明两只袜子的颜色不同\"\u003e向色盲证明两只袜子的颜色不同\u003c/h1\u003e\n\u003cp\u003eP有一只红袜子和黄袜子，她的一个色盲朋友V不相信P的袜子颜色不同，P如何才能让V相信这是真的呢？一个简单的办法如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eP把两只袜子给V，V每只手拿了一只袜子\u003c/li\u003e\n\u003cli\u003eP转过身背对V\u003c/li\u003e\n\u003cli\u003eV抛一枚硬币，如果头面朝上，则保持两只袜子不动，否则交换左右手的袜子\u003c/li\u003e\n\u003cli\u003eP转过身，V问P是否交换过袜子\u003c/li\u003e\n\u003cli\u003e如果P回答错误，则V不相信；否则，重复100次实验，如果P都回答正确，则V相信这两只袜子是不同颜色的\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e如果两只袜子的颜色确实不一样，则P通过区分两只袜子的颜色能正确回答V有没有交换过袜子。但是如果两只袜子颜色一样，则不管V有没有交换过，P都无法分辨这两只袜子，所以只好猜V有没有交换，而猜对的概率只有1/2，重复100次，都猜对的概率只有\\((1/2)^{100}\\)，这是一个非常小的数，可以认为几乎不会发生，即出错的概率极低。\u003c/p\u003e\n\u003cp\u003e这就是交互式证明的一个例子，上述证明有三个特点：1）交互过程，整个证明需要P和V进行交互才能完成；2）具有随机性，即V需要抛一枚硬币，来决定是否交换袜子；3）零知识，虽然V最终相信了这两只袜子是不同颜色的，但V还是不知道这两只袜子是什么颜色的。\u003c/p\u003e\n\u003cp\u003e下面我们给出交互式证明的形式化定义。\u003c/p\u003e\n\u003ch1 id=\"交互式证明interactive-proofs-ip\"\u003e交互式证明（Interactive Proofs, IP）\u003c/h1\u003e\n\u003cp\u003e令\\(k\\)是\\(N\\rightarrow N\\)的一个函数，我们称语言\\(L\\)属于\\(IP[k]\\)，如果存在一个\\(k(|x|)\\)多项式时间概率图灵机TM \\(V\\)，使得：\u003c/p\u003e\n$$\n\\begin{equation}\n\\begin{cases}\nx\\in L \\Longrightarrow\\exists P\\quad Pr[V \\text{ accepts }x, V(x)=1]\\geq 2/3 \\\\\nx\\notin L \\Longrightarrow\\forall P\\quad Pr[V \\text{ accepts }x, V(x)=1]\\leq 1/3\n\\end{cases}\n\\end{equation}\n$$\u003cp\u003e定义\u003c/p\u003e\n$$IP=\\underset{c}{\\bigcup} IP[n^c]$$\u003cp\u003e上述定义的第一条称为“完备性”（Completeness）：如果\\(x\\in L\\)，则存在一个证明者P（prover），使得验证者V（verfier）能以多项式时间接受\\(x\\)，且接受的概率大于2/3；第二条称为“可靠性”（Soundness），如果\\(x\\notin L\\)，则对于所有证明者P，V接受\\(x\\)的概率都不会超过1/3。\u003c/p\u003e\n\u003cp\u003e对应到上面的例子，完备性：当两只袜子的颜色确实不同时，V接受的概率为1\u0026gt;2/3；可靠性：当两只袜子的颜色相同时，重复100次实验，V接受的概率只有\\((1/2)^{100}\u003c1/3\\)。\u003c/p\u003e\n\u003cp\u003eIP这个复杂性类就是所有IP[k]的并集。在IP中，P的能力是无穷的，但它不一定是诚实的；V能力较弱，只能进行多项式时间的计算。\u003c/p\u003e\n\u003cp\u003e下面我们给出另外两个交互式证明的协议。\u003c/p\u003e\n\u003ch1 id=\"图不同构graph-non-isomorphism-gni的交互式证明\"\u003e图不同构（Graph Non-isomorphism, GNI）的交互式证明\u003c/h1\u003e\n\u003cp\u003e如果图\\(G_1\\)和\\(G_2\\)可以通过对顶点进行恰当标记来将它们转换为同一个图，则称\\(G_1\\)和\\(G_2\\)同构，记为\\(G_1\\cong G_2\\)。换句话说，如果\\(G_1\\)和\\(G_2\\)同构，则在\\(G_1\\)的所有顶点标签上存在一个置换\\(\\pi\\)使得\\(\\pi (G_1)=G_2\\)，其中\\(\\pi (G_1)\\)是将\\(\\pi\\)作用到\\(G_1\\)的各个顶点上之后得到的图。下图就是两个同构图，右边给出了置换\\(\\pi\\)。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"isomorphism graph\" loading=\"lazy\" src=\"/posts/2016-07-14-the-interesting-interactive-proofs/isomorphism-graph.webp\"\u003e\u003c/p\u003e\n\u003cp\u003e图同构的补集为图不同构（Graph Non-isomorphism, GNI），即判定给定的两个图是否不同构。下面是GNI的一个交互式证明过程。\u003c/p\u003e\n\u003cp\u003e给定两个图\\(G_1\\)和\\(G_2\\)，证明者P想要向验证者V证明\\(G_1\\ncong G_2\\)。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eV：随机选一个\\(i\\in \\{1,2\\}\\)，对\\(G_i\\)做一个随机的置换，得到新图\\(H\\)，则有\\(H\\cong G_i\\)，将\\(H\\)发送给P\u003c/li\u003e\n\u003cli\u003eP：发送\\(j\\)给V\u003c/li\u003e\n\u003cli\u003eV：如果\\(i\\neq j\\)，则拒绝；否则重复100次实验，都有\\(i==j\\)，则相信\\(G_1\\ncong G_2\\)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e完备性：如果\\(G_1\\ncong G_2\\)，则\\(H\\)只和\\(G_1, G_2\\)中的一个图同构，P因为能力无穷，一定能找出和\\(H\\)同构的图\\(G_j\\)，且满足\\(j==i\\)。\u003c/p\u003e","title":"有趣的交互式证明"},{"content":"\n\\(\\LaTeX\\)的强大我就不赘述了，这里简单介绍一下怎样在Windows快速配置一个完美好用的\\(\\LaTeX\\)环境。\n我大学刚接触\\(\\LaTeX\\)的时候，使用的是CTeX，CTeX是一个大礼包，整合了编译器编辑器等，但是由于久不更新，很多宏包和语法都变了，而且CTeX附带的WinEdt是商业软件，30天之后需要收费，我又不想用盗版，所以就打算自己配置\\(\\LaTeX\\)环境。\n目前使用的是MiKTeX+Texmaker的完美组合！MiKTeX是\\(\\LaTeX\\)编译器，Texmaker是\\(\\LaTeX\\)编辑器。两者都是开源软件。\nMiKTeX非常棒的地方在于“MiKTeX has the ability to install needed packages automatically (on-the-fly)”，就是说，你用MiKTeX时，不需要担心某个宏包是否存在，你只管用就是了，MiKTeX会在你第一次用到某个宏包时，自动从网上下载，非常方便。正因为这样，MiKTeX的安装包很小，只有175MB。当然，因为是on-the-fly的，所以必须联网使用，而且MiKTeX只有Windows版本。\nMiKTeX自带了一个TeXworks编辑器的，但是这软件用户体验并不好。我以前一直都用WinEdt，很好用，但是它是商业软件，我又不想盗版（说到底是没钱…），所以换了Texmaker。Texmaker可以媲美WinEdt，软件布局合理，各种快捷键用起来也很方便。不过在上手之前要简单配置一下。\n如果是写英文文章，点击“快速构建”左边的箭头（或者F1快捷键），就能一键编译并刷新pdf视图。但是默认的快速构建使用的引擎是PdfLaTeX，如果你是中文用户，使用了xeCJK宏包，则必须使用XeLaTeX引擎编译，所以依次点击“选项-\u0026gt;配置Texmaker-\u0026gt;（左边）快速构建”，选择快速构建命令为”XeLaTeX + View PDF”。\n构建好的PDF默认是以弹窗的形式展现的，我们可以设置让代码和PDF并排显示，这样方便在PDF和源代码之间切换，配置如下：\nTexmaker自带了一个PDF阅读器，当然你也可以使用外部阅读器，比如非常棒的Sumatra PDF，只需填入Sumatra PDF的路径跟上%.pdf，并选中External Viewer。\nTexmaker还有一个很好用的功能是“正向/反向搜索”。反向搜索是点击PDF某个位置，会跳到tex源代码对应位置，快捷方式是ctrl+click。正向搜索是点击tex源代码某个位置，会跳到PDF对应的位置，默认快捷方式ctrl+space，但是这个快捷方式好像用不了，可以自行配置成其他快捷方式，比如ctrl+1，我当时是打开下图的快捷方式窗口才发现这个问题的。\n正反向搜索都可以通过鼠标右键菜单实现，但是快捷键还是更方便的。最重要的一点是，源文件*.tex所在路径不能有中文！！！要不然正反向搜索不能用，这点很重要，我当时郁闷了好久。\n另外还可以配置一下编辑器的字体，勾选”Backup documents every 10 min”之类的。\nOK，大功告成，这种三段式的界面、F1快速构建以及正向/反向搜索，用起来真是太顺手了，Just Enjoy \\(\\LaTeX\\)~\n下面是我常用的\\(\\LaTeX\\)中文模板：\nLaTeXDemo.pdf\nLaTeXDemo.tex\n","permalink":"http://localhost:1313/posts/2016-05-16-an-easy-to-use-latex-toolkit/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://www.ctan.org/lion/ctan_lion_350x350.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\\(\\LaTeX\\)的强大我就不赘述了，这里简单介绍一下怎样在Windows快速配置一个完美好用的\\(\\LaTeX\\)环境。\u003c/p\u003e\n\u003cp\u003e我大学刚接触\\(\\LaTeX\\)的时候，使用的是\u003ca href=\"http://www.ctex.org/HomePage\"\u003eCTeX\u003c/a\u003e，CTeX是一个大礼包，整合了编译器编辑器等，但是由于久不更新，很多宏包和语法都变了，而且CTeX附带的WinEdt是商业软件，30天之后需要收费，我又不想用盗版，所以就打算自己配置\\(\\LaTeX\\)环境。\u003c/p\u003e\n\u003cp\u003e目前使用的是\u003ca href=\"http://miktex.org/\"\u003eMiKTeX\u003c/a\u003e+\u003ca href=\"http://www.xm1math.net/texmaker/\"\u003eTexmaker\u003c/a\u003e的完美组合！MiKTeX是\\(\\LaTeX\\)编译器，Texmaker是\\(\\LaTeX\\)编辑器。两者都是开源软件。\u003c/p\u003e\n\u003cp\u003eMiKTeX非常棒的地方在于“MiKTeX has the ability to install needed packages automatically (on-the-fly)”，就是说，你用MiKTeX时，不需要担心某个宏包是否存在，你只管用就是了，MiKTeX会在你第一次用到某个宏包时，自动从网上下载，非常方便。正因为这样，MiKTeX的安装包很小，只有175MB。当然，因为是on-the-fly的，所以必须联网使用，而且MiKTeX只有Windows版本。\u003c/p\u003e\n\u003cp\u003eMiKTeX自带了一个\u003ca href=\"https://www.tug.org/texworks/\"\u003eTeXworks\u003c/a\u003e编辑器的，但是这软件用户体验并不好。我以前一直都用WinEdt，很好用，但是它是商业软件，我又不想盗版（说到底是没钱…），所以换了Texmaker。Texmaker可以媲美WinEdt，软件布局合理，各种快捷键用起来也很方便。不过在上手之前要简单配置一下。\u003c/p\u003e\n\u003cp\u003e如果是写英文文章，点击“快速构建”左边的箭头（或者F1快捷键），就能一键编译并刷新pdf视图。但是默认的快速构建使用的引擎是PdfLaTeX，如果你是中文用户，使用了xeCJK宏包，则必须使用XeLaTeX引擎编译，所以依次点击“选项-\u0026gt;配置Texmaker-\u0026gt;（左边）快速构建”，选择快速构建命令为”XeLaTeX + View PDF”。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Texmaker-1\" loading=\"lazy\" src=\"/posts/2016-05-16-an-easy-to-use-latex-toolkit/Texmaker-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e构建好的PDF默认是以弹窗的形式展现的，我们可以设置让代码和PDF并排显示，这样方便在PDF和源代码之间切换，配置如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Texmaker-2\" loading=\"lazy\" src=\"/posts/2016-05-16-an-easy-to-use-latex-toolkit/Texmaker-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003eTexmaker自带了一个PDF阅读器，当然你也可以使用外部阅读器，比如非常棒的\u003ca href=\"http://www.sumatrapdfreader.org/free-pdf-reader.html\"\u003eSumatra PDF\u003c/a\u003e，只需填入Sumatra PDF的路径跟上%.pdf，并选中External Viewer。\u003c/p\u003e\n\u003cp\u003eTexmaker还有一个很好用的功能是“正向/反向搜索”。反向搜索是点击PDF某个位置，会跳到tex源代码对应位置，快捷方式是ctrl+click。正向搜索是点击tex源代码某个位置，会跳到PDF对应的位置，默认快捷方式ctrl+space，但是这个快捷方式好像用不了，可以自行配置成其他快捷方式，比如ctrl+1，我当时是打开下图的快捷方式窗口才发现这个问题的。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Texmaker-3\" loading=\"lazy\" src=\"/posts/2016-05-16-an-easy-to-use-latex-toolkit/Texmaker-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e正反向搜索都可以通过鼠标右键菜单实现，但是快捷键还是更方便的。最重要的一点是，源文件*.tex所在路径不能有中文！！！要不然正反向搜索不能用，这点很重要，我当时郁闷了好久。\u003c/p\u003e\n\u003cp\u003e另外还可以配置一下编辑器的字体，勾选”Backup documents every 10 min”之类的。\u003c/p\u003e\n\u003cp\u003eOK，大功告成，这种三段式的界面、F1快速构建以及正向/反向搜索，用起来真是太顺手了，Just Enjoy \\(\\LaTeX\\)~\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/www.xm1math.net/texmaker/texmakertop_big.png\"\u003e\u003c/p\u003e\n\u003cp\u003e下面是我常用的\\(\\LaTeX\\)中文模板：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/2016-05-16-an-easy-to-use-latex-toolkit/LaTeXDemo.pdf\"\u003eLaTeXDemo.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/2016-05-16-an-easy-to-use-latex-toolkit/LaTeXDemo.tex\"\u003eLaTeXDemo.tex\u003c/a\u003e\u003c/p\u003e","title":"LaTeX写作完美解决方案"},{"content":"SVM回顾 支持向量机（SVM）的一大特点是最大化间距（max margin）。对于如上图的二分类问题，虽然有很多线可以将左右两部分分开，但是只有中间的红线效果是最好的，因为它的可活动范围（margin）是最大的，从直观上来说很好理解。\n对于线性二分类问题，假设分类面为\n$$\\begin{equation} u=\\vec w \\cdot \\vec x-b \\end{equation}$$则margin为\n$$\\begin{equation} m=\\frac{1}{||w||_2} \\end{equation}$$根据max margin规则和约束条件，得到如下优化问题，我们要求的就是参数\\(\\vec w\\)和\\(b\\)：\n$$\\begin{equation} \\min\\limits_{\\vec w,b}\\frac{1}{2}||\\vec w||^2 \\quad\\text{subject to}\\quad y_i(\\vec w\\cdot \\vec x_i-b) \\geq 1, \\forall i,\\end{equation}$$对于正样本，类标号\\(y_i\\)为+1，反之则为-1。根据拉格朗日对偶，(3)可以转换为如下的二次规划（QP）问题，其中\\(\\alpha_i\\)为拉格朗日乘子。\n$$\\begin{equation} \\min\\limits_{\\vec \\alpha}\\Psi(\\vec\\alpha)=\\min\\limits_{\\vec \\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^Ny_iy_j(\\vec x_i\\cdot\\vec x_j)\\alpha_i\\alpha_j-\\sum_{i=1}^N\\alpha_i,\\end{equation}$$其中N为样本数量。上式还需满足如下两个约束条件：\n$$\\begin{equation} \\alpha_i\\geq 0, \\forall i,\\end{equation}$$$$\\begin{equation} \\sum_{i=1}^Ny_i\\alpha_i=0.\\end{equation}$$一旦求解出所有的拉格朗日乘子，则我们可以通过如下的公式得到分类面参数\\(\\vec w\\)和\\(b\\)。\n$$\\begin{equation}\\vec w=\\sum_{i=1}^Ny_i\\alpha_i\\vec x_i,\\quad b=\\vec w\\cdot\\vec x_k-y_k\\quad\\text{for some}\\quad\\alpha_k\u003e0.\\end{equation}$$当然并不是所有的数据都可以完美的线性划分，可能有少量数据就是混在对方阵营，这时可以通过引入松弛变量\\(\\xi_i\\)得到软间隔形式的SVM：\n$$\\begin{equation} \\min\\limits_{\\vec w,b,\\vec\\xi}\\frac{1}{2}||\\vec w||^2+C\\sum_{i=1}^N\\xi_i \\quad\\text{subject to}\\quad y_i(\\vec w\\cdot \\vec x_i-b) \\geq 1-\\xi_i, \\forall i,\\end{equation}$$其中的\\(\\xi_i\\)为松弛变量，能假装把错的样本分对，\\(C\\)对max margin和margin failures的trades off。对于这个新的优化问题，约束变成了一个box constraint：\n$$\\begin{equation}0\\leq\\alpha_i \\leq C,\\forall i.\\end{equation}$$而松弛变量\\(\\xi_i\\)不再出现在对偶公式中了。\n对于线性不可分的数据，可以用核函数\\(K\\)将其投影到高维空间，这样就可分了，由此得到一般的分类面公式：\n$$\\begin{equation}u=\\sum_{j=1}^Ny_j\\alpha_jK(\\vec x_j,\\vec x)-b,\\end{equation}$$终极优化问题就变成了下面这个样子：\n$$\\begin{equation} \\min\\limits_{\\vec \\alpha}\\Psi(\\vec\\alpha)=\\min\\limits_{\\vec \\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^Ny_iy_jK(\\vec x_i,\\vec x_j)\\alpha_i\\alpha_j-\\sum_{i=1}^N\\alpha_i,\\\\0\\leq\\alpha_i \\leq C,\\forall i,\\\\ \\sum_{i=1}^Ny_i\\alpha_i=0.\\end{equation}$$要满足的KKT条件为：\n$$ \\begin{equation} \\begin{cases} \\alpha_i = 0 \\Leftrightarrow y_iu_i \\geq 1,\\\\ 0 \u003c \\alpha_i \u003c C \\Leftrightarrow y_iu_i = 1,\\\\ \\alpha_i = C \\Leftrightarrow y_iu_i \\leq 1.\\\\ \\end{cases} \\end{equation} $$SMO算法 为了求解式(11)，SMO算法通过启发式方法选择两个\\(\\alpha_i,\\alpha_j\\)当变量，固定其他\\(\\alpha_k\\)，然后用解析的方法求解两个变量的二次规划问题。关于这两个变量的解应该更接近原始的二次规划问题，更重要的是，这时子问题可以通过解析方法求解，避免了矩阵运算，大大提高了整个算法的计算速度。如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。\n不失一般性，假设选择的两个拉格朗日乘子是\\(\\alpha_1,\\alpha_2\\)，因为\\(\\alpha_i\\)和变量\\(x_i\\)是一一对应的，所以也可以说选择了变量\\(x_1,x_2\\)。固定其他变量\\(\\alpha_i (i=3,4,…,N)\\)\n此时，式(11)的优化问题转换为如下的优化问题：\n$$\\begin{equation} \\min\\limits_{\\alpha_1,\\alpha_2}W(\\alpha_1,\\alpha_2)=\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2\\\\-(\\alpha_1+\\alpha_2)+y_1\\alpha_1\\sum_{i=3}^Ny_i\\alpha_iK_{i1}+y_2\\alpha_2\\sum_{i=3}^Ny_i\\alpha_iK_{i2}\\end{equation}$$满足\n$$\\begin{equation}\\alpha_1y_1+\\alpha_2y_2=-\\sum_{i=3}^Ny_i\\alpha_i=\\zeta\\end{equation}$$因为只有\\(\\alpha_1,\\alpha_2\\)是变量，其他\\(\\alpha_i (i=3,4,…,N)\\)是固定的（可以认为是常数），所以式(13)省略了不含\\(\\alpha_1,\\alpha_2\\)的常数项。又因为\\(y_i=\\pm 1\\)，\\(\\alpha_i\\)有限制(9)，所以\\(\\alpha_1,\\alpha_2\\)被限制在\\([0,C]\\times[0,C]\\)的盒子里，且位于对角线上，如图Figure 1.\n由于\\(\\alpha_2^{new}\\)需满足(9)，所以最优值\\(\\alpha_2^{new}\\)的取值范围必须满足条件\n$$\\begin{equation}L\\leq\\alpha_2^{new}\\leq H\\end{equation}$$其中L与H是\\(\\alpha_2^{new}\\)所在的对角线段端点的界。如果\\(y_1\\neq y_2\\)（Figure 1.左图），则\n$$\\begin{equation}L=max(0,\\alpha_2^{old}-\\alpha_1^{old}),\\quad H=min(C,C+\\alpha_2^{old}-\\alpha_1^{old}).\\end{equation}$$如果\\(y_1= y_2\\)（Figure 1.右图），则\n$$\\begin{equation}L=max(0,\\alpha_2^{old}+\\alpha_1^{old}-C),\\quad H=min(C,\\alpha_2^{old}+\\alpha_1^{old}).\\end{equation}$$令\n$$\\begin{equation}\\eta=K(\\vec x_1,\\vec x_1)+K(\\vec x_2,\\vec x_2)-2K(\\vec x_1,\\vec x_2).\\end{equation}$$因为\\(y_i=\\pm 1\\)，所以\\(y_i^2=1\\)，式(14)同乘以\\(y_1\\)，用\\(\\alpha_2\\)表示\\(\\alpha_1\\)并代入式(13)，得到只含一个变量\\(\\alpha_2\\)的表达式，该表达式对\\(\\alpha_2\\)求偏导并等于0，求出\\(\\alpha_2\\)的更新公式为：\n$$\\begin{equation}\\alpha_2^{new}=\\alpha_2^{old}+\\frac{y_2(E_1-E_2)}{\\eta},\\end{equation}$$其中\\(E_i=u_i-y_i\\)是第\\(i\\)个样本的训练误差。经过截断之后的\\(\\alpha_2^{new}\\)为：\n$$ \\begin{equation} \\alpha_2^{new, clipped} = \\begin{cases} H, \u0026 \\mbox{if }\\alpha_2^{new}\\geq H;\\\\ \\alpha_2^{new}, \u0026 \\mbox{if }L \u003c \\alpha_2^{new} \u003c H;\\\\ L, \u0026 \\mbox{if }\\alpha_2^{new}\\leq L. \\end{cases} \\end{equation} $$将\\(\\alpha_2^{new,clipped}\\)的结果代入式(14)，得到\\(\\alpha_1\\)的更新公式：\n$$\\begin{equation}\\alpha_1^{new}=\\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new,clipped}).\\end{equation}$$当\\(0 \u003c \\alpha_1^{new} \u003c C\\)时，由式(12)中间的条件可以得到阈值\\(b_1\\)的更新公式：\n$$\\begin{equation}b_1^{new}=b^{old}-E_1-y_1K_{11}(\\alpha_1^{new}-\\alpha_1^{old})-y_2K_{21}(\\alpha_2^{new,clipped}-\\alpha_2^{old}),\\end{equation}$$同理，当\\(0 \u003c \\alpha_2^{new,clipped} \u003c C\\)时，得到阈值\\(b_2\\)的更新公式：\n$$\\begin{equation}b_2^{new}=b^{old}-E_2-y_1K_{12}(\\alpha_1^{new}-\\alpha_1^{old})-y_2K_{22}(\\alpha_2^{new,clipped}-\\alpha_2^{old}).\\end{equation}$$综合以上，阈值\\(b\\)的更新公式为：\n$$ \\begin{equation} b^{new}= \\begin{cases} b_1^{new}, \u0026 \\mbox{if } 0 \u003c \\alpha_1^{new} \u003c C; \\\\ b_2^{new}, \u0026 \\mbox{if } 0 \u003c \\alpha_2^{new,clipped} \u003c C;\\\\ (b_1^{new}+b_2^{new})/2, \u0026 \\mbox{otherwise}. \\end{cases} \\end{equation} $$至此，如果我们选定了两个变量\\(\\alpha_2,\\alpha_1\\)，则可以通过公式(19)和(21)来更新它们，并通过公式(24)更新阈值\\(b\\)，通过多次迭代逼近最优结果。但是怎样选择\\(\\alpha_2,\\alpha_1\\)呢，SMO通过启发式方法选择！\n对于第一个样本\\(\\alpha_2\\)，我们选择违反KKT条件式(12)（最严重）的样本\\(\\alpha_2\\)；由式(19)可知，\\(\\alpha_2^{new}\\)是依赖于\\(|E_1-E_2|\\)的，为了加快计算，一种简单的做法是选择\\(\\alpha_1\\)，使其对应的\\(|E_1-E_2|\\)最大。\n至此，我们有了启发式选择两个变量\\(\\alpha_2,\\alpha_1\\)的方法，以及求解这两个变量二次规划的解析方法，下面介绍SMO算法的具体实现。\nMATLAB代码实现 算法伪代码如下：\n输入：训练数据\\(T=\\{(\\vec x_1,y_1),(\\vec x_2,y_2),…,(\\vec x_N,y_N)\\}\\)，其中\\(\\vec x_i \\in R^n\\)，\\(y_i\\in\\{-1,+1\\}\\)，\\(i=1,2,…,N\\)，精度为\\(\\epsilon\\)。\n输出：拉格朗日乘子\\(\\vec\\alpha=\\{\\alpha_1,\\alpha_2,…,\\alpha_N\\}\\)和阈值\\(b\\)的近似解。\n初始化\\(\\vec \\alpha=\\vec 0, b=0\\) 选一个违反KKT条件的变量\\(\\alpha_2\\) 选一个使得\\(|E_1-E_2|\\)最大的变量\\(\\alpha_1\\) 根据公式(19)~(21)更新变量\\(\\alpha_2,\\alpha_1\\) 根据公式(22)~(24)更新阈值\\(b\\) 如果不满足结束条件，则转2；否则算法结束 算法的结束条件是：如果所有变量都已经检查过，且没有变量可以进一步优化时，算法结束。\n下面是SMO算法的MATLAB简化实现，省略了论文[1]的公式(19)。代码主流程参考论文[1]中的伪代码，部分实现细节参考[2]。\n本代码和MATLAB自带的seqminopt.m算法接口相同，可直接替换使用。下载代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 function [alphas, offset] = bitjoy_seqminopt(data, targetLabels, boxConstraints, ... kernelFunc, smoOptions) % https://bitjoy.net/posts/2016-05-02-svm-smo-algorithm/ % 参考文献: % [1] A Fast Algorithm for Training Support Vector Machines, % http://research.microsoft.com/pubs/69644/tr-98-14.pdf % [2] CSDN, http://blog.csdn.net/techq/article/details/6171688 N = length(data); % 样本数量 alphas = zeros([N,1]); % 所有拉格朗日乘子初始化为0 offset = 0.0; % 偏移量b也初始化为0 numChanged = 0; % 拉格朗日乘子改变的个数 examineAll = 1; % 是否需要检查所有拉格朗日乘子 % 主流程 while numChanged \u0026gt; 0 || examineAll numChanged = 0; if examineAll == 1 for i = 1 : N numChanged = numChanged + examineExample(i); end else for i = 1 : N if alphas(i) ~= 0 \u0026amp;\u0026amp; alphas(i) ~= boxConstraints(i) numChanged = numChanged + examineExample(i); end end end if examineAll == 1 examineAll = 0; elseif numChanged == 0 examineAll = 1; end end % 计算当前参数下第idx个样本的函数输出 function [svm_o] = wxpb(idx) svm_o = 0.0; for j = 1 : N svm_o = svm_o + alphas(j) * targetLabels(j) * kernelFunc(data(j,:),data(idx,:)); end svm_o = svm_o + offset; end % 选定第二个变量i2后，根据max|E1-E2|的启发式规则，选择i1； % 如果没有满足条件的i1，返回-1. function [i1] = selectSecondChoice(i2, E2) i1 = -1; maxDelta = -1; for j = 1 : N if j ~= i2 \u0026amp;\u0026amp; alphas(j) ~= 0 \u0026amp;\u0026amp; alphas(j) ~= boxConstraints(j) Ej = wxpb(j) - targetLabels(j); if abs(E2 - Ej) \u0026gt; maxDelta i1 = j; maxDelta = abs(E2 - Ej); end end end end % 根据选定的两个变量i1,i2，代入更新公式计算； % 最后还更新了偏移量offset，也就是y=wx+b中的b。 function [flag] = takeStep(i1, i2) alpha1 = alphas(i1); y1 = targetLabels(i1); E1 = wxpb(i1) - y1; alpha2 = alphas(i2); y2 = targetLabels(i2); E2 = wxpb(i2) - y2; s = y1 * y2; if y1 ~= y2 L = max(0, alpha2 - alpha1); H = min(boxConstraints(i2), boxConstraints(i1) + alpha2 - alpha1); else L = max(0, alpha2 + alpha1 - boxConstraints(i1)); H = min(boxConstraints(i2), alpha2 + alpha1); end if L == H flag = 0; return; end k11 = kernelFunc(data(i1,:),data(i1,:)); k12 = kernelFunc(data(i1,:),data(i2,:)); k22 = kernelFunc(data(i2,:),data(i2,:)); eta = k11 + k22 - 2 * k12; if eta \u0026gt; 0 a2 = alpha2 + y2 * (E1 - E2) / eta; %截断 if a2 \u0026lt; L a2 = L; elseif a2 \u0026gt; H a2 = H; end else % 并未处理该case，论文[1]公式(19)有更详细的方法 flag = 0; return; end if abs(a2 - alpha2) \u0026lt; eps * (a2 + alpha2 + eps) flag = 0; return; end a1 = alpha1 + s * (alpha2 - a2); alphas(i1) = a1; alphas(i2) = a2; % 更新offset b1 = offset - E1 - y1 * k11 * (a1 - alpha1) - y2 * k12 * (a2 - alpha2); b2 = offset - E2 - y1 * k12 * (a1 - alpha1) - y2 * k22 * (a2 - alpha2); if a1 \u0026gt; 0 \u0026amp;\u0026amp; a1 \u0026lt; boxConstraints(i1) offset = b1; elseif a2 \u0026gt; 0 \u0026amp;\u0026amp; a2 \u0026lt; boxConstraints(i2) offset = b2; else offset = (b1 + b2) / 2; end flag = 1; end % 检查样本。 % 首先判断i2是否满足KKT条件，如果不满足， % 则根据启发式规则再选择i1样本， % 然后更新i1和i2的拉格朗日乘子。 function [flag] = examineExample(i2) y2 = targetLabels(i2); alpha2 = alphas(i2); E2 = wxpb(i2) - y2; r2 = E2 * y2; if (r2 \u0026lt; -smoOptions.TolKKT \u0026amp;\u0026amp; alpha2 \u0026lt; boxConstraints(i2)) || (r2 \u0026gt; smoOptions.TolKKT \u0026amp;\u0026amp; alpha2 \u0026gt; 0) i1 = selectSecondChoice(i2, E2); if i1 == -1 i1 = floor(1 + rand() * N); % 随机选一个i1 while i1 == i2 i1 = floor(1 + rand() * N); end flag = takeStep(i1,i2); else flag = takeStep(i1,i2); end else flag = 0; end end end 参考资料\n[1]. J.C. Platt: A Fast Algorithm for Training Support Vector Machines\n[2]. CSDN, techq, SVM算法实现（一）\n[3]. 国科大叶齐祥老师机器学习方法与应用课程资料\n[4]. 支持向量机（SVM）的详细推导过程及注解（一）\n","permalink":"http://localhost:1313/posts/2016-05-02-svm-smo-algorithm/","summary":"\u003ch1 id=\"svm回顾\"\u003eSVM回顾\u003c/h1\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/20/SVM_Example_of_Hyperplanes.png\"\u003e\u003c/p\u003e\n\u003cp\u003e支持向量机（SVM）的一大特点是最大化间距（max margin）。对于如上图的二分类问题，虽然有很多线可以将左右两部分分开，但是只有中间的红线效果是最好的，因为它的可活动范围（margin）是最大的，从直观上来说很好理解。\u003c/p\u003e\n\u003cp\u003e对于线性二分类问题，假设分类面为\u003c/p\u003e\n$$\\begin{equation} u=\\vec w \\cdot \\vec x-b \\end{equation}$$\u003cp\u003e则margin为\u003c/p\u003e\n$$\\begin{equation} m=\\frac{1}{||w||_2} \\end{equation}$$\u003cp\u003e根据max margin规则和约束条件，得到如下优化问题，我们要求的就是参数\\(\\vec w\\)和\\(b\\)：\u003c/p\u003e\n$$\\begin{equation} \\min\\limits_{\\vec w,b}\\frac{1}{2}||\\vec w||^2 \\quad\\text{subject to}\\quad y_i(\\vec w\\cdot \\vec x_i-b) \\geq 1, \\forall i,\\end{equation}$$\u003cp\u003e对于正样本，类标号\\(y_i\\)为+1，反之则为-1。根据拉格朗日对偶，(3)可以转换为如下的二次规划（QP）问题，其中\\(\\alpha_i\\)为拉格朗日乘子。\u003c/p\u003e\n$$\\begin{equation} \\min\\limits_{\\vec \\alpha}\\Psi(\\vec\\alpha)=\\min\\limits_{\\vec \\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^Ny_iy_j(\\vec x_i\\cdot\\vec x_j)\\alpha_i\\alpha_j-\\sum_{i=1}^N\\alpha_i,\\end{equation}$$\u003cp\u003e其中N为样本数量。上式还需满足如下两个约束条件：\u003c/p\u003e\n$$\\begin{equation} \\alpha_i\\geq 0, \\forall i,\\end{equation}$$$$\\begin{equation} \\sum_{i=1}^Ny_i\\alpha_i=0.\\end{equation}$$\u003cp\u003e一旦求解出所有的拉格朗日乘子，则我们可以通过如下的公式得到分类面参数\\(\\vec w\\)和\\(b\\)。\u003c/p\u003e\n$$\\begin{equation}\\vec w=\\sum_{i=1}^Ny_i\\alpha_i\\vec x_i,\\quad b=\\vec w\\cdot\\vec x_k-y_k\\quad\\text{for some}\\quad\\alpha_k\u003e0.\\end{equation}$$\u003cp\u003e当然并不是所有的数据都可以完美的线性划分，可能有少量数据就是混在对方阵营，这时可以通过引入松弛变量\\(\\xi_i\\)得到软间隔形式的SVM：\u003c/p\u003e\n$$\\begin{equation} \\min\\limits_{\\vec w,b,\\vec\\xi}\\frac{1}{2}||\\vec w||^2+C\\sum_{i=1}^N\\xi_i \\quad\\text{subject to}\\quad y_i(\\vec w\\cdot \\vec x_i-b) \\geq 1-\\xi_i, \\forall i,\\end{equation}$$\u003cp\u003e其中的\\(\\xi_i\\)为松弛变量，能假装把错的样本分对，\\(C\\)对max margin和margin failures的trades off。对于这个新的优化问题，约束变成了一个box constraint：\u003c/p\u003e\n$$\\begin{equation}0\\leq\\alpha_i \\leq C,\\forall i.\\end{equation}$$\u003cp\u003e而松弛变量\\(\\xi_i\\)不再出现在对偶公式中了。\u003c/p\u003e","title":"SVM之序列最小最优化算法（SMO算法）"},{"content":"\nyn说最近在备考GMAT和托福，把手机都清理了只为专心学习。xx说TCP/IP大作业要用Qt做一个网络监控的软件，问我Qt好不好学。\nGRE遇见Qt，会擦出怎样的火花呢~没错，我用Qt写了一个强化背诵GRE单词的软件——Cracking 3000\n当时的一本GRE单词书有3000个单词，用杨鹏17天刷过之后，很多都记不住，于是想有没有办法把记不住的单词筛选出来，集中力量强化记忆。网上已经有3000的Excel表，所以我很自然的想到了把表格导入软件，用软件快速测试，并把不认识的单词筛选到新的Excel表格中。这样就可以把不认识的单词表打印出来，记完之后再导入软件进行新一轮的测试筛选，直到不认识的单词数为零。\n有了软件需求，代码实现起来就很快了。由于当时用Qt库比较多，所以直接拿来用了。软件实现这一块，主要是Excel表格的导入和导出，需要查一些文档，其他的就很简单了。\n虽然和GRE纠缠了一个多月，最终却没有考，但是想起当初早上6点爬起来背单词，晚上回宿舍抹黑写代码的情景，心情还是有一点小小的激动！以后类似的体验估计不会太多吧。\n最后祝yn GT考试顺利，xx大作业圆满完成！\nCracking_3000软件安装包及说明\n","permalink":"http://localhost:1313/posts/2016-03-16-hello-gre-do-you-like-me/","summary":"\u003cp\u003e\u003cimg alt=\"GRE3000\" loading=\"lazy\" src=\"/posts/2016-03-16-hello-gre-do-you-like-me/GRE3000.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eyn说最近在备考GMAT和托福，把手机都清理了只为专心学习。xx说TCP/IP大作业要用Qt做一个网络监控的软件，问我Qt好不好学。\u003c/p\u003e\n\u003cp\u003eGRE遇见Qt，会擦出怎样的火花呢~没错，我用Qt写了一个强化背诵GRE单词的软件——Cracking 3000\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Cracking-3000-1\" loading=\"lazy\" src=\"/posts/2016-03-16-hello-gre-do-you-like-me/Cracking-3000-1.png\"\u003e \u003cimg alt=\"Cracking-3000-2\" loading=\"lazy\" src=\"/posts/2016-03-16-hello-gre-do-you-like-me/Cracking-3000-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e当时的一本GRE单词书有3000个单词，用杨鹏17天刷过之后，很多都记不住，于是想有没有办法把记不住的单词筛选出来，集中力量强化记忆。网上已经有3000的Excel表，所以我很自然的想到了把表格导入软件，用软件快速测试，并把不认识的单词筛选到新的Excel表格中。这样就可以把不认识的单词表打印出来，记完之后再导入软件进行新一轮的测试筛选，直到不认识的单词数为零。\u003c/p\u003e\n\u003cp\u003e有了软件需求，代码实现起来就很快了。由于当时用Qt库比较多，所以直接拿来用了。软件实现这一块，主要是Excel表格的导入和导出，需要查一些文档，其他的就很简单了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"GRE3000-2\" loading=\"lazy\" src=\"/posts/2016-03-16-hello-gre-do-you-like-me/GRE3000-2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e虽然和GRE纠缠了一个多月，最终却没有考，但是想起当初早上6点爬起来背单词，晚上回宿舍抹黑写代码的情景，心情还是有一点小小的激动！以后类似的体验估计不会太多吧。\u003c/p\u003e\n\u003cp\u003e最后祝yn GT考试顺利，xx大作业圆满完成！\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/2016-03-16-hello-gre-do-you-like-me/Cracking_3000.zip\"\u003eCracking_3000软件安装包及说明\u003c/a\u003e\u003c/p\u003e","title":"和GRE纠缠的岁月"},{"content":"刚坐上去北京的列车，就收到了妈妈的微信语音：霖，早上收拾东西怎么忘了带上我给你洗好的鞋呀。我这才想起早上妈妈把洗好的鞋和叠好的衣服放在我房间，我却忘了带鞋。\n后来和爸妈在群里聊了起来。当我问爸爸什么时候返回学校时，他却说前天突然请假回家惹老板不高兴了，可能要被炒鱿鱼。是，老爸在那个学校当老师十几年了，我平时老数落他当老师工资那么低，为什么不改行，可突然听到这个消息，心里却不是滋味。\n其实老爸没必要请假回来的。前几天我发脾气，老爸好像真的决定转行搞种植业了，托我在淘宝买了好多枸杞树，自己带回了五十棵脐橙树苗，还准备去某个地方考察什么药材。\n离家前一天，妈妈特地跑到县城买了好多排骨回来，还煮了十个土鸡蛋要我带着路上吃。老爸买了好多苹果、香蕉、猕猴桃要我带着路上吃。今天早上收拾行李的时候，从来不动手的爸爸，也抢着往我包里塞各种牛奶和水果。\n二十多年了，经历了多少次的离家，从来没有像今天这样的不舍。二十多年了，我突然发现爸爸妈妈变矮了，爸爸的额头黑得发亮，妈妈的眼角也长出了好多鱼尾纹。\n今年难得有一个月的寒假，但我整天忙着看论文、书和电视剧，和爸妈的交流反而少了。有天吃过晚饭，发现妈妈独自坐在客厅戳着她的手机。我问过才发现原来妈妈想看哥哥和他女朋友的照片，但是怎么都弄不出来，我帮妈妈找出来之后，还教她怎么用微信和qq，妈妈说wifi图标像降落伞，我说你什么时候想上网就把降落伞打开，我说你如果想和哥哥聊天，就按住底部的按钮，等到出现小喇叭之后就可以说话了，说完放开手，听到“嗖“”的一声，说的话就发过去了，但是妈妈经常忘记打开降落伞，经常忘记按小喇叭。。。\n刚刷QQ空间的时候，看到一个同学的说说“马上又要去坐火车回武汉了，在家的时间越来越少了，没能好好陪陪父母，我不是称职的儿子。”\n坐在火车上，看着窗外闪过的霓虹灯，突然觉得这个世界好陌生好无情，每个人在时间面前是多么的渺小。\n2016年2月26日于z68列车上。\n","permalink":"http://localhost:1313/posts/2016-02-26-leave-home-again/","summary":"\u003cp\u003e刚坐上去北京的列车，就收到了妈妈的微信语音：霖，早上收拾东西怎么忘了带上我给你洗好的鞋呀。我这才想起早上妈妈把洗好的鞋和叠好的衣服放在我房间，我却忘了带鞋。\u003c/p\u003e\n\u003cp\u003e后来和爸妈在群里聊了起来。当我问爸爸什么时候返回学校时，他却说前天突然请假回家惹老板不高兴了，可能要被炒鱿鱼。是，老爸在那个学校当老师十几年了，我平时老数落他当老师工资那么低，为什么不改行，可突然听到这个消息，心里却不是滋味。\u003c/p\u003e\n\u003cp\u003e其实老爸没必要请假回来的。前几天我发脾气，老爸好像真的决定转行搞种植业了，托我在淘宝买了好多枸杞树，自己带回了五十棵脐橙树苗，还准备去某个地方考察什么药材。\u003c/p\u003e\n\u003cp\u003e离家前一天，妈妈特地跑到县城买了好多排骨回来，还煮了十个土鸡蛋要我带着路上吃。老爸买了好多苹果、香蕉、猕猴桃要我带着路上吃。今天早上收拾行李的时候，从来不动手的爸爸，也抢着往我包里塞各种牛奶和水果。\u003c/p\u003e\n\u003cp\u003e二十多年了，经历了多少次的离家，从来没有像今天这样的不舍。二十多年了，我突然发现爸爸妈妈变矮了，爸爸的额头黑得发亮，妈妈的眼角也长出了好多鱼尾纹。\u003c/p\u003e\n\u003cp\u003e今年难得有一个月的寒假，但我整天忙着看论文、书和电视剧，和爸妈的交流反而少了。有天吃过晚饭，发现妈妈独自坐在客厅戳着她的手机。我问过才发现原来妈妈想看哥哥和他女朋友的照片，但是怎么都弄不出来，我帮妈妈找出来之后，还教她怎么用微信和qq，妈妈说wifi图标像降落伞，我说你什么时候想上网就把降落伞打开，我说你如果想和哥哥聊天，就按住底部的按钮，等到出现小喇叭之后就可以说话了，说完放开手，听到“嗖“”的一声，说的话就发过去了，但是妈妈经常忘记打开降落伞，经常忘记按小喇叭。。。\u003c/p\u003e\n\u003cp\u003e刚刷QQ空间的时候，看到一个同学的说说“马上又要去坐火车回武汉了，在家的时间越来越少了，没能好好陪陪父母，我不是称职的儿子。”\u003c/p\u003e\n\u003cp\u003e坐在火车上，看着窗外闪过的霓虹灯，突然觉得这个世界好陌生好无情，每个人在时间面前是多么的渺小。\u003c/p\u003e\n\u003cp\u003e2016年2月26日于z68列车上。\u003c/p\u003e","title":"爸妈老了"},{"content":"现在是2016年2月4日，距离农历新年不到4天，结束了半年的国科大研一生活，躺在被窝里，松了一口气……\n来国科大之前，在贴吧上了解到国科大雁栖湖校区地处偏远农村，周边几乎没有娱乐场所；但同时学校的软硬件设施非常的棒：豪华单人间，研究员甚至院士亲自授课等等。所以对国科大雁栖湖校区满是憧憬。至今还清楚的记得坐校车从玉泉路过来时，沿途看到APEC主会场的鸟蛋、国科大桥、钟楼以及国科大正门几个大字时的激动心情~\n入住国科大，着实被UCAS的蓝天白云、青山绿水给迷住了。\n当然，凡事有利必有弊，因为这里远离市区，环境好，但正因为远离市区，几乎没有年轻人的娱乐活动，想要看个电影唱个歌少说也得跑城里，再要想感受下帝都奢靡的生活，必须各种倒车近2个小时到市里。\n研一这上学期，半年只进市里两次，一次是买山地车，一次是回所里开会。购物主要靠天猫超市。\n九十月份，大家都和大一新生似的，各种疯玩，野长城、雁栖湖、慕田峪、青龙峡、密云水库。进入十一月，新鲜劲过去了，又开始各种赶大小作业，复习考试。\n这是我这学期的课表，看着课好像不多，每天都有半天休息，但是真的感觉回到了大三呀！尤其数据挖掘、信息检索、矩阵论一周上两次课，当天上完的课如果没有及时复习，隔一天再学新内容完全跟不上啊，而且矩阵论每次课都有好多作业啊，这数学课不做练习完全消化不了呀。更神的课还要数周五的卜神算法，君不见，每到周四晚上，西A、西B两栋宿舍，灯火通明，大家都在熬夜赶算法作业啊，不熬个两三点都不好意思和别人说你熬夜了呀。大家可以感受一下我整理的卜神算法作业~~\n正是因为这奇葩的课程安排，这半年几乎没有12点前睡过觉，估计平均是1:30才睡觉，早上8点多才起，中午也没午休。想想大学的时候按时作息，真是惭愧。期间有一次听说搜狐一同届华科毕业生猝死，朋友圈传得沸沸扬扬，大伙都吓得要命，纷纷表示绝不熬夜，早睡早起，我那天也是吓坏了，决定早睡，11:30就爬床上了，但是不知道是因为紧张还是熬夜习惯了，辗转反侧，到12点多才睡着的。\n我们研一在国科大上课是有补助的，但是在帝都完全不够用啊，而且CS相关的几个所补助都比ICT高，so当时还公车上书，各种写联名信、起义，经过半年之久的持久战，所里终于答应从2016年开始给我们涨500块钱的工资。涨了之后差不多够吃饭了。\n虽然这半年课业繁重，但是也抽空锻炼了身体，天气不是很冷的时候，隔一天就会去夜跑；而且选了乒乓球课，从直拍转为了横拍，并且在课上结识了路路，打球好厉害的一个女生，每次老师来指导的时候，都叫路路温柔点 O__O “…\n另外花了一千多块钱买了一辆二手山地车，骑着到处转悠了一下。很有缘的是，认识了一位才女。本来我们骑行社一块去美利达准备买车，但是由于种种原因我和小欣都没买，然后我们一块坐小黑车回村，在车上聊着聊着就认识，没想到后来还成为了好朋友，小欣的台球和乒乓球都打得不错，琴棋书画样样精通。突然发现身边的学霸才子佳人好多，更加深刻感受到有些东西不是你努力就能够弥补的，天赋、眼界、才艺、品味、性格……\n来国科大的这半年，自我感觉变化最大的是自己变得爱说话了，而且带着一种zhuang bi气息，不知道是不是受某几个我一直崇拜的人的影响。有时候静下心来想想都不敢相信之前的话是我说的，和大学时的我完全判若两人。当然这种事情有利也有弊，还在慢慢找平衡点，可能正如CL说的“话怎么说是一回事，内心要知道自己想要的”。\n这半年突然害怕一个人上学，一个人吃饭，一个人自习了，更喜欢face-to-face的交谈，少了对网络的依赖，不知道是不是因为性格的变化、环境的变化、抑或是认识的人多了，有了念想。\n半年时光，认识了不多不少几个好朋友：良辰、发文章、牛牛、欣儿、路路，有你们真好，谢谢你们~\n2016猴赛雷，即将从研一的上课转入课题组工作，很关键的一年，加油！\n","permalink":"http://localhost:1313/posts/2016-02-04-half-year-experience-report-in-ucas/","summary":"\u003cp\u003e现在是2016年2月4日，距离农历新年不到4天，结束了半年的国科大研一生活，躺在被窝里，松了一口气……\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ucas-2015-1\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e来国科大之前，在贴吧上了解到国科大雁栖湖校区地处偏远农村，周边几乎没有娱乐场所；但同时学校的软硬件设施非常的棒：豪华单人间，研究员甚至院士亲自授课等等。所以对国科大雁栖湖校区满是憧憬。至今还清楚的记得坐校车从玉泉路过来时，沿途看到APEC主会场的鸟蛋、国科大桥、钟楼以及国科大正门几个大字时的激动心情~\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ucas-2015-7\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-7.jpg\"\u003e \u003cimg alt=\"ucas-2015-3\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-3.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e入住国科大，着实被UCAS的蓝天白云、青山绿水给迷住了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ucas-2015-2\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-2.jpg\"\u003e \u003cimg alt=\"ucas-2015-4\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-4.jpg\"\u003e \u003cimg alt=\"ucas-2015-5\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-5.jpg\"\u003e \u003cimg alt=\"ucas-2015-6\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-6.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e当然，凡事有利必有弊，因为这里远离市区，环境好，但正因为远离市区，几乎没有年轻人的娱乐活动，想要看个电影唱个歌少说也得跑城里，再要想感受下帝都奢靡的生活，必须各种倒车近2个小时到市里。\u003c/p\u003e\n\u003cp\u003e研一这上学期，半年只进市里两次，一次是买山地车，一次是回所里开会。购物主要靠天猫超市。\u003c/p\u003e\n\u003cp\u003e九十月份，大家都和大一新生似的，各种疯玩，野长城、雁栖湖、慕田峪、青龙峡、密云水库。进入十一月，新鲜劲过去了，又开始各种赶大小作业，复习考试。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ucas-schedule-2015-fall\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-schedule-2015-fall.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是我这学期的课表，看着课好像不多，每天都有半天休息，但是真的感觉回到了大三呀！尤其数据挖掘、信息检索、矩阵论一周上两次课，当天上完的课如果没有及时复习，隔一天再学新内容完全跟不上啊，而且矩阵论每次课都有好多作业啊，这数学课不做练习完全消化不了呀。更神的课还要数周五的卜神算法，君不见，每到周四晚上，西A、西B两栋宿舍，灯火通明，大家都在熬夜赶算法作业啊，不熬个两三点都不好意思和别人说你熬夜了呀。\u003ca href=\"https://bitjoy.net/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/\"\u003e大家可以感受一下我整理的卜神算法作业~~\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e正是因为这奇葩的课程安排，这半年几乎没有12点前睡过觉，估计平均是1:30才睡觉，早上8点多才起，中午也没午休。想想大学的时候按时作息，真是惭愧。期间有一次听说搜狐一同届华科毕业生猝死，朋友圈传得沸沸扬扬，大伙都吓得要命，纷纷表示绝不熬夜，早睡早起，我那天也是吓坏了，决定早睡，11:30就爬床上了，但是不知道是因为紧张还是熬夜习惯了，辗转反侧，到12点多才睡着的。\u003c/p\u003e\n\u003cp\u003e我们研一在国科大上课是有补助的，但是在帝都完全不够用啊，而且CS相关的几个所补助都比ICT高，so当时还公车上书，各种写联名信、起义，经过半年之久的持久战，所里终于答应从2016年开始给我们涨500块钱的工资。涨了之后差不多够吃饭了。\u003c/p\u003e\n\u003cp\u003e虽然这半年课业繁重，但是也抽空锻炼了身体，天气不是很冷的时候，隔一天就会去夜跑；而且选了乒乓球课，从直拍转为了横拍，并且在课上结识了路路，打球好厉害的一个女生，每次老师来指导的时候，都叫路路温柔点 O__O “…\u003c/p\u003e\n\u003cp\u003e另外花了一千多块钱买了一辆二手山地车，骑着到处转悠了一下。很有缘的是，认识了一位才女。本来我们骑行社一块去美利达准备买车，但是由于种种原因我和小欣都没买，然后我们一块坐小黑车回村，在车上聊着聊着就认识，没想到后来还成为了好朋友，小欣的台球和乒乓球都打得不错，琴棋书画样样精通。突然发现身边的学霸才子佳人好多，更加深刻感受到有些东西不是你努力就能够弥补的，天赋、眼界、才艺、品味、性格……\u003c/p\u003e\n\u003cp\u003e来国科大的这半年，自我感觉变化最大的是自己变得爱说话了，而且带着一种zhuang bi气息，不知道是不是受某几个我一直崇拜的人的影响。有时候静下心来想想都不敢相信之前的话是我说的，和大学时的我完全判若两人。当然这种事情有利也有弊，还在慢慢找平衡点，可能正如CL说的“话怎么说是一回事，内心要知道自己想要的”。\u003c/p\u003e\n\u003cp\u003e这半年突然害怕一个人上学，一个人吃饭，一个人自习了，更喜欢face-to-face的交谈，少了对网络的依赖，不知道是不是因为性格的变化、环境的变化、抑或是认识的人多了，有了念想。\u003c/p\u003e\n\u003cp\u003e半年时光，认识了不多不少几个好朋友：良辰、发文章、牛牛、欣儿、路路，有你们真好，谢谢你们~\u003c/p\u003e\n\u003cp\u003e2016猴赛雷，即将从研一的上课转入课题组工作，很关键的一年，加油！\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/image.thepaper.cn/www/image/4/753/520.jpg\"\u003e\u003c/p\u003e","title":"国科大半年体验报告"},{"content":"这学期选修了卜老师的算法课，都说这课是神课，上过之后果然是神课。同样是算法课，别人12月底就考完了，我们要1月底才考试。\n本课程主要讲了以下几个专题：\nDivide-and-conquer Dynamic programming Greedy Linear programming Linear programming: duality Network flow Problem hardness: Polynomial-time reduction NP-Completeness Approximation algorithm 前三个专题的算法大多数本科时学过的，但是经卜老师讲一遍还会有新的收获。后六个专题接触较少，学到了很多新算法。\n下图是卜老师每节课必讲的问题求解思路图：\n（待我回家把图画出来…）\n本课程最神的要数课后作业了，一般deadline是周五，每到周四晚上，大家都做好熬通宵赶作业的准备，没熬到两三点都不好意思睡觉，我同学有一次甚至熬到了第二天六点！\n每次作业大概有10题，前7题是算法设计，后3题是算法实现，每题都不是省油的灯，不过如果把每道题都理解消化，算法及编程能力会有很大的提高。\n下面是我整理出来的算法题目和个人解答，大家感受一下。（仅供完成作业之后交流使用，拒绝抄袭！）\nAssignment1_DandC.zip A1sol.pdf | A1sol.tex A1sol_supplement.pdf | A1sol_supplement.tex_.zip Assignment2_DP.zip A2sol.pdf | A2sol.tex_.zip A2sol_supplement.pdf | A2sol_supplement.tex Assignment3_Greedy.zip A3sol.pdf | A3sol.tex_.zip A3sol_supplement.pdf | A3sol_supplement.tex Assignment4_LP.zip A4sol.pdf | A4sol.tex A4sol_supplement.pdf | A4sol_supplement.tex Assignment5_NF.zip A5sol.pdf | A5sol.tex A5sol_supplement.pdf | A5sol_supplement.tex Assignment6_NP.pdf A6sol.pdf | A6sol.tex Assignment7_App.pdf A7sol.pdf | A7sol.tex ","permalink":"http://localhost:1313/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/","summary":"\u003cp\u003e这学期选修了\u003ca href=\"http://bioinfo.ict.ac.cn/~dbu/AlgorithmCourses/CS711008Z/CS711008Z_2015.html\"\u003e卜老师的算法课\u003c/a\u003e，都说这课是神课，上过之后果然是神课。同样是算法课，别人12月底就考完了，我们要1月底才考试。\u003c/p\u003e\n\u003cp\u003e本课程主要讲了以下几个专题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDivide-and-conquer\u003c/li\u003e\n\u003cli\u003eDynamic programming\u003c/li\u003e\n\u003cli\u003eGreedy\u003c/li\u003e\n\u003cli\u003eLinear programming\u003c/li\u003e\n\u003cli\u003eLinear programming: duality\u003c/li\u003e\n\u003cli\u003eNetwork flow\u003c/li\u003e\n\u003cli\u003eProblem hardness: Polynomial-time reduction\u003c/li\u003e\n\u003cli\u003eNP-Completeness\u003c/li\u003e\n\u003cli\u003eApproximation algorithm\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e前三个专题的算法大多数本科时学过的，但是经卜老师讲一遍还会有新的收获。后六个专题接触较少，学到了很多新算法。\u003c/p\u003e\n\u003cp\u003e下图是卜老师每节课必讲的问题求解思路图：\u003c/p\u003e\n\u003cp\u003e（待我回家把图画出来…）\u003c/p\u003e\n\u003cp\u003e本课程最神的要数课后作业了，一般deadline是周五，每到周四晚上，大家都做好熬通宵赶作业的准备，没熬到两三点都不好意思睡觉，我同学有一次甚至熬到了第二天六点！\u003c/p\u003e\n\u003cp\u003e每次作业大概有10题，前7题是算法设计，后3题是算法实现，每题都不是省油的灯，不过如果把每道题都理解消化，算法及编程能力会有很大的提高。\u003c/p\u003e\n\u003cp\u003e下面是我整理出来的算法题目和个人解答，大家感受一下。（\u003cstrong\u003e仅供完成作业之后交流使用，拒绝抄袭！\u003c/strong\u003e）\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment1_DandC.zip\"\u003eAssignment1_DandC.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A1sol.pdf\"\u003eA1sol.pdf\u003c/a\u003e   |  \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A1sol.tex\"\u003eA1sol.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A1sol_supplement.pdf\"\u003eA1sol_supplement.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A1sol_supplement.tex_.zip\"\u003eA1sol_supplement.tex_.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment2_DP.zip\"\u003eAssignment2_DP.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A2sol.pdf\"\u003eA2sol.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A2sol.tex_.zip\"\u003eA2sol.tex_.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A2sol_supplement.pdf\"\u003eA2sol_supplement.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A2sol_supplement.tex\"\u003eA2sol_supplement.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment3_Greedy.zip\"\u003eAssignment3_Greedy.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A3sol.pdf\"\u003eA3sol.pdf\u003c/a\u003e  |  \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A3sol.tex_.zip\"\u003eA3sol.tex_.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A3sol_supplement.pdf\"\u003eA3sol_supplement.pdf\u003c/a\u003e  |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A3sol_supplement.tex\"\u003eA3sol_supplement.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment4_LP.zip\"\u003eAssignment4_LP.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A4sol.pdf\"\u003eA4sol.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A4sol.tex\"\u003eA4sol.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A4sol_supplement.pdf\"\u003eA4sol_supplement.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A4sol_supplement.tex\"\u003eA4sol_supplement.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment5_NF.zip\"\u003eAssignment5_NF.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A5sol.pdf\"\u003eA5sol.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A5sol.tex\"\u003eA5sol.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A5sol_supplement.pdf\"\u003eA5sol_supplement.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A5sol_supplement.tex\"\u003eA5sol_supplement.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment6_NP.pdf\"\u003eAssignment6_NP.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A6sol.pdf\"\u003eA6sol.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A6sol.tex\"\u003eA6sol.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment7_App.pdf\"\u003eAssignment7_App.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A7sol.pdf\"\u003eA7sol.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A7sol.tex\"\u003eA7sol.tex\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"卜神算法作业整理"},{"content":"至此，整个新闻搜索引擎构建完毕，总体效果令人满意，不过还是有很多可以改进的地方。下面总结一下本系统的优点和不足。\n优点\n倒排索引存储方式。因为不同词项的倒排记录表长度一般不同，所以没办法以常规的方式存入关系型数据库。通过将一个词项的倒排记录表序列化成一个字符串再存入数据库，读取的时候通过反序列化获得相关数据，整个结构类似于邻接表的形式。\n推荐阅读实现方式。利用特征提取的方法，用25个关键词表示一篇新闻，大大减小了文档词项矩阵规模，提高计算效率的同时不影响推荐新闻相关性。\n借用了Reddit的热度公式，融合了时间因素。\n不足\n构建索引时，为了降低索引规模，提高算法速度，我们将纯数字词项过滤了，同时忽略了词项大小写。虽然索引规模下降了，但是牺牲了搜索引擎的正确率。\n构建索引时，采用了jieba的精确分词模式，比如句子“我来到北京清华大学”的分词结果为“我/ 来到/ 北京/ 清华大学”，“清华大学”作为一个整体被当作一个词项，如果搜索关键词是“清华”，则该句子不能匹配，但显然这个句子和“清华”相关。所以后续可以采用结巴的搜索引擎分词模式，虽然索引规模增加了，但能提升搜索引擎的召回率。\n在推荐阅读模块，虽然进行了维度约减，但是当数据量较大时（数十万条新闻），得到的文档词项矩阵也是巨大的，会远远超过现有PC的内存大小。所以可以先对新闻进行粗略的聚类，再在类内计算两两cosine相似度，得到值得推荐的新闻。\n在热度公式中，虽然借用了Reddit的公式，大的方向是正确的，但是引入了参数\\(k_1\\)和\\(k_2\\)，而且将其简单的设置为1。如果能够由专家给出或者经过机器学习训练得到，则热度公式的效果会更好。\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-09-introduction-to-building-a-search-engine-7/","summary":"\u003cp\u003e至此，整个新闻搜索引擎构建完毕，总体效果令人满意，不过还是有很多可以改进的地方。下面总结一下本系统的优点和不足。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e优点\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e倒排索引存储方式。因为不同词项的倒排记录表长度一般不同，所以没办法以常规的方式存入关系型数据库。通过将一个词项的倒排记录表序列化成一个字符串再存入数据库，读取的时候通过反序列化获得相关数据，整个结构类似于邻接表的形式。\u003c/p\u003e\n\u003cp\u003e推荐阅读实现方式。利用特征提取的方法，用25个关键词表示一篇新闻，大大减小了文档词项矩阵规模，提高计算效率的同时不影响推荐新闻相关性。\u003c/p\u003e\n\u003cp\u003e借用了Reddit的热度公式，融合了时间因素。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e不足\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e构建索引时，为了降低索引规模，提高算法速度，我们将纯数字词项过滤了，同时忽略了词项大小写。虽然索引规模下降了，但是牺牲了搜索引擎的正确率。\u003c/p\u003e\n\u003cp\u003e构建索引时，采用了jieba的精确分词模式，比如句子“我来到北京清华大学”的分词结果为“我/ 来到/ 北京/ 清华大学”，“清华大学”作为一个整体被当作一个词项，如果搜索关键词是“清华”，则该句子不能匹配，但显然这个句子和“清华”相关。所以后续可以采用结巴的搜索引擎分词模式，虽然索引规模增加了，但能提升搜索引擎的召回率。\u003c/p\u003e\n\u003cp\u003e在推荐阅读模块，虽然进行了维度约减，但是当数据量较大时（数十万条新闻），得到的文档词项矩阵也是巨大的，会远远超过现有PC的内存大小。所以可以先对新闻进行粗略的聚类，再在类内计算两两cosine相似度，得到值得推荐的新闻。\u003c/p\u003e\n\u003cp\u003e在热度公式中，虽然借用了Reddit的公式，大的方向是正确的，但是引入了参数\\(k_1\\)和\\(k_2\\)，而且将其简单的设置为1。如果能够由专家给出或者经过机器学习训练得到，则热度公式的效果会更好。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e完整可运行的新闻搜索引擎Demo请看我的Github项目\u003ca href=\"https://github.com/01joy/news_search_engine\"\u003enews_search_engine\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e以下是系列博客：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-1/\"\u003e和我一起构建搜索引擎（一）简介\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-2/\"\u003e和我一起构建搜索引擎（二）网络爬虫\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-3/\"\u003e和我一起构建搜索引擎（三）构建索引\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-4/\"\u003e和我一起构建搜索引擎（四）检索模型\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-5/\"\u003e和我一起构建搜索引擎（五）推荐阅读\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-6/\"\u003e和我一起构建搜索引擎（六）系统展示\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-7/\"\u003e和我一起构建搜索引擎（七）总结展望\u003c/a\u003e\u003c/p\u003e","title":"和我一起构建搜索引擎（七）总结展望"},{"content":"前几个博客已经介绍完搜索引擎的所有功能，为了实现更好的用户体验，需要一个web界面。这一部分是另一个队员做的，我这里借用他的代码。\n我们利用开源的Flask Web框架搭建了展示系统，搜索引擎只需要两个界面，一个是搜索界面，另一个是展示详细新闻的页面（实际搜索引擎没有这个页面）。编写好这两个模板页面并调用前面给出的接口，得到数据，展示出来就可以。\n这一部分没有太多需要讲解的算法，直接上效果图（点击图片可以查看大图）。\n图1. 搜索页面\n图2. 新闻详情页面\n由于数据量不大，只有1000条新闻，所以第一页中后面几个结果相关度就不是很高了。但是经过测试，在大数据量的情况下，不论是搜索的速度、准确率、召回率以及推荐阅读的相关度，都达到了不错的效果。\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-09-introduction-to-building-a-search-engine-6/","summary":"\u003cp\u003e前几个博客已经介绍完搜索引擎的所有功能，为了实现更好的用户体验，需要一个web界面。这一部分是另一个队员做的，我这里借用他的代码。\u003c/p\u003e\n\u003cp\u003e我们利用开源的\u003ca href=\"http://flask.pocoo.org/\"\u003eFlask Web框架\u003c/a\u003e搭建了展示系统，搜索引擎只需要两个界面，一个是搜索界面，另一个是展示详细新闻的页面（实际搜索引擎没有这个页面）。编写好这两个模板页面并调用前面给出的接口，得到数据，展示出来就可以。\u003c/p\u003e\n\u003cp\u003e这一部分没有太多需要讲解的算法，直接上效果图（点击图片可以查看大图）。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图1. 搜索页面\" loading=\"lazy\" src=\"/posts/2016-01-09-introduction-to-building-a-search-engine-6/News-Search-Engine1.webp\"\u003e\n图1. 搜索页面\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图2. 新闻详情页面\" loading=\"lazy\" src=\"/posts/2016-01-09-introduction-to-building-a-search-engine-6/News-Search-Engine2.webp\"\u003e\n图2. 新闻详情页面\u003c/p\u003e\n\u003cp\u003e由于数据量不大，只有1000条新闻，所以第一页中后面几个结果相关度就不是很高了。但是经过测试，在大数据量的情况下，不论是搜索的速度、准确率、召回率以及推荐阅读的相关度，都达到了不错的效果。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e完整可运行的新闻搜索引擎Demo请看我的Github项目\u003ca href=\"https://github.com/01joy/news_search_engine\"\u003enews_search_engine\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e以下是系列博客：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-1/\"\u003e和我一起构建搜索引擎（一）简介\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-2/\"\u003e和我一起构建搜索引擎（二）网络爬虫\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-3/\"\u003e和我一起构建搜索引擎（三）构建索引\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-4/\"\u003e和我一起构建搜索引擎（四）检索模型\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-5/\"\u003e和我一起构建搜索引擎（五）推荐阅读\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-6/\"\u003e和我一起构建搜索引擎（六）系统展示\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-7/\"\u003e和我一起构建搜索引擎（七）总结展望\u003c/a\u003e\u003c/p\u003e","title":"和我一起构建搜索引擎（六）系统展示"},{"content":"虽然主要的检索功能实现了，但是我们还需要一个“推荐阅读”的功能。当用户浏览某条具体新闻时，我们在页面底端给出5条和该新闻相关的新闻，也就是一个最简单的推荐系统。\n搜狐新闻“相关新闻”模块\n推荐模块的思路是度量两两新闻之间的相似度，取相似度最高的前5篇新闻作为推荐阅读的新闻。\n我们前面讲过，一篇文档可以用一个向量表示，向量中的每个值是不同词项t在该文档d中的词频tf。但是一篇较短的文档（如新闻）的关键词并不多，所以我们可以提取每篇新闻的关键词，用这些关键词的tfidf值构成文档的向量表示，这样能够大大减少相似度计算量，同时保持较好的推荐效果。\njieba分词组件自带关键词提取功能，并能返回关键词的tfidf值。所以对每篇新闻，我们先提取tfidf得分最高的前25个关键词，用这25个关键词的tfidf值作为文档的向量表示。由此能够得到一个1000*m的文档词项矩阵M，矩阵每行表示一个文档，每列表示一个词项，m为1000个文档的所有互异的关键词（大概10000个）。矩阵M当然也是稀疏矩阵。\n得到文档词项矩阵M之后，我们利用sklearn的pairwise_distances函数计算M中行向量之间的cosine相似度，对每个文档，得到与其最相似的前5篇新闻id，并把结果写入数据库。\n推荐阅读模块的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Wed Dec 23 14:06:10 2015 @author: bitjoy.net \u0026#34;\u0026#34;\u0026#34; from os import listdir import xml.etree.ElementTree as ET import jieba import jieba.analyse import sqlite3 import configparser from datetime import * import math import pandas as pd import numpy as np from sklearn.metrics import pairwise_distances class RecommendationModule: stop_words = set() k_nearest = [] config_path = \u0026#39;\u0026#39; config_encoding = \u0026#39;\u0026#39; doc_dir_path = \u0026#39;\u0026#39; doc_encoding = \u0026#39;\u0026#39; stop_words_path = \u0026#39;\u0026#39; stop_words_encoding = \u0026#39;\u0026#39; idf_path = \u0026#39;\u0026#39; db_path = \u0026#39;\u0026#39; def __init__(self, config_path, config_encoding): self.config_path = config_path self.config_encoding = config_encoding config = configparser.ConfigParser() config.read(config_path, config_encoding) self.doc_dir_path = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_dir_path\u0026#39;] self.doc_encoding = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_encoding\u0026#39;] self.stop_words_path = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;stop_words_path\u0026#39;] self.stop_words_encoding = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;stop_words_encoding\u0026#39;] self.idf_path = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;idf_path\u0026#39;] self.db_path = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;db_path\u0026#39;] f = open(self.stop_words_path, encoding = self.stop_words_encoding) words = f.read() self.stop_words = set(words.split(\u0026#39;\\n\u0026#39;)) def write_k_nearest_matrix_to_db(self): conn = sqlite3.connect(self.db_path) c = conn.cursor() c.execute(\u0026#34;\u0026#39;DROP TABLE IF EXISTS knearest\u0026#39;\u0026#34;) c.execute(\u0026#34;\u0026#39;CREATE TABLE knearest(id INTEGER PRIMARY KEY, first INTEGER, second INTEGER, third INTEGER, fourth INTEGER, fifth INTEGER)\u0026#39;\u0026#34;) for docid, doclist in self.k_nearest: c.execute(\u0026#34;INSERT INTO knearest VALUES (?, ?, ?, ?, ?, ?)\u0026#34;, tuple([docid] + doclist)) conn.commit() conn.close() def is_number(self, s): try: float(s) return True except ValueError: return False def construct_dt_matrix(self, files, topK = 200): jieba.analyse.set_stop_words(self.stop_words_path) jieba.analyse.set_idf_path(self.idf_path) M = len(files) N = 1 terms = {} dt = [] for i in files: root = ET.parse(self.doc_dir_path + i).getroot() title = root.find(\u0026#39;title\u0026#39;).text body = root.find(\u0026#39;body\u0026#39;).text docid = int(root.find(\u0026#39;id\u0026#39;).text) tags = jieba.analyse.extract_tags(title + \u0026#39;。\u0026#39; + body, topK=topK, withWeight=True) #tags = jieba.analyse.extract_tags(title, topK=topK, withWeight=True) cleaned_dict = {} for word, tfidf in tags: word = word.strip().lower() if word == \u0026#39;\u0026#39; or self.is_number(word): continue cleaned_dict[word] = tfidf if word not in terms: terms[word] = N N += 1 dt.append([docid, cleaned_dict]) dt_matrix = [[0 for i in range(N)] for j in range(M)] i =0 for docid, t_tfidf in dt: dt_matrix[i][0] = docid for term, tfidf in t_tfidf.items(): dt_matrix[i][terms[term]] = tfidf i += 1 dt_matrix = pd.DataFrame(dt_matrix) dt_matrix.index = dt_matrix[0] print(\u0026#39;dt_matrix shape:(%d %d)\u0026#39;%(dt_matrix.shape)) return dt_matrix def construct_k_nearest_matrix(self, dt_matrix, k): tmp = np.array(1 – pairwise_distances(dt_matrix[dt_matrix.columns[1:]], metric = \u0026#34;cosine\u0026#34;)) similarity_matrix = pd.DataFrame(tmp, index = dt_matrix.index.tolist(), columns = dt_matrix.index.tolist()) for i in similarity_matrix.index: tmp = [int(i),[]] j = 0 while j \u0026lt;= k: max_col = similarity_matrix.loc[i].idxmax(axis = 1) similarity_matrix.loc[i][max_col] = -1 if max_col != i: tmp[1].append(int(max_col)) #max column name j += 1 self.k_nearest.append(tmp) def gen_idf_file(self): files = listdir(self.doc_dir_path) n = float(len(files)) idf = {} for i in files: root = ET.parse(self.doc_dir_path + i).getroot() title = root.find(\u0026#39;title\u0026#39;).text body = root.find(\u0026#39;body\u0026#39;).text seg_list = jieba.lcut(title + \u0026#39;。\u0026#39; + body, cut_all=False) seg_list = set(seg_list) – self.stop_words for word in seg_list: word = word.strip().lower() if word == \u0026#39;\u0026#39; or self.is_number(word): continue if word not in idf: idf[word] = 1 else: idf[word] = idf[word] + 1 idf_file = open(self.idf_path, \u0026#39;w\u0026#39;, encoding = \u0026#39;utf-8\u0026#39;) for word, df in idf.items(): idf_file.write(\u0026#39;%s %.9f\\n\u0026#39;%(word, math.log(n / df))) idf_file.close() def find_k_nearest(self, k, topK): self.gen_idf_file() files = listdir(self.doc_dir_path) dt_matrix = self.construct_dt_matrix(files, topK) self.construct_k_nearest_matrix(dt_matrix, k) self.write_k_nearest_matrix_to_db() if __name__ == \u0026#34;__main__\u0026#34;: print(\u0026#39;—–start time: %s—–\u0026#39;%(datetime.today())) rm = RecommendationModule(\u0026#39;../config.ini\u0026#39;, \u0026#39;utf-8\u0026#39;) rm.find_k_nearest(5, 25) print(\u0026#39;—–finish time: %s—–\u0026#39;%(datetime.today())) 这个模块的代码量最多，主要原因是需要构建文档词项矩阵，并且计算k邻居矩阵。矩阵数据结构的设计需要特别注意，否则会严重影响系统的效率。我刚开始把任务都扔给了pandas.DataFrame，后来发现当两个文档向量合并时，需要join连接操作，当数据量很大时，非常耗时，所以改成了先用python原始的list存储，最后一次性构造一个完整的pandas.DataFrame，速度比之前快了不少。\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-09-introduction-to-building-a-search-engine-5/","summary":"\u003cp\u003e虽然主要的检索功能实现了，但是我们还需要一个“推荐阅读”的功能。当用户浏览某条具体新闻时，我们在页面底端给出5条和该新闻相关的新闻，也就是一个最简单的推荐系统。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"搜狐新闻“相关新闻”模块\" loading=\"lazy\" src=\"/posts/2016-01-09-introduction-to-building-a-search-engine-5/sohu-news3.webp\"\u003e\n搜狐新闻“相关新闻”模块\u003c/p\u003e\n\u003cp\u003e推荐模块的思路是度量两两新闻之间的相似度，取相似度最高的前5篇新闻作为推荐阅读的新闻。\u003c/p\u003e\n\u003cp\u003e我们前面讲过，一篇文档可以用一个向量表示，向量中的每个值是不同词项t在该文档d中的词频tf。但是一篇较短的文档（如新闻）的关键词并不多，所以我们可以提取每篇新闻的关键词，用这些关键词的tfidf值构成文档的向量表示，这样能够大大减少相似度计算量，同时保持较好的推荐效果。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/fxsjy/jieba#3-%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96\"\u003ejieba分词组件自带关键词提取功能\u003c/a\u003e，并能返回关键词的tfidf值。所以对每篇新闻，我们先提取tfidf得分最高的前25个关键词，用这25个关键词的tfidf值作为文档的向量表示。由此能够得到一个1000*m的文档词项矩阵M，矩阵每行表示一个文档，每列表示一个词项，m为1000个文档的所有互异的关键词（大概10000个）。矩阵M当然也是稀疏矩阵。\u003c/p\u003e\n\u003cp\u003e得到文档词项矩阵M之后，我们利用\u003ca href=\"http://sklearn.metrics.pairwise.pairwise_distances/\"\u003esklearn的pairwise_distances函数\u003c/a\u003e计算M中行向量之间的cosine相似度，对每个文档，得到与其最相似的前5篇新闻id，并把结果写入数据库。\u003c/p\u003e\n\u003cp\u003e推荐阅读模块的代码如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 46\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 47\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 48\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 49\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 50\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 51\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 52\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 53\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 54\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 55\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 56\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 57\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 58\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 59\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 60\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 61\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 62\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 63\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 64\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 65\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 66\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 67\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 68\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 69\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 70\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 71\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 72\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 73\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 74\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 75\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 76\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 77\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 78\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 79\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 80\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 81\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 82\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 83\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 84\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 85\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 86\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 87\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 88\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 89\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 90\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 91\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 92\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 93\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 94\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 95\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 96\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 97\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 98\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 99\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e100\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e101\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e102\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e103\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e104\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e105\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e106\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e107\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e108\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e109\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e110\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e111\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e112\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e113\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e114\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e115\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e116\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e117\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e118\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e119\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e120\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e121\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e122\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e123\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e124\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e125\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e126\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e127\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e128\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e129\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e130\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e131\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e132\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e133\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e134\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e135\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e136\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e137\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e138\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e139\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e140\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e141\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e142\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e143\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e144\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e145\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e146\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e147\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e148\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e149\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e150\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e151\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e152\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e153\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e154\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e155\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e156\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e157\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e158\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -*- coding: utf-8 -*-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eCreated on Wed Dec 23 14:06:10 2015\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e@author: bitjoy.net\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e os \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e listdir\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e xml.etree.ElementTree \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e ET\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e jieba\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e jieba.analyse\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e sqlite3\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e configparser\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e datetime \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e math\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pandas \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e pd\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.metrics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pairwise_distances\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eRecommendationModule\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    stop_words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    k_nearest \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    doc_dir_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    doc_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    stop_words_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    stop_words_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    idf_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    db_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e(self, config_path, config_encoding):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config_path\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config_encoding\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e configparser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConfigParser()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread(config_path, config_encoding)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_dir_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_dir_path\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_encoding\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;stop_words_path\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;stop_words_encoding\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eidf_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;idf_path\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edb_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;db_path\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        f \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e open(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words_path, encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words_encoding)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e f\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set(words\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ewrite_k_nearest_matrix_to_db\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sqlite3\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econnect(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edb_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecursor()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#39;DROP TABLE IF EXISTS knearest\u0026#39;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#39;CREATE TABLE knearest(id INTEGER PRIMARY KEY, first INTEGER, second INTEGER, third INTEGER, fourth INTEGER, fifth INTEGER)\u0026#39;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e docid, doclist \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ek_nearest:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;INSERT INTO knearest VALUES (?, ?, ?, ?, ?, ?)\u0026#34;\u003c/span\u003e, tuple([docid] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e doclist))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecommit()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eis_number\u003c/span\u003e(self, s):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003etry\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            float(s)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eexcept\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eValueError\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003econstruct_dt_matrix\u003c/span\u003e(self, files, topK \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e200\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eanalyse\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset_stop_words(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eanalyse\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset_idf_path(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eidf_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        M \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e len(files)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        terms \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        dt \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e files:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            root \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eparse(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_dir_path \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egetroot()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            title \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;body\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            docid \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;id\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            tags \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eanalyse\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eextract_tags(title \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;。\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e body, topK\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003etopK, withWeight\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e#tags = jieba.analyse.extract_tags(title, topK=topK, withWeight=True)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            cleaned_dict \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e word, tfidf \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e tags:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                word \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e word\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estrip()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eor\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eis_number(word):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                cleaned_dict[word] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e tfidf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e terms:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    terms[word] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e N\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    N \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    dt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend([docid, cleaned_dict])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    dt_matrix \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(N)] \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e j \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(M)]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        i \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e docid, t_tfidf \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e dt:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            dt_matrix[i][\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e docid\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e term, tfidf \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e t_tfidf\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                dt_matrix[i][terms[term]] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e tfidf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                i \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        dt_matrix \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataFrame(dt_matrix)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        dt_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eindex \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e dt_matrix[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;dt_matrix shape:(\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e)\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(dt_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e dt_matrix\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003econstruct_k_nearest_matrix\u003c/span\u003e(self, dt_matrix, k):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        tmp \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003earray(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e pairwise_distances(dt_matrix[dt_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecolumns[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e:]], metric \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;cosine\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        similarity_matrix \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataFrame(tmp, index \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e dt_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eindex\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etolist(), columns \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e dt_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eindex\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etolist())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e similarity_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eindex:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            tmp \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [int(i),[]]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            j \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e j \u003cspan style=\"color:#f92672\"\u003e\u0026lt;=\u003c/span\u003e k:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                max_col \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e similarity_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eloc[i]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eidxmax(axis \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                similarity_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eloc[i][max_col] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e max_col \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e i:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    tmp[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(int(max_col)) \u003cspan style=\"color:#75715e\"\u003e#max column name\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    j \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ek_nearest\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(tmp)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egen_idf_file\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        files \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e listdir(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_dir_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        n \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e float(len(files))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        idf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e files:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            root \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eparse(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_dir_path \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egetroot()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            title \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;body\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            seg_list \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elcut(title \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;。\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e body, cut_all\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            seg_list \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set(seg_list) \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e seg_list:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                word \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e word\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estrip()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eor\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eis_number(word):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e idf:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    idf[word] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    idf[word] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e idf[word] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                idf_file \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e open(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eidf_path, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;w\u0026#39;\u003c/span\u003e, encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;utf-8\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e word, df \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e idf\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    idf_file\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%.9f\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(word, math\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elog(n \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e df)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                idf_file\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003efind_k_nearest\u003c/span\u003e(self, k, topK):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egen_idf_file()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        files \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e listdir(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_dir_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        dt_matrix \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econstruct_dt_matrix(files, topK)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econstruct_k_nearest_matrix(dt_matrix, k)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite_k_nearest_matrix_to_db()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;—–start time: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e—–\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(datetime\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etoday()))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e RecommendationModule(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../config.ini\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;utf-8\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind_k_nearest(\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e25\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;—–finish time: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e—–\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(datetime\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etoday()))\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e这个模块的代码量最多，主要原因是需要构建文档词项矩阵，并且计算k邻居矩阵。矩阵数据结构的设计需要特别注意，否则会严重影响系统的效率。我刚开始把任务都扔给了pandas.DataFrame，后来发现当两个文档向量合并时，需要join连接操作，当数据量很大时，非常耗时，所以改成了先用python原始的list存储，最后一次性构造一个完整的pandas.DataFrame，速度比之前快了不少。\u003c/p\u003e","title":"和我一起构建搜索引擎（五）推荐阅读"},{"content":"构建好倒排索引之后，就可以开始检索了。\n检索模型有很多，比如向量空间模型、概率模型、语言模型等。其中最有名的、检索效果最好的是基于概率的BM25模型。\n给定一个查询Q和一篇文档d，d对Q的BM25得分公式为\n$$BM25_{score}(Q,d)=\\sum_{t\\in Q}w(t,d)$$$$w(t,d)=\\frac{qtf}{k_3+qtf}\\times \\frac{k_1\\times tf}{tf+k_1(1-b+b\\times l_d/avg\\_l)}\\times log_2\\frac{N-df+0.5}{df+0.5}$$公式中变量含义如下：\n\\(qtf\\)：查询中的词频 \\(tf\\)：文档中的词频 \\(l_d\\)：文档长度 \\(avg\\_l\\)：平均文档长度 \\(N\\)：文档数量 \\(df\\)：文档频率 \\(b,k_1,k_3\\)：可调参数 这个公式看起来很复杂，我们把它分解一下，其实很容易理解。第一个公式是外部公式，一个查询Q可能包含多个词项，比如“苹果手机”就包含“苹果”和“手机”两个词项，我们需要分别计算“苹果”和“手机”对某个文档d的贡献分数w(t,d)，然后将他们加起来就是整个文档d相对于查询Q的得分。\n第二个公式就是计算某个词项t在文档d中的得分，它包括三个部分。第一个部分是词项t在查询Q中的得分，比如查询“中国人说中国话”中“中国”出现了两次，此时qtf=2，说明这个查询希望找到的文档和“中国”更相关，“中国”的权重应该更大，但是通常情况下，查询Q都很短，而且不太可能包含相同的词项，所以这个因子是一个常数，我们在实现的时候可以忽略。\n第二部分类似于TFIDF模型中的TF项。也就是说某个词项t在文档d中出现次数越多，则t越重要，但是文档长度越长，tf也倾向于变大，所以使用文档长度除以平均长度\\(l_d/avg\\_l\\)起到某种归一化的效果，\\(k_1\\)和\\(b\\)是可调参数。\n第三部分类似于TFIDF模型中的IDF项。也就是说虽然“的”、“地”、“得”等停用词在某文档d中出现的次数很多，但是他们在很多文档中都出现过，所以这些词对d的贡献分并不高，接近于0；反而那些很稀有的词如”糖尿病“能够很好的区分不同文档，这些词对文档的贡献分应该较高。\n所以根据BM25公式，我们可以很快计算出不同文档t对查询Q的得分情况，然后按得分高低排序给出结果。\n下面是给定一个查询句子sentence，根据BM25公式给出文档排名的函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def result_by_BM25(self, sentence): seg_list = jieba.lcut(sentence, cut_all=False) n, cleaned_dict = self.clean_list(seg_list) BM25_scores = {} for term in cleaned_dict.keys(): r = self.fetch_from_db(term) if r is None: continue df = r[1] w = math.log2((self.N - df + 0.5) / (df + 0.5)) docs = r[2].split(\u0026#39;\\n\u0026#39;) for doc in docs: docid, date_time, tf, ld = doc.split(\u0026#39;\\t\u0026#39;) docid = int(docid) tf = int(tf) ld = int(ld) s = (self.K1 * tf * w) / (tf + self.K1 * (1 - self.B + self.B * ld / self.AVG_L)) if docid in BM25_scores: BM25_scores[docid] = BM25_scores[docid] + s else: BM25_scores[docid] = s BM25_scores = sorted(BM25_scores.items(), key = operator.itemgetter(1)) BM25_scores.reverse() if len(BM25_scores) == 0: return 0, [] else: return 1, BM25_scores 首先将句子分词得到所有查询词项，然后从数据库中取出词项对应的倒排记录表，对记录表中的所有文档，计算其BM25得分，最后按得分高低排序作为查询结果。\n类似的，我们还可以对所有文档按时间先后顺序排序，越新鲜的新闻排名越高；还可以按新闻的热度排序，越热门的新闻排名越高。\n关于热度公式，我们认为一方面要兼顾相关度，另一方面也要考虑时间因素，所以是BM25打分和时间打分的一个综合。\n比较有名的热度公式有两个，一个是Hacker News的，另一个是Reddit的，他们的公式分别为：\n图1. hacker news ranking algorithm [1]\n图2. reddit ranking algorithm [2]\n可以看出，他们都是将新闻/评论的一个原始得分和时间组合起来，只是一个用除法，一个用加法。所以我们也依葫芦画瓢，”自创“了一个简单的热度公式：\n$$hot_{score}=k_1log(BM25_{score})+\\frac{k_2}{t_{now}-t_{news}}$$用BM25得分加上新闻时间和当前时间的差值的倒数，\\(k_1\\)和\\(k_2\\)也是可调参数。\n按时间排序和按热度排序的函数和按BM25打分排序的函数类似，这里就不贴出来了，详细情况可以看我的Github项目News_IR_Demo。\n至此，搜索引擎的搜索功能已经实现了，你可以试着修改./web/search_engine.py的第167行的关键词，看看搜索结果是否和你预想的排序是一样的。不过由于我们的数据量只有1000个新闻，并不能涵盖所有关键词，更多的测试可以留给大家线下完成。\n[1]. http://amix.dk/blog/post/19574\n[2]. http://amix.dk/blog/post/19588\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-07-introduction-to-building-a-search-engine-4/","summary":"\u003cp\u003e构建好倒排索引之后，就可以开始检索了。\u003c/p\u003e\n\u003cp\u003e检索模型有很多，比如向量空间模型、概率模型、语言模型等。其中最有名的、检索效果最好的是基于概率的BM25模型。\u003c/p\u003e\n\u003cp\u003e给定一个查询Q和一篇文档d，d对Q的BM25得分公式为\u003c/p\u003e\n$$BM25_{score}(Q,d)=\\sum_{t\\in Q}w(t,d)$$$$w(t,d)=\\frac{qtf}{k_3+qtf}\\times \\frac{k_1\\times tf}{tf+k_1(1-b+b\\times l_d/avg\\_l)}\\times log_2\\frac{N-df+0.5}{df+0.5}$$\u003cp\u003e公式中变量含义如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\\(qtf\\)：查询中的词频\u003c/li\u003e\n\u003cli\u003e\\(tf\\)：文档中的词频\u003c/li\u003e\n\u003cli\u003e\\(l_d\\)：文档长度\u003c/li\u003e\n\u003cli\u003e\\(avg\\_l\\)：平均文档长度\u003c/li\u003e\n\u003cli\u003e\\(N\\)：文档数量\u003c/li\u003e\n\u003cli\u003e\\(df\\)：文档频率\u003c/li\u003e\n\u003cli\u003e\\(b,k_1,k_3\\)：可调参数\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这个公式看起来很复杂，我们把它分解一下，其实很容易理解。第一个公式是外部公式，一个查询Q可能包含多个词项，比如“苹果手机”就包含“苹果”和“手机”两个词项，我们需要分别计算“苹果”和“手机”对某个文档d的贡献分数w(t,d)，然后将他们加起来就是整个文档d相对于查询Q的得分。\u003c/p\u003e\n\u003cp\u003e第二个公式就是计算某个词项t在文档d中的得分，它包括三个部分。第一个部分是词项t在查询Q中的得分，比如查询“中国人说中国话”中“中国”出现了两次，此时qtf=2，说明这个查询希望找到的文档和“中国”\u003cstrong\u003e更\u003c/strong\u003e相关，“中国”的权重应该更大，但是通常情况下，查询Q都很短，而且不太可能包含相同的词项，所以这个因子是一个常数，我们在实现的时候可以忽略。\u003c/p\u003e\n\u003cp\u003e第二部分类似于\u003ca href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\"\u003eTFIDF模型\u003c/a\u003e中的TF项。也就是说某个词项t在文档d中出现次数越多，则t越重要，但是文档长度越长，tf也倾向于变大，所以使用文档长度除以平均长度\\(l_d/avg\\_l\\)起到某种归一化的效果，\\(k_1\\)和\\(b\\)是可调参数。\u003c/p\u003e\n\u003cp\u003e第三部分类似于\u003ca href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\"\u003eTFIDF模型\u003c/a\u003e中的IDF项。也就是说虽然“的”、“地”、“得”等停用词在某文档d中出现的次数很多，但是他们在很多文档中都出现过，所以这些词对d的贡献分并不高，接近于0；反而那些很稀有的词如”糖尿病“能够很好的区分不同文档，这些词对文档的贡献分应该较高。\u003c/p\u003e\n\u003cp\u003e所以根据BM25公式，我们可以很快计算出不同文档t对查询Q的得分情况，然后按得分高低排序给出结果。\u003c/p\u003e\n\u003cp\u003e下面是给定一个查询句子sentence，根据BM25公式给出文档排名的函数\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e27\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eresult_by_BM25\u003c/span\u003e(self, sentence):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tseg_list \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elcut(sentence, cut_all\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tn, cleaned_dict \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclean_list(seg_list)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tBM25_scores \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e term \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cleaned_dict\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ekeys():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\tr \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efetch_from_db(term)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e r \u003cspan style=\"color:#f92672\"\u003eis\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eNone\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\tdf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e r[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\tw \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e math\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elog2((self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eN \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e df \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e (df \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\tdocs \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e r[\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e doc \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e docs:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\tdocid, date_time, tf, ld \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e doc\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\tdocid \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(docid)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\ttf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(tf)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\tld \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(ld)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\ts \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eK1 \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e tf \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e w) \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e (tf \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eK1 \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e (\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eB \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eB \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e ld \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eAVG_L))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e docid \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e BM25_scores:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\tBM25_scores[docid] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e BM25_scores[docid] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e s\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\tBM25_scores[docid] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e s\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tBM25_scores \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sorted(BM25_scores\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems(), key \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e operator\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitemgetter(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tBM25_scores\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ereverse()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e len(BM25_scores) \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, BM25_scores\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e首先将句子分词得到所有查询词项，然后从数据库中取出词项对应的倒排记录表，对记录表中的所有文档，计算其BM25得分，最后按得分高低排序作为查询结果。\u003c/p\u003e","title":"和我一起构建搜索引擎（四）检索模型"},{"content":"目前正是所谓的“大数据”时代，数据量多到难以计数，怎样结构化的存储以便于分析计算，是当前的一大难题。上一篇博客我们简单抓取了1000个搜狐新闻数据，搜索的过程就是从这1000个新闻中找出和关键词相关的新闻来，那么怎样快速搜索呢，总不可能依次打开xml文件一个字一个字的找吧，这时就需要借助倒排索引这个强大的数据结构。\n在讲倒排索引之前，我们先介绍一下布尔检索。布尔检索只是简单返回包含某个关键词的文档，比如查询“苹果手机”，则返回所有包含“苹果”和“手机”关键词的文档，布尔检索并不对返回结果排序，所以有可能返回的第一个文档是“某个男孩边吃苹果边玩手机…“。\n实现布尔检索并不难，我们需要构建一个如下图的词项文档矩阵：\n图1. 布尔检索中的词项文档矩阵\n每行对应一个词项，每列对应一个文档，如果该值为1，表示该行词项出现在该列文档中。比如词项”苹果“出现在doc1和doc3文档中，如果我们要找同时出现”苹果“和”手机“的文档，只需把他们对应的向量取出来进行”与“操作，此为101\u0026amp;011=001，所以doc3同时出现了”苹果“和”手机“两个关键词，我们将其返回。\n布尔检索虽然很快，但是它也有很多缺陷，比如不能对结果排序，词项只有出现和不出现两种状态，但是一篇文档中出现10次“苹果“和只出现1次”苹果“，他们的相关度肯定是不相同的。所以需要对布尔检索进行改进。\n在扫描文档时，不但记录某词项出现与否，还记录该词项出现的次数，即词项频率(tf)；同时我们记录该文档的长度(ld)，以及某词项在不同文档中出现的次数，即文档频率(df)。\n图2. 倒排索引结构图\n这样我们就得到了如上图的倒排索引。左边部分被称为词典，存储的是1000个新闻中所有不同的词项；右边部分被称为倒排记录表，存储的是出现Term_i的那些文档信息。倒排索引中存储的变量都是为了给后续检索模型使用。\n讲到这里，我们需要解决如下几个问题。\n怎样得到一篇文档中的所有词项。给我们一篇新闻稿子，人类很容易分辨出”苹果“和”手机“是两个不同的词项，但是计算机怎么知道是这两个词呢？为什么不是”苹”、”国手“和”机“呢？这就需要进行中文分词，我们可以借助开源的jieba中文分词组件来完成，jieba分词能够将一个中文句子切成一个个词项，这样我们就可以统计tf, df了。 有些词，如”的“、”地“、”得“、”如果“等，几乎每篇文档都会出现，他们起不到很好的区分文档的效果，这类词被称为”停用词“，我们需要把他们去掉。去停词的步骤可以在jieba分词之后完成。 怎样存储倒排记录表。假设1000个文档共有20000个不同的词项，如果用类似图1的矩阵形式存储，需要耗费100020000=210^7个存储单元，但是图1往往是一个稀疏矩阵，因为一个文档中可能只出现了200个不同的词项，剩余的19800个词项都是空的。用矩阵方式存储时空效率都不高。所以我们可以采用图2的方式，词典用B-树或hash存储，倒排记录表用邻接链表存储方式，这样能大大减少存储空间。如果我们要将图2保存到数据库，可以对倒排记录表序列化成一个长的字符串，写入到一个单元格，读取的时候再反序列化。比如每个Doc内部用’\\t’连接，Doc之间用’\\n’连接，读取的时候split即可。 倒排索引构建算法使用内存式单遍扫描索引构建方法（SPIMI），其实就是依次对每篇新闻进行分词，如果出现新的词项则插入到词典中，否则将该文档的信息追加到词项对应的倒排记录表中。SPIMI的伪代码如下：\n图3. SPIMI算法伪代码\n下面是构建索引的所有代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Sat Dec 5 23:31:22 2015 @author: bitjoy.net \u0026#34;\u0026#34;\u0026#34; from os import listdir import xml.etree.ElementTree as ET import jieba import sqlite3 import configparser class Doc: docid = 0 date_time = \u0026#39;\u0026#39; tf = 0 ld = 0 def __init__(self, docid, date_time, tf, ld): self.docid = docid self.date_time = date_time self.tf = tf self.ld = ld def __repr__(self): return(str(self.docid) + \u0026#39;\\t\u0026#39; + self.date_time + \u0026#39;\\t\u0026#39; + str(self.tf) + \u0026#39;\\t\u0026#39; + str(self.ld)) def __str__(self): return(str(self.docid) + \u0026#39;\\t\u0026#39; + self.date_time + \u0026#39;\\t\u0026#39; + str(self.tf) + \u0026#39;\\t\u0026#39; + str(self.ld)) class IndexModule: stop_words = set() postings_lists = {} config_path = \u0026#39;\u0026#39; config_encoding = \u0026#39;\u0026#39; def __init__(self, config_path, config_encoding): self.config_path = config_path self.config_encoding = config_encoding config = configparser.ConfigParser() config.read(config_path, config_encoding) f = open(config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;stop_words_path\u0026#39;], encoding = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;stop_words_encoding\u0026#39;]) words = f.read() self.stop_words = set(words.split(\u0026#39;\\n\u0026#39;)) def is_number(self, s): try: float(s) return True except ValueError: return False def clean_list(self, seg_list): cleaned_dict = {} n = 0 for i in seg_list: i = i.strip().lower() if i != \u0026#39;\u0026#39; and not self.is_number(i) and i not in self.stop_words: n = n + 1 if i in cleaned_dict: cleaned_dict[i] = cleaned_dict[i] + 1 else: cleaned_dict[i] = 1 return n, cleaned_dict def write_postings_to_db(self, db_path): conn = sqlite3.connect(db_path) c = conn.cursor() c.execute(\u0026#34;\u0026#39;DROP TABLE IF EXISTS postings\u0026#39;\u0026#34;) c.execute(\u0026#34;\u0026#39;CREATE TABLE postings(term TEXT PRIMARY KEY, df INTEGER, docs TEXT)\u0026#39;\u0026#34;) for key, value in self.postings_lists.items(): doc_list = \u0026#39;\\n\u0026#39;.join(map(str,value[1])) t = (key, value[0], doc_list) c.execute(\u0026#34;INSERT INTO postings VALUES (?, ?, ?)\u0026#34;, t) conn.commit() conn.close() def construct_postings_lists(self): config = configparser.ConfigParser() config.read(self.config_path, self.config_encoding) files = listdir(config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_dir_path\u0026#39;]) AVG_L = 0 for i in files: root = ET.parse(config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_dir_path\u0026#39;] + i).getroot() title = root.find(\u0026#39;title\u0026#39;).text body = root.find(\u0026#39;body\u0026#39;).text docid = int(root.find(\u0026#39;id\u0026#39;).text) date_time = root.find(\u0026#39;datetime\u0026#39;).text seg_list = jieba.lcut(title + \u0026#39;。\u0026#39; + body, cut_all=False) ld, cleaned_dict = self.clean_list(seg_list) AVG_L = AVG_L + ld for key, value in cleaned_dict.items(): d = Doc(docid, date_time, value, ld) if key in self.postings_lists: self.postings_lists[key][0] = self.postings_lists[key][0] + 1 # df++ self.postings_lists[key][1].append(d) else: self.postings_lists[key] = [1, [d]] # [df, [Doc]] AVG_L = AVG_L / len(files) config.set(\u0026#39;DEFAULT\u0026#39;, \u0026#39;N\u0026#39;, str(len(files))) config.set(\u0026#39;DEFAULT\u0026#39;, \u0026#39;avg_l\u0026#39;, str(AVG_L)) with open(self.config_path, ‘w’, encoding = self.config_encoding) as configfile: config.write(configfile) self.write_postings_to_db(config[‘DEFAULT’][‘db_path’]) if __name__ == \u0026#34;__main__\u0026#34;: im = IndexModule(\u0026#39;../config.ini\u0026#39;, \u0026#39;utf-8\u0026#39;) im.construct_postings_lists() 运行之后会在./data/下生成一个ir.db数据库文件，这就是构建好的索引数据库。\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-07-introduction-to-building-a-search-engine-3/","summary":"\u003cp\u003e目前正是所谓的“大数据”时代，数据量多到难以计数，怎样结构化的存储以便于分析计算，是当前的一大难题。上一篇博客我们简单抓取了1000个搜狐新闻数据，搜索的过程就是从这1000个新闻中找出和关键词相关的新闻来，那么怎样快速搜索呢，总不可能依次打开xml文件一个字一个字的找吧，这时就需要借助倒排索引这个强大的数据结构。\u003c/p\u003e\n\u003cp\u003e在讲倒排索引之前，我们先介绍一下布尔检索。布尔检索只是简单返回包含某个关键词的文档，比如查询“苹果手机”，则返回所有包含“苹果”和“手机”关键词的文档，布尔检索并不对返回结果排序，所以有可能返回的第一个文档是“某个男孩边吃苹果边玩手机…“。\u003c/p\u003e\n\u003cp\u003e实现布尔检索并不难，我们需要构建一个如下图的词项文档矩阵：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图1. 布尔检索中的词项文档矩阵\" loading=\"lazy\" src=\"/posts/2016-01-07-introduction-to-building-a-search-engine-3/td_matrix.png\"\u003e\n图1. 布尔检索中的词项文档矩阵\u003c/p\u003e\n\u003cp\u003e每行对应一个词项，每列对应一个文档，如果该值为1，表示该行词项出现在该列文档中。比如词项”苹果“出现在doc1和doc3文档中，如果我们要找同时出现”苹果“和”手机“的文档，只需把他们对应的向量取出来进行”与“操作，此为101\u0026amp;011=001，所以doc3同时出现了”苹果“和”手机“两个关键词，我们将其返回。\u003c/p\u003e\n\u003cp\u003e布尔检索虽然很快，但是它也有很多缺陷，比如不能对结果排序，词项只有出现和不出现两种状态，但是一篇文档中出现10次“苹果“和只出现1次”苹果“，他们的相关度肯定是不相同的。所以需要对布尔检索进行改进。\u003c/p\u003e\n\u003cp\u003e在扫描文档时，不但记录某词项出现与否，还记录该词项出现的次数，即词项频率(tf)；同时我们记录该文档的长度(ld)，以及某词项在不同文档中出现的次数，即文档频率(df)。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图2. 倒排索引结构图\" loading=\"lazy\" src=\"/posts/2016-01-07-introduction-to-building-a-search-engine-3/inverted-index.png\"\u003e\n图2. 倒排索引结构图\u003c/p\u003e\n\u003cp\u003e这样我们就得到了如上图的倒排索引。左边部分被称为词典，存储的是1000个新闻中所有不同的词项；右边部分被称为倒排记录表，存储的是出现Term_i的那些文档信息。倒排索引中存储的变量都是为了给后续检索模型使用。\u003c/p\u003e\n\u003cp\u003e讲到这里，我们需要解决如下几个问题。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e怎样得到一篇文档中的所有词项。给我们一篇新闻稿子，人类很容易分辨出”苹果“和”手机“是两个不同的词项，但是计算机怎么知道是这两个词呢？为什么不是”苹”、”国手“和”机“呢？这就需要进行中文分词，我们可以借助开源的\u003ca href=\"https://github.com/fxsjy/jieba\"\u003ejieba中文分词组件\u003c/a\u003e来完成，jieba分词能够将一个中文句子切成一个个词项，这样我们就可以统计tf, df了。\u003c/li\u003e\n\u003cli\u003e有些词，如”的“、”地“、”得“、”如果“等，几乎每篇文档都会出现，他们起不到很好的区分文档的效果，这类词被称为”停用词“，我们需要把他们去掉。去停词的步骤可以在jieba分词之后完成。\u003c/li\u003e\n\u003cli\u003e怎样存储倒排记录表。假设1000个文档共有20000个不同的词项，如果用类似图1的矩阵形式存储，需要耗费1000\u003cem\u003e20000=2\u003c/em\u003e10^7个存储单元，但是图1往往是一个稀疏矩阵，因为一个文档中可能只出现了200个不同的词项，剩余的19800个词项都是空的。用矩阵方式存储时空效率都不高。所以我们可以采用图2的方式，词典用B-树或hash存储，倒排记录表用邻接链表存储方式，这样能大大减少存储空间。如果我们要将图2保存到数据库，可以对倒排记录表序列化成一个长的字符串，写入到一个单元格，读取的时候再反序列化。比如每个Doc内部用’\\t’连接，Doc之间用’\\n’连接，读取的时候split即可。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e倒排索引构建算法使用内存式单遍扫描索引构建方法（SPIMI），其实就是依次对每篇新闻进行分词，如果出现新的词项则插入到词典中，否则将该文档的信息追加到词项对应的倒排记录表中。SPIMI的伪代码如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图3. SPIMI算法伪代码\" loading=\"lazy\" src=\"/posts/2016-01-07-introduction-to-building-a-search-engine-3/SPIMI-algo.png\"\u003e\n图3. SPIMI算法伪代码\u003c/p\u003e\n\u003cp\u003e下面是构建索引的所有代码：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 46\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 47\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 48\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 49\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 50\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 51\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 52\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 53\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 54\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 55\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 56\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 57\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 58\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 59\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 60\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 61\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 62\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 63\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 64\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 65\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 66\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 67\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 68\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 69\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 70\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 71\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 72\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 73\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 74\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 75\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 76\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 77\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 78\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 79\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 80\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 81\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 82\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 83\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 84\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 85\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 86\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 87\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 88\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 89\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 90\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 91\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 92\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 93\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 94\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 95\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 96\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 97\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 98\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 99\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e100\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e101\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e102\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e103\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e104\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e105\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e106\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e107\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e108\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e109\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e110\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e111\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e112\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e113\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e114\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -*- coding: utf-8 -*-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eCreated on Sat Dec 5 23:31:22 2015\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e@author: bitjoy.net\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e os \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e listdir\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e xml.etree.ElementTree \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e ET\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e jieba\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e sqlite3\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e configparser\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eDoc\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    docid \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    date_time \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    tf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ld \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e(self, docid, date_time, tf, ld):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edocid \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e docid\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edate_time \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e date_time\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e tf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eld \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ld\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__repr__\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e(str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edocid) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edate_time \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etf) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eld))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__str__\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e(str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edocid) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edate_time \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etf) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eld))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eIndexModule\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    stop_words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    postings_lists \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e(self, config_path, config_encoding):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config_path\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config_encoding\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e configparser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConfigParser()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread(config_path, config_encoding)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        f \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e open(config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;stop_words_path\u0026#39;\u003c/span\u003e], encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;stop_words_encoding\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e f\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set(words\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eis_number\u003c/span\u003e(self, s):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003etry\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            float(s)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eexcept\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eValueError\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eclean_list\u003c/span\u003e(self, seg_list):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cleaned_dict \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        n \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e seg_list:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            i \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e i\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estrip()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eis_number(i) \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                n \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e n \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cleaned_dict:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                cleaned_dict[i] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cleaned_dict[i] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                cleaned_dict[i] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e n, cleaned_dict\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ewrite_postings_to_db\u003c/span\u003e(self, db_path):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sqlite3\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econnect(db_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecursor()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#39;DROP TABLE IF EXISTS postings\u0026#39;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#39;CREATE TABLE postings(term TEXT PRIMARY KEY, df INTEGER, docs TEXT)\u0026#39;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e key, value \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            doc_list \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ejoin(map(str,value[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            t \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (key, value[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e], doc_list)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;INSERT INTO postings VALUES (?, ?, ?)\u0026#34;\u003c/span\u003e, t)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecommit()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003econstruct_postings_lists\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e configparser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConfigParser()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_path, self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_encoding)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        files \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e listdir(config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_dir_path\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        AVG_L \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e files:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            root \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eparse(config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_dir_path\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egetroot()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            title \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;body\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            docid \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;id\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            date_time \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;datetime\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            seg_list \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elcut(title \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;。\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e body, cut_all\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            ld, cleaned_dict \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclean_list(seg_list)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            AVG_L \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e AVG_L \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e ld\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e key, value \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cleaned_dict\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                d \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Doc(docid, date_time, value, ld)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e key \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists[key][\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists[key][\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# df++\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists[key][\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(d)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists[key] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, [d]] \u003cspan style=\"color:#75715e\"\u003e# [df, [Doc]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    AVG_L \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e AVG_L \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e len(files)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;N\u0026#39;\u003c/span\u003e, str(len(files)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;avg_l\u0026#39;\u003c/span\u003e, str(AVG_L))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e open(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_path, \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003ew\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e, encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_encoding) \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e configfile:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite(configfile)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite_postings_to_db(config[\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003eDEFAULT\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e][\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003edb_path\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    im \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e IndexModule(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../config.ini\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;utf-8\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    im\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econstruct_postings_lists()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e运行之后会在./data/下生成一个ir.db数据库文件，这就是构建好的索引数据库。\u003c/p\u003e","title":"和我一起构建搜索引擎（三）构建索引"},{"content":"网络爬虫又称网络蜘蛛、Web采集器等，它是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。\n我们在设计网络爬虫的时候需要注意两点：\n鲁棒性。Web中有些服务器会制造采集器陷阱（spider traps），这些陷阱服务器实际上是Web页面的生成器，它能在某个域下生成无数网页，从而使采集器陷入到一个无限的采集循环中去。采集器必须能从这些陷阱中跳出来。当然，这些陷阱倒不一定都是恶意的，有时可能是网站设计疏忽所导致的结果。\n礼貌性。Web服务器具有一些隐式或显式的政策来控制采集器访问它们的频率。设计采集器时必须要遵守这些代表礼貌性的访问政策。\n采集器的基本架构如下图所示。\n基本上是“抓取→分析→得到新的URL→再抓取→再分析”这样一个死循环过程。\n以上内容摘自王斌老师翻译的《信息检索导论》课本。\n由于我们要做的是一个新闻搜索引擎，所以抓取的是新闻数据，对于爬虫，网上也有很多的开源程序，如nutch等，Github上还有人专门开发了抓取新闻的组件newspaper，可以很方便的提取新闻标题、正文、时间等信息。不过用python写爬虫也是分分钟的事情，下面我们一起来试一试。\n首先找一个新闻网站，为简单起见，要找那种结构清晰、html代码便于解析的门户网站，比如搜狐新闻、参考消息等。\n搜狐新闻的国内要闻列表如下：\n结构非常清楚，左边是带URL的标题，右边括号里有新闻时间。这一页列表就有200条新闻，如果我们要获取1000条，只要不断模拟点击下一页即可。下一页的URL也只是在首页的基础上加上_xxx.shtml，xxx就是不同的页码。\n查看列表的html源码，得知列表都在类名为newsblue1的td中，所以只需要解析html源码就可以得到新闻标题、URL和时间，python解析html可以用BeautifulSoup包，非常方便。\n进入到新闻详细页面，正文部分如下：\n查看html源码，正文位于类名为text clear的div中，据此可以很方便的提取新闻正文。\n得到一条新闻的所有数据之后，我们需要将之结构化成xml文件，借助相应的xml包可以很方便的完成这项工作。xml格式定义如下：\n注意爬虫需要访问网络，难免会出现一些异常，所以捕获异常是非常有必要的。另外，搜狐每篇新闻正文后面都会有一段’//’开始的注释，这个需要过滤掉，短于140个字的新闻我也过滤掉了。整个搜索系统的配置参数都存储在config.ini文件中。\n下面是完整的python 3.4+代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Sat Dec 19 11:57:01 2015 @author: bitjoy.net \u0026#34;\u0026#34;\u0026#34; from bs4 import BeautifulSoup import urllib.request import xml.etree.ElementTree as ET import configparser def get_news_pool(root, start, end): news_pool = [] for i in range(start,end,-1): page_url = \u0026#39;\u0026#39; if i != start: page_url = root +\u0026#39;_%d.shtml\u0026#39;%(i) else: page_url = root + \u0026#39;.shtml\u0026#39; try: response = urllib.request.urlopen(page_url) except Exception as e: print(\u0026#34;—–%s: %s—–\u0026#34;%(type(e), page_url)) continue html = response.read() soup = BeautifulSoup(html) td = soup.find(\u0026#39;td\u0026#39;, class_ = \u0026#34;newsblue1\u0026#34;) a = td.find_all(\u0026#39;a\u0026#39;) span = td.find_all(\u0026#39;span\u0026#39;) for i in range(len(a)): date_time = span[i].string url = a[i].get(\u0026#39;href\u0026#39;) title = a[i].string news_info = [\u0026#39;2016-\u0026#39;+date_time[1:3]+\u0026#39;-\u0026#39;+date_time[4:-1]+\u0026#39;:00\u0026#39;,url,title] news_pool.append(news_info) return(news_pool) def crawl_news(news_pool, min_body_len, doc_dir_path, doc_encoding): i = 1 for news in news_pool: try: response = urllib.request.urlopen(news[1]) except Exception as e: print(\u0026#34;—–%s: %s—–\u0026#34;%(type(e), news[1])) continue html = response.read() soup = BeautifulSoup(html) try: body = soup.find(\u0026#39;div\u0026#39;, class_ = \u0026#34;text clear\u0026#34;).find(\u0026#39;div\u0026#39;).get_text() except Exception as e: print(\u0026#34;—–%s: %s—–\u0026#34;%(type(e), news[1])) continue if \u0026#39;//\u0026#39; in body: body = body[:body.index(\u0026#39;//\u0026#39;)] body = body.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;) if len(body) \u0026lt;= min_body_len: continue doc = ET.Element(\u0026#34;doc\u0026#34;) ET.SubElement(doc, \u0026#34;id\u0026#34;).text = \u0026#34;%d\u0026#34;%(i) ET.SubElement(doc, \u0026#34;url\u0026#34;).text = news[1] ET.SubElement(doc, \u0026#34;title\u0026#34;).text = news[2] ET.SubElement(doc, \u0026#34;datetime\u0026#34;).text = news[0] ET.SubElement(doc, \u0026#34;body\u0026#34;).text = body tree = ET.ElementTree(doc) tree.write(doc_dir_path + \u0026#34;%d.xml\u0026#34;%(i), encoding = doc_encoding, xml_declaration = True) i += 1 if __name__ == \u0026#39;__main__\u0026#39;: config = configparser.ConfigParser() config.read(\u0026#39;../config.ini\u0026#39;, \u0026#39;utf-8\u0026#39;) root = \u0026#39;http://news.sohu.com/1/0903/61/subject212846158\u0026#39; news_pool = get_news_pool(root, 854, 849) crawl_news(news_pool, 140, config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_dir_path\u0026#39;], config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_encoding\u0026#39;]) print(\u0026#39;done!\u0026#39;) 完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-04-introduction-to-building-a-search-engine-2/","summary":"\u003cp\u003e网络爬虫又称网络蜘蛛、Web采集器等，它是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。\u003c/p\u003e\n\u003cp\u003e我们在设计网络爬虫的时候需要注意两点：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e鲁棒性\u003c/strong\u003e。Web中有些服务器会制造采集器陷阱（spider traps），这些陷阱服务器实际上是Web页面的生成器，它能在某个域下生成无数网页，从而使采集器陷入到一个无限的采集循环中去。采集器必须能从这些陷阱中跳出来。当然，这些陷阱倒不一定都是恶意的，有时可能是网站设计疏忽所导致的结果。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e礼貌性\u003c/strong\u003e。Web服务器具有一些隐式或显式的政策来控制采集器访问它们的频率。设计采集器时必须要遵守这些代表礼貌性的访问政策。\u003c/p\u003e\n\u003cp\u003e采集器的基本架构如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"the basic crawler architecture\" loading=\"lazy\" src=\"/posts/2016-01-04-introduction-to-building-a-search-engine-2/the-basic-crawler-architecture.png\"\u003e\u003c/p\u003e\n\u003cp\u003e基本上是“抓取→分析→得到新的URL→再抓取→再分析”这样一个死循环过程。\u003c/p\u003e\n\u003cp\u003e以上内容摘自王斌老师翻译的《信息检索导论》课本。\u003c/p\u003e\n\u003cp\u003e由于我们要做的是一个新闻搜索引擎，所以抓取的是新闻数据，对于爬虫，网上也有很多的开源程序，如nutch等，Github上还有人专门开发了抓取新闻的组件\u003ca href=\"https://github.com/codelucas/newspaper\"\u003enewspaper\u003c/a\u003e，可以很方便的提取新闻标题、正文、时间等信息。不过用python写爬虫也是分分钟的事情，下面我们一起来试一试。\u003c/p\u003e\n\u003cp\u003e首先找一个新闻网站，为简单起见，要找那种结构清晰、html代码便于解析的门户网站，比如\u003ca href=\"http://news.sohu.com/1/0903/61/subject212846158.shtml\"\u003e搜狐新闻\u003c/a\u003e、\u003ca href=\"http://www.cankaoxiaoxi.com/china/szyw/\"\u003e参考消息\u003c/a\u003e等。\u003c/p\u003e\n\u003cp\u003e搜狐新闻的国内要闻列表如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"sohu news1\" loading=\"lazy\" src=\"/posts/2016-01-04-introduction-to-building-a-search-engine-2/sohu-news1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结构非常清楚，左边是带URL的标题，右边括号里有新闻时间。这一页列表就有200条新闻，如果我们要获取1000条，只要不断模拟点击下一页即可。下一页的URL也只是在首页的基础上加上_xxx.shtml，xxx就是不同的页码。\u003c/p\u003e\n\u003cp\u003e查看列表的html源码，得知列表都在类名为newsblue1的td中，所以只需要解析html源码就可以得到新闻标题、URL和时间，python解析html可以用BeautifulSoup包，非常方便。\u003c/p\u003e\n\u003cp\u003e进入到新闻详细页面，正文部分如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"sohu news2\" loading=\"lazy\" src=\"/posts/2016-01-04-introduction-to-building-a-search-engine-2/sohu-news2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e查看html源码，正文位于类名为text clear的div中，据此可以很方便的提取新闻正文。\u003c/p\u003e\n\u003cp\u003e得到一条新闻的所有数据之后，我们需要将之结构化成xml文件，借助相应的xml包可以很方便的完成这项工作。xml格式定义如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"xml format\" loading=\"lazy\" src=\"/posts/2016-01-04-introduction-to-building-a-search-engine-2/xml-format.png\"\u003e\u003c/p\u003e\n\u003cp\u003e注意爬虫需要访问网络，难免会出现一些异常，所以捕获异常是非常有必要的。另外，搜狐每篇新闻正文后面都会有一段’//’开始的注释，这个需要过滤掉，短于140个字的新闻我也过滤掉了。整个搜索系统的配置参数都存储在config.ini文件中。\u003c/p\u003e\n\u003cp\u003e下面是完整的python 3.4+代码。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e46\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e47\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e48\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e49\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e50\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e51\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e52\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e53\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e54\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e55\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e56\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e57\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e58\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e59\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e60\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e61\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e62\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e63\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e64\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e65\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e66\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e67\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e68\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e69\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e70\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e71\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e72\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e73\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e74\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e75\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -*- coding: utf-8 -*-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eCreated on Sat Dec 19 11:57:01 2015\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e@author: bitjoy.net\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e bs4 \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e BeautifulSoup\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e urllib.request\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e xml.etree.ElementTree \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e ET\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e configparser\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eget_news_pool\u003c/span\u003e(root, start, end):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    news_pool \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(start,end,\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        page_url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e start:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            page_url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;_\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e.shtml\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(i)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            page_url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;.shtml\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003etry\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            response \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e urllib\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erequest\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eurlopen(page_url)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eexcept\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eException\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;—–\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e—–\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(type(e), page_url))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        html \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e response\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        soup \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e BeautifulSoup(html)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        td \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e soup\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;td\u0026#39;\u003c/span\u003e, class_ \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;newsblue1\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        a \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e td\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind_all(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;a\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        span \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e td\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind_all(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;span\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(len(a)):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            date_time \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e span[i]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estring\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e a[i]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eget(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;href\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            title \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e a[i]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estring\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            news_info \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;2016-\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003edate_time[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e:\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;-\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003edate_time[\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e:\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;:00\u0026#39;\u003c/span\u003e,url,title]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            news_pool\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(news_info)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e(news_pool)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecrawl_news\u003c/span\u003e(news_pool, min_body_len, doc_dir_path, doc_encoding):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    i \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e news \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e news_pool:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003etry\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            response \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e urllib\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erequest\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eurlopen(news[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eexcept\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eException\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;—–\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e—–\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(type(e), news[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        html \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e response\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        soup \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e BeautifulSoup(html)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003etry\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e soup\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;div\u0026#39;\u003c/span\u003e, class_ \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text clear\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;div\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eget_text()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eexcept\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eException\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;—–\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e—–\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(type(e), news[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;//\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e body:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e body[:body\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eindex(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;//\u0026#39;\u003c/span\u003e)]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e body\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ereplace(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34; \u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e len(body) \u003cspan style=\"color:#f92672\"\u003e\u0026lt;=\u003c/span\u003e min_body_len:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        doc \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eElement(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;doc\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSubElement(doc, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(i)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSubElement(doc, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;url\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e news[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSubElement(doc, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;title\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e news[\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSubElement(doc, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;datetime\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e news[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSubElement(doc, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;body\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e body\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        tree \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eElementTree(doc)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        tree\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite(doc_dir_path \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e.xml\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(i), encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e doc_encoding, xml_declaration \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        i \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;__main__\u0026#39;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e configparser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConfigParser()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../config.ini\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;utf-8\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    root \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;http://news.sohu.com/1/0903/61/subject212846158\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    news_pool \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e get_news_pool(root, \u003cspan style=\"color:#ae81ff\"\u003e854\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e849\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    crawl_news(news_pool, \u003cspan style=\"color:#ae81ff\"\u003e140\u003c/span\u003e, config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_dir_path\u0026#39;\u003c/span\u003e], config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_encoding\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;done!\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003chr\u003e\n\u003cp\u003e完整可运行的新闻搜索引擎Demo请看我的Github项目\u003ca href=\"https://github.com/01joy/news_search_engine\"\u003enews_search_engine\u003c/a\u003e。\u003c/p\u003e","title":"和我一起构建搜索引擎（二）网络爬虫"},{"content":"我们上网用得最多的一项服务应该是搜索，不管大事小情，都喜欢百度一下或谷歌一下，那么百度和谷歌是怎样从浩瀚的网络世界中快速找到你想要的信息呢，这就是搜索引擎的艺术，属于信息检索的范畴。\n这学期学习了《现代信息检索》课程，使用的是Stanford的教材Introduction to Information Retrieval，网上有电子版，大家可以参考。\n本课程的大作业是完成一个新闻搜索引擎，要求是这样的：定向采集3-4个新闻网站，实现这些网站信息的抽取、索引和检索。网页数目不少于10万条。能按相关度、时间和热度（需要自己定义）进行排序，能实现相似新闻的自动聚类。\n截止日期12月31日，我们已经在规定时间完成了该系统，自认为检索效果不错，所以在此把过程记录如下，欢迎讨论。\n网上有很多开源的全文搜索引擎，比如Lucene、Sphinx、Whoosh等，都提供了很好的API，开发者只需要调用相关接口就可以实现一个全功能的搜索引擎。不过既然学习了IR这门课，自然要把相关技术实践一下，所以我们打算自己实现一个搜索引擎。\n这是简介部分，主要介绍整个搜索引擎的思路和框架。\n上图为本搜索引擎的框架图。首先爬虫程序从特定的几个新闻网站抓取新闻数据，然后过滤网页中的图片、视频、广告等无关元素，抽取新闻的主体内容，得到结构化的xml数据。然后一方面使用内存式单遍扫描索引构建方法（SPIMI）构建倒排索引，供检索模型使用；另一方面根据向量空间模型计算两两新闻之间的余弦相似度，供推荐模块使用。最后利用概率检索模型中的BM25公式计算给定关键词下的文档相关性评分，BM25打分结合时间因素得到热度评分，根据评分给出排序结果。\n在后续博文中，我会详细介绍每个部分的实现。\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-04-introduction-to-building-a-search-engine-1/","summary":"\u003cp\u003e我们上网用得最多的一项服务应该是搜索，不管大事小情，都喜欢百度一下或谷歌一下，那么百度和谷歌是怎样从浩瀚的网络世界中快速找到你想要的信息呢，这就是搜索引擎的艺术，属于信息检索的范畴。\u003c/p\u003e\n\u003cp\u003e这学期学习了《现代信息检索》课程，使用的是Stanford的教材\u003ca href=\"http://nlp.stanford.edu/IR-book/\"\u003eIntroduction to Information Retrieval\u003c/a\u003e，网上有电子版，大家可以参考。\u003c/p\u003e\n\u003cp\u003e本课程的大作业是完成一个新闻搜索引擎，要求是这样的：定向采集3-4个新闻网站，实现这些网站信息的抽取、索引和检索。网页数目不少于10万条。能按相关度、时间和热度（需要自己定义）进行排序，能实现相似新闻的自动聚类。\u003c/p\u003e\n\u003cp\u003e截止日期12月31日，我们已经在规定时间完成了该系统，自认为检索效果不错，所以在此把过程记录如下，欢迎讨论。\u003c/p\u003e\n\u003cp\u003e网上有很多开源的全文搜索引擎，比如Lucene、Sphinx、Whoosh等，都提供了很好的API，开发者只需要调用相关接口就可以实现一个全功能的搜索引擎。不过既然学习了IR这门课，自然要把相关技术实践一下，所以我们打算自己实现一个搜索引擎。\u003c/p\u003e\n\u003cp\u003e这是简介部分，主要介绍整个搜索引擎的思路和框架。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"search engine outline\" loading=\"lazy\" src=\"/posts/2016-01-04-introduction-to-building-a-search-engine-1/search-engine-architecture.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上图为本搜索引擎的框架图。首先爬虫程序从特定的几个新闻网站抓取新闻数据，然后过滤网页中的图片、视频、广告等无关元素，抽取新闻的主体内容，得到结构化的xml数据。然后一方面使用内存式单遍扫描索引构建方法（SPIMI）构建倒排索引，供检索模型使用；另一方面根据向量空间模型计算两两新闻之间的余弦相似度，供推荐模块使用。最后利用概率检索模型中的BM25公式计算给定关键词下的文档相关性评分，BM25打分结合时间因素得到热度评分，根据评分给出排序结果。\u003c/p\u003e\n\u003cp\u003e在后续博文中，我会详细介绍每个部分的实现。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e完整可运行的新闻搜索引擎Demo请看我的Github项目\u003ca href=\"https://github.com/01joy/news_search_engine\"\u003enews_search_engine\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e以下是系列博客：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-1/\"\u003e和我一起构建搜索引擎（一）简介\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-2/\"\u003e和我一起构建搜索引擎（二）网络爬虫\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-3/\"\u003e和我一起构建搜索引擎（三）构建索引\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-4/\"\u003e和我一起构建搜索引擎（四）检索模型\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-5/\"\u003e和我一起构建搜索引擎（五）推荐阅读\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-6/\"\u003e和我一起构建搜索引擎（六）系统展示\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-7/\"\u003e和我一起构建搜索引擎（七）总结展望\u003c/a\u003e\u003c/p\u003e","title":"和我一起构建搜索引擎（一）简介"},{"content":"突然发现，从小到大，自己做事都做得很慢，别人一会做完的作业，我可能要花好几个小时。但拿作业一对比，明显能看出差距，自己精雕细琢的作品不是别人随随便便就能比的。\n最近几次和同学合作完成大作业也遇到了类似的情况，数据抓取的同学给我的数据，不是格式不对就是内容缺胳膊少腿，质量极其差，还不愿修改，曰：只是做一个演示系统，有数据就行了。他不知道他这样的数据给我，我们后面做得再好，最终的演示效果也不会好，他这样的随意，后面的人不知要多花多少时间来弥补。我也无意跟他多费口舌，自己挽起袖子重做了他的工作。\n类似的事情，我遇到的不在少数，和别人沟通的时间远远超过了自己完成任务的时间。所以往往一个很简单的工作，我要花比别人多两到三倍的时间。这个过程就像工匠在雕琢自己的作品，是不计时间的，直到自己认为完美为止。这大概就是老罗所说的工匠精神吧。\n在一个完美主义者的眼里，这是一个千疮百孔的世界。\n糟糕的文档排版，错别字和错误标点一堆，一群人并排走挡了后面或对面的人，开水房离宿舍十万八千里，蚊香的设计，电脑接口位置的设计，U盘接口的设计，凸出的摄像头，插队，说脏话。。。\n当然也有同学劝我，这些东西差不多就行了，何必花这么多时间做这么好干什么，还不如去看个电影打个球。也经常听人说Take it easy，别太认真，认真你就输了。\n但是我始终相信，态度决定一切。你一天认真做了，别人不一定看得到，但坚持一个月甚至一年，总会有志同道合的人发现你，而你的坚持也将一点点的改变这个行业这个世界。就像老罗做手机，虽然销量不怎么样，但他的工匠精神、他的情怀，值得每一个人尊敬。T2统一听筒和各种传感器的位置、消失的电源键、消失的SIM卡插槽、消失的金属中框断点完全是超出iPhone的美好设计。希望老罗的情怀之路能够坚持下去、越走越远。\n","permalink":"http://localhost:1313/posts/2016-01-04-attitude-is-everything/","summary":"\u003cp\u003e突然发现，从小到大，自己做事都做得很慢，别人一会做完的作业，我可能要花好几个小时。但拿作业一对比，明显能看出差距，自己精雕细琢的作品不是别人随随便便就能比的。\u003c/p\u003e\n\u003cp\u003e最近几次和同学合作完成大作业也遇到了类似的情况，数据抓取的同学给我的数据，不是格式不对就是内容缺胳膊少腿，质量极其差，还不愿修改，曰：只是做一个演示系统，有数据就行了。他不知道他这样的数据给我，我们后面做得再好，最终的演示效果也不会好，他这样的随意，后面的人不知要多花多少时间来弥补。我也无意跟他多费口舌，自己挽起袖子重做了他的工作。\u003c/p\u003e\n\u003cp\u003e类似的事情，我遇到的不在少数，和别人沟通的时间远远超过了自己完成任务的时间。所以往往一个很简单的工作，我要花比别人多两到三倍的时间。这个过程就像工匠在雕琢自己的作品，是不计时间的，直到自己认为完美为止。这大概就是老罗所说的工匠精神吧。\u003c/p\u003e\n\u003cp\u003e在一个完美主义者的眼里，这是一个千疮百孔的世界。\u003c/p\u003e\n\u003cp\u003e糟糕的文档排版，错别字和错误标点一堆，一群人并排走挡了后面或对面的人，开水房离宿舍十万八千里，蚊香的设计，电脑接口位置的设计，U盘接口的设计，凸出的摄像头，插队，说脏话。。。\u003c/p\u003e\n\u003cp\u003e当然也有同学劝我，这些东西差不多就行了，何必花这么多时间做这么好干什么，还不如去看个电影打个球。也经常听人说Take it easy，别太认真，认真你就输了。\u003c/p\u003e\n\u003cp\u003e但是我始终相信，态度决定一切。你一天认真做了，别人不一定看得到，但坚持一个月甚至一年，总会有志同道合的人发现你，而你的坚持也将一点点的改变这个行业这个世界。就像老罗做手机，虽然销量不怎么样，但他的工匠精神、他的情怀，值得每一个人尊敬。T2统一听筒和各种传感器的位置、消失的电源键、消失的SIM卡插槽、消失的金属中框断点完全是超出iPhone的美好设计。希望老罗的情怀之路能够坚持下去、越走越远。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"smartisan-T2-2015\" loading=\"lazy\" src=\"/posts/2016-01-04-attitude-is-everything/smartisan-T2-2015.jpg\"\u003e\u003c/p\u003e","title":"认真你就赢了"},{"content":"2015年过得好快，梳理一下，2015年的时间线大概是这样的：\n3月来北京计算所做毕设→5月返回武大修改论文→5月30公开答辩→6月毕业季→7月回北京计算所→8月回家陪父母→9月国科大开学→持续高强度的学习→2016元旦还在图书馆研究NPC问题。\n2015年给我的总体感受是很忙，但忙的事情都很琐碎，并没有什么大的里程碑事件，不过以下三件事情我认为值得一提。\n本科四年修成正果，研究生三年新的起航 买了一辆属于自己的山地车，1k2，虽然是二手的，但足够我骑着它去看世界了:-) 也许是在城市里待久了，我特别享受这种亲近大自然的感觉，蓝天、白云、草原、大海这些美景永远也看不够。\n在国科大认识了两个好基友，虽然都是单身汪，但至少想看电影吃火锅的时候还可以有个伴。（此处居然少了三人合照） 2015年共写了14篇博客，包含3篇技术博客，bitjoy.net 历史累计PV1039，UV520，IP502。\n展望2016年，大的方向基本都确定了，目标如下：\n完成国科大下学期的课程任务 接手pLink软件 刷完LeetCode所有题目 读10本书 去电影院看10场电影（2015下半年在怀柔村里没看一部电影/(ㄒoㄒ)/~~） 改正坐姿 大家一起见证！\n","permalink":"http://localhost:1313/posts/2016-01-03-2016-happy-new-year/","summary":"\u003cp\u003e2015年过得好快，梳理一下，2015年的时间线大概是这样的：\u003c/p\u003e\n\u003cp\u003e3月来北京计算所做毕设→5月返回武大修改论文→5月30公开答辩→6月毕业季→7月回北京计算所→8月回家陪父母→9月国科大开学→持续高强度的学习→2016元旦还在图书馆研究NPC问题。\u003c/p\u003e\n\u003cp\u003e2015年给我的总体感受是很忙，但忙的事情都很琐碎，并没有什么大的里程碑事件，不过以下三件事情我认为值得一提。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e本科四年修成正果，研究生三年新的起航\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"whu_certificate\" loading=\"lazy\" src=\"/posts/2016-01-03-2016-happy-new-year/whu_certificate.jpg\"\u003e\n\u003cimg alt=\"ucas_admission\" loading=\"lazy\" src=\"/posts/2016-01-03-2016-happy-new-year/ucas_admission.jpg\"\u003e\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e买了一辆属于自己的山地车，1k2，虽然是二手的，但足够我骑着它去看世界了:-)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"bike\" loading=\"lazy\" src=\"/posts/2016-01-03-2016-happy-new-year/bike.jpg\"\u003e\n\u003cimg alt=\"2015_cycling_1\" loading=\"lazy\" src=\"/posts/2016-01-03-2016-happy-new-year/2015_cycling_1.jpg\"\u003e\n\u003cimg alt=\"2015_cycling_2\" loading=\"lazy\" src=\"/posts/2016-01-03-2016-happy-new-year/2015_cycling_2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e也许是在城市里待久了，我特别享受这种亲近大自然的感觉，蓝天、白云、草原、大海这些美景永远也看不够。\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e在国科大认识了两个好基友，虽然都是单身汪，但至少想看电影吃火锅的时候还可以有个伴。（此处居然少了三人合照）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e2015年共写了14篇博客，包含3篇技术博客，bitjoy.net 历史累计PV1039，UV520，IP502。\u003c/p\u003e\n\u003cp\u003e展望2016年，大的方向基本都确定了，目标如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e完成国科大下学期的课程任务\u003c/li\u003e\n\u003cli\u003e接手pLink软件\u003c/li\u003e\n\u003cli\u003e刷完LeetCode所有题目\u003c/li\u003e\n\u003cli\u003e读10本书\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影（2015下半年在怀柔村里没看一部电影/(ㄒoㄒ)/~~）\u003c/li\u003e\n\u003cli\u003e改正坐姿\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e大家一起见证！\u003c/p\u003e","title":"2016新年快乐"},{"content":"\n第一次在北方过冬，今年北京11月6日就下雪了，然而我在广州的小伙伴还穿着短袖吃着冰棍呢。。。\n北京2015年的第一场雪，比以往时候来的更早一些\n今天又下起了第二场雪，下了整整两天的大雪，然而我房间的暖气却不暖了，大叔来修了两次，无功而返，说是一楼的宿舍暖气都有问题，当初设计有缺陷(╯‵□′)╯︵┻━┻\n这样也好，给了自己去图书馆的理由❉\n","permalink":"http://localhost:1313/posts/2015-11-22-snow-in-beijing/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/Von_Koch_curve.gif\"\u003e\u003c/p\u003e\n\u003cp\u003e第一次在北方过冬，今年北京11月6日就下雪了，然而我在广州的小伙伴还穿着短袖吃着冰棍呢。。。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"2015_11_06_beijing_snow\" loading=\"lazy\" src=\"/posts/2015-11-22-snow-in-beijing/2015_11_06_beijing_snow.jpg\"\u003e\n北京2015年的第一场雪，比以往时候来的更早一些\u003c/p\u003e\n\u003cp\u003e今天又下起了第二场雪，下了整整两天的大雪，然而我房间的暖气却不暖了，大叔来修了两次，无功而返，说是一楼的宿舍暖气都有问题，当初设计有缺陷(╯‵□′)╯︵┻━┻\u003c/p\u003e\n\u003cp\u003e这样也好，给了自己去图书馆的理由❉\u003c/p\u003e","title":"北国的雪"},{"content":"安装WIN10一个月以来，校园有线网经常间歇性断网，通常是20分钟不到就断了，需要重启或者把有线连接关闭再打开才可以。在微博上问过微软客服也无果，后来Google到某国外的解决办法，现记录如下。\n说到底WIN10断网的问题还是和驱动有关，先看一下我的有线网卡Broadcom NetLink (TM) Gigabit Ethernet，驱动信息是这样的：\n还是13年的驱动，版本号是15.6.0.14，于是第一想到的是更新驱动。点击驱动右键选更新-\u0026gt;自动搜索更新的驱动程序软件-\u0026gt;提示“已安装适合设备的最佳驱动程序软件”，但这明明不是最新的驱动啊！\n于是在Broadcom的官网上找到了最新驱动win_b57_x64-17.2.0.2，版本号是17.2.0.2，更新日期2015-10-27，原来这才是最新的驱动。\n（2018.1.25更新：上面的地址已失效，最新地址请点击此处，并选择DOWNLOADS→Software→NetLink®/NetXtreme® I Desktop/Mobile/Server (x64)，也可以从本站下载。）\n在安装最新驱动之前，我们需要关闭WIN10的自动更新驱动功能，因为WIN10会认为它的15.6.0.14版本是最新的，在windows update时把实际最新的17.2.0.2版本替换掉。具体做法是在Cortana中搜索“更改设备安装设置”并打开，选择否，从不安装来自Windows更新的驱动程序软件，如下。\n然后重启进入安全模式，再次在设备管理器中右键点击网卡驱动，选择更新-\u0026gt;浏览计算机以查找驱动程序软件-\u0026gt;从计算机的设备驱动程序列表中选取-\u0026gt;点击从磁盘安装按钮-\u0026gt;浏览找到你之前在网上下载的最新驱动（*.inf格式）-\u0026gt;选中-\u0026gt;依次确定。刷新之后再次查看驱动信息如下：\n可以看到驱动已经更新到最新的版本了。再次重启进入正常模式，目前用了两天了也没有再断过网。\n其他WIN10驱动问题应该也可以用类似的方法解决。\n（话说我的WIN10偶尔会死机，就是用着用着突然鼠标和键盘完全动不了了，只能强制重启，有谁知道这是怎么回事吗？）\n","permalink":"http://localhost:1313/posts/2015-11-13-solution-to-win10s-network-problem/","summary":"\u003cp\u003e安装WIN10一个月以来，校园有线网经常间歇性断网，通常是20分钟不到就断了，需要重启或者把有线连接关闭再打开才可以。在微博上问过微软客服也无果，\u003ca href=\"http://www.pcadvisor.co.uk/forum/windows-29/windows-10-no-internet-trough-ethernet-4540238/\"\u003e后来Google到某国外的解决办法\u003c/a\u003e，现记录如下。\u003c/p\u003e\n\u003cp\u003e说到底WIN10断网的问题还是和驱动有关，先看一下我的有线网卡Broadcom NetLink (TM) Gigabit Ethernet，驱动信息是这样的：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"bcm-driver-before\" loading=\"lazy\" src=\"/posts/2015-11-13-solution-to-win10s-network-problem/bcm-driver-before.png\"\u003e\u003c/p\u003e\n\u003cp\u003e还是13年的驱动，版本号是15.6.0.14，于是第一想到的是更新驱动。点击驱动右键选更新-\u0026gt;自动搜索更新的驱动程序软件-\u0026gt;提示“已安装适合设备的最佳驱动程序软件”，但这明明不是最新的驱动啊！\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.broadcom.com/support/ethernet-nic-netxtreme-i-desktop-mobile\"\u003e于是在Broadcom的官网上找到了最新驱动win_b57_x64-17.2.0.2\u003c/a\u003e，版本号是17.2.0.2，更新日期2015-10-27，原来这才是最新的驱动。\u003c/p\u003e\n\u003cp\u003e（\u003cstrong\u003e2018.1.25\u003c/strong\u003e更新：上面的地址已失效，\u003ca href=\"https://www.broadcom.cn/products/ethernet-connectivity/controllers/bcm5720#downloads\"\u003e最新地址请点击此处\u003c/a\u003e，并选择DOWNLOADS→Software→NetLink®/NetXtreme® I Desktop/Mobile/Server (x64)，\u003ca href=\"/posts/2015-11-13-solution-to-win10s-network-problem/win_b57_x64-17.2.0.2.zip\"\u003e也可以从本站下载\u003c/a\u003e。）\u003c/p\u003e\n\u003cp\u003e在安装最新驱动之前，我们需要关闭WIN10的自动更新驱动功能，因为WIN10会认为它的15.6.0.14版本是最新的，在windows update时把实际最新的17.2.0.2版本替换掉。具体做法是在Cortana中搜索“\u003cstrong\u003e更改设备安装设置\u003c/strong\u003e”并打开，选择否，从不安装来自Windows更新的驱动程序软件，如下。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"change-device-installation\" loading=\"lazy\" src=\"/posts/2015-11-13-solution-to-win10s-network-problem/change-device-installation.png\"\u003e\u003c/p\u003e\n\u003cp\u003e然后\u003ca href=\"http://jingyan.baidu.com/article/fea4511a72cb38f7ba912543.html\"\u003e重启进入安全模式\u003c/a\u003e，再次在设备管理器中右键点击网卡驱动，选择更新-\u0026gt;浏览计算机以查找驱动程序软件-\u0026gt;从计算机的设备驱动程序列表中选取-\u0026gt;点击从磁盘安装按钮-\u0026gt;浏览找到你之前在网上下载的最新驱动（*.inf格式）-\u0026gt;选中-\u0026gt;依次确定。刷新之后再次查看驱动信息如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"bcm-driver-after\" loading=\"lazy\" src=\"/posts/2015-11-13-solution-to-win10s-network-problem/bcm-driver-after.png\"\u003e\u003c/p\u003e\n\u003cp\u003e可以看到驱动已经更新到最新的版本了。再次重启进入正常模式，目前用了两天了也没有再断过网。\u003c/p\u003e\n\u003cp\u003e其他WIN10驱动问题应该也可以用类似的方法解决。\u003c/p\u003e\n\u003cp\u003e（话说我的WIN10偶尔会死机，就是用着用着突然鼠标和键盘完全动不了了，只能强制重启，有谁知道这是怎么回事吗？）\u003c/p\u003e","title":"解决Win10间歇性断网的问题"},{"content":"大家好，施一公老师的26篇博客大概可以分为四类：1）讲述个人生活经历2）评论社会问题3）讨论国家科技和人才引进政策4）介绍学习方法。这些博客比较全面地反应了施一公的求学经历、由学生到教授的转变过程以及回国之后为中国人才引进所做出的努力。\n给我感触最深的有3点：1）环境对人的影响很大2）坚持总会有所收获3）做一个有担当、有社会责任感的科研人。\n1）环境对人的影响很大 《从\u0026lt;高考1977\u0026gt;说起》这篇博客详细介绍了施一公高考前的家庭情况，施一公的父亲是哈工大毕业，母亲是北京矿业学院（今中国矿业大学）毕业，在上世纪五六十年代，父母都是名校大学毕业，可谓是少有的知识分子家庭。施一公还有两个姐姐、一个哥哥、一个表哥和一个表姐，哥哥姐姐们刻苦的学习、父亲悉心的辅导以及不错的高考成绩对施一公产生了很大的影响，争强好胜的施一公自然不甘示弱，以84年全国数学联赛省第一名的成绩保送清华。诚然，施一公的成绩和他自己的刻苦努力分不开，但是从小良好的家庭氛围也功不可没。\n2）坚持总会有所收获 这可以从施一公的两个例子中看出。\n《今天3000米》讲到施一公从82年跑步的“倒数第一“、“颜面尽失”之后开始坚持长跑，直到89年从未间断，在85年的清华新生运动会3000米竞走中，以16分10秒轻松获得第一名。\n跑步这件事我也深有体会，我高中几乎没有体育锻炼，大一刚入学的体能测试中，1000米项目跑了4’15’’，小组倒数第一，跑完全程脸都发白，当时真的担心大学因体育挂科毕不了业。后来大三下的时候，开始坚持跑步，一开始每晚跑3圈，一个月后加一圈，最后稳定在每晚跑5圈，一直坚持到毕业。在毕业体能测试中，还是1000米项目，我居然跑了3’42’’，小组顺数第一名，跑完之后虽然有点累，但并不感觉难受，连我自己都不太相信。\n《如何做一名优秀的博士生：（一）时间的付出》中，施一公讲到他在留学期间的时间付出。“留学的第一年，我情绪波动很大，内心浮躁而迷茫，根本无心念书。”“第二年，每周五天、每天从上午9点做实验到晚上7、8点，周末也会去两个半天。””到了第三年，晚上常常干到11点多，赶最后一班校车从霍普金斯医学院回Homewood campus（我住在附近）。””研究生阶段后期，我的刻苦在实验室是出了名的。每天晚上做实验到半夜三点左右，回到住处躺下来睡觉时常常已是四点以后；但每天早晨八点都会被窗外纽约第一大道(First Avenue)上的汽车喧闹声吵醒，九点左右又回到实验室开始了新的一天…”\n施一公几年如一日的坚持没有白费，他顺利毕业并获得名校终生教职席位。\n其实正如H老师所说“以大多数人努力的程度，根本还没到拼智商的时候。” 坚持做一件事，点滴积累，做到极致。无论什么事情，坚持做下去，一定能有所收获，对于体力活更是如此。\n3）做一个有担当、有社会责任感的科研人 在读施一公博客的时候，心潮澎湃，热血沸腾，无论是施一公自己排除万难坚持回国的行动，还是施一公回国之后号召海龟回国的倡议，亦或是施一公为人才引进，千人计划建言献策的付出，都真真切切的体现了他的强烈的爱国热情。\n施一公回国后的去私心、敢担当、有作为，坚持职业操守，“我申请基金的时候一定不和评委在评审前或评审后做任何形式的私下沟通；我当评委的时候一定不和申请人在评审前或评审后做任何形式的私下沟通”等都在用切身行动一点点改善国内的科研环境。\n在pFind组，H老师也时常教导我们要对学术保留一点敬畏之心，做好科研，尽自己一份力改善国际社会对中国学术界的看法。\n总的来说，施一公老师的博客内容丰富，让我受益匪浅，也给了我很多启发，关于如何做一名合格的研究生，我还完全是门外汉，前面的师兄师姐都给出了很多方法论的解读，我也把施一公关于如何做一名优秀的博士生的几个要点罗列如下，希望用此标准来要求自己。\n如何一名优秀的博士生：\n时间的付出 方法论的转变 正确分析负面结果 耗费时间的完美主义阻碍创新进取 科研文献与学术讲座的取与舍 挑战传统思维 祝大家工作顺利！\n-bitJoy\n","permalink":"http://localhost:1313/posts/2015-10-31-review-about-shiyigongs-blogs/","summary":"\u003cp\u003e大家好，\u003ca href=\"http://blog.sciencenet.cn/home.php?mod=space\u0026amp;uid=46212\u0026amp;do=blog\u0026amp;view=me\u0026amp;from=space\"\u003e施一公老师的26篇博客\u003c/a\u003e大概可以分为四类：1）讲述个人生活经历2）评论社会问题3）讨论国家科技和人才引进政策4）介绍学习方法。这些博客比较全面地反应了施一公的求学经历、由学生到教授的转变过程以及回国之后为中国人才引进所做出的努力。\u003c/p\u003e\n\u003cp\u003e给我感触最深的有3点：1）环境对人的影响很大2）坚持总会有所收获3）做一个有担当、有社会责任感的科研人。\u003c/p\u003e\n\u003ch1 id=\"1环境对人的影响很大\"\u003e1）环境对人的影响很大\u003c/h1\u003e\n\u003cp\u003e《从\u0026lt;高考1977\u0026gt;说起》这篇博客详细介绍了施一公高考前的家庭情况，施一公的父亲是哈工大毕业，母亲是北京矿业学院（今中国矿业大学）毕业，在上世纪五六十年代，父母都是名校大学毕业，可谓是少有的知识分子家庭。施一公还有两个姐姐、一个哥哥、一个表哥和一个表姐，哥哥姐姐们刻苦的学习、父亲悉心的辅导以及不错的高考成绩对施一公产生了很大的影响，争强好胜的施一公自然不甘示弱，以84年全国数学联赛省第一名的成绩保送清华。诚然，施一公的成绩和他自己的刻苦努力分不开，但是从小良好的家庭氛围也功不可没。\u003c/p\u003e\n\u003ch1 id=\"2坚持总会有所收获\"\u003e2）坚持总会有所收获\u003c/h1\u003e\n\u003cp\u003e这可以从施一公的两个例子中看出。\u003c/p\u003e\n\u003cp\u003e《今天3000米》讲到施一公从82年跑步的“倒数第一“、“颜面尽失”之后开始坚持长跑，直到89年从未间断，在85年的清华新生运动会3000米竞走中，以16分10秒轻松获得第一名。\u003c/p\u003e\n\u003cp\u003e跑步这件事我也深有体会，我高中几乎没有体育锻炼，大一刚入学的体能测试中，1000米项目跑了4’15’’，小组倒数第一，跑完全程脸都发白，当时真的担心大学因体育挂科毕不了业。后来大三下的时候，开始坚持跑步，一开始每晚跑3圈，一个月后加一圈，最后稳定在每晚跑5圈，一直坚持到毕业。在毕业体能测试中，还是1000米项目，我居然跑了3’42’’，小组顺数第一名，跑完之后虽然有点累，但并不感觉难受，连我自己都不太相信。\u003c/p\u003e\n\u003cp\u003e《如何做一名优秀的博士生：（一）时间的付出》中，施一公讲到他在留学期间的时间付出。“留学的第一年，我情绪波动很大，内心浮躁而迷茫，根本无心念书。”“第二年，每周五天、每天从上午9点做实验到晚上7、8点，周末也会去两个半天。””到了第三年，晚上常常干到11点多，赶最后一班校车从霍普金斯医学院回Homewood campus（我住在附近）。””研究生阶段后期，我的刻苦在实验室是出了名的。每天晚上做实验到半夜三点左右，回到住处躺下来睡觉时常常已是四点以后；但每天早晨八点都会被窗外纽约第一大道(First Avenue)上的汽车喧闹声吵醒，九点左右又回到实验室开始了新的一天…”\u003c/p\u003e\n\u003cp\u003e施一公几年如一日的坚持没有白费，他顺利毕业并获得名校终生教职席位。\u003c/p\u003e\n\u003cp\u003e其实正如H老师所说“\u003cstrong\u003e以大多数人努力的程度，根本还没到拼智商的时候。\u003c/strong\u003e” 坚持做一件事，点滴积累，做到极致。无论什么事情，坚持做下去，一定能有所收获，对于体力活更是如此。\u003c/p\u003e\n\u003ch1 id=\"3做一个有担当有社会责任感的科研人\"\u003e3）做一个有担当、有社会责任感的科研人\u003c/h1\u003e\n\u003cp\u003e在读施一公博客的时候，心潮澎湃，热血沸腾，无论是施一公自己排除万难坚持回国的行动，还是施一公回国之后号召海龟回国的倡议，亦或是施一公为人才引进，千人计划建言献策的付出，都真真切切的体现了他的强烈的爱国热情。\u003c/p\u003e\n\u003cp\u003e施一公回国后的去私心、敢担当、有作为，坚持职业操守，“我申请基金的时候一定不和评委在评审前或评审后做任何形式的私下沟通；我当评委的时候一定不和申请人在评审前或评审后做任何形式的私下沟通”等都在用切身行动一点点改善国内的科研环境。\u003c/p\u003e\n\u003cp\u003e在pFind组，H老师也时常教导我们要\u003cstrong\u003e对学术保留一点敬畏之心\u003c/strong\u003e，做好科研，尽自己一份力改善国际社会对中国学术界的看法。\u003c/p\u003e\n\u003cp\u003e总的来说，施一公老师的博客内容丰富，让我受益匪浅，也给了我很多启发，关于如何做一名合格的研究生，我还完全是门外汉，前面的师兄师姐都给出了很多方法论的解读，我也把施一公关于如何做一名优秀的博士生的几个要点罗列如下，希望用此标准来要求自己。\u003c/p\u003e\n\u003cp\u003e如何一名优秀的博士生：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e时间的付出\u003c/li\u003e\n\u003cli\u003e方法论的转变\n\u003col\u003e\n\u003cli\u003e正确分析负面结果\u003c/li\u003e\n\u003cli\u003e耗费时间的完美主义阻碍创新进取\u003c/li\u003e\n\u003cli\u003e科研文献与学术讲座的取与舍\u003c/li\u003e\n\u003cli\u003e挑战传统思维\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e祝大家工作顺利！\u003c/p\u003e\n\u003cp\u003e-bitJoy\u003c/p\u003e","title":"读施一公博客有感"},{"content":"进入研究生生涯完成的第一个新生培训作业是“2.5亿个浮点数的外部排序算法”，前后折腾了将近一个月，结果是在i7处理器上，限制512MB内存，排序用时250秒左右。\n这个作业的常规思路大部分人都能想到，按块读取文件-\u0026gt;atof转换为double-\u0026gt;内部快速排序或基数排序-\u0026gt;dtoa转换为char*-\u0026gt;按块写入文件。这里面中间的三个过程都很耗时，特别是atof和dtoa，因为精度只要求保留9位小数，所以可以自己实现atof和dtoa来加速，也可以使用多线程加速。\n整个作业都是基于对IEEE754浮点数的深刻理解展开的，所以下面详细讲解浮点数的一些知识。\nIEEE754双精度浮点数 目前大多数CPU内浮点数的表示都遵循IEEE754标准，IEEE754双精度浮点数（double）表示如下图所示。\nIEEE754 double在内存中的形式[1]\nsign bit：符号位，1位，用来表示正负号，0表示非负；1表示负 exponent：指数位，11位，用来表示次方数，是一个无符号数 fraction：尾数位，52位，用来表示精确度，也是一个无符号数，有些资料也叫做mantissa或significand 这种表示形式有两点需要注意。\n第一，既然exponent是无符号的，那么怎样表示负指数呢？\nIEEE754规定，二进制串中算得的e需要减去一个偏移量bias，对于double，bias=1023，即e’=e-bias。因为\\(e\\in[0,2^{11}-1]\\)，所以最终\\(e’\\in[-2^{10}+1,2^{10}]\\)。但是如果把e本身看作有符号数e”，则\\(e”\\in[-2^{10},2^{10}-1]\\)，既然e”和e’相差微小，为什么不直接把e看成有符号数，而非要把它看成无符号数，再减去一个偏移量bias呢？\n这是因为如果把e看成无符号数再减偏移量，浮点数大小比较速度更快。引用维基百科的一段话：\nBy arranging the fields so that the sign bit is in the most significant bit position, the biased exponent in the middle, then the mantissa in the least significant bits, the resulting value will be ordered properly, whether it’s interpreted as a floating point or integer value. This allows high speed comparisons of floating point numbers using fixed point hardware.\n对于两个正浮点数a和b，如果a\u0026gt;b，则a的二进制字符串的字典序也相应的在b的后面；对于负数则正好相反。也就是说，无论是把这个数看成浮点数还是整数，都可以通过只比较两个数的二进制串得出大小关系，而不需要通过公式计算其十进制值再比较大小，这显然加快了比较速度。\n浮点数的这个特性使得对浮点数排序也可以使用基数排序！很神奇吧，具体是这样的：先对二进制串进行分组，按先低位组后高位组对其进行基数排序；当到最高位组时，把负数放到正数的前面逆序排列，正数常规排列，得到的就是有序的排列。\n比如，假设把数看成无符号数时，会得到下面的基数排序结果，此时需要调整顺序，把正数统一移到后面，就是代码第50行：index += negatives；把负数移到前面的同时逆序排列，相当于在0的上面画一条线，然后-1,-2,-7以这条线做一个翻折对称，所以新的负数的下标变成了第48行的：index = n – index – 1。\n0　:　0000 1　:　0001 2　:　0010 4　:　0100 -1　:　1001 -2　:　1010 -7　:　1111　变成：\n-7　:　1111　-2　:　1010 -1　:　1001 0　:　0000 1　:　0001 2　:　0010 4　:　0100 具体的实现可以看这个讨论，下面是我实现的C++版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 void RadixSort(std::vector\u0026lt;double\u0026gt; \u0026amp;nums) { int n = nums.size(); vector\u0026lt;LL\u0026gt; t(n), a(n); for (int i = 0; i \u0026lt; n; i++) a[i] = *(LL*)(\u0026amp;nums[i]); //将double的二进制转换为long long int groupLength = 16; //可自定义 int bitLength = 64; int len = 1 \u0026lt;\u0026lt; groupLength; vector\u0026lt;int\u0026gt; count(len), pref(len); int groups = bitLength / groupLength; int mask = len - 1; int negatives = 0, positives = 0; for (int c = 0, shift = 0; c \u0026lt; groups; c++, shift += groupLength) { // reset count array fill(count.begin(), count.end(), 0); // counting elements of the c-th group for (int i = 0; i \u0026lt; n; i++) { ++count[(a[i] \u0026gt;\u0026gt; shift) \u0026amp; mask]; // additionally count all negative // values in first round if (c == 0 \u0026amp;\u0026amp; a[i] \u0026lt; 0) ++negatives; } if (c == 0) positives = n - negatives; // calculating prefixes pref[0] = 0; for (int i = 1; i \u0026lt; len; i++) pref[i] = pref[i - 1] + count[i - 1]; // from a[] to t[] elements ordered by c-th group for (int i = 0; i \u0026lt; n; i++) { // Get the right index to sort the number in int index = pref[(a[i] \u0026gt;\u0026gt; shift) \u0026amp; mask]++; if (c == groups - 1) { // We\u0026#39;re in the last (most significant) group, if the // number is negative, order them inversely in front // of the array, pushing positive ones back. if (a[i] \u0026lt; 0) index = n - index - 1; else index += negatives; } t[index] = a[i]; } // a[]=t[] and start again until the last group if (c != groups - 1) { for (int j = 0; j \u0026lt; n; j++) a[j] = t[j]; } } // Convert back the ints to the double array for (int i = 0; i \u0026lt; n; i++) nums[i] = *(double*)(\u0026amp;t[i]); //重新把long long 的二进制转换为double } 浮点数的基数排序肯定会比快速排序快，至于快多少我就没有测试了。\n二，IEEE754浮点数都是规格化浮点数。\n规格化（normalized）浮点数是指尾数f的最高位非0。如果指数e的范围是无限的，则可以通过对尾数f移位并调整指数e的大小对v进行规格化；如果e的范围是有限的，则有些数并不能被规格化（f移位过多，导致调整后的e超出其范围）。有最小指数的不可规格化浮点数称为非规格化数（denormals）。[3]\n当所有的数都是以规格化数或非规格化数表示时，他们是唯一的。[3]\nIEEE754也能表示非规格化浮点数，但不在本文的讨论范围。\n既然IEEE754浮点数都是规格化浮点数，则他们的f最高位都是1（非0），所以可以省略这一位。也就是说IEEE754双精度浮点数的尾数实际上有53位，只是最高位都是1，所以都省略掉了。\n了解了这两点之后，我们就可以理解为什么将IEEE754双精度浮点数转换为十进制数的公式是下面这个样子了。\n[1]\n或者\n[1]\n在接下来的讨论中，假设一个浮点数为v，其尾数为\\(f_v\\)，指数为\\(e_v\\)，基为b（通常为2），则有\\(v=f_v\\times b^{e_v}\\)。对于IEEE754 double，因为尾数省略了最高位1，所以有\\(hidden=2^{52}\\)，二进制串中的尾数为\\(f_{IEEE}\\)，真正的尾数为\\(f_v=hidden + f_{IEEE}\\)，真正的指数为\\(e_v=e_{IEEE}-bias\\)，所以有\\(v=f_v\\times 2^{e_v}\\)。\n舍入机制 因为浮点数并不能表示所有的实数，所以将实数映射到浮点数的时候，需要一个舍入机制，有两种舍入机制：\nup：向上进位，使用\\([x]^\\uparrow\\)表示； even：选择偶数，使用\\([x]^\\Box\\)表示，比如在十进制中，1.5→2、0.5→0；这是IEEE的默认策略。 当四舍五入的策略不重要时，使用\\([x]^*\\)表示。我们使用\\(\\widetilde x=[x]_p^s\\)来表示规格化浮点数\\(\\widetilde x\\)（x上一根波浪线）的尾数位数（精度）为p，在规格化的过程中使用了s的四舍五入策略。\nULP ULP的全称为unit in the last place，可以理解为尾数相差一个单位时，浮点数的差值。因为x被四舍五入到最接近x的值\\(\\widetilde x\\)，所以有\\(|\\widetilde x-x|\\leqslant 0.5\\times b^e=0.5ulp\\)。\n邻居和边界 令\\(v=f_v\\times b^{e_v}\\)是一个正浮点数，则v的前驱节点\\(v^-\\)是v的上一个可以表示的浮点数；v的后继节点\\(v^+\\)是v的下一个可以表示的浮点数。如果v是最小值，则\\(v^-=0\\)；如果v是最大值，则\\(v^+=v+(v-v^-)\\)。\\(v^-\\)和\\(v^+\\)都是v的邻居，他们和v具有相同的距离。\n两个相邻的数\\(v_1\\)和\\(v_2\\)的边界为他们的算术平均\\(m=(v_1+v_2)/2\\)。根据定义，边界值是不能被表示的。每个浮点数v都有2个边界：\\(m^-=(v^-+v)/2\\)、\\(m^+=(v^++v)/2\\)。明显的，任何实数\\(m^- \u003c w \u003c m^+\\)都将四舍五入到v，也就是说在\\((m^-,m^+)\\)之间的实数是无法用计算机表示的。\nGrisu原是1970年代意大利动画片中的主角，它是一条想成为消防员的小龙，配图是动画片VCD的封面[2]\n下面开始介绍Grisu算法，参考论文Printing Floating-Point Numbers Quickly and Accurately with Integers[3]。\nGrisu算法 自定义数据结构 本文使用整数来实现浮点数的转换，数据结构如下：\n1 2 3 4 typedef struct diy_fp{ uint64_t f; int e; }diy_fp; diy_fp中的f表示尾数，e表示指数。f的精度为q=64，高于IEEE754双精度浮点数的精度p=53。\ndiy_fp有两种运算，减法和乘法。减法为指数相等，尾数相减，结果可能没有规格化；乘法如下：\n$$x\\otimes y=[(f_x\\times f_y)/2^q ]^\\uparrow \\times 2^{e_x+e_y+q}$$乘法结果要四舍五入到64位，所以会有一些错误，但是错误不超过0.5ulp，结果可能没有规格化。\n预计算10的幂 Grisu算法需要用到10的幂的规格化结果，提前计算好这些结果能加速Grisu运行。函数diy_fp cached_power(int k);能够直接返回\\(10^k\\)的规格化浮点数。\n假设输入浮点数为v，其指数为e，需要寻找的10的幂即为\\(\\widetilde{c_k}=f_{c_k }\\times 2^{e_{c_k}}=[10^k ]_q^*\\)，且指数满足\\(\\alpha \\leqslant e_{c_k}+e \\leqslant \\gamma\\)。推导过程为：同时对\\(\\widetilde{c_k}\\)两边取\\(log_{10}\\)得到\\(k=log_{10}(f_{c_k }\\times 2^{e_{c_k}})\\)，将\\(e_{c_k}\\)的下界\\(e_{c_k}\\geqslant\\alpha-e\\)代入，得到\\(k=\\lceil log_{10}(f_{c_k}\\times 2^{\\alpha -e})\\rceil\\)，又因为\\(\\widetilde{c_k}\\)所表示的10的幂的尾数精度为\\(q\\)，所以\\(f_{c_k}\\)的下界为\\(2^{q-1}\\)，代入前一个式子得到下式：\n$$k=\\lceil log_{10}2^{\\alpha -e+q-1}\\rceil=\\lceil (\\alpha -e+q-1)\\times 1/log_{2}10\\rceil$$Grisu算法描述 假设浮点数v的指数是负数，则v可表示为\\(\\frac{f_v}{2^{-e_v}}\\)，如果能找到一个t，使得\\(1\\leqslant \\frac{f_v\\times 10^t}{2^{-e_v}}\u003c10\\)，则v的十进制尾数为\\(\\frac{f_v\\times 10^t}{2^{-e_v}}\\)，十进制指数为-t；我们可以很容易获取\\(\\frac{f_v\\times 10^t}{2^{-e_v}}\\)的各位数字。\n所以Grisu要解决的问题就是怎样快速的将\\(f_v\\times 2^{e_v}\\)转换为\\(D\\times 10^k\\)的形式，并且D尽量小。这样我们可以很容易的获取D的各位数字，和其十进制指数k。问题进一步转换为求k，使得\\(D=v\\times 10^{-k}=f_v\\times 2^{e_v}\\times f_{c_{-k}}\\times 2^{e_{c_{-k}}}\\)尽量的小，所以要让\\(e_{c_{-k}}\\)和\\(e_v\\)能尽量的抵消掉，这就是为什么在预计算10的幂中要求\\(\\alpha \\leqslant e_{c_k}+e \\leqslant \\gamma\\)，且\\(\\alpha\\)和\\(\\gamma\\)都比较小，但是论文表明\\(\\alpha\\)和\\(\\gamma\\)并不是越小越好。\n下面是Grisu算法的具体描述\n输入：精度为p的正浮点数v 前提：diy_fp的精度q≥p+2，且\\(\\widetilde{c_k}=[10^k ]_q^*\\)已经提前计算好了。 输出：十进制字符串V，且满足\\([V]_p^\\Box=v\\)，也就是说再次读取字符串V时，能四舍五入成浮点数v。 过程：\n求v的规格化浮点数表示diy_fp w 寻找满足\\(\\alpha \\leqslant e_c+e_w+q\\leqslant \\gamma\\)的10的幂\\(\\widetilde{c_{-k}}=f_c\\times 2^{e_c}=[10^{-k}]_q^*\\) 计算乘积\\(\\widetilde D=f_D\\times 2^{e_D}=w\\otimes\\widetilde{c_{-k}}\\) 定义\\(V=\\widetilde D\\times 10^k\\)，输出\\(\\widetilde D\\)的十进制表示，字符\u0026rsquo;e\u0026rsquo;和k的十进制表示。 Grisu算法中，\\(\\widetilde D\\)相当于v的十进制尾数，k相当于v的十进制指数。\nGrisu2算法 Grisu算法虽然快，但是得到的结果并不是最短的，比如Grisu可能会把1.0打印成10000000000000000000e-19。Grisu2算法使用了额外的二进制位使得对于99%的输入都能输出最短的字符串表示。\n主要长度 令v是一个正实数，n, l和s是整数，有\\(l\\geqslant 1, 10^{l-1}\\leqslant s\\leqslant 10^l, v=s\\times 10^{n-l}\\)，并且l越小越好，则s的前l个数字是v的主要数字（leading digits），l就是v的主要长度（leading length）。\n通俗的说就是把V的不必要的前导和后尾0去掉后的长度，比如\\(1.23=123\\times 10^{-2}\\)=\\(1230\\times 10^{-3}\\)，但是\\(1230\\times 10^{-3}\\)就多了一个不必要的后尾0，所以1.23的主要长度是3。\n定理6.2 令x和y是2个实数，且\\(x\\leqslant y\\)。令k是满足y mod \\(10^k\\leqslant y-x\\)的最大整数，则有\\(V=\\lfloor \\frac{y}{10^k}\\rfloor \\times 10^k\\)满足\\(x\\leqslant V\\leqslant y\\)。并且V的主要长度（leading length）是所有在[x,y]范围内最小的。\nGrisu2算法正是在Grisu的基础上，利用定理6.2输出了最短的长度。\nGrisu2算法描述 输入：同Grisu算法 前提：diy_fp的精度q≥p+3，且\\(\\widetilde {c_k}=[10^k ]_q^*\\)已经提前计算好了。 输出：十进制数字\\(d_i\\)，i属于[0,n]，和整数\\(\\kappa\\)使得\\(V=d_0 … d_n\\times 10^\\kappa\\)满足\\([V]_p^\\Box =v\\) 过程：\n计算v的边界\\(m^-\\)和\\(m^+\\)（\\(v\\mp 0.5ulp\\)） diy_fp \\(w^+=m^+\\); diy_fp \\(w^-=m^-;\\) 且\\(e_w^-=e_w^+\\) 寻找满足\\(\\alpha \\leqslant e_c+e_w^++q\\leqslant \\gamma\\)的10的幂\\(\\widetilde{c_{-k}}=f_c\\times 2^{e_c}=[10^{-k}]_q^*\\) 计算\\(\\widetilde{M^-}=w^- \\otimes \\widetilde{c_{-k}}\\)；\\(\\widetilde{M^+}=w^+ \\otimes \\widetilde{c_{-k}}\\)；\\(M_{\\uparrow }^-=\\widetilde{M^-}+1ulp\\)；\\(M_{\\downarrow }^+=\\widetilde{M^+}-1ulp\\)；\\(\\delta =M_{\\downarrow }^+-M_{\\uparrow }^-\\) 找到最大的\\(\\kappa\\)使得\\(M_{\\downarrow }^+ mod 10^{\\kappa }\\leqslant \\delta\\)，并且定义\\(P=\\lfloor \\frac{M_{\\downarrow }^+}{10^{\\kappa }}\\rfloor\\) 定义\\(V=P\\times 10^{k+\\kappa }\\)，输出P的十进制数字和\\(K=k+\\kappa\\) Grisu2算法的前三步工作和Grisu算法类似，4,5步的工作近似求Grisu算法的\\(\\widetilde D\\)的最短表示，利用的正是定理6.2，所以最终的指数是由k和\\(\\kappa\\)两部分构成的。\n该论文表示，Grisu2算法比sprintf快四倍左右，根据milo的测试[4] ，经过优化的Grisu2算法milo比sprintf快九倍，据说Google的V8 JavaScript引擎就是使用了Grisu算法，速度才很快的。\ndtoa-benchmark，grisu2是sprintf的5.7倍，milo（优化过的grisu2算法）是sprintf的9.1倍[4]\n至于milo是怎么优化Grisu的，可以参考他的博客，完整的代码可以参考他的Github项目[4]。\n关于完整全面的浮点数介绍，可以参考文献[5]。\n参考：\n[1]. https://en.wikipedia.org/wiki/Double-precision_floating-point_format\n[2]. http://miloyip.com/images/grisu.jpg\n[3]. Printing Floating-Point Numbers Quickly and Accurately with Integers第2节\n[4]. dtoa-benchmark\n[5]. What Every Computer Scientist Should Know About Floating-Point Arithmetic\n","permalink":"http://localhost:1313/posts/2015-08-30-introduction-to-floating-point-numbers-and-grisu-algorithm/","summary":"\u003cp\u003e进入研究生生涯完成的第一个新生培训作业是“2.5亿个浮点数的外部排序算法”，前后折腾了将近一个月，结果是在i7处理器上，限制512MB内存，排序用时250秒左右。\u003c/p\u003e\n\u003cp\u003e这个作业的常规思路大部分人都能想到，按块读取文件-\u0026gt;atof转换为double-\u0026gt;内部快速排序或基数排序-\u0026gt;dtoa转换为char*-\u0026gt;按块写入文件。这里面中间的三个过程都很耗时，特别是atof和dtoa，因为精度只要求保留9位小数，所以可以自己实现atof和dtoa来加速，也可以使用多线程加速。\u003c/p\u003e\n\u003cp\u003e整个作业都是基于对IEEE754浮点数的深刻理解展开的，所以下面详细讲解浮点数的一些知识。\u003c/p\u003e\n\u003ch1 id=\"ieee754双精度浮点数\"\u003eIEEE754双精度浮点数\u003c/h1\u003e\n\u003cp\u003e目前大多数CPU内浮点数的表示都遵循IEEE754标准，IEEE754双精度浮点数（double）表示如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"IEEE754 double在内存中的形式[1]\" loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/a/a9/IEEE_754_Double_Floating_Point_Format.svg\"\u003e\nIEEE754 double在内存中的形式[1]\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esign bit：符号位，1位，用来表示正负号，0表示非负；1表示负\u003c/li\u003e\n\u003cli\u003eexponent：指数位，11位，用来表示次方数，是一个无符号数\u003c/li\u003e\n\u003cli\u003efraction：尾数位，52位，用来表示精确度，也是一个无符号数，有些资料也叫做mantissa或significand\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这种表示形式有两点需要注意。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e第一，既然exponent是无符号的，那么怎样表示负指数呢？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIEEE754规定，二进制串中算得的e需要减去一个偏移量bias，对于double，bias=1023，即e’=e-bias。因为\\(e\\in[0,2^{11}-1]\\)，所以最终\\(e’\\in[-2^{10}+1,2^{10}]\\)。但是如果把e本身看作有符号数e”，则\\(e”\\in[-2^{10},2^{10}-1]\\)，既然e”和e’相差微小，为什么不直接把e看成有符号数，而非要把它看成无符号数，再减去一个偏移量bias呢？\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://stackoverflow.com/questions/2612775/why-ieee-floating-point-number-calculate-exponent-using-a-biased-form\"\u003e这是因为如果把e看成无符号数再减偏移量，浮点数大小比较速度更快。\u003c/a\u003e引用\u003ca href=\"https://en.wikipedia.org/wiki/Exponent_bias\"\u003e维基百科\u003c/a\u003e的一段话：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBy arranging the fields so that the sign bit is in the most significant bit position, the biased exponent in the middle, then the mantissa in the least significant bits, the resulting value will be ordered properly, whether it’s interpreted as a floating point or integer value. This allows high speed comparisons of floating point numbers using fixed point hardware.\u003c/p\u003e","title":"浮点数知识及Grisu算法介绍"},{"content":"去年暑假在北大计算所实习的时候，任务之一就是批量下载百度图片。当时没学python，用c#实现了一个简易版本的批量下载器，如下图。\nC#版本百度图片批量下载器（抓的是百度的wap站点，现在好像不能用了）\n当时“时间紧，任务重“，既没仔细研究百度图片API，也没处理好界面线程阻塞的问题。这个问题其实很有意思，趁着暑假在家，实现了一个比较完美的python版本，先上效果图。\npython3版本百度图片批量下载器\n新版使用了python-3.4.3.amd64.msi + PyQt5-5.5-gpl-Py3.4-Qt5.5.0-x64.exe + eric6-6.0.8.zip + cx_Freeze-4.3.4-cp34-none-win_amd64.whl，完整项目在我的GitHub上。大致有如下几点工作：\n研究百度图片API，获取原始图片URL列表 使用python3进行多线程下载 利用pyqt5实现界面 利用cx_Freeze4打包整个程序 下面记录每个步骤的要点，供后人参考。\n百度图片API 正常使用百度图片搜索的时候，URL是这样的：\nhttp://image.baidu.com/search/index?ct=201326592\u0026z=0\u0026tn=baiduimage\u0026ipn=r\u0026word=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6\u0026pn=0\u0026istype=2\u0026ie=utf-8\u0026oe=utf-8\u0026cl=2\u0026lm=-1\u0026st=-1\u0026fr=\u0026fmq=1439374041843_R\u0026ic=0\u0026se=\u0026sme=\u0026width=0\u0026height=0\u0026face=0\n里面有很多参数，有些我们并不需要，精简之后大概是这样的：\nhttp://image.baidu.com/i?tn=baiduimage\u0026ie=utf-8\u0026word=%E7%BE%8E%E5%A5%B3\u0026pn=\u0026rn=\u0026z=\nword为搜索关键词；pn为page number当前是第几页，实际含义是image id，表示第几张图片，从0开始；rn为每一页的图片数量，最大为60；z表示图片尺寸，z=9特大尺寸，z=3大尺寸，z=2中等尺寸，z=1小尺寸，z=0所有尺寸。\n但是这个URL是给”人“看的，下一页的图片是动态加载的，其html代码的图片URL数量固定。一番查询之后发现，将tn=baiduimage换成tn=resultjson_com能获取到所有图片URL的json，json当然是给”猴“看的，这样就能轻松获取到所有图片的URL。\n慢着，仔细看看json中的objURL，是一串连”猴“都看不懂的字符串，原来百度把图片真实URL加密了，好在加密方法是简单的字符映射，参考这篇博客成功解密。\n更新：tn=resultjson_com的objURL是加密了，但是tn=resultjson的objURL并没有加密，所以采用tn=resultjson最佳。\n通过控制pn和rn就能获取指定数量的图片URL，但是我发现rn最大只能为60，并且不同的pn可能会有相同的图片url（比如pn=0和pn=1都有ippr_z2C$qAzdH3FAzdH3Fooo_z\u0026amp;e3Bd8vs7k_z\u0026amp;e3Bv54_z\u0026amp;e3BvgAzdH3F7rs5w1utsjAzdH3Fda8nAzdH3Fa080AzdH3Fda8na080aldm9bb8m_z\u0026amp;e3B3r2这个objURL），所以使用python的集合（set）去重。\n更新：pn实际上指图片的id，pn=0、rn=60能获取到从0~59这60个URL列表，pn=1、rn=60能获取到从1~60这60个URL列表，所以pn=0和pn=1的列表中当然有59个是重复的。正确的做法是pn=0、rn=60获取0~59这60个URL列表，然后pn=60、rn=60获取60~119这60个列表，以此类推，这样获取到的URL就不会有重复的了。\n获取图片URL列表的简要代码如下：\n1 2 3 4 5 6 7 8 9 10 11 def ParseJSON(self, pn, rn, st): url = \u0026#39;http://image.baidu.com/i?tn=resultjson_com\u0026amp;amp;amp;ie=utf-8\u0026amp;amp;amp;word=%s\u0026amp;amp;amp;pn=%d\u0026amp;amp;amp;rn=%d\u0026amp;amp;amp;z=%d\u0026#39;%(self.word, pn, rn, self.size) #print(url) request = urllib.request.Request(url = url, headers = my_header) html = urllib.request.urlopen(request).read() hjson = json.loads(html.decode(\u0026#39;gbk\u0026#39;)) for i in range(0, len(hjson[\u0026#39;data\u0026#39;])-1):#最后一个数据为空 img_url = self.DecodeURL(hjson[\u0026#39;data\u0026#39;][i][\u0026#39;objURL\u0026#39;]) if img_url not in st: st.add(img_url)#去重 self.progressBar_updated_signal.emit()#更新进度条 DecodeURL是解密函数。很奇怪，json最后一个数据是空的。\n更新：文章末尾的最新代码已经不需要set去重和DecodeURL解密了。\npython3多线程下载 多线程下载图片主要参考了这个例子，只是将其转换为python3的形式，不得不感叹python的易用性，创建线程和下载图片一两行代码就可以完成，太方便了。这个例子有点像单生产者多消费者模型，创建了4个线程之后，并不需要告诉a线程下载哪几张图片，这4个线程会自定从队列里获取，互斥变量的访问也不会出错，减轻了程序员很多任务。\n关于python抓取网络资源的介绍，这篇博客介绍得很全面。有些URL并不是图片的真正地址，访问之后还会进行跳转，这种情况在使用urlretrieve下载图片时可能会抛出URLError异常。其他还可能遇到timeout、HTTPError、OSError等异常，可以使用下面的方式一次性捕获所有异常。\n1 2 3 4 try: urllib.request.urlretrieve(img_url, self.dir + \u0026#39;/\u0026#39; + img_url.split(\u0026#39;/\u0026#39;)[-1]) except Exception as e: print(\u0026#34;—–%s: %s—–n\u0026#34;%(type(e), img_url)) pyqt5实现界面 大二大三的时候接触过c++ qt4 gui编程，现在改python了，不过基本思想差不多，pyqt和c++qt的api基本相同，所以借助eric6实现了python和qt的联接。\n关于eric的使用教程，网上很多，这个讲解很详细，不过如果在windows上，安装没那么复杂，安装eric之前先安装好python3和pyqt5就行了，不用任何配置。eric熟练之后很方便了，直接拖拽控件画界面。\n在使用pyqt5的时候有一些坑需要注意，尤其是我使用的是最新版的python3和qt5，网上的资料不是很多。我遇到的两个主要坑是qt的信号和槽以及界面线程阻塞的问题。\n1 2 3 4 5 6 def on_download_pushButton_clicked(self): if self.check_option() == 1: de = DownloadEngine(self.word_lineEdit.text(), self.size_radio_group.checkedId(), self.num_spinBox.value(), self.dir_lineEdit.text(), self.thread_spinBox.value(),self.progressBar) de.Run() msg_box = QMessageBox(QMessageBox.Information, \u0026#34;提示\u0026#34;, \u0026#34;下载完毕\u0026#34;) msg_box.exec_() 上面这一段是我最开始的按钮槽函数，点击下载按钮之后，实例化一个DownloadEngine，然后de.Run()开始下载，要等到下载完毕de.Run()返回后，流程才会往下走，弹出提示框。但是下载过程耗时较长，这样整个界面线程阻塞，程序出现假死状态。\n后来参考A、B两个例子，令DownloadEngine继承QThread，用de.start()开启一个新的下载线程，界面线程继续往下走，当de下载完毕后，发送download_done_signal信号，界面接收到该信号后弹出提示框。第二版程序大概如下：\n1 2 3 4 5 def on_download_pushButton_clicked(self): if self.check_option() == 1: de = DownloadEngine(self.word_lineEdit.text(), self.size_radio_group.checkedId(), self.num_spinBox.value(), self.dir_lineEdit.text(), self.thread_spinBox.value(), self.progressBar) de.start() de.download_done_signal.connect(self.download_done_slot) 一切似乎很正确，但是点击下载按钮后程序崩溃，提示pythonw.exe已停止工作，我开始以为是python或者pyqt5的问题，重装一遍问题依旧。百思不得其解之后，用命令行运行py程序，cmd提示QThread: Destroyed while thread is still running，原来de是该函数的局部变量，当de.start()后，函数继续往下走直到结束，但是de的下载任务可能还没完成，所以导致上面的错误。解决办法就是将de变为该类的成员变量，将de改为self.de。\n有了这一个编写信号和槽函数的经验之后，准备实现进度条的功能，同样要保证界面线程不阻塞，所以创建了progressBar_updated_signal信号和相应槽函数。\n但是因为界面是直接和DownloadEngine联系的，但真正的下载工作是在ImageDownloadThread完成，所以progressBar_updated_signal信号最原始的发出地是ImageDownloadThread。这里面的信号传递关系如下图所示：\n“进度条更新”信号传递关系\npython3多线程下载代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 # -*- coding: utf-8 -*- import urllib.request import json import socket import queue from PyQt5.QtCore import * global my_header my_header = {\u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0\u0026#39;} global bad bad = 0 class ImageDownloadThread(QThread): sub_progressBar_updated_signal = pyqtSignal() def __init__(self,queue_in, dir_in): #进程间通过队列通信，所以每个进程需要用到同一个队列初始化 super(ImageDownloadThread,self).__init__() self.my_queue=queue_in self.dir = dir_in #self.setDaemon(True) #守护线程 self.start() #启动线程 #使用队列实现进程间通信 def run(self): while (True): global bad img_url = self.my_queue.get() socket.setdefaulttimeout(5)#这里对整个socket层设置超时时间。后续连接中如果再使用到socket，不必再设置 try: urllib.request.urlretrieve(img_url, self.dir + \u0026#39;/\u0026#39; + img_url.split(\u0026#39;/\u0026#39;)[-1]) except Exception as e: print(\u0026#34;—–%s: %s—–n\u0026#34;%(type(e), img_url)) bad += 1 self.sub_progressBar_updated_signal.emit() if self.my_queue.empty(): break self.my_queue.task_done() #当使用者线程调用 task_done() 以表示检索了该项目、并完成了所有的工作时，那么未完成的任务的总数就会减少。 class DownloadEngine(QThread): download_done_signal = pyqtSignal(int) status_changed_signal = pyqtSignal(str) progressBar_updated_signal = pyqtSignal() def __init__(self, word_in, size_in, num_in, dir_in, thread_num_in): super(DownloadEngine,self).__init__() self.word = urllib.parse.quote(word_in) self.size = size_in self.num = num_in self.dir = dir_in self.thread_num = thread_num_in def ParseJSON(self, pn, rn, qe): url = \u0026#39;http://image.baidu.com/i?tn=resultjson\u0026amp;ie=utf-8\u0026amp;word=%s\u0026amp;pn=%d\u0026amp;rn=%d\u0026amp;z=%d\u0026#39;%(self.word, pn, rn, self.size) #print(url) request = urllib.request.Request(url = url, headers = my_header) html = urllib.request.urlopen(request).read() hjson = json.loads(html.decode(\u0026#39;gbk\u0026#39;)) for i in range(0, len(hjson[‘data’])-1):#最后一个数据为空 qe.put(hjson[\u0026#39;data\u0026#39;][i][\u0026#39;objURL\u0026#39;]) self.progressBar_updated_signal.emit()#更新进度条 def GetImgUrlQueue(self): img_url_queue = queue.Queue(0) if self.num \u0026lt;= 60: self.ParseJSON(0, self.num, img_url_queue) else: n = self.num / 60 n = int(n) for i in range(n): self.ParseJSON(i * 60, 60, img_url_queue) self.ParseJSON(n * 60, self.num – n * 60, img_url_queue) return img_url_queue def sub_update_progressBar(self): self.progressBar_updated_signal.emit() def run(self): global bad bad = 0 self.status_changed_signal.emit(\u0026#39;获取URL\u0026#39;) img_url_queue = self.GetImgUrlQueue() threads = [] self.status_changed_signal.emit(‘下载图片’) #多线程爬去图片 for i in range(self.thread_num): thread=ImageDownloadThread(img_url_queue, self.dir) thread.sub_progressBar_updated_signal.connect(self.sub_update_progressBar) threads.append(thread) #合并进程，当子进程结束时，主进程才可以执行 for thread in threads: thread.wait() self.status_changed_signal.emit(\u0026#39;下载完成\u0026#39;) self.download_done_signal.emit(bad) pyqt5界面代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Module implementing MainDialog. \u0026#34;\u0026#34;\u0026#34; from PyQt5.QtCore import * from PyQt5.QtWidgets import * from Ui_main import Ui_Dialog from DownloadEngine import DownloadEngine import webbrowser class MainDialog(QDialog, Ui_Dialog): \u0026#34;\u0026#34;\u0026#34; Class documentation goes here. \u0026#34;\u0026#34;\u0026#34; def __init__(self, parent=None): \u0026#34;\u0026#34;\u0026#34; Constructor @param parent reference to the parent widget (QWidget) \u0026#34;\u0026#34;\u0026#34; super(MainDialog, self).__init__(parent) self.setupUi(self) self.size_radio_group = QButtonGroup() self.size_radio_group.addButton(self.total_radioButton, 0) self.size_radio_group.addButton(self.XL_radioButton, 9) self.size_radio_group.addButton(self.L_radioButton, 3) self.size_radio_group.addButton(self.M_radioButton, 2) self.size_radio_group.addButton(self.S_radioButton, 1) self.count = 0 def check_option(self): if self.word_lineEdit.text() == \u0026#34;\u0026#34;: msg_box = QMessageBox(QMessageBox.Warning, \u0026#34;警告\u0026#34;, \u0026#34;请输入搜索关键词！\u0026#34;) msg_box.exec_() return 0 if self.dir_lineEdit.text() == \u0026#34;\u0026#34;: msg_box = QMessageBox(QMessageBox.Warning, \u0026#34;警告\u0026#34;, \u0026#34;请选择图片存储目录！\u0026#34;) msg_box.exec_() return 0 return 1 @pyqtSlot() def on_dir_pushButton_clicked(self): \u0026#34;\u0026#34;\u0026#34; Slot documentation goes here. \u0026#34;\u0026#34;\u0026#34; # TODO: not implemented yet dir = QFileDialog.getExistingDirectory(self, \u0026#34;选择图片存储目录\u0026#34;,\u0026#34;.\u0026#34;) self.dir_lineEdit.setText(dir) @pyqtSlot() def on_download_pushButton_clicked(self): \u0026#34;\u0026#34;\u0026#34; Slot documentation goes here. \u0026#34;\u0026#34;\u0026#34; # TODO: not implemented yet self.progressBar.setValue(0) if self.check_option() == 1: self.progressBar.setMaximum(self.num_spinBox.value()) self.download_pushButton.setEnabled(False) self.de = DownloadEngine(self.word_lineEdit.text(), self.size_radio_group.checkedId(), self.num_spinBox.value(), self.dir_lineEdit.text(), self.thread_spinBox.value()) self.de.start() self.de.status_changed_signal.connect(self.status_changed_slot) self.de.download_done_signal.connect(self.download_done_slot) self.de.progressBar_updated_signal.connect(self.progressBar_updated_slot) @pyqtSlot() def on_src_pushButton_clicked(self): \u0026#34;\u0026#34;\u0026#34; Slot documentation goes here. \u0026#34;\u0026#34;\u0026#34; # TODO: not implemented yet webbrowser.open(\u0026#34;https://github.com/Beeder/BaiduImageDownloader\u0026#34;) def progressBar_updated_slot(self): self.count += 1 self.progressBar.setValue(self.count) def status_changed_slot(self, tip): self.status_label.setText(tip) self.count = 0 if tip != \u0026#39;下载完成\u0026#39;: self.progressBar.setValue(0) def download_done_slot(self, bad): msg_box = QMessageBox(QMessageBox.Information, \u0026#34;提示\u0026#34;, \u0026#34;下载完毕n成功%d,失败%d\u0026#34;%(self.num_spinBox.value() – bad, bad)) msg_box.exec_() self.download_pushButton.setEnabled(True) if __name__ == \u0026#34;__main__\u0026#34;: import sys app = QApplication(sys.argv) Dialog = MainDialog() Dialog.show() sys.exit(app.exec_()) cx_Freeze4打包 目前有好几个python打包程序，但是只有cx_Freeze明确表示支持python3，所以非他莫属了。\n在sourceforge下载cx_Freeze-4.3.3.win-amd64-py3.4.msi，安装；根据官方指南编写setup.py代码，将setup.py放到工程根目录下，执行python setup.py bdist_msi；报错`AttributeError:’module’object has no attribute ‘fix_up_module’。原来这是cx_Freeze-4.3.3版本的一个bug，利用替换的方法，安装4.3.4版本，问题解决。\n打包完成之后生成一个BaiduImageDownloader-0.1-amd64.msi文件，拷贝到其他电脑上也可正常运行。Winows7 64位用户可以点击下载BaiduImageDownloader-0.1-amd64.msi。\n（完）\n","permalink":"http://localhost:1313/posts/2015-08-13-baidu-image-downloader-python3-pyqt5-eric6-cx_freeze4/","summary":"\u003cp\u003e去年暑假在北大计算所实习的时候，任务之一就是批量下载百度图片。当时没学python，用c#实现了一个简易版本的批量下载器，如下图。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"C#版本百度图片批量下载器（抓的是百度的wap站点，现在好像不能用了）\" loading=\"lazy\" src=\"/posts/2015-08-13-baidu-image-downloader-python3-pyqt5-eric6-cx_freeze4/BaiduImageDownloader1.png\"\u003e\nC#版本百度图片批量下载器（抓的是百度的wap站点，现在好像不能用了）\u003c/p\u003e\n\u003cp\u003e当时“时间紧，任务重“，既没仔细研究百度图片API，也没处理好界面线程阻塞的问题。这个问题其实很有意思，趁着暑假在家，实现了一个比较完美的python版本，先上效果图。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"python3版本百度图片批量下载器\" loading=\"lazy\" src=\"/posts/2015-08-13-baidu-image-downloader-python3-pyqt5-eric6-cx_freeze4/BaiduImageDownloader2.png\"\u003e\npython3版本百度图片批量下载器\u003c/p\u003e\n\u003cp\u003e新版使用了\u003ca href=\"https://www.python.org/ftp/python/3.4.3/python-3.4.3.amd64.msi\"\u003epython-3.4.3.amd64.msi\u003c/a\u003e + \u003ca href=\"http://sourceforge.net/projects/pyqt/files/PyQt5/PyQt-5.5/PyQt5-5.5-gpl-Py3.4-Qt5.5.0-x64.exe\"\u003ePyQt5-5.5-gpl-Py3.4-Qt5.5.0-x64.exe\u003c/a\u003e + \u003ca href=\"http://downloads.sourceforge.net/project/eric-ide/eric6/stable/6.0.8/eric6-6.0.8.zip?r=http%3A%2F%2Fsourceforge.net%2Fprojects%2Feric-ide%2Ffiles%2Feric6%2Fstable%2F\u0026amp;ts=1439435222\u0026amp;use_mirror=nchc\"\u003eeric6-6.0.8.zip\u003c/a\u003e + \u003ca href=\"http://www.lfd.uci.edu/~gohlke/pythonlibs/3i673h27/cx_Freeze-4.3.4-cp34-none-win_amd64.whl\"\u003ecx_Freeze-4.3.4-cp34-none-win_amd64.whl\u003c/a\u003e，完整项目在\u003ca href=\"https://github.com/01joy/BaiduImageDownloader\"\u003e我的GitHub上\u003c/a\u003e。大致有如下几点工作：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e研究百度图片API，获取原始图片URL列表\u003c/li\u003e\n\u003cli\u003e使用python3进行多线程下载\u003c/li\u003e\n\u003cli\u003e利用pyqt5实现界面\u003c/li\u003e\n\u003cli\u003e利用cx_Freeze4打包整个程序\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e下面记录每个步骤的要点，供后人参考。\u003c/p\u003e\n\u003ch1 id=\"百度图片api\"\u003e百度图片API\u003c/h1\u003e\n\u003cp\u003e正常使用百度图片搜索的时候，URL是这样的：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://image.baidu.com/search/index?ct=201326592\u0026amp;z=0\u0026amp;tn=baiduimage\u0026amp;ipn=r\u0026amp;word=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6\u0026amp;pn=0\u0026amp;istype=2\u0026amp;ie=utf-8\u0026amp;oe=utf-8\u0026amp;cl=2\u0026amp;lm=-1\u0026amp;st=-1\u0026amp;fr=\u0026amp;fmq=1439374041843_R\u0026amp;ic=0\u0026amp;se=\u0026amp;sme=\u0026amp;width=0\u0026amp;height=0\u0026amp;face=0\"\u003ehttp://image.baidu.com/search/index?ct=201326592\u0026z=0\u0026tn=baiduimage\u0026ipn=r\u0026word=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6\u0026pn=0\u0026istype=2\u0026ie=utf-8\u0026oe=utf-8\u0026cl=2\u0026lm=-1\u0026st=-1\u0026fr=\u0026fmq=1439374041843_R\u0026ic=0\u0026se=\u0026sme=\u0026width=0\u0026height=0\u0026face=0\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e里面有很多参数，有些我们并不需要，精简之后大概是这样的：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://image.baidu.com/i?tn=baiduimage\u0026amp;ie=utf-8\u0026amp;word=%E7%BE%8E%E5%A5%B3\u0026amp;pn=\u0026amp;rn=\u0026amp;z=\"\u003ehttp://image.baidu.com/i?tn=baiduimage\u0026ie=utf-8\u0026word=%E7%BE%8E%E5%A5%B3\u0026pn=\u0026rn=\u0026z=\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eword为搜索关键词；pn为page number\u003cdel\u003e当前是第几页，\u003c/del\u003e\u003cstrong\u003e实际含义是image id\u003c/strong\u003e，表示第几张图片，从0开始；rn为每一页的图片数量，最大为60；z表示图片尺寸，z=9特大尺寸，z=3大尺寸，z=2中等尺寸，z=1小尺寸，z=0所有尺寸。\u003c/p\u003e\n\u003cp\u003e但是这个URL是给”人“看的，下一页的图片是动态加载的，其html代码的图片URL数量固定。一番查询之后发现，将tn=baiduimage换成tn=resultjson_com能获取到所有图片URL的json，json当然是给”猴“看的，这样就能轻松获取到所有图片的URL。\u003c/p\u003e\n\u003cp\u003e慢着，仔细看看json中的objURL，是一串连”猴“都看不懂的字符串，原来百度把图片真实URL加密了，好在加密方法是简单的字符映射，参考\u003ca href=\"http://blog.csdn.net/hbuxiaoshe/article/details/44780653\"\u003e这篇博客\u003c/a\u003e成功解密。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e更新：tn=resultjson_com的objURL是加密了，但是tn=resultjson的objURL并没有加密，所以采用tn=resultjson最佳。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e通过控制pn和rn就能获取指定数量的图片URL，但是我发现rn最大只能为60，并且不同的pn可能会有相同的图片url（比如\u003ca href=\"http://image.baidu.com/i?tn=resultjson_com\u0026amp;ie=utf-8\u0026amp;word=mit\u0026amp;pn=0\u0026amp;rn=60\u0026amp;z=9\"\u003epn=0\u003c/a\u003e和\u003ca href=\"http://image.baidu.com/i?tn=resultjson_com\u0026amp;ie=utf-8\u0026amp;word=mit\u0026amp;pn=1\u0026amp;rn=60\u0026amp;z=9\"\u003epn=1\u003c/a\u003e都有ippr_z2C$qAzdH3FAzdH3Fooo_z\u0026amp;e3Bd8vs7k_z\u0026amp;e3Bv54_z\u0026amp;e3BvgAzdH3F7rs5w1utsjAzdH3Fda8nAzdH3Fa080AzdH3Fda8na080aldm9bb8m_z\u0026amp;e3B3r2这个objURL），所以使用python的集合（set）去重。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e更新：pn实际上指图片的id，pn=0、rn=60能获取到从0~59这60个URL列表，pn=1、rn=60能获取到从1~60这60个URL列表，所以pn=0和pn=1的列表中当然有59个是重复的。正确的做法是pn=0、rn=60获取0~59这60个URL列表，然后pn=60、rn=60获取60~119这60个列表，以此类推，这样获取到的URL就不会有重复的了。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e获取图片URL列表的简要代码如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eParseJSON\u003c/span\u003e(self, pn, rn, st):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;http://image.baidu.com/i?tn=resultjson_com\u0026amp;amp;amp;ie=utf-8\u0026amp;amp;amp;word=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026amp;amp;amp;pn=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026amp;amp;amp;rn=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026amp;amp;amp;z=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eword, pn, rn, self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esize)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e#print(url)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    request \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e urllib\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erequest\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eRequest(url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e url, headers \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e my_header)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    html \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e urllib\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erequest\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eurlopen(request)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    hjson \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e json\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eloads(html\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edecode(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;gbk\u0026#39;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, len(hjson[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;data\u0026#39;\u003c/span\u003e])\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e):\u003cspan style=\"color:#75715e\"\u003e#最后一个数据为空\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        img_url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDecodeURL(hjson[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;data\u0026#39;\u003c/span\u003e][i][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;objURL\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e img_url \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e st:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            st\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd(img_url)\u003cspan style=\"color:#75715e\"\u003e#去重\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eprogressBar_updated_signal\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eemit()\u003cspan style=\"color:#75715e\"\u003e#更新进度条\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eDecodeURL是解密函数。很奇怪，json最后一个数据是空的。\u003c/p\u003e","title":"百度图片批量下载器（python3 + pyqt5 + eric6 + cx_Freeze4）"},{"content":"2015年6月27日，武汉大学在梅园操场举办了2015年毕业典礼。\n武汉大学2015年毕业典礼，正在发言的是雷军学长[1]\n武汉大学2015年毕业典礼[2]\n坐在台下，顿感恍惚，四年前同样在梅操，同样是这些人，我们举办开学典礼。\n武汉大学2011年开学典礼[3]\n原来梅操到梅操的距离只有四年。\n毕业典礼结束，下午集体收拾行李，大家都默默无语。晚上去红牛—-大一第一次聚餐的地方—-吃最后的晚餐，这次聚餐喝了两箱啤酒，6瓶白酒！白酒下肚，前一口酒落地又向上翻滚，和后一口酒相互撞击，四年的往事喷涌而出。离别之际，每个人都把自己的心声说出来了，说出了自己的家境、对某某的感情、一个宿舍的兄弟情，说出了自己的抱负、未来的理想，再互相拥抱、道一声珍重。\n山水一程，三生有幸[4]\n回到宿舍，所有人吐得一塌糊涂，昏睡过去。也许这就是离别的滋味，折磨着你，让你难受，只有把它吐出来，离开了，平静了，一切就好了。\n第二天醒来，发现隔壁宿舍的几个哥们已经走了，宿舍冷清了许多。去小卖部买了一些非必需品，只为把卡里的几十块钱用掉。打包行李，准备出发。\n毕业了，离开了，那些大一的迷茫、兼职，大二的信息安全竞赛，大三繁重的课业、为保研奋斗的数学建模竞赛，大四悠闲的生活也将躲藏在记忆的某个角落，不再被轻易的发现。武大的樱花、牌坊、樱顶、新图、青楼、梅操电影、珞珈之声、每天晚上在奥场穿着17号球衣跑步的女生、一起练笛的同学、在梅园食堂吃饭的一对情侣、幽默装逼的室友，这一幅幅画面，也将随着时间的车轮，慢慢消散。\n天下没有不散的筵席，我们来到这个世上，就注定要历经悲欢离合。在中国最美丽的大学，度过了我人生中最美好的年华，山水一程，三生有幸，感谢有你。\n别怕，梦的方向叫做闯，青春还没散场！\n参考：\n[1]. 武汉大学官方微博\n[2]. 武大新闻网：http://news.whu.edu.cn/info/1002/43788.htm\n[3]. 守望珞珈的新浪博客：http://blog.sina.com.cn/s/blog_4da93d1f0100t6v9.html\n[4]. 珞珈山水bbs毕业封面：http://bbs.whu.edu.cn/\n","permalink":"http://localhost:1313/posts/2015-06-28-farewell-to-whu/","summary":"\u003cp\u003e2015年6月27日，武汉大学在梅园操场举办了2015年毕业典礼。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"武汉大学2015年毕业典礼，正在发言的是雷军学长[1]\" loading=\"lazy\" src=\"https://i0.wp.com/ww1.sinaimg.cn/large/634fd979jw1etiecww24cj20hs0dctbj.jpg\"\u003e\n武汉大学2015年毕业典礼，正在发言的是雷军学长[1]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"武汉大学2015年毕业典礼[2]\" loading=\"lazy\" src=\"https://i0.wp.com/news.whu.edu.cn/_mediafile/whu_news/2015/06/27/29kc177qza.jpg\"\u003e\n武汉大学2015年毕业典礼[2]\u003c/p\u003e\n\u003cp\u003e坐在台下，顿感恍惚，四年前同样在梅操，同样是这些人，我们举办开学典礼。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"武汉大学2011年开学典礼[3]\" loading=\"lazy\" src=\"https://i0.wp.com/ww3.sinaimg.cn/large/005Muw7zjw1etjl1cs943j30j60bpgpm.jpg\"\u003e\n武汉大学2011年开学典礼[3]\u003c/p\u003e\n\u003cp\u003e原来梅操到梅操的距离只有四年。\u003c/p\u003e\n\u003cp\u003e毕业典礼结束，下午集体收拾行李，大家都默默无语。晚上去红牛—-大一第一次聚餐的地方—-吃最后的晚餐，这次聚餐喝了两箱啤酒，6瓶白酒！白酒下肚，前一口酒落地又向上翻滚，和后一口酒相互撞击，四年的往事喷涌而出。离别之际，每个人都把自己的心声说出来了，说出了自己的家境、对某某的感情、一个宿舍的兄弟情，说出了自己的抱负、未来的理想，再互相拥抱、道一声珍重。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"whu_bbs_graduation_2015\" loading=\"lazy\" src=\"/posts/2015-06-28-farewell-to-whu/whu_bbs_graduation_2015.jpg\"\u003e\n山水一程，三生有幸[4]\u003c/p\u003e\n\u003cp\u003e回到宿舍，所有人吐得一塌糊涂，昏睡过去。也许这就是离别的滋味，折磨着你，让你难受，只有把它吐出来，离开了，平静了，一切就好了。\u003c/p\u003e\n\u003cp\u003e第二天醒来，发现隔壁宿舍的几个哥们已经走了，宿舍冷清了许多。去小卖部买了一些非必需品，只为把卡里的几十块钱用掉。打包行李，准备出发。\u003c/p\u003e\n\u003cp\u003e毕业了，离开了，那些大一的迷茫、兼职，大二的信息安全竞赛，大三繁重的课业、为保研奋斗的数学建模竞赛，大四悠闲的生活也将躲藏在记忆的某个角落，不再被轻易的发现。武大的樱花、牌坊、樱顶、新图、青楼、梅操电影、珞珈之声、每天晚上在奥场穿着17号球衣跑步的女生、一起练笛的同学、在梅园食堂吃饭的一对情侣、幽默装逼的室友，这一幅幅画面，也将随着时间的车轮，慢慢消散。\u003c/p\u003e\n\u003cp\u003e天下没有不散的筵席，我们来到这个世上，就注定要历经悲欢离合。在中国最美丽的大学，度过了我人生中最美好的年华，山水一程，三生有幸，感谢有你。\u003c/p\u003e\n\u003cp\u003e别怕，梦的方向叫做闯，青春还没散场！\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. 武汉大学官方微博\u003c/p\u003e\n\u003cp\u003e[2]. 武大新闻网：\u003ca href=\"http://news.whu.edu.cn/info/1002/43788.htm\"\u003ehttp://news.whu.edu.cn/info/1002/43788.htm\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[3]. 守望珞珈的新浪博客：\u003ca href=\"http://blog.sina.com.cn/s/blog_4da93d1f0100t6v9.html\"\u003ehttp://blog.sina.com.cn/s/blog_4da93d1f0100t6v9.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[4]. 珞珈山水bbs毕业封面：\u003ca href=\"http://bbs.whu.edu.cn/\"\u003ehttp://bbs.whu.edu.cn/\u003c/a\u003e\u003c/p\u003e","title":"再见武大"},{"content":"今天参加了【珞珈阅读广场第89期】《礼物》（影像阅读），感触很多，收获也很多，其中最大的收获就是体会到了交流的乐趣。\n【珞珈阅读广场第89期】《礼物》（影像阅读）[1]\n影片《礼物》讲述了一位功成名就的大叔和身为小偷的女生相互救赎的故事。大叔白手起家，一路打拼，最后坐拥万贯家财，却抛弃了妻子和女儿，导致妻子自杀；女生的父亲早逝，母亲生活又不检点，女生曾差点一刀把母亲捅死，为了偿还男朋友的债务，女生甚至当起了小偷。在一次行窃过程中被大叔抓住，大叔要求女生当他的司机和搬运工，带他去东京，给女儿送一个礼物。在去东京的路上，大叔走访了自己曾经到过的很多地方，但大多数都是激情满怀而去，失望而归。到达东京，当女生知道大叔要送给女儿的礼物是自己的心脏，自己的生命的时候，女生陷入了两难的境地。但是女生最终答应的大叔的请求，帮其送出了礼物，大叔得到了救赎，女生也因为大叔的一句“要好好活着”而坚强乐观的面对生活。\n90分钟的观影结束之后，主持人抛出了如下几个问题。\n关于《礼物》这部电影，主持人提出的若干问题\n因为上学期我也参加过珞珈阅读广场的观影活动，当时也体会到了与他人交流的乐趣，现在马上要毕业了，所以跃跃欲试，想和同学们交流一下。正好第一个问题主持人点名叫我谈一谈。我当时谈了一下我对大叔这种献出自己生命拯救外孙女的行为表示了理解，并表示自己也会做出类似的事情。在场的另外一个老师就表达了他的观点，他对日本这种“野蛮粗暴”的拯救方式不太理解，也不太赞同，大叔最后的死亡过程类似于日本武士的剖腹自尽。\n关于第三个问题，大家也畅所欲言，从很多个方面谈了自己的想法，大部分还是认为大叔想要给外孙女抽一个好彩头的观点。其实这个观点要到最后大叔把这个“大吉”签绑到外孙女的病床上才能感觉得到，在对后面内容不知道的情况下，我认为最合理的解释应该是这一行为体现了大叔好强甚至“蛮横”的性格，因为他抽签的时候说自己从白手起家到现在亿万富翁，就像中了头彩一样，那么我现在抽签，也要像我经商一样，取得最好的结果。有的同学甚至解读出了大叔“执念”这一层含义。\n对于第四个问题，好几个同学分享了自己的经历或者想法。我当时表达了“虽然你现在面临不幸，请不要过多的抱怨，珍惜当下，因为你现在所遭遇的，正是你将来所怀念的；当你再故地重游的时候，也许像这位大叔一样，再也找不到当初那种美好的感觉了。“，并顺带告诉学弟学妹们，珍惜在武大的美好时光，自己马上要毕业了，对武大的一花一木都非常的不舍。\n最后一个问题，主持人给出了很好的解答，并且阐明了要拯救一个身处绝望的人的困难性，很精彩。\n交流过程中很有意思的一件事情是，主持人给出了这样一个观点”婚姻或家庭不幸的人，其子女的性格往往也会偏离常态，并且子女的婚姻或家庭也很可能会不幸。”，对于这个观点，大家的反应比较激烈，特别是在场的那位老师，表达了他的反对意见。我因为自己的家庭环境原因，反而表示了积极的一面，就是父母婚姻不幸的人，其孩子有可能反而更加珍惜婚姻，珍惜家庭，所以家庭有可能比一般家庭更加幸福。当然也有同学表示对爱的人抓得太紧，有可能适得其反，导致婚姻的破裂。主持人讲了这样一段话，很好的表达了这个观点：\n让爱恰到好处－不让疯长的孤独烧毁世界，也不让泛滥的博爱窒息自由。\n电影从晚上7:00到8:30，讨论从8:30到10:00。讨论结束的时候，主持人把本期两本书分别赠送给了我和另一位硕士毕业生，我的赠书是《那一天》。\n赠书《那一天》[2]\n讨论结束，临走的时候大家还意犹未尽，主持人对大家的讨论表示感谢，此时有一位同学表示主持人的发言也很不错。确实，整个讨论环节，主持人很好的带动起了大家发言的兴趣和积极性，包括问题的设置，主持人的点名提问以及主持人自身精彩的解说，都非常好的带动了现场的气氛，打开了观众的话匣子。不久前我刚好参加过这位主持人主持的”周末艺苑·外院专场“演出，当时主持人随机应变的能力和绝妙的口才给我留下了深刻的印象，学弟不错，加油！\n这次观影交流达到了真正交流的目的，观众中有大一大二的新生，有即将毕业的本科生和研究生，也有已经成家的中年老师，大家基于各自的背景，表达自己的想法，聆听他人的观点，达到了很好的思维发散、观点碰撞的目的。此时我想到了高中背的萧伯纳讲过的一句话：\n两个人各有一个苹果，交换之后，每个人还是只有一个苹果；然而，当两个人各有一种思想，交换之后，每个人却拥有了两种思想。\n前几天观看了踪点剧社的两部毕业大戏《理想》和《禁闭》，两部很有深意的话剧，话剧结束的时候，也有一个短暂的交流会，也很精彩。\n人很多时候会沉浸在自我的世界中，产生很多偏见，此时不妨听一听他人的观点，也许会豁然开朗或眼前一亮，觉得世界真奇妙。最后用H老师的一句话结束：技术上要多钻研，技术外要多沟通，生存两个法则。\n希望每个人都能发现并享受交流的乐趣。\n参考：\n[1]. 珞珈阅读广场第89期公告：http://www.lib.whu.edu.cn/news/view.asp?id=3354\n[2]. 豆瓣读书《那一天》：http://book.douban.com/subject/25904481/\n","permalink":"http://localhost:1313/posts/2015-06-12-the-joy-of-communication/","summary":"\u003cp\u003e今天参加了\u003ca href=\"http://www.lib.whu.edu.cn/news/view.asp?id=3354\"\u003e【珞珈阅读广场第89期】《礼物》（影像阅读）\u003c/a\u003e，感触很多，收获也很多，其中最大的收获就是体会到了交流的乐趣。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"【珞珈阅读广场第89期】《礼物》（影像阅读）[1]\" loading=\"lazy\" src=\"https://i0.wp.com/www.lib.whu.edu.cn/news/tc/readSqua89.jpg\"\u003e\n【珞珈阅读广场第89期】《礼物》（影像阅读）[1]\u003c/p\u003e\n\u003cp\u003e影片\u003ca href=\"http://movie.douban.com/subject/25878911/\"\u003e《礼物》\u003c/a\u003e讲述了一位功成名就的大叔和身为小偷的女生相互救赎的故事。大叔白手起家，一路打拼，最后坐拥万贯家财，却抛弃了妻子和女儿，导致妻子自杀；女生的父亲早逝，母亲生活又不检点，女生曾差点一刀把母亲捅死，为了偿还男朋友的债务，女生甚至当起了小偷。在一次行窃过程中被大叔抓住，大叔要求女生当他的司机和搬运工，带他去东京，给女儿送一个礼物。在去东京的路上，大叔走访了自己曾经到过的很多地方，但大多数都是激情满怀而去，失望而归。到达东京，当女生知道大叔要送给女儿的礼物是自己的心脏，自己的生命的时候，女生陷入了两难的境地。但是女生最终答应的大叔的请求，帮其送出了礼物，大叔得到了救赎，女生也因为大叔的一句“要好好活着”而坚强乐观的面对生活。\u003c/p\u003e\n\u003cp\u003e90分钟的观影结束之后，主持人抛出了如下几个问题。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"关于《礼物》这部电影，主持人提出的若干问题\" loading=\"lazy\" src=\"/posts/2015-06-12-the-joy-of-communication/questions-about-gift.jpg\"\u003e\n关于《礼物》这部电影，主持人提出的若干问题\u003c/p\u003e\n\u003cp\u003e因为上学期我也参加过珞珈阅读广场的观影活动，当时也体会到了与他人交流的乐趣，现在马上要毕业了，所以跃跃欲试，想和同学们交流一下。正好第一个问题主持人点名叫我谈一谈。我当时谈了一下我对大叔这种献出自己生命拯救外孙女的行为表示了理解，并表示自己也会做出类似的事情。在场的另外一个老师就表达了他的观点，他对日本这种“野蛮粗暴”的拯救方式不太理解，也不太赞同，大叔最后的死亡过程类似于日本武士的剖腹自尽。\u003c/p\u003e\n\u003cp\u003e关于第三个问题，大家也畅所欲言，从很多个方面谈了自己的想法，大部分还是认为大叔想要给外孙女抽一个好彩头的观点。其实这个观点要到最后大叔把这个“大吉”签绑到外孙女的病床上才能感觉得到，在对后面内容不知道的情况下，我认为最合理的解释应该是这一行为体现了大叔好强甚至“蛮横”的性格，因为他抽签的时候说自己从白手起家到现在亿万富翁，就像中了头彩一样，那么我现在抽签，也要像我经商一样，取得最好的结果。有的同学甚至解读出了大叔“执念”这一层含义。\u003c/p\u003e\n\u003cp\u003e对于第四个问题，好几个同学分享了自己的经历或者想法。我当时表达了“虽然你现在面临不幸，请不要过多的抱怨，珍惜当下，因为你现在所遭遇的，正是你将来所怀念的；当你再故地重游的时候，也许像这位大叔一样，再也找不到当初那种美好的感觉了。“，并顺带告诉学弟学妹们，珍惜在武大的美好时光，自己马上要毕业了，对武大的一花一木都非常的不舍。\u003c/p\u003e\n\u003cp\u003e最后一个问题，主持人给出了很好的解答，并且阐明了要拯救一个身处绝望的人的困难性，很精彩。\u003c/p\u003e\n\u003cp\u003e交流过程中很有意思的一件事情是，主持人给出了这样一个观点”婚姻或家庭不幸的人，其子女的性格往往也会偏离常态，并且子女的婚姻或家庭也很可能会不幸。”，对于这个观点，大家的反应比较激烈，特别是在场的那位老师，表达了他的反对意见。我因为自己的家庭环境原因，反而表示了积极的一面，就是父母婚姻不幸的人，其孩子有可能反而更加珍惜婚姻，珍惜家庭，所以家庭有可能比一般家庭更加幸福。当然也有同学表示对爱的人抓得太紧，有可能适得其反，导致婚姻的破裂。主持人讲了这样一段话，很好的表达了这个观点：\u003c/p\u003e\n\u003cp\u003e让爱恰到好处－不让疯长的孤独烧毁世界，也不让泛滥的博爱窒息自由。\u003c/p\u003e\n\u003cp\u003e电影从晚上7:00到8:30，讨论从8:30到10:00。讨论结束的时候，主持人把本期两本书分别赠送给了我和另一位硕士毕业生，我的赠书是\u003ca href=\"http://book.douban.com/subject/25904481/\"\u003e《那一天》\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"赠书《那一天》[2]\" loading=\"lazy\" src=\"https://img1.doubanio.com/lpic/s27950209.jpg\"\u003e\n赠书《那一天》[2]\u003c/p\u003e\n\u003cp\u003e讨论结束，临走的时候大家还意犹未尽，主持人对大家的讨论表示感谢，此时有一位同学表示主持人的发言也很不错。确实，整个讨论环节，主持人很好的带动起了大家发言的兴趣和积极性，包括问题的设置，主持人的点名提问以及主持人自身精彩的解说，都非常好的带动了现场的气氛，打开了观众的话匣子。不久前我刚好参加过这位主持人主持的”周末艺苑·外院专场“演出，当时主持人随机应变的能力和绝妙的口才给我留下了深刻的印象，学弟不错，加油！\u003c/p\u003e\n\u003cp\u003e这次观影交流达到了真正交流的目的，观众中有大一大二的新生，有即将毕业的本科生和研究生，也有已经成家的中年老师，大家基于各自的背景，表达自己的想法，聆听他人的观点，达到了很好的思维发散、观点碰撞的目的。此时我想到了高中背的萧伯纳讲过的一句话：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e两个人各有一个苹果，交换之后，每个人还是只有一个苹果；然而，当两个人各有一种思想，交换之后，每个人却拥有了两种思想。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e前几天观看了踪点剧社的两部毕业大戏《理想》和《禁闭》，两部很有深意的话剧，话剧结束的时候，也有一个短暂的交流会，也很精彩。\u003c/p\u003e\n\u003cp\u003e人很多时候会沉浸在自我的世界中，产生很多偏见，此时不妨听一听他人的观点，也许会豁然开朗或眼前一亮，觉得世界真奇妙。最后用H老师的一句话结束：\u003cstrong\u003e技术上要多钻研，技术外要多沟通，生存两个法则。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e希望每个人都能发现并享受交流的乐趣。\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. 珞珈阅读广场第89期公告：\u003ca href=\"http://www.lib.whu.edu.cn/news/view.asp?id=3354\"\u003ehttp://www.lib.whu.edu.cn/news/view.asp?id=3354\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[2]. 豆瓣读书《那一天》：\u003ca href=\"http://book.douban.com/subject/25904481/\"\u003ehttp://book.douban.com/subject/25904481/\u003c/a\u003e\u003c/p\u003e","title":"交流的乐趣"},{"content":"前几天给毕设指导老师发邮件，麻烦老师写申优推荐理由，老师回复我说她的儿子这几天高考，她不在学院，最晚要9号才能帮我写。我才意识到又是一年高考时，距离自己参加高考已经过去了四个春秋，但高中生活的场景在脑海中却依稀可见，想来是要写一篇文章追忆那平凡或不平凡的高中生活。\n在我读初中的时候，县里只有一所高中－县一中，每年中考之前，县一中都会组织一次提前批考试，如果提前批考试被录取了，正式中考的时候只要过线就能上，和现在的大学自主招生有点类似。我当时考试成绩不错，大概是全县十名左右，被分到所谓的奥赛班了。正式中考完之后，县五中开始正式招生，五中是抚州市一个老板办的私立学校，据说办学模式借鉴临川一中。当时县五中为了抢占优质生源，承诺只要去五中读书，除了3年学费全免之外，还额外奖励3000块钱，而且如果高考考上清华北大，奖励10万，考上其他十大名牌大学，奖励3万。当时考虑到五中刚开始办学，又借鉴临川一中的办学模式，老师大多是抚州的“名师”，教学质量应该不错，而且去五中读书能省不少钱，爸爸建议我去五中。我当时其实是不太情愿的，毕竟一中奥赛班聚集了全县最优秀的老师和同学，不论是环境还是各项措施，都比刚办学的五中要好。不过我还是以“只要自己认真读，在哪个学校都能取得好成绩”的理由安慰自己，去了五中。\n说实话，五中学生的层次确实要比一中学生差。不过好在学校把几十个成绩比较好的安排在了一个班里，配备了更好的老师，实行特殊管理。\n在我的印象中，高中的三年过得都一样，高三并没有比高一高二累多少，或者说高一高二并没有比高三轻松多少。每天早上6点准时起床，洗漱完之后6:20做早操，高三的时候不用做早操，改早自习了。大概6:50吃早餐，7:10开始早读。因为是理科，早读的内容不外乎英语单词、英语作文、语文背诵诗词、语文作文。8:05开始正式上课，上午四节课一直上到11:40。下课之后回家吃饭，因为妈妈在校内陪读，所以午餐能在15分钟之内解决，然后马上回到教室做几道题或者看一两个作文素材。下午1点准时午休半个小时，2:05正式上课，下午三节课一直上到4:40，好像高三的时候改成了四节课，记不清了。和中午一样，快速解决晚餐，马上回到教室，首先复习或学习语文字词，包括拼音和常考成语，当时基本把《现代汉语词典》翻熟了；然后正式晚自习，一直到晚上11点才回家洗漱睡觉。当时学校规定其他班学生10点之后就必须离开教室，但是我们班特殊规定可以自习到11点。回家洗漱完之后大概11:30了，高三的时候，我还经常在睡之前打着小电筒复习一下白天学习的内容。\n学校每两个星期放1天假，再过两个星期放2天半假，大多数同学只有在2天半假的时候才回家一趟。放假期间，除了做一两套卷子，大多数时间是在看电视，另外会去书店逛一次，不过买的大多数是高考复习资料，仅有的算得上是课外读物的就是《疯狂阅读》或者《读者》之类的了，小县城的书店也没有其他的“闲书”。高考要求阅读的几篇经典名著，几乎没有完整阅读过纸质版，高三的时候为了应付高考，时间紧，任务重，直接从机房下载了《巴黎圣母院》、《堂吉诃德》等改编电影，这才稍微了解了一下主要内容。学校也没有像样的图书馆，在石城那个小县城，不可能买到这些“高大上”的书。我相信大城市的很多高中生肯定看过很多这类世界名著，周末或者放假的时候也是在忙着学琴棋书画。这可能就是所谓的城乡教育差别吧，虽然这种差别在高考的时候体现得并不明显，农村的孩子在高中稍微刻苦一下，也能上不错的大学；但是一旦到了大学，大城市的孩子和我这种从农村走出来的孩子的差别一下就能看出来。大城市的孩子不论是在交际、口才、学识、才艺等方面都能轻松碾压农村的孩子，农村孩子虽然你很刻苦，卷面成绩不错，但是知识面不够宽广，格局比较小，几乎没有才艺；并不是城里人歧视你，不和你玩，但是和你聊美术，你懂吗，和你聊莎士比亚剧作，你看过吗，和你排练音乐舞蹈，你会吗。来到武大之后，我对这种城乡读书孩子之间的差别真的深有体会，无论我多么努力，好像总达不到他们的高度，总是无法融入他们的生活。\n高中不像大学，每个班有固定的教师，每个人有固定的座位，有自己的“左邻右舍”，坐在座位上，真的感觉很温暖。每到下课的时候，班上都闹哄哄的，同班同学之间的交流也很多，班集体的荣誉感以及个人的归属感也很强。每次打扫卫生的时候，几乎要经过每个同学的座位，问一问有没有垃圾要处理的；每次发考卷的时候，也会左顾右盼，相互逗个乐。一年中要数元旦晚会最为热闹，犹记得高三那年元旦，我为了演唱“海阔天空”，每天回家吃饭的时候就听mp3，走在路上也会小声哼唱，当然对于五音不全的我，演唱效果并不是很好:-) 晚会当天下午开始布置场地，所有人把书搬回宿舍，清空教室，在玻璃窗上贴上气球，圣诞树贴纸，或者某个小画家直接在上面画一幅画，电风扇和墙上都会挂上彩带；在教室四周摆上课桌，课桌上摆上事先买好的瓜子、花生、糖果、饮料等；同学们借来音响话筒，老师也把自己的笔记本搬到教室，一场简朴晚会现场就布置好了。晚会的所有工作人员、演员、主持人都是自己班上的同学，大家欢聚一堂，过着小集体的节日，有时候在同学和主持人的怂恿下，老师们也会激情献唱一首。现在回想起来，这种小集体归属感真的很美好，高中毕业之后，我大概再也没有过这种感觉了。\n春节过后，就是高考的紧要关头了，每周一的班会课上，老师都会给我们加油鼓劲，告诉我们大学有多轻松美好。百日会战那天，班主任甚至亲自泼墨，写下“辛苦数日，幸福一生”的对联，贴在教室的后墙上。高三，每天就是不停的做卷子、刷题，日考、周考、月考，不断的考试，往往上一张考卷还没有讲评，下一次考试又到了。考得多了，对成绩也不那么看重了，不过也基本稳定在前三。\n图片来自[1]\n距离高考只剩一个星期的时候，题量开始下降，老师也变得温柔起来，开始提醒我们注意饮食，调整生物钟，保持充足睡眠等。考前3天，学校放假，让我们回家吃好喝好，放松心情。考前1天，看考场，当时坐我前面的一个同学找到我，叫我给他抄，并威胁我如果不给他抄，则影响我考试，碰到这样一个人渣，对我的心情还是有一定影响的，我也没敢把这件事告诉我妈。当天晚上英语老师找我谈话，宽慰我，跟我说考试的时候不要遮住试卷，他能不能抄到是他的事了，况且他最多只能抄到选择填空题，主观题还得靠真才实学。高考那两天，全校其他年级放假，为的是给所有考生创造一个安静舒适的校园环境，这一点给学校点赞。高考第一天和第二天上午还算顺利，正常发挥，第二天午休没有睡着，下午考英语的时候，听力几乎没有进入状态，哈欠连连，英语是我考得最差的一科了，当然英语本来就不是我的强项。\n考完之后，回到家中，妈妈给我和哥哥洗了两个甜瓜吃，寓意我们苦尽甘来:-) 高考完的那个暑假，妈妈说让我们好好在家待着休养生息，所以基本过着猪一样的生活。6月底高考成绩出来了，六百多吧，和估分差不多，纠结的是填志愿。当时也不像现在，互联网这么发达，基本上是通过《全国普通高等学校报考指南》了解每个学校，对提前批的情况也不甚了解。后来根据往年分数线以及自己的兴趣，报了武大、吉大、川大这几个学校，很幸运，录取了第一志愿第一专业WHUCS。说实话，在填志愿之前，我只知道清华北大这两个学校，对武大这所“全国最美丽的大学”一无所知，我不是狂妄自大，而是孤陋寡闻。\n高中真的很累、很辛苦，要想坚持下去，一定要找一个可靠的精神支柱，不论是做什么事都是这样，一直以来，支持我勇往直前的都是我的亲人和我想要改变命运的决心！要说高中3年的收获，那就是它磨砺了我的意志，增强了我忍受孤独的能力，当然高中并不是我最孤独的时候，至少有我前面提到的小集体归属感；高中三年，也认识了很要好的同学兼老乡WQ、WS和MZ。\n对了，高中那会每个人都会有一个座右铭，我也不例外，很大众化，汪国真的“既然选择了远方，便只顾风雨兼程”。惊闻汪老师于2015年4月26日逝世，令人嘘唏。\n参考：\n[1]. 永不过时的高考记忆\n","permalink":"http://localhost:1313/posts/2015-06-08-my-high-school-life/","summary":"\u003cp\u003e前几天给毕设指导老师发邮件，麻烦老师写申优推荐理由，老师回复我说她的儿子这几天高考，她不在学院，最晚要9号才能帮我写。我才意识到又是一年高考时，距离自己参加高考已经过去了四个春秋，但高中生活的场景在脑海中却依稀可见，想来是要写一篇文章追忆那平凡或不平凡的高中生活。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"classroom\" loading=\"lazy\" src=\"/posts/2015-06-08-my-high-school-life/classroom.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e在我读初中的时候，县里只有一所高中－县一中，每年中考之前，县一中都会组织一次提前批考试，如果提前批考试被录取了，正式中考的时候只要过线就能上，和现在的大学自主招生有点类似。我当时考试成绩不错，大概是全县十名左右，被分到所谓的奥赛班了。正式中考完之后，县五中开始正式招生，五中是抚州市一个老板办的私立学校，据说办学模式借鉴临川一中。当时县五中为了抢占优质生源，承诺只要去五中读书，除了3年学费全免之外，还额外奖励3000块钱，而且如果高考考上清华北大，奖励10万，考上其他十大名牌大学，奖励3万。当时考虑到五中刚开始办学，又借鉴临川一中的办学模式，老师大多是抚州的“名师”，教学质量应该不错，而且去五中读书能省不少钱，爸爸建议我去五中。我当时其实是不太情愿的，毕竟一中奥赛班聚集了全县最优秀的老师和同学，不论是环境还是各项措施，都比刚办学的五中要好。不过我还是以“只要自己认真读，在哪个学校都能取得好成绩”的理由安慰自己，去了五中。\u003c/p\u003e\n\u003cp\u003e说实话，五中学生的层次确实要比一中学生差。不过好在学校把几十个成绩比较好的安排在了一个班里，配备了更好的老师，实行特殊管理。\u003c/p\u003e\n\u003cp\u003e在我的印象中，高中的三年过得都一样，高三并没有比高一高二累多少，或者说高一高二并没有比高三轻松多少。每天早上6点准时起床，洗漱完之后6:20做早操，高三的时候不用做早操，改早自习了。大概6:50吃早餐，7:10开始早读。因为是理科，早读的内容不外乎英语单词、英语作文、语文背诵诗词、语文作文。8:05开始正式上课，上午四节课一直上到11:40。下课之后回家吃饭，因为妈妈在校内陪读，所以午餐能在15分钟之内解决，然后马上回到教室做几道题或者看一两个作文素材。下午1点准时午休半个小时，2:05正式上课，下午三节课一直上到4:40，好像高三的时候改成了四节课，记不清了。和中午一样，快速解决晚餐，马上回到教室，首先复习或学习语文字词，包括拼音和常考成语，当时基本把《现代汉语词典》翻熟了；然后正式晚自习，一直到晚上11点才回家洗漱睡觉。当时学校规定其他班学生10点之后就必须离开教室，但是我们班特殊规定可以自习到11点。回家洗漱完之后大概11:30了，高三的时候，我还经常在睡之前打着小电筒复习一下白天学习的内容。\u003c/p\u003e\n\u003cp\u003e学校每两个星期放1天假，再过两个星期放2天半假，大多数同学只有在2天半假的时候才回家一趟。放假期间，除了做一两套卷子，大多数时间是在看电视，另外会去书店逛一次，不过买的大多数是高考复习资料，仅有的算得上是课外读物的就是《疯狂阅读》或者《读者》之类的了，小县城的书店也没有其他的“闲书”。高考要求阅读的几篇经典名著，几乎没有完整阅读过纸质版，高三的时候为了应付高考，时间紧，任务重，直接从机房下载了《巴黎圣母院》、《堂吉诃德》等改编电影，这才稍微了解了一下主要内容。学校也没有像样的图书馆，在石城那个小县城，不可能买到这些“高大上”的书。我相信大城市的很多高中生肯定看过很多这类世界名著，周末或者放假的时候也是在忙着学琴棋书画。这可能就是所谓的城乡教育差别吧，虽然这种差别在高考的时候体现得并不明显，农村的孩子在高中稍微刻苦一下，也能上不错的大学；但是一旦到了大学，大城市的孩子和我这种从农村走出来的孩子的差别一下就能看出来。大城市的孩子不论是在交际、口才、学识、才艺等方面都能轻松碾压农村的孩子，农村孩子虽然你很刻苦，卷面成绩不错，但是知识面不够宽广，格局比较小，几乎没有才艺；并不是城里人歧视你，不和你玩，但是和你聊美术，你懂吗，和你聊莎士比亚剧作，你看过吗，和你排练音乐舞蹈，你会吗。来到武大之后，我对这种城乡读书孩子之间的差别真的深有体会，无论我多么努力，好像总达不到他们的高度，总是无法融入他们的生活。\u003c/p\u003e\n\u003cp\u003e高中不像大学，每个班有固定的教师，每个人有固定的座位，有自己的“左邻右舍”，坐在座位上，真的感觉很温暖。每到下课的时候，班上都闹哄哄的，同班同学之间的交流也很多，班集体的荣誉感以及个人的归属感也很强。每次打扫卫生的时候，几乎要经过每个同学的座位，问一问有没有垃圾要处理的；每次发考卷的时候，也会左顾右盼，相互逗个乐。一年中要数元旦晚会最为热闹，犹记得高三那年元旦，我为了演唱“海阔天空”，每天回家吃饭的时候就听mp3，走在路上也会小声哼唱，当然对于五音不全的我，演唱效果并不是很好:-) 晚会当天下午开始布置场地，所有人把书搬回宿舍，清空教室，在玻璃窗上贴上气球，圣诞树贴纸，或者某个小画家直接在上面画一幅画，电风扇和墙上都会挂上彩带；在教室四周摆上课桌，课桌上摆上事先买好的瓜子、花生、糖果、饮料等；同学们借来音响话筒，老师也把自己的笔记本搬到教室，一场简朴晚会现场就布置好了。晚会的所有工作人员、演员、主持人都是自己班上的同学，大家欢聚一堂，过着小集体的节日，有时候在同学和主持人的怂恿下，老师们也会激情献唱一首。现在回想起来，这种小集体归属感真的很美好，高中毕业之后，我大概再也没有过这种感觉了。\u003c/p\u003e\n\u003cp\u003e春节过后，就是高考的紧要关头了，每周一的班会课上，老师都会给我们加油鼓劲，告诉我们大学有多轻松美好。百日会战那天，班主任甚至亲自泼墨，写下“辛苦数日，幸福一生”的对联，贴在教室的后墙上。高三，每天就是不停的做卷子、刷题，日考、周考、月考，不断的考试，往往上一张考卷还没有讲评，下一次考试又到了。考得多了，对成绩也不那么看重了，不过也基本稳定在前三。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图片来自[1]\" loading=\"lazy\" src=\"https://i0.wp.com/a3.att.hudong.com/66/83/300000908235130708833431958_950.jpg\"\u003e\n图片来自[1]\u003c/p\u003e\n\u003cp\u003e距离高考只剩一个星期的时候，题量开始下降，老师也变得温柔起来，开始提醒我们注意饮食，调整生物钟，保持充足睡眠等。考前3天，学校放假，让我们回家吃好喝好，放松心情。考前1天，看考场，当时坐我前面的一个同学找到我，叫我给他抄，并威胁我如果不给他抄，则影响我考试，碰到这样一个人渣，对我的心情还是有一定影响的，我也没敢把这件事告诉我妈。当天晚上英语老师找我谈话，宽慰我，跟我说考试的时候不要遮住试卷，他能不能抄到是他的事了，况且他最多只能抄到选择填空题，主观题还得靠真才实学。高考那两天，全校其他年级放假，为的是给所有考生创造一个安静舒适的校园环境，这一点给学校点赞。高考第一天和第二天上午还算顺利，正常发挥，第二天午休没有睡着，下午考英语的时候，听力几乎没有进入状态，哈欠连连，英语是我考得最差的一科了，当然英语本来就不是我的强项。\u003c/p\u003e\n\u003cp\u003e考完之后，回到家中，妈妈给我和哥哥洗了两个甜瓜吃，寓意我们苦尽甘来:-) 高考完的那个暑假，妈妈说让我们好好在家待着休养生息，所以基本过着猪一样的生活。6月底高考成绩出来了，六百多吧，和估分差不多，纠结的是填志愿。当时也不像现在，互联网这么发达，基本上是通过《全国普通高等学校报考指南》了解每个学校，对提前批的情况也不甚了解。后来根据往年分数线以及自己的兴趣，报了武大、吉大、川大这几个学校，很幸运，录取了第一志愿第一专业WHUCS。说实话，在填志愿之前，我只知道清华北大这两个学校，对武大这所“全国最美丽的大学”一无所知，我不是狂妄自大，而是孤陋寡闻。\u003c/p\u003e\n\u003cp\u003e高中真的很累、很辛苦，要想坚持下去，一定要找一个可靠的精神支柱，不论是做什么事都是这样，一直以来，支持我勇往直前的都是我的亲人和我想要改变命运的决心！要说高中3年的收获，那就是它磨砺了我的意志，增强了我忍受孤独的能力，当然高中并不是我最孤独的时候，至少有我前面提到的小集体归属感；高中三年，也认识了很要好的同学兼老乡WQ、WS和MZ。\u003c/p\u003e\n\u003cp\u003e对了，高中那会每个人都会有一个座右铭，我也不例外，很大众化，汪国真的“既然选择了远方，便只顾风雨兼程”。惊闻汪老师于2015年4月26日逝世，令人嘘唏。\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. \u003ca href=\"http://tupian.baike.com/84178/2.html\"\u003e永不过时的高考记忆\u003c/a\u003e\u003c/p\u003e","title":"我的高中生活"},{"content":"5月19号的中午吃完饭后随手刷了一下朋友圈，发现MS表哥分享了一个链接，说家乡发生了十多年未遇洪灾。仔细看了一下，发现这次洪灾真的很严重，然后就给妈妈打了个电话，妈妈说从昨天下午开始下大雨，到晚上下暴雨，我家后院有一座小山，也出现了滑坡；附近的一座桥也淹了，家门口的一片农田（不是我家的）也全被淹了。妈妈说她也是头一回看到这么大而持久的暴雨。\n后来看QQ空间，全是关于家乡灾情的状态，很多新闻媒体也报道了。县的下游地区受灾比较严重，隔壁有个叫横江的村镇，这个镇因为坐落在横江水旁边，受灾最严重，听说整个镇断水断电，都快成孤岛了。我有一个亲戚在横江，他们家一层楼就被淹了，很多人家养的鸡鸭鱼猪等都被冲走了，几乎所有的农田被淹，损失真的很严重。\n横江镇政府被淹情形[1]\n这是通往横江镇的公路，下坡后就是横江镇，可以看到整个村镇已经被淹[2]\n停在路边的小汽车都被冲走了！[2]\n整个村子一片汪洋[2]\n特写[2]\n估计我家后山的滑坡比这严重很多[1]\n我记得我小的时候（大概五六岁？），可能是1998年吧，也发生过一次特大洪水，当时也是晚上，爷爷把我和哥哥叫起来，爬到后山上去避难了。当时虽然洪水也大，但也就一晚上，第二天很快就退了，不像这次持续的特大暴雨。\n晚些时候我又打电话给妈妈，听妈妈说因为怕山体滑坡，已经去半山腰上的烤烟房里过夜了。那个烤烟房是我还没出生（或者我很小）的时候盖的土坯房，专门用来烤烟的。 后来一家人常年在外，也没怎么管它，很多瓦都碎了。前几年我爸回家补漏，现在是不会漏水了。但是毕竟年代久远了，而且是土坯房，又下那么大雨，还是不放心。想到妈妈一个人在家，晚上要住在一个没有水没有电的土坯房里，心里真不是滋味。以后有钱了应该把烤烟房改建成砖房，万一出现什么灾害，也有个落脚的地方。\n刚看了下天气预报，现在还在下雨，周六周一还有雷阵雨，希望不要再滑坡了。\n石城5月份历史天气，从15号开始连下了6天的大雨，以18,19,20为最[4]\n在这次灾害中，也要感谢当今发达的互联网，让全国各地的人实时了解家乡的情况，很多救援队也纷纷赶赴家乡展开救援。希望洪水快快退去，还家乡人民安宁的生活。\n此次特大灾害直播：http://mp.weixin.qq.com/s?__biz=MjM5MzEzODgyMA==\u0026amp;mid=206189476\u0026amp;idx=1\u0026amp;sn=3e2cc4f01b7ff42f933420063f4e5a74#rd\n520，对石城说“我爱你”：http://v.qq.com/boke/page/z/0/2/z0154dvf6d2.html\n灾后场景：http://mp.weixin.qq.com/s?__biz=MzAwNDUzMjg5OA==\u0026amp;mid=208435542\u0026amp;idx=1\u0026amp;sn=346f8e17448bf3a976c55591636b6e02\u0026amp;scene=1\u0026amp;from=singlemessage\u0026amp;isappinstalled=0#rd\n2015年5月21号石城新闻：http://v.qq.com/boke/page/w/0/4/w015442vnu4.html\n相关媒体报道：\nhttp://pic.people.com.cn/n/2015/0520/c1016-27031534.html?k=1\nhttp://news.163.com/15/0521/05/AQ48GSQU00014Q4P.html\nhttp://news.163.com/15/0521/06/AQ4CVUIF00014AEE.html\n参考：\n[1]. 赣江源头微信公众号文章\n[2]. 石城热线微信公众号文章\n[3]. 百度搜索“石城天气”，数据来源中国天气网\n[4]. 数据来源：http://15tianqi.cn/shicheng5yuetianqi/\n","permalink":"http://localhost:1313/posts/2015-05-21-severe-flooding-hits-my-hometown/","summary":"\u003cp\u003e5月19号的中午吃完饭后随手刷了一下朋友圈，发现MS表哥分享了一个\u003ca href=\"http://mp.weixin.qq.com/s?__biz=MzAwNDUzMjg5OA==\u0026amp;mid=208222165\u0026amp;idx=1\u0026amp;sn=8586e0f5d9830cfb4cf51921bfe1269a\u0026amp;scene=2\u0026amp;from=timeline\u0026amp;isappinstalled=0#rd\"\u003e链接\u003c/a\u003e，说家乡发生了十多年未遇洪灾。仔细看了一下，发现这次洪灾真的很严重，然后就给妈妈打了个电话，妈妈说从昨天下午开始下大雨，到晚上下暴雨，我家后院有一座小山，也出现了滑坡；附近的一座桥也淹了，家门口的一片农田（不是我家的）也全被淹了。妈妈说她也是头一回看到这么大而持久的暴雨。\u003c/p\u003e\n\u003cp\u003e后来看QQ空间，全是关于家乡灾情的状态，很多新闻媒体也报道了。县的下游地区受灾比较严重，隔壁有个叫横江的村镇，这个镇因为坐落在横江水旁边，受灾最严重，听说整个镇断水断电，都快成孤岛了。我有一个亲戚在横江，他们家一层楼就被淹了，很多人家养的鸡鸭鱼猪等都被冲走了，几乎所有的农田被淹，损失真的很严重。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"横江镇政府被淹情形[1]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-1.jpg\"\u003e\n横江镇政府被淹情形[1]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"这是通往横江镇的公路，下坡后就是横江镇，可以看到整个村镇已经被淹[2]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-2.jpg\"\u003e\n这是通往横江镇的公路，下坡后就是横江镇，可以看到整个村镇已经被淹[2]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"停在路边的小汽车都被冲走了！[2]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-3.jpg\"\u003e\n停在路边的小汽车都被冲走了！[2]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"整个村子一片汪洋[2]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-4.jpg\"\u003e\n整个村子一片汪洋[2]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"特写[2]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-5.jpg\"\u003e\n特写[2]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"估计我家后山的滑坡比这严重很多[1]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-6.jpg\"\u003e\n估计我家后山的滑坡比这严重很多[1]\u003c/p\u003e\n\u003cp\u003e我记得我小的时候（大概五六岁？），可能是1998年吧，也发生过一次特大洪水，当时也是晚上，爷爷把我和哥哥叫起来，爬到后山上去避难了。当时虽然洪水也大，但也就一晚上，第二天很快就退了，不像这次持续的特大暴雨。\u003c/p\u003e\n\u003cp\u003e晚些时候我又打电话给妈妈，听妈妈说因为怕山体滑坡，已经去半山腰上的烤烟房里过夜了。那个烤烟房是我还没出生（或者我很小）的时候盖的土坯房，专门用来烤烟的。 后来一家人常年在外，也没怎么管它，很多瓦都碎了。前几年我爸回家补漏，现在是不会漏水了。但是毕竟年代久远了，而且是土坯房，又下那么大雨，还是不放心。想到妈妈一个人在家，晚上要住在一个没有水没有电的土坯房里，心里真不是滋味。以后有钱了应该把烤烟房改建成砖房，万一出现什么灾害，也有个落脚的地方。\u003c/p\u003e\n\u003cp\u003e刚看了下天气预报，现在还在下雨，周六周一还有雷阵雨，希望不要再滑坡了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"石城5月份历史天气，从15号开始连下了6天的大雨，以18,19,20为最[4]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/shicheng_history_weather.png\"\u003e\n石城5月份历史天气，从15号开始连下了6天的大雨，以18,19,20为最[4]\u003c/p\u003e\n\u003cp\u003e在这次灾害中，也要感谢当今发达的互联网，让全国各地的人实时了解家乡的情况，很多救援队也纷纷赶赴家乡展开救援。希望洪水快快退去，还家乡人民安宁的生活。\u003c/p\u003e\n\u003cp\u003e此次特大灾害直播：\u003ca href=\"http://mp.weixin.qq.com/s?__biz=MjM5MzEzODgyMA==\u0026amp;mid=206189476\u0026amp;idx=1\u0026amp;sn=3e2cc4f01b7ff42f933420063f4e5a74#rd\"\u003ehttp://mp.weixin.qq.com/s?__biz=MjM5MzEzODgyMA==\u0026amp;mid=206189476\u0026amp;idx=1\u0026amp;sn=3e2cc4f01b7ff42f933420063f4e5a74#rd\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e520，对石城说“我爱你”：\u003ca href=\"http://v.qq.com/boke/page/z/0/2/z0154dvf6d2.html\"\u003ehttp://v.qq.com/boke/page/z/0/2/z0154dvf6d2.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e灾后场景：\u003ca href=\"http://mp.weixin.qq.com/s?__biz=MzAwNDUzMjg5OA==\u0026amp;mid=208435542\u0026amp;idx=1\u0026amp;sn=346f8e17448bf3a976c55591636b6e02\u0026amp;scene=1\u0026amp;from=singlemessage\u0026amp;isappinstalled=0#rd\"\u003ehttp://mp.weixin.qq.com/s?__biz=MzAwNDUzMjg5OA==\u0026amp;mid=208435542\u0026amp;idx=1\u0026amp;sn=346f8e17448bf3a976c55591636b6e02\u0026amp;scene=1\u0026amp;from=singlemessage\u0026amp;isappinstalled=0#rd\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e2015年5月21号石城新闻：\u003ca href=\"http://v.qq.com/boke/page/w/0/4/w015442vnu4.html\"\u003ehttp://v.qq.com/boke/page/w/0/4/w015442vnu4.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e相关媒体报道：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://pic.people.com.cn/n/2015/0520/c1016-27031534.html?k=1\"\u003ehttp://pic.people.com.cn/n/2015/0520/c1016-27031534.html?k=1\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://news.163.com/15/0521/05/AQ48GSQU00014Q4P.html\"\u003ehttp://news.163.com/15/0521/05/AQ48GSQU00014Q4P.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://news.163.com/15/0521/06/AQ4CVUIF00014AEE.html\"\u003ehttp://news.163.com/15/0521/06/AQ4CVUIF00014AEE.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. 赣江源头微信公众号文章\u003c/p\u003e\n\u003cp\u003e[2]. 石城热线微信公众号文章\u003c/p\u003e\n\u003cp\u003e[3]. 百度搜索“石城天气”，数据来源中国天气网\u003c/p\u003e\n\u003cp\u003e[4]. 数据来源：\u003ca href=\"http://15tianqi.cn/shicheng5yuetianqi/\"\u003ehttp://15tianqi.cn/shicheng5yuetianqi/\u003c/a\u003e\u003c/p\u003e","title":"家乡遭受几十年一遇的洪灾"},{"content":"其实在中学的时候就遇到过“刺猬困境”，只是那时候不知道这个名词，直接促使我了解这方面内容的原因是和XN的一次不愉快的交谈。\n那是半个多月以前的事情了，有一天中午我们一起吃完午饭回来，我们身后突然有一辆小车要启动，XN就一个劲叫我靠边走；但与此同时我边上迎面来了一辆电动车，于是我就和XN说我这边也有车，但是XN回我一句“叫你靠边走你不靠边走”，当时我听了很不舒服，还是告诉她我这边也有车，但是语气稍微重了点，我以为XN会到此为止，没想到她还是回了一句“你那么大声干嘛呀”……因为那天心情就不太好，还被她这么一说，那天我就没理她了。\n本来以为以XN的性格，这件事会很快过去，没想到XN一气之下不和我一起上下班了，也不和我一起打球吃饭了。我觉得事情还是有那么一点“严重”，这都还没正式入组，就和最熟悉的组员闹僵了，以后三年该怎么办呀，所以第二天晚上我还是诚恳的向XN道歉了。\n后来回想起这件事，确实是我的错。XN当时叫我靠边是一番好意，她多说了几句，我一个男生反倒先急了，确实是气度不够大。\n不过说实话，XN也并不是完全没有错，在我和她解释之后，她还不依不饶，似乎有点过了。她之前也跟我讲过自己有一次在班群里开某同学的玩笑，导致那个同学和她理论的事情。\n写到这的时候又让我想起了有一次教XN打乒乓球的时候，在公共场合批评她打球不够认真，哪里哪里打得不好之类的，当时我自己觉得不以为然，教她打球，当然要指出她的不对之处，但是后来XN告诉我她很生气，哪有我这样一直批评学生的老师啊。我仔细一想，对呀，我为什么在公共场合批评她打得不好呢，我是她的教练吗，我和她熟到能无话不谈吗，好像没有，所以这就是我没有把握好人际交往的一个度。我和XN只是同届同门师弟妹，我们之间应该保持一定的距离，距离太近了，说话做事无所顾忌，迟早会对某一方产生伤害，导致矛盾。\n刺猬困境是由叔本华提出的一个概念。寒冷的冬天，刺猬本想拥作一团、互相取暖，但一靠近便被彼此刺伤了；想分开避免扎伤，又觉得寒冷而想再彼此靠近。几个反复后，刺猬发现它们最好保持一点距离。\n与人交往也是这样，当两个人关系逐渐亲密起来，成为所谓的好基友、好闺蜜的时候，往往容易忽视对方的感受，说出一些伤人的话。\n高中的时候因为成绩还可以，班上的LL同学经常向我请教问题，我每次都会很耐心很热情的帮他解答，慢慢的关系比较好，他就经常和我一起上下课，课间跑到我的位置上和我聊天开玩笑，甚至左拍一下我右捏一下我。这让我感到很不舒服，感觉个人空间被入侵，后来我就慢慢的和他疏远，保持一个合理的距离。\n我和XN的那次不愉快交谈，就相当于刺猬间相互取暖导致彼此受伤的过程，不过我相信，经过几个回合，我们能慢慢的找到合适的距离，既相互取暖，又不至于受伤。\nEdward T. Hall’s personal reaction bubbles, showing radius in feet and meters[1]\n上图是爱德华·霍尔提出的人际交往的四个距离，从内到外依次是亲密距离-\u0026gt;个人距离-\u0026gt;社交距离-\u0026gt;公共距离。和朋友同事之间的距离应该保持在个人距离0.45米。\nP.S.希望能够找到那个和我共享亲密距离的人-:)\n参考：\n[1]. 维基百科“Personal space”条目：http://en.wikipedia.org/wiki/Personal_space\n","permalink":"http://localhost:1313/posts/2015-05-02-hedgehogs-dilemma-and-interpersonal-distance/","summary":"\u003cp\u003e其实在中学的时候就遇到过“刺猬困境”，只是那时候不知道这个名词，直接促使我了解这方面内容的原因是和XN的一次不愉快的交谈。\u003c/p\u003e\n\u003cp\u003e那是半个多月以前的事情了，有一天中午我们一起吃完午饭回来，我们身后突然有一辆小车要启动，XN就一个劲叫我靠边走；但与此同时我边上迎面来了一辆电动车，于是我就和XN说我这边也有车，但是XN回我一句“叫你靠边走你不靠边走”，当时我听了很不舒服，还是告诉她我这边也有车，但是语气稍微重了点，我以为XN会到此为止，没想到她还是回了一句“你那么大声干嘛呀”……因为那天心情就不太好，还被她这么一说，那天我就没理她了。\u003c/p\u003e\n\u003cp\u003e本来以为以XN的性格，这件事会很快过去，没想到XN一气之下不和我一起上下班了，也不和我一起打球吃饭了。我觉得事情还是有那么一点“严重”，这都还没正式入组，就和最熟悉的组员闹僵了，以后三年该怎么办呀，所以第二天晚上我还是诚恳的向XN道歉了。\u003c/p\u003e\n\u003cp\u003e后来回想起这件事，确实是我的错。XN当时叫我靠边是一番好意，她多说了几句，我一个男生反倒先急了，确实是气度不够大。\u003c/p\u003e\n\u003cp\u003e不过说实话，XN也并不是完全没有错，在我和她解释之后，她还不依不饶，似乎有点过了。她之前也跟我讲过自己有一次在班群里开某同学的玩笑，导致那个同学和她理论的事情。\u003c/p\u003e\n\u003cp\u003e写到这的时候又让我想起了有一次教XN打乒乓球的时候，在公共场合批评她打球不够认真，哪里哪里打得不好之类的，当时我自己觉得不以为然，教她打球，当然要指出她的不对之处，但是后来XN告诉我她很生气，哪有我这样一直批评学生的老师啊。我仔细一想，对呀，我为什么在公共场合批评她打得不好呢，我是她的教练吗，我和她熟到能无话不谈吗，好像没有，所以这就是我没有把握好人际交往的一个度。我和XN只是同届同门师弟妹，我们之间应该保持一定的距离，距离太近了，说话做事无所顾忌，迟早会对某一方产生伤害，导致矛盾。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hedgehogs dilemma\" loading=\"lazy\" src=\"/posts/2015-05-02-hedgehogs-dilemma-and-interpersonal-distance/hedgehogs-dilemma.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e刺猬困境是由叔本华提出的一个概念。寒冷的冬天，刺猬本想拥作一团、互相取暖，但一靠近便被彼此刺伤了；想分开避免扎伤，又觉得寒冷而想再彼此靠近。几个反复后，刺猬发现它们最好保持一点距离。\u003c/p\u003e\n\u003cp\u003e与人交往也是这样，当两个人关系逐渐亲密起来，成为所谓的好基友、好闺蜜的时候，往往容易忽视对方的感受，说出一些伤人的话。\u003c/p\u003e\n\u003cp\u003e高中的时候因为成绩还可以，班上的LL同学经常向我请教问题，我每次都会很耐心很热情的帮他解答，慢慢的关系比较好，他就经常和我一起上下课，课间跑到我的位置上和我聊天开玩笑，甚至左拍一下我右捏一下我。这让我感到很不舒服，感觉个人空间被入侵，后来我就慢慢的和他疏远，保持一个合理的距离。\u003c/p\u003e\n\u003cp\u003e我和XN的那次不愉快交谈，就相当于刺猬间相互取暖导致彼此受伤的过程，不过我相信，经过几个回合，我们能慢慢的找到合适的距离，既相互取暖，又不至于受伤。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/upload.wikimedia.org/wikipedia/commons/thumb/3/35/Personal_Space.svg/640px-Personal_Space.svg.png\"\u003e\nEdward T. Hall’s personal reaction bubbles, showing radius in feet and meters[1]\u003c/p\u003e\n\u003cp\u003e上图是爱德华·霍尔提出的人际交往的四个距离，从内到外依次是亲密距离-\u0026gt;个人距离-\u0026gt;社交距离-\u0026gt;公共距离。和朋友同事之间的距离应该保持在个人距离0.45米。\u003c/p\u003e\n\u003cp\u003eP.S.希望能够找到那个和我共享亲密距离的人-:)\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. 维基百科“Personal space”条目：\u003ca href=\"http://en.wikipedia.org/wiki/Personal_space\"\u003ehttp://en.wikipedia.org/wiki/Personal_space\u003c/a\u003e\u003c/p\u003e","title":"“刺猬困境”与人际交往距离"},{"content":"其实很早之前就打算学习繁体字了，但是直接驱动我开始行动的还是上周末的一件事。\n上周末组内一起去天津蓟县盘山景区游玩，进入景区看到的第一个“景点”就是下面的乾隆御笔\n乾隆《游盘山记》\n当时JL师姐念了一遍，有几个繁体字不认识，在场的其他人也都模棱两可。我当时就后悔为什么不早点把繁体字学了呢。所以回所之后马上买了下面的这本《繁简字对照字典》，决定每天看一两页。\n《繁简字对照字典》[1]\n网上也有《游盘山记》的简体版，如下\n连太行，拱神京，放碣石，距沧溟，走蓟野，枕长城，盖蓟州之天作，俯临重壑，如众星拱北而莫敢与争者也。—-乾隆御笔\n对照图片中的繁体字，学习一下。\n有些繁体字和简体字不是一一对应的，比如同样是“汇”字，“汇聚”对应的繁体字为“匯聚”，而“词汇”对应的繁体字为“詞彙”，这一点需要注意，网上有开源的繁简字转换工具，可以看这里。\n关于繁简字的争论，网上已经很多了，我也不想评论，我只想说，学习繁体字完全是个人兴趣，我觉得繁体字很美，很有意思，一个字可以研究半天，外出游玩的时候也能顺带“和古人交流交流”，所以就学了-:)\n参考：\n[1]. 豆瓣读书《繁简字对照字典》：http://book.douban.com/subject/2234412/\n","permalink":"http://localhost:1313/posts/2015-04-21-learning-traditional-chinese-characters/","summary":"\u003cp\u003e其实很早之前就打算学习繁体字了，但是直接驱动我开始行动的还是上周末的一件事。\u003c/p\u003e\n\u003cp\u003e上周末组内一起去天津蓟县盘山景区游玩，进入景区看到的第一个“景点”就是下面的乾隆御笔\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"乾隆《游盘山记》\" loading=\"lazy\" src=\"/posts/2015-04-21-learning-traditional-chinese-characters/panshan.jpg\"\u003e\n乾隆《游盘山记》\u003c/p\u003e\n\u003cp\u003e当时JL师姐念了一遍，有几个繁体字不认识，在场的其他人也都模棱两可。我当时就后悔为什么不早点把繁体字学了呢。所以回所之后马上买了下面的这本《繁简字对照字典》，决定每天看一两页。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://img3.doubanio.com/lpic/s2988301.jpg\"\u003e\n《繁简字对照字典》[1]\u003c/p\u003e\n\u003cp\u003e网上也有《游盘山记》的简体版，如下\u003c/p\u003e\n\u003cp\u003e连太行，拱神京，放碣石，距沧溟，走蓟野，枕长城，盖蓟州之天作，俯临重壑，如众星拱北而莫敢与争者也。—-乾隆御笔\u003c/p\u003e\n\u003cp\u003e对照图片中的繁体字，学习一下。\u003c/p\u003e\n\u003cp\u003e有些繁体字和简体字不是一一对应的，比如同样是“汇”字，“汇聚”对应的繁体字为“匯聚”，而“词汇”对应的繁体字为“詞彙”，这一点需要注意，网上有开源的繁简字转换工具，可以看\u003ca href=\"http://opencc.byvoid.com/\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.zhihu.com/question/25389359\"\u003e关于繁简字的争论，网上已经很多了\u003c/a\u003e，我也不想评论，我只想说，学习繁体字完全是个人兴趣，我觉得繁体字很美，很有意思，一个字可以研究半天，外出游玩的时候也能顺带“和古人交流交流”，所以就学了-:)\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. 豆瓣读书《繁简字对照字典》：\u003ca href=\"http://book.douban.com/subject/2234412/\"\u003ehttp://book.douban.com/subject/2234412/\u003c/a\u003e\u003c/p\u003e","title":"我学繁体字"},{"content":"师姐推荐了最新一期《人物周刊》上的一篇文章–“不求助的人”。这篇文章讲到，在上月末德国空难事件中，副驾驶卢比茨选择了坠毁飞机。调查人员随后了解到，他可能患有抑郁症，同时，他的弱视或许在持续恶化。视力和心理问题给卢比茨的职业前景笼上阴影，而他没有选择求助。\n“求助”似乎从来不是首选项。遇到难题自己解决，不行就上网搜搜方法，还不行，则“咬紧牙关熬过去”。不求助的表面理由是不麻烦别人，但更真实的担忧大概是“如果开口求助，别人会认为我能力低下，我会因此丧失各种机会”。本质原因在于，给自己定的目标是“在他人面前表现出众”。\n目标可细分为两种，“精熟型目标（mastery goal）”和“绩效型目标（performancegoal）”。精熟型目标更重视过程而非结果，认为目的是自我提升，不是获得肯定。哪怕现在还“做不到”，但通过不断努力也能有所进步。既然目标是“成为更好的自己”，那么遇到困难时，自然会寻求帮助。而绩效型目标只看最终结果，你要么“能做到”，要么“不能做到”，要么力压众人表现出色，要么挑战失败沦为笑柄。既然目的是“从他人那里获得肯定”，感觉上像是“示弱”的求助就不会被列入选项。\n“不求助之人”并不少见。不过，“绩效型目标”者不知道的是，求助他人时，其实会提升此人对你的评价。每个人都觉得自己智慧过人，可以为别人授业解惑，而“懂得向聪明的我询问智慧建言的人，一定也是聪明人”。沃顿商学院的研究者发现，脑力竞赛中接到“搭档”求助的人，赛后给搭档打了更高的能力分。2010年，美国西北大学研究发现，老板其实更喜欢那些遇到困境会主动求援的下属，某种意义上，“求指点迷津”可能是对老板最好的恭维。\n说到底，不管目标是获得成长还是赞赏，求助都是帮助达成目标的大道。越早寻求帮助，越有机会让自己成长，也越有可能掌握技能、成功解决问题，周遭人对你的评价也会因此上升。反倒是不求助的人，万一拖到事情无法收拾，自己的自信和风评都会落到极低。\n这篇文章讲得很有道理，我发现身边就有很多不求助的人。他们从小到大很少向别人求助，自己能做的事尽量自己做，遇到难题也尽量自己扛，给人一种能力很强、很自信的印象；同时他们也很鄙视那些经常向别人求助的人，认为这些人“就知道问别人，这么简单的都不会”。你可以称赞“不求助的人”独立自主、坚韧刻苦，但是从某种程度上这恰恰反映了他们内心的不自信，他们害怕自己的求助暴露了自己的智商，显得自己水平不够。他们属于绩效型人群，只看重结果，不看重过程，如果当上领导，下属的压力肯定不小，久而久之，就会产生类似上面的案例，宁愿选择坠毁，也不愿向他人求助。\n不求助的人因为很少向他人求助，他们的交际圈也很窄，他们经常把自己封锁起来，甚至把主动伸出援手的人拒之门外。长期的封闭往往导致一些心理和精神疾病，以至于做出一些病态的选择。\n其实，和”不求助的人“的想法相反，文中沃顿商学院的研究结果很有道理，遇到问题喜欢求助的人，反而会受到别人较高的评价，因为被求助者会潜意识的认为“懂得向聪明的我询问智慧建言的人，一定也是聪明人”，说不定双方还能由此发展出一段不错的关系；而且越早求助，就能越早解决问题。这一箭多雕的事情，恐怕是不求助的人没有想到的吧。\n从某种程度上来说，我自己也是一个“不求助的人”，H老师估计早就猜透了，我希望能够在读研期间“收获一点成就感、一点自信心”，从绩效型人群转移到精熟型人群。\n","permalink":"http://localhost:1313/posts/2015-04-18-someone-who-doesnt-ask-for-help/","summary":"\u003cp\u003e师姐推荐了最新一期《人物周刊》上的一篇文章–\u003ca href=\"https://mp.weixin.qq.com/s?__biz=MTY0MzI5NDcwMQ==\u0026amp;mid=206411797\u0026amp;idx=1\u0026amp;sn=c0da9db2ed5a6a5bdedbfb871e45c8e6#rd\"\u003e“不求助的人”\u003c/a\u003e。这篇文章讲到，在上月末德国空难事件中，副驾驶卢比茨选择了坠毁飞机。调查人员随后了解到，他可能患有抑郁症，同时，他的弱视或许在持续恶化。视力和心理问题给卢比茨的职业前景笼上阴影，\u003cstrong\u003e而他没有选择求助\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e“求助”似乎从来不是首选项。遇到难题自己解决，不行就上网搜搜方法，还不行，则“咬紧牙关熬过去”。不求助的表面理由是不麻烦别人，但更真实的担忧大概是“如果开口求助，别人会认为我能力低下，我会因此丧失各种机会”。本质原因在于，给自己定的目标是“在他人面前表现出众”。\u003c/p\u003e\n\u003cp\u003e目标可细分为两种，“精熟型目标（mastery goal）”和“绩效型目标（performancegoal）”。精熟型目标更重视过程而非结果，认为目的是自我提升，不是获得肯定。哪怕现在还“做不到”，但通过不断努力也能有所进步。既然目标是“成为更好的自己”，那么遇到困难时，自然会寻求帮助。而绩效型目标只看最终结果，你要么“能做到”，要么“不能做到”，要么力压众人表现出色，要么挑战失败沦为笑柄。既然目的是“从他人那里获得肯定”，感觉上像是“示弱”的求助就不会被列入选项。\u003c/p\u003e\n\u003cp\u003e“不求助之人”并不少见。不过，“绩效型目标”者不知道的是，求助他人时，其实会提升此人对你的评价。每个人都觉得自己智慧过人，可以为别人授业解惑，而“懂得向聪明的我询问智慧建言的人，一定也是聪明人”。沃顿商学院的研究者发现，脑力竞赛中接到“搭档”求助的人，赛后给搭档打了更高的能力分。2010年，美国西北大学研究发现，老板其实更喜欢那些遇到困境会主动求援的下属，某种意义上，“求指点迷津”可能是对老板最好的恭维。\u003c/p\u003e\n\u003cp\u003e说到底，不管目标是获得成长还是赞赏，求助都是帮助达成目标的大道。越早寻求帮助，越有机会让自己成长，也越有可能掌握技能、成功解决问题，周遭人对你的评价也会因此上升。反倒是不求助的人，万一拖到事情无法收拾，自己的自信和风评都会落到极低。\u003c/p\u003e\n\u003cp\u003e这篇文章讲得很有道理，我发现身边就有很多不求助的人。他们从小到大很少向别人求助，自己能做的事尽量自己做，遇到难题也尽量自己扛，给人一种能力很强、很自信的印象；同时他们也很鄙视那些经常向别人求助的人，认为这些人“就知道问别人，这么简单的都不会”。你可以称赞“不求助的人”独立自主、坚韧刻苦，但是从某种程度上这恰恰反映了他们内心的不自信，他们害怕自己的求助暴露了自己的智商，显得自己水平不够。他们属于绩效型人群，只看重结果，不看重过程，如果当上领导，下属的压力肯定不小，久而久之，就会产生类似上面的案例，宁愿选择坠毁，也不愿向他人求助。\u003c/p\u003e\n\u003cp\u003e不求助的人因为很少向他人求助，他们的交际圈也很窄，他们经常把自己封锁起来，甚至把主动伸出援手的人拒之门外。长期的封闭往往导致一些心理和精神疾病，以至于做出一些病态的选择。\u003c/p\u003e\n\u003cp\u003e其实，和”不求助的人“的想法相反，文中沃顿商学院的研究结果很有道理，遇到问题喜欢求助的人，反而会受到别人较高的评价，因为被求助者会潜意识的认为“懂得向聪明的我询问智慧建言的人，一定也是聪明人”，说不定双方还能由此发展出一段不错的关系；而且越早求助，就能越早解决问题。这一箭多雕的事情，恐怕是不求助的人没有想到的吧。\u003c/p\u003e\n\u003cp\u003e从某种程度上来说，我自己也是一个“不求助的人”，H老师估计早就猜透了，我希望能够在读研期间\u003ca href=\"http://pfind.net/people/hesimin/Chinese/Favorite%20Books.htm\"\u003e“收获一点成就感、一点自信心”\u003c/a\u003e，从绩效型人群转移到精熟型人群。\u003c/p\u003e","title":"不求助的人"},{"content":"趁着周末，看了韩寒导演的处女作《后会无期》，说来奇怪，看的过程中没有丝毫感觉，情节松散，直到听到了片尾曲”平凡之路“，内心为之一颤，想来应该写点什么纪念一下。\n影片中三个年轻人离开家乡小岛，一路向西，横穿中国大陆，路上落下了胡生，错过了假装”小姐“的”骗子“，告别了一直”恋爱“着的笔友，遇到了善恶莫测的奇怪旅人，送走了最好的朋友，只有流浪的小狗留在了身边。几番告白，几番告别，勾勒出几段截然不同的平凡人生之路。\n突然间，我从影片中看到了萧瑟冷漠的世界，看到了饱经沧桑的老人挣扎着，反抗着，但最终离开了。\n影片给我影响最深的两句话是：\n你连世界都没观过，你哪来的世界观。 如果要告别，一定要用力一点,因为任何多看一眼,都有可能成为最后一眼,多说一句,都可能是最后一句。 经常有人惊讶于我小小年纪就表现得如此成熟，不知道是不是因为从小跟着父母外出闯荡，经历得多了，世界观不一样了。\n很小的时候就跟着父母去了广东JY，当时父亲帮别人挖煤，后来当过老师，开过早餐店，开过出租车。挖煤的时候每天都要在臭气熏天的河里挖半天煤，然后用船运回去，又要顶着炎炎夏日做半天的煤。每天完事之后脚乌漆墨黑，老茧长得跟树皮一样。开早餐店的时候，每天凌晨三四点就要起床开始和面，做包子，熬豆浆，炸油条。忙完了早餐还要去学校上班。\n那一年有天晚上，爸爸妈妈正在收拾店铺，准备第二天早晨的面料，YT睡到半夜突然KTBM，脚一直在发抖，我被吵醒之后马上告诉了爸爸妈妈。当时都已经很晚了，地段也比较偏僻，路上少有行人，幸好隔壁开茶叶店的老板还没有走，他用摩托车把YT送到了医院。那一天晚上格外的冷，我只记得妈妈站在医院门口不停的祈祷着什么。后来几经折腾，转院到汕头的大医院，病床好像在走廊里，医院的快餐比家里的还好吃。\n一家人出门在外，父母的工资很低，在外读书，一学期的学费要几百块钱，再加上YT的那场大病，家里的经济压力着实不小。父母经常为一些事大吵大闹，有几次还大打出手，作为小孩子的我只能哭着求着他们不要再吵不要再打了，过往的事件历历在目，那都是血和泪的记忆。\n在外漂泊的日子过得很辛苦，对于大人如此，对于我们这些青少年也一样。走在路上经常会被一些本地的小混混打，晚自习回家一定要结伴而行，不要走人少的路。我记得有一次我和表哥一起走在街上，一个骑自行车的小混混从我们背后踢了我们两脚，当时表哥正要反击，我把他拉住了，因为我知道，外地人在这势单力薄，根本不是这些人的对手，自己吃点亏，能不惹麻烦尽量不惹麻烦。但是这个小事给我的印象很深。\n也许是在外打拼的日子太苦，初一下学期，我、YT和妈妈回老家了，爸爸继续在JY打拼着。回到家之后，去了一个稍微好一点的初中，妈妈在我们身边陪读。\n因为在JY的时候，家里很穷，但是过年的时候，父母总还是会给我们买新衣服，所以每年就给我们买便宜又得体的西装。回到老家之后，城里的同学看我们经常穿西装，索性给我们取了一个外号”西装“，这导致我后来对西装厌恶至极。\n有一天晚上，晚自习回家，我和YT刚走出校门的时候，有一群小混混和我们逆行，他们跑的时候不小心把水溅到我们身上了，我想想也就算了，但是YT不服，故意把水溅到他们身上。我当时暗想坏了，他们会不会回来找我们算账啊，果不其然，没过多久，一帮人就追着我们打，幸好当时有一个老师路过，我向她”求救“才得以脱险回家。\n这些小事反应到我的性格上来就是忍气吞声，处世中庸，”吃亏是福“。这种性格在某种程度上也是一件好事，高中三年给我省了很多麻烦，也能让我沉下心来，埋头苦干，高考的时候考了全县第十一名，考取WHU也算是对我那几年的一个回报。其实农村孩子这种”两耳不闻窗外事，一心只读圣贤书“的单一发展，也给我视野狭窄、其他技能缺失埋下了伏笔，这里暂时按下不表。\n我还在JY的时候，有一天妈妈说我们要马上回家看外公，外公病了。回到老家之后，我和YT在院子里玩耍，后来妈妈拉我去见了外公最后一面。那大概是我记忆中第一次亲人离别。后来妈妈告诉我，外公当时还怪我到家之后为什么没有马上去看他呢，对呀，我当时为什么没有马上去看外公而是在院子里玩耍呢，也许那时候还不知道什么是离别吧。\n后来又经历了曾祖母的离别。记忆最深的是爷爷的离别。那大概是一年前吧，我当时正在图书馆准备保研的事情，突然爷爷给我打了一个电话，爷爷很少给我打电话的，而且那时候我们好像还不知道对方的手机号码，爷爷说是从YT那知道我的号码的。他问了我一些近况，叫我要好好照顾自己的身体，不要担心他；他还说他给YT也打了电话，给CY打电话但是没有打通；末了，他说这个电话没别的意思，就这样吧，挂了。我可以明显的感受到电话那头爷爷凄凉孤独的心，这通电话听起来很怪，我马上给爸爸打了个电话，告诉了他情况，爸爸说爷爷一个人在家，也许是太孤单了，或者是犯了老年痴呆症，爸爸还说爷爷也经常打类似的电话给他。是啊，奶奶在我很小的时候就去世了，爷爷一个人孤苦伶仃生活了将近二十年，纵然有三个儿子一个女儿，但是几个儿子儿媳之间为了老人的赡养问题竟成陌路，小儿子老大不小了也还没有成家，是孤独的在这个世界苟活着，给儿子儿媳带来更多的麻烦还是默默的离开，给年轻人省去一个包袱，爷爷心里恐怕早已有了答案。\n几天之后，噩耗传来，没想到那竟成了我和爷爷最后的通话。\n给爷爷办后事的时候，几个叔叔姑姑都回来了，这竟是我记忆中唯一一次看到大家坐在一张桌子上吃饭。\n很多人都说我冷漠、沉默寡言，其实我小时候不是这样的，可能是这些年经历的事太多了，我对很多事情漠不关心，很多不必要的、无意义的话也不讲了，很多点头之交的朋友也不联系了，也变得不喜欢和人争辩了；我开始喜欢独处，喜欢一个人走在路上，看过往匆匆行人，喜欢看《文化苦旅》、《一九八四》、《百年孤独》、《活着》……\n身边很多同学喜欢三五成群出门，经常见他们和各种各样的朋友打招呼。有一次我和一个夏令营认识的同学打招呼，XN居然诧异的告诉我这是她第一次在ICT发现我也有认识的人。为什么要有那么多朋友呢，可能你会告诉我你的QQ好友都上千了，但是真正在你社交圈里的朋友，能够和你交心的朋友，超过10个吗？逢年过节，为了维系那990个你都不记得他/她的模样的朋友，群发着各种短信，朋友圈、QQ空间、微博里不停的给别人点赞，有必要吗？我已经厌倦了这些虚情假意。\n令我很感动的是，前几天，我正在实验室敲代码的时候，接到了WQ的一个电话，我跟他抱怨了一下在北京的各种不顺，他跟我说一个人出门在外，要好好照顾自己；同时另一个好友WS也经常跟我说想和我聊聊。这让我感到非常温暖，虽然我现在一无所有，但是有一两个至交，足以。\n我已经走过了二十多年，是时候走出去看看世界，说不准世界观就形成了呢。\n","permalink":"http://localhost:1313/posts/2015-04-12-my-ordinary-life/","summary":"\u003cp\u003e趁着周末，看了韩寒导演的处女作《后会无期》，说来奇怪，看的过程中没有丝毫感觉，情节松散，直到听到了片尾曲”平凡之路“，内心为之一颤，想来应该写点什么纪念一下。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"The-Continent1.jpg\" loading=\"lazy\" src=\"/posts/2015-04-12-my-ordinary-life/The-Continent1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e影片中三个年轻人离开家乡小岛，一路向西，横穿中国大陆，路上落下了胡生，错过了假装”小姐“的”骗子“，告别了一直”恋爱“着的笔友，遇到了善恶莫测的奇怪旅人，送走了最好的朋友，只有流浪的小狗留在了身边。几番告白，几番告别，勾勒出几段截然不同的平凡人生之路。\u003c/p\u003e\n\u003cp\u003e突然间，我从影片中看到了萧瑟冷漠的世界，看到了饱经沧桑的老人挣扎着，反抗着，但最终离开了。\u003c/p\u003e\n\u003cp\u003e影片给我影响最深的两句话是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e你连世界都没观过，你哪来的世界观。\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e如果要告别，一定要用力一点,因为任何多看一眼,都有可能成为最后一眼,多说一句,都可能是最后一句。\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e经常有人惊讶于我小小年纪就表现得如此成熟，不知道是不是因为从小跟着父母外出闯荡，经历得多了，世界观不一样了。\u003c/p\u003e\n\u003cp\u003e很小的时候就跟着父母去了广东JY，当时父亲帮别人挖煤，后来当过老师，开过早餐店，开过出租车。挖煤的时候每天都要在臭气熏天的河里挖半天煤，然后用船运回去，又要顶着炎炎夏日做半天的煤。每天完事之后脚乌漆墨黑，老茧长得跟树皮一样。开早餐店的时候，每天凌晨三四点就要起床开始和面，做包子，熬豆浆，炸油条。忙完了早餐还要去学校上班。\u003c/p\u003e\n\u003cp\u003e那一年有天晚上，爸爸妈妈正在收拾店铺，准备第二天早晨的面料，YT睡到半夜突然KTBM，脚一直在发抖，我被吵醒之后马上告诉了爸爸妈妈。当时都已经很晚了，地段也比较偏僻，路上少有行人，幸好隔壁开茶叶店的老板还没有走，他用摩托车把YT送到了医院。那一天晚上格外的冷，我只记得妈妈站在医院门口不停的祈祷着什么。后来几经折腾，转院到汕头的大医院，病床好像在走廊里，医院的快餐比家里的还好吃。\u003c/p\u003e\n\u003cp\u003e一家人出门在外，父母的工资很低，在外读书，一学期的学费要几百块钱，再加上YT的那场大病，家里的经济压力着实不小。父母经常为一些事大吵大闹，有几次还大打出手，作为小孩子的我只能哭着求着他们不要再吵不要再打了，过往的事件历历在目，那都是血和泪的记忆。\u003c/p\u003e\n\u003cp\u003e在外漂泊的日子过得很辛苦，对于大人如此，对于我们这些青少年也一样。走在路上经常会被一些本地的小混混打，晚自习回家一定要结伴而行，不要走人少的路。我记得有一次我和表哥一起走在街上，一个骑自行车的小混混从我们背后踢了我们两脚，当时表哥正要反击，我把他拉住了，因为我知道，外地人在这势单力薄，根本不是这些人的对手，自己吃点亏，能不惹麻烦尽量不惹麻烦。但是这个小事给我的印象很深。\u003c/p\u003e\n\u003cp\u003e也许是在外打拼的日子太苦，初一下学期，我、YT和妈妈回老家了，爸爸继续在JY打拼着。回到家之后，去了一个稍微好一点的初中，妈妈在我们身边陪读。\u003c/p\u003e\n\u003cp\u003e因为在JY的时候，家里很穷，但是过年的时候，父母总还是会给我们买新衣服，所以每年就给我们买便宜又得体的西装。回到老家之后，城里的同学看我们经常穿西装，索性给我们取了一个外号”西装“，这导致我后来对西装厌恶至极。\u003c/p\u003e\n\u003cp\u003e有一天晚上，晚自习回家，我和YT刚走出校门的时候，有一群小混混和我们逆行，他们跑的时候不小心把水溅到我们身上了，我想想也就算了，但是YT不服，故意把水溅到他们身上。我当时暗想坏了，他们会不会回来找我们算账啊，果不其然，没过多久，一帮人就追着我们打，幸好当时有一个老师路过，我向她”求救“才得以脱险回家。\u003c/p\u003e\n\u003cp\u003e这些小事反应到我的性格上来就是忍气吞声，处世中庸，”吃亏是福“。这种性格在某种程度上也是一件好事，高中三年给我省了很多麻烦，也能让我沉下心来，埋头苦干，高考的时候考了全县第十一名，考取WHU也算是对我那几年的一个回报。其实农村孩子这种”两耳不闻窗外事，一心只读圣贤书“的单一发展，也给我视野狭窄、其他技能缺失埋下了伏笔，这里暂时按下不表。\u003c/p\u003e\n\u003cp\u003e我还在JY的时候，有一天妈妈说我们要马上回家看外公，外公病了。回到老家之后，我和YT在院子里玩耍，后来妈妈拉我去见了外公最后一面。那大概是我记忆中第一次亲人离别。后来妈妈告诉我，外公当时还怪我到家之后为什么没有马上去看他呢，对呀，我当时为什么没有马上去看外公而是在院子里玩耍呢，也许那时候还不知道什么是离别吧。\u003c/p\u003e\n\u003cp\u003e后来又经历了曾祖母的离别。记忆最深的是爷爷的离别。那大概是一年前吧，我当时正在图书馆准备保研的事情，突然爷爷给我打了一个电话，爷爷很少给我打电话的，而且那时候我们好像还不知道对方的手机号码，爷爷说是从YT那知道我的号码的。他问了我一些近况，叫我要好好照顾自己的身体，不要担心他；他还说他给YT也打了电话，给CY打电话但是没有打通；末了，他说这个电话没别的意思，就这样吧，挂了。我可以明显的感受到电话那头爷爷凄凉孤独的心，这通电话听起来很怪，我马上给爸爸打了个电话，告诉了他情况，爸爸说爷爷一个人在家，也许是太孤单了，或者是犯了老年痴呆症，爸爸还说爷爷也经常打类似的电话给他。是啊，奶奶在我很小的时候就去世了，爷爷一个人孤苦伶仃生活了将近二十年，纵然有三个儿子一个女儿，但是几个儿子儿媳之间为了老人的赡养问题竟成陌路，小儿子老大不小了也还没有成家，是孤独的在这个世界苟活着，给儿子儿媳带来更多的麻烦还是默默的离开，给年轻人省去一个包袱，爷爷心里恐怕早已有了答案。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"The-Continent2.jpg\" loading=\"lazy\" src=\"/posts/2015-04-12-my-ordinary-life/The-Continent2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e几天之后，噩耗传来，没想到那竟成了我和爷爷最后的通话。\u003c/p\u003e\n\u003cp\u003e给爷爷办后事的时候，几个叔叔姑姑都回来了，这竟是我记忆中唯一一次看到大家坐在一张桌子上吃饭。\u003c/p\u003e\n\u003cp\u003e很多人都说我冷漠、沉默寡言，其实我小时候不是这样的，可能是这些年经历的事太多了，我对很多事情漠不关心，很多不必要的、无意义的话也不讲了，很多点头之交的朋友也不联系了，也变得不喜欢和人争辩了；我开始喜欢独处，喜欢一个人走在路上，看过往匆匆行人，喜欢看《文化苦旅》、《一九八四》、《百年孤独》、《活着》……\u003c/p\u003e\n\u003cp\u003e身边很多同学喜欢三五成群出门，经常见他们和各种各样的朋友打招呼。有一次我和一个夏令营认识的同学打招呼，XN居然诧异的告诉我这是她第一次在ICT发现我也有认识的人。为什么要有那么多朋友呢，可能你会告诉我你的QQ好友都上千了，但是真正在你社交圈里的朋友，能够和你交心的朋友，超过10个吗？逢年过节，为了维系那990个你都不记得他/她的模样的朋友，群发着各种短信，朋友圈、QQ空间、微博里不停的给别人点赞，有必要吗？我已经厌倦了这些虚情假意。\u003c/p\u003e\n\u003cp\u003e令我很感动的是，前几天，我正在实验室敲代码的时候，接到了WQ的一个电话，我跟他抱怨了一下在北京的各种不顺，他跟我说一个人出门在外，要好好照顾自己；同时另一个好友WS也经常跟我说想和我聊聊。这让我感到非常温暖，虽然我现在一无所有，但是有一两个至交，足以。\u003c/p\u003e\n\u003cp\u003e我已经走过了二十多年，是时候走出去看看世界，说不准世界观就形成了呢。\u003c/p\u003e","title":"人生，平凡之路"},{"content":"2015年2月28日到达北京，到现在一月有余，是时候月度总结了。\n这是我第三次来北京 2014年的6月份第一次来北京，去北大计算所实习十天。感受了一下北大计算所p老师及其博士生z的冷漠。我还记得刚到的那天晚上一个人孤零零的走在北京的大街上寻找北大方正员工宿舍的场景。每天重复着朝九晚十的固定模式，每个人都像机器一样的工作，实验室只有敲击键盘的声音，偶尔看到p的时候他也是阴沉着脸，临走的时候还被z批评了。虽然p承诺说只要我通过夏令营面试就会要我，但是这明显就是废话呀；而且这段不算愉快的实习经历让我对p的实验室产生了厌恶。\n回学校之后开始准备期末考试和7月份的夏令营，并于2014年7月9日到达北京，这次的行程包括北大计算所和中科院计算所的夏令营。北大计算所因为机试和综合排名不占优势，败下阵来，不过现在想想当初幸好没有进p的实验室，要不然天天对着p的臭脸，估计要疯。\n中科院计算所的风格完全不像北大，中科院真正做到了海纳百川，一视同仁，不像北大看不起校外学生。来北京之前和ict的h老师沟通过很多次，发现我和h老师的性格很相似，我非常崇拜h老师严谨的处事风格，经过认真的准备，我也顺利通过了ict的夏令营，然后跑去深圳siat玩耍了。\n北京的物价真是贵 这次是来ict完成我的本科毕业设计的，大概要待到5月初。一来从家里不方便带太多东西，二来想想要在北京待三年，所以准备在北京重新置办生活用品。\n2月28刚到北京，去了师兄推荐的家乐福大超市。超市很大，有两层，不过里面的东西真是贵。也就买了被子、三件套、桶、盆，花了四五百，很可恨的是，很薄的春秋单人被子，居然要169，这还是最便宜的，床垫比被子还贵，要199。\n没必要的用品可以不买，但是饭不能不吃。以前在学校一天也就十几块，到北京后发现，一顿饭就十多块了。早餐一个鸡蛋要2块，一个小包子要1.5块，哎，想想以前自己家做早餐的时候1块钱4个大肉包子啊，当初怎么不多吃几个呢。\n餐厅里的饭菜确实贵，不过国科大食堂的饭菜既便宜又好吃，赞一个。\n北京的雾霾 来北京之前对北京的雾霾也略有耳闻，觉得那是北京人的小题大做，武汉也是扬尘满天飞，我也活得好好的啊；不过真来北京之后，确实受不了，放眼望去，街道上灰蒙蒙的一片，路上的特大广告牌都看不清。也许是雾霾加干燥的原因，刚来北京那一个星期，嗓子特别不舒服，每天早晨刷牙的时候都恶心干呕，不过后来慢慢好了。\n3月28那天，北京还遭遇了沙尘暴，那场景真像外星人袭击地球。所以来北京，口罩是必需品。\n3月28日北京的沙尘暴[1]\n北京的风景 来北京的前3个星期，忙于组内布置的任务，也是过着朝九晚十的生活，后来想想也不能天天这样，我应该出去透透气，于是选择了颐和园，3月份属于颐和园景区的淡季，加上学生证，门票不贵。颐和园不愧是皇家园林，里面的景色确实很漂亮，有万寿山、昆明湖，湖水还算干净。\n我去的那天天气很好，空气质量也不错；人虽多，但不算拥挤，部分原因是颐和园真是太大了。我在游玩的过程中看到了很多外国旅游团，听到过英语、法语、日语、韩语、粤语、中文，让我默默联想起了八国联军侵略中国的场景。\n在游玩的过程中遇到了一个很温馨的“外国小家庭”，妈妈是外国人，爸爸应该也是外国人，爸爸和妈妈聊天的时候是用英语，但是爸爸和儿子却是用中文，他们还有一个坐在婴儿车里的小女儿，儿子坐在湖边玩ipad，小女儿在车里哭个不停…\n虽然颐和园的风景很好，但是里面居然有很多蚊子（小虫子）！期间有一个虫子撞到我的嘴巴，还有一个钻进了我的耳朵。可能是春天到了，万物复苏，加上颐和园内潮湿，植物丰富，给虫子提供了很好的环境。\n下面放几张游玩时拍的照片。\n谁能告诉我这是什么乐器\n学太极的外国人\n这也是迎春花？\n这个角度的颐和园很美\n长廊\n准备回去的时候看到的，各种长枪大炮，据说是在抓拍一种珍稀鸟类。。。\n颐和园的风景虽好，但就自然风光来说，比不上南方早已复苏的大自然。\n家乡的油菜花，比颐和园的迎春花漂亮多了-:)\n武大樱花已经争奇斗艳了，北方还是萧瑟一片[2]\n后记 没来北京之前，我想着以后一定要在北京工作，还要争取在北京安家落户。虽然北京生活压力很大，但是机会和挑战也多，这不就是我要的生活吗；而且北京是全国的政治、经济和文化中心，可以很方便的去感受中国几千年的文化和历史。在北京生活一个月之后，我的想法变了，很重要的原因是北京的雾霾和气候。雾霾不是一个人一天两天能够解决的，你必须尽量减少不必要的出行，跑步健身什么的就更别提了；北京属于北温带季风气候，冬天很干燥，而且放眼望去看不到绿色，ict旁边种的树，光秃秃的只有树干。这种钢筋混凝土构筑起来的城市，我真的不喜欢。\n和南方的一些城市相比，北京真的不适宜居住，尤其对于南方人！\n参考 [1]. 新京报网：http://www.bjnews.com.cn/news/2015/03/28/358122.html\n[2]. 中新网：http://www.chinanews.com/tp/hd2011/2015/03-21/496005.shtml\n","permalink":"http://localhost:1313/posts/2015-03-30-one-month-in-beijing/","summary":"\u003cp\u003e2015年2月28日到达北京，到现在一月有余，是时候月度总结了。\u003c/p\u003e\n\u003ch1 id=\"这是我第三次来北京\"\u003e这是我第三次来北京\u003c/h1\u003e\n\u003cp\u003e2014年的6月份第一次来北京，去北大计算所实习十天。感受了一下北大计算所p老师及其博士生z的冷漠。我还记得刚到的那天晚上一个人孤零零的走在北京的大街上寻找北大方正员工宿舍的场景。每天重复着朝九晚十的固定模式，每个人都像机器一样的工作，实验室只有敲击键盘的声音，偶尔看到p的时候他也是阴沉着脸，临走的时候还被z批评了。虽然p承诺说只要我通过夏令营面试就会要我，但是这明显就是废话呀；而且这段不算愉快的实习经历让我对p的实验室产生了厌恶。\u003c/p\u003e\n\u003cp\u003e回学校之后开始准备期末考试和7月份的夏令营，并于2014年7月9日到达北京，这次的行程包括北大计算所和中科院计算所的夏令营。北大计算所因为机试和综合排名不占优势，败下阵来，不过现在想想当初幸好没有进p的实验室，要不然天天对着p的臭脸，估计要疯。\u003c/p\u003e\n\u003cp\u003e中科院计算所的风格完全不像北大，中科院真正做到了海纳百川，一视同仁，不像北大看不起校外学生。来北京之前和ict的h老师沟通过很多次，发现我和h老师的性格很相似，我非常崇拜h老师严谨的处事风格，经过认真的准备，我也顺利通过了ict的夏令营，然后跑去深圳siat玩耍了。\u003c/p\u003e\n\u003ch1 id=\"北京的物价真是贵\"\u003e北京的物价真是贵\u003c/h1\u003e\n\u003cp\u003e这次是来ict完成我的本科毕业设计的，大概要待到5月初。一来从家里不方便带太多东西，二来想想要在北京待三年，所以准备在北京重新置办生活用品。\u003c/p\u003e\n\u003cp\u003e2月28刚到北京，去了师兄推荐的家乐福大超市。超市很大，有两层，不过里面的东西真是贵。也就买了被子、三件套、桶、盆，花了四五百，很可恨的是，很薄的春秋单人被子，居然要169，这还是最便宜的，床垫比被子还贵，要199。\u003c/p\u003e\n\u003cp\u003e没必要的用品可以不买，但是饭不能不吃。以前在学校一天也就十几块，到北京后发现，一顿饭就十多块了。早餐一个鸡蛋要2块，一个小包子要1.5块，哎，想想以前自己家做早餐的时候1块钱4个大肉包子啊，当初怎么不多吃几个呢。\u003c/p\u003e\n\u003cp\u003e餐厅里的饭菜确实贵，不过国科大食堂的饭菜既便宜又好吃，赞一个。\u003c/p\u003e\n\u003ch1 id=\"北京的雾霾\"\u003e北京的雾霾\u003c/h1\u003e\n\u003cp\u003e来北京之前对北京的雾霾也略有耳闻，觉得那是北京人的小题大做，武汉也是扬尘满天飞，我也活得好好的啊；不过真来北京之后，确实受不了，放眼望去，街道上灰蒙蒙的一片，路上的特大广告牌都看不清。也许是雾霾加干燥的原因，刚来北京那一个星期，嗓子特别不舒服，每天早晨刷牙的时候都恶心干呕，不过后来慢慢好了。\u003c/p\u003e\n\u003cp\u003e3月28那天，北京还遭遇了沙尘暴，那场景真像外星人袭击地球。所以来北京，口罩是必需品。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"3月28日北京的沙尘暴\" loading=\"lazy\" src=\"https://i0.wp.com/y0.ifengimg.com/cmpp/2015/03/28/14/1120a1ac-4317-49ca-8114-5962823270e1_size48_w600_h400.jpg\"\u003e\n3月28日北京的沙尘暴[1]\u003c/p\u003e\n\u003ch1 id=\"北京的风景\"\u003e北京的风景\u003c/h1\u003e\n\u003cp\u003e来北京的前3个星期，忙于组内布置的任务，也是过着朝九晚十的生活，后来想想也不能天天这样，我应该出去透透气，于是选择了颐和园，3月份属于颐和园景区的淡季，加上学生证，门票不贵。颐和园不愧是皇家园林，里面的景色确实很漂亮，有万寿山、昆明湖，湖水还算干净。\u003c/p\u003e\n\u003cp\u003e我去的那天天气很好，空气质量也不错；人虽多，但不算拥挤，部分原因是颐和园真是太大了。我在游玩的过程中看到了很多外国旅游团，听到过英语、法语、日语、韩语、粤语、中文，让我默默联想起了八国联军侵略中国的场景。\u003c/p\u003e\n\u003cp\u003e在游玩的过程中遇到了一个很温馨的“外国小家庭”，妈妈是外国人，爸爸应该也是外国人，爸爸和妈妈聊天的时候是用英语，但是爸爸和儿子却是用中文，他们还有一个坐在婴儿车里的小女儿，儿子坐在湖边玩ipad，小女儿在车里哭个不停…\u003c/p\u003e\n\u003cp\u003e虽然颐和园的风景很好，但是里面居然有很多蚊子（小虫子）！期间有一个虫子撞到我的嘴巴，还有一个钻进了我的耳朵。可能是春天到了，万物复苏，加上颐和园内潮湿，植物丰富，给虫子提供了很好的环境。\u003c/p\u003e\n\u003cp\u003e下面放几张游玩时拍的照片。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"谁能告诉我这是什么乐器\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace1.jpg\"\u003e\n谁能告诉我这是什么乐器\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"学太极的外国人\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace2.jpg\"\u003e\n学太极的外国人\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"这也是迎春花？\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace3.jpg\"\u003e\n这也是迎春花？\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"这个角度的颐和园很美\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace4.jpg\"\u003e\n这个角度的颐和园很美\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"长廊\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace5.jpg\"\u003e\n长廊\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"准备回去的时候看到的，各种长枪大炮，据说是在抓拍一种珍稀鸟类。。。\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace6.jpg\"\u003e\n准备回去的时候看到的，各种长枪大炮，据说是在抓拍一种珍稀鸟类。。。\u003c/p\u003e\n\u003cp\u003e颐和园的风景虽好，但就自然风光来说，比不上南方早已复苏的大自然。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"家乡的油菜花，比颐和园的迎春花漂亮多了-:)\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/rape-flowers.jpg\"\u003e\n家乡的油菜花，比颐和园的迎春花漂亮多了-:)\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"武大樱花已经争奇斗艳了，北方还是萧瑟一片\" loading=\"lazy\" src=\"https://i0.wp.com/i2.chinanews.com/simg/hd/2015/03/21/7e9527ca232f4253bfb86eef15cee517.jpg\"\u003e\n武大樱花已经争奇斗艳了，北方还是萧瑟一片[2]\u003c/p\u003e\n\u003ch1 id=\"后记\"\u003e后记\u003c/h1\u003e\n\u003cp\u003e没来北京之前，我想着以后一定要在北京工作，还要争取在北京安家落户。虽然北京生活压力很大，但是机会和挑战也多，这不就是我要的生活吗；而且北京是全国的政治、经济和文化中心，可以很方便的去感受中国几千年的文化和历史。在北京生活一个月之后，我的想法变了，很重要的原因是北京的雾霾和气候。雾霾不是一个人一天两天能够解决的，你必须尽量减少不必要的出行，跑步健身什么的就更别提了；北京属于北温带季风气候，冬天很干燥，而且放眼望去看不到绿色，ict旁边种的树，光秃秃的只有树干。这种钢筋混凝土构筑起来的城市，我真的不喜欢。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e和南方的一些城市相比，北京真的不适宜居住，尤其对于南方人！\u003c/strong\u003e\u003c/p\u003e\n\u003ch1 id=\"参考\"\u003e参考\u003c/h1\u003e\n\u003cp\u003e[1]. 新京报网：\u003ca href=\"http://www.bjnews.com.cn/news/2015/03/28/358122.html\"\u003ehttp://www.bjnews.com.cn/news/2015/03/28/358122.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[2]. 中新网：\u003ca href=\"http://www.chinanews.com/tp/hd2011/2015/03-21/496005.shtml\"\u003ehttp://www.chinanews.com/tp/hd2011/2015/03-21/496005.shtml\u003c/a\u003e\u003c/p\u003e","title":"One month in Beijing"},{"content":"在上一题POJ Problem 1837: Balance中，我们曾讲到，如果只有两个挂钩，问题会好办得多，就拿题目给的样例数据来说：\nSample Input 2 4 -2 3 3 4 5 8 Sample Output 2 如上图所示，给定重量为3,4,5,8的砝码，放在一个左右臂长分别为2和3的天平上，要使天平平衡，问有多少种方法。\n这个问题可以稍加转换，假设放在左边的重量为x，右边为y，则有如下方程组成立：\n$$ \\begin{cases} x+y=3+4+5+8=20\\\\ 2x=3y \\end{cases} $$马上解出x=12,y=8。这样就相当于把原问题转换为：已知序列3,4,5,8，问从中取若干个数使和为12（或8）的方案数有多少个？ 因为取出数字和为8，则剩余和为12，所以和为8和12的方案数是相等的。\n因为这里只有4个数字，一眼就能看出有(3,4,5)，(4,8)能使和为12，即只有两种方案。如果给的数字较多较大，该怎样写代码求出呢？可以使用动态规划求解。\n设dp[i][j]表示从前i个数中选若干个数使得和为j的方案数，则我们可以得到这样的状态转换方程：\n$$ \\begin{cases} dp[i][j]=1\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\text{if}i=0\\\u0026\\\u0026j=0\\\\ dp[i][j]=dp[i-1][j]\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\text{if}w[i]\u003ej\\\\ dp[i][j]=dp[i-1][j]+dp[i-1][j-w[i]]\\qquad\\quad\\text{if}w[i]\u003c=j \\end{cases} $$ 当i=0\u0026amp;\u0026amp;j=0时，dp[i][j]=1表示从0个数中取若干个数使得和为0，当然只有1种方案，那就是什么都不取 当w[i]\u0026gt;j时，第i个数用不上，因为你单个数字都超过j了，怎么使和为j呢，所以直接dp[i][j]=dp[i-1][j] 当w[i]\u0026lt;=j时，第i个数可以用了，这个时候分两种情况，用或者不用第i个数，如果不用，则和w[i]\u0026gt;j时一样dp[i][j]=dp[i-1][j]，如果用的话，则要从前i-1个数中取若干个数使和为j-w[i]，也就是dp[i-1][j-w[i]]，这样总的方案数就是用和不用第i个数的方案数之和，即dp[i][j]=dp[i-1][j]+dp[i-1][j-w[i]] 下面是针对这个例子我手算的一个图：\n以上面的内容设计一个OJ题如下：\n描述： 给定一个正整数数字序列，从中取出若干个数字，使得这些数字之和为某一个特定的值，求所有取法的方案数。 输入： 输入包含多个测试用例，每个测试用例的第一行有两个数N,S，N表示这个数字序列共有多少个数字；S表示取出的数字之和为S。后面一行包含N个正整数。 N,S为0程序结束 输出： 每个测试用例输出一行，表示从N个数中取若干个数使得和为S的方案总数。 样例输入： 4 8 3 4 5 8 4 12 3 4 5 8 10 10 10 9 8 7 6 5 4 3 2 1 0 0 样例输出： 2 2 10 知道了状态转换方程，我们可以很快的写出以上OJ的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #include \u0026lt;algorithm\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; int main() { int n, s, sum; while (cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; s \u0026amp;\u0026amp; n \u0026amp;\u0026amp; s) { vector\u0026lt;int\u0026gt; w(n + 1); vector\u0026lt;int\u0026gt; dp(s + 1, 0); sum = 0; w[0] = 0; /* 额外添加的第0个数字为0 */ for (int i = 1; i \u0026lt;= n; i++) { cin \u0026gt;\u0026gt; w[i]; sum += w[i]; /* 所有数字之和 */ } if (sum \u0026lt; s) /* 如果所有数字加起来都小于s，则怎么取都不存在和为s的方案 */ { cout \u0026lt;\u0026lt; \u0026#34;0\u0026#34; \u0026lt;\u0026lt; endl; continue; } sort(w.begin(), w.end()); /* 首先对这些数字从小到大排序，因为取大的数字的时候会用到取小的数字的结果 */ dp[0] = 1; /* 相当于dp[0][0]=1; */ for (int i = 1; i \u0026lt;= n; i++) { for (int j = s; j \u0026gt;= 1; j – ) /* 从后往前测试，这样只需要一行空间 */ { if (w[i] \u0026lt;= j) dp[j] += dp[j - w[i]]; } } cout \u0026lt;\u0026lt; dp[s] \u0026lt;\u0026lt; endl; } return (0); } 代码中添加了几个操作，首先如果所有数字之和都小于s，则肯定无解；其次，我们先对数字序列从小到大排序，这样DP填表；最后我们填表的时候是从右往左填的，这样只需要一行空间dp[j]，而不是二维dp[i][j]。\n","permalink":"http://localhost:1313/posts/2014-11-15-subset-sum-problem/","summary":"\u003cp\u003e在上一题POJ Problem 1837: Balance中，我们曾讲到，如果只有两个挂钩，问题会好办得多，就拿题目给的样例数据来说：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSample Input\n2 4\n-2 3\n3 4 5 8\n\nSample Output\n2\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cimg alt=\"number-balance.png\" loading=\"lazy\" src=\"/posts/2014-11-15-subset-sum-problem/number-balance.png\"\u003e\n如上图所示，给定重量为3,4,5,8的砝码，放在一个左右臂长分别为2和3的天平上，要使天平平衡，问有多少种方法。\u003c/p\u003e\n\u003cp\u003e这个问题可以稍加转换，假设放在左边的重量为x，右边为y，则有如下方程组成立：\u003c/p\u003e\n$$\n\\begin{cases}\nx+y=3+4+5+8=20\\\\\n2x=3y\n\\end{cases}\n$$\u003cp\u003e马上解出x=12,y=8。这样就相当于把原问题转换为：\u003cstrong\u003e已知序列3,4,5,8，问从中取若干个数使和为12（或8）的方案数有多少个？\u003c/strong\u003e 因为取出数字和为8，则剩余和为12，所以和为8和12的方案数是相等的。\u003c/p\u003e\n\u003cp\u003e因为这里只有4个数字，一眼就能看出有(3,4,5)，(4,8)能使和为12，即只有两种方案。如果给的数字较多较大，该怎样写代码求出呢？可以使用动态规划求解。\u003c/p\u003e\n\u003cp\u003e设dp[i][j]表示从前i个数中选若干个数使得和为j的方案数，则我们可以得到这样的状态转换方程：\u003c/p\u003e\n$$\n\\begin{cases}\ndp[i][j]=1\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\text{if}i=0\\\u0026\\\u0026j=0\\\\\ndp[i][j]=dp[i-1][j]\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\text{if}w[i]\u003ej\\\\\ndp[i][j]=dp[i-1][j]+dp[i-1][j-w[i]]\\qquad\\quad\\text{if}w[i]\u003c=j\n\\end{cases}\n$$\u003col\u003e\n\u003cli\u003e当i=0\u0026amp;\u0026amp;j=0时，dp[i][j]=1表示从0个数中取若干个数使得和为0，当然只有1种方案，那就是什么都不取\u003c/li\u003e\n\u003cli\u003e当w[i]\u0026gt;j时，第i个数用不上，因为你单个数字都超过j了，怎么使和为j呢，所以直接dp[i][j]=dp[i-1][j]\u003c/li\u003e\n\u003cli\u003e当w[i]\u0026lt;=j时，第i个数可以用了，这个时候分两种情况，用或者不用第i个数，如果不用，则和w[i]\u0026gt;j时一样dp[i][j]=dp[i-1][j]，如果用的话，则要从前i-1个数中取若干个数使和为j-w[i]，也就是dp[i-1][j-w[i]]，这样总的方案数就是用和不用第i个数的方案数之和，即dp[i][j]=dp[i-1][j]+dp[i-1][j-w[i]]\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e下面是针对这个例子我手算的一个图：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"20141115_170507.jpg\" loading=\"lazy\" src=\"/posts/2014-11-15-subset-sum-problem/20141115_170507.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e以上面的内容设计一个OJ题如下：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e描述：\n给定一个正整数数字序列，从中取出若干个数字，使得这些数字之和为某一个特定的值，求所有取法的方案数。\n\n输入：\n输入包含多个测试用例，每个测试用例的第一行有两个数N,S，N表示这个数字序列共有多少个数字；S表示取出的数字之和为S。后面一行包含N个正整数。\nN,S为0程序结束\n\n输出：\n每个测试用例输出一行，表示从N个数中取若干个数使得和为S的方案总数。\n\n样例输入：\n4 8\n3 4 5 8\n4 12\n3 4 5 8\n10 10\n10 9 8 7 6 5 4 3 2 1\n0 0\n\n样例输出：\n2\n2\n10\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e知道了状态转换方程，我们可以很快的写出以上OJ的代码：\u003c/p\u003e","title":"从一个数字序列中取若干个数字使得和为某个数，问共有多少种取数方案？"},{"content":"一、第一次使用Github的步骤： 在这个页面中填写Repo名称 不要勾选Initialize this repository with a README 点击Create repository 在本地使用Git命令行工具进入到和第1步填写Repo相同名称的文件夹中（此文件夹中已包含你要push到Github上的内容），执行以下几个命令： 1 2 3 4 5 6 git init touch README.md #optional git add . git commit -m \u0026#39;your comment\u0026#39; git remote add origin https://github.com/UserName/RepoName git push origin master 如果你在第2步中勾选了Initialize this repository with a README，那么在第4步中省略touch README.md并且在git add .之前，执行第5行代码，然后git pull origin master将远端（remote）的内容pull到本地 关于Git命令中的fetch和pull的区别，请看这篇博文 关于Git bash和Github的连接，请看这篇博文 二、Git命令中fetch和pull的区别（转载） Git中从远程的分支获取最新的版本到本地有这样2个命令：\ngit fetch：相当于是从远程获取最新版本到本地，不会自动merge 1 2 3 git fetch origin master git log -p master..origin/master git merge origin/master 以上命令的含义：首先从远程的origin的master主分支下载最新的版本到origin/master分支上，然后比较本地的master分支和origin/master分支的差别，最后进行合并。\n上述过程其实可以用以下更清晰的方式来进行：\n1 2 3 git fetch origin master:tmp git diff tmp git merge tmp 从远程获取最新的版本到本地的test分支上，之后再进行比较合并。\ngit pull：相当于是从远程获取最新版本并merge到本地 1 git pull origin master 上述命令其实相当于git fetch + git merge。\n在实际使用中，git fetch更安全一些，因为在merge前，我们可以查看更新情况，然后再决定是否合并。\n","permalink":"http://localhost:1313/posts/2014-11-11-git-notes/","summary":"\u003ch1 id=\"一第一次使用github的步骤\"\u003e一、第一次使用Github的步骤：\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e在\u003ca href=\"https://github.com/new\"\u003e这个页面\u003c/a\u003e中填写Repo名称\u003c/li\u003e\n\u003cli\u003e不要勾选Initialize this repository with a README\u003c/li\u003e\n\u003cli\u003e点击Create repository\u003c/li\u003e\n\u003cli\u003e在本地使用Git命令行工具进入到和第1步填写Repo相同名称的文件夹中（此文件夹中已包含你要push到Github上的内容），执行以下几个命令：\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e6\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit init\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etouch README.md \u003cspan style=\"color:#75715e\"\u003e#optional\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit add .\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit commit -m \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;your comment\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit remote add origin https://github.com/UserName/RepoName\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit push origin master\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003col start=\"5\"\u003e\n\u003cli\u003e如果你在第2步中勾选了Initialize this repository with a README，那么在第4步中省略touch README.md并且在git add .之前，执行第5行代码，然后git pull origin master将远端（remote）的内容pull到本地\u003c/li\u003e\n\u003cli\u003e关于Git命令中的fetch和pull的区别，请看\u003ca href=\"https://blog.csdn.net/wfdtxz/article/details/8632811\"\u003e这篇博文\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e关于Git bash和Github的连接，请看\u003ca href=\"https://www.cnblogs.com/fnng/archive/2011/08/25/2153807.html\"\u003e这篇博文\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"二git命令中fetch和pull的区别转载\"\u003e二、Git命令中fetch和pull的区别（\u003ca href=\"https://blog.csdn.net/wfdtxz/article/details/8632811\"\u003e转载\u003c/a\u003e）\u003c/h1\u003e\n\u003cp\u003eGit中从远程的分支获取最新的版本到本地有这样2个命令：\u003c/p\u003e","title":"Git相关笔记"},{"content":"\n基本信息 论文标题：Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm 作者单位：阿里巴巴 论文链接：https://arxiv.org/abs/2510.14321 来源：arxiv 一、问题 电商emb召回场景，目前的方法都是直接字面语义上的对比学习训练（direct-embedding methods），即q2i的对比学习训练。对于复杂、困难的query，语义理解能力不足，比如下图Fig1中的query=\u0026ldquo;比茶更提神的饮料\u0026rdquo;，仍然会召回很多茶，因为字面理解没有理解query背后的深层含义。\n二、方法 使用LLM强大的推理能力（reasoning），先推理出CoT，然后基于CoT再产emb。比如上面的例子中，经过LLM推理之后，推理出咖啡、红牛等关键词，通过这些关键词再去产emb然后召回，效果就好很多。\n2.1 训练样本构造方法 如下图Fig2中的Data Construction部分：\n收集线上query，尤其是那种困难query，就是在现有direct-embedding表现不好的query 把这些query喂给现有召回模型，得到召回商品集合① 然后使用强大的Qwen3-30B-A3B-Instruct生产CoT扩展信息 Unconstrained Reasoning：首先不加任何限制地生产CoT，尽可能利用大模型的世界知识和推理能力，生产充分完全的CoT信息 Information Extraction：由于上一步产出的CoT信息太长了，不利于线上推理，因此把上一步产出的CoT和原始query再次输入给大模型，让大模型抽取其中的关键信息，以keyword list形式输出 Post Processing：最后对上一步抽取的关键词进行后处理，去除重复词，去除query中已有的词等，得到精简、干净的关键词列表，列表最大长度是16 接着把query和CoT喂给已有的向量召回模型，得到扩展的召回商品集合② 由于要训练模型的Reasoning能力，所以只取出集合②-①的差集部分，这部分是CoT带来的增益商品集合 最后使用相关性模型对商品集合②-①进行过滤，过滤出相关的商品 通过上述步骤，产出约7.5kw的\u0026lt;query, CoT, item\u0026gt;三元组 把上述样本划分成两部分，7.1kw的\u0026lt;query, CoT, item\u0026gt;三元组用于Cold start预训练；剩余400w的\u0026lt;query, item\u0026gt;用于RL微调 2.2 Cold Start预训练 对应图Fig2左下角部分，该模块通过大规模的\u0026lt;query, CoT,item\u0026gt;三元组数据预训练，想要达到两个目的：一是让基础模型具备think能力；二是让基础模型产出的emb和下游q2i任务对齐。\n这里使用的基础模型是Qwen2.5-3B-Instruct，比生产CoT的模型（Qwen3-30B-A3B-Instruct）小，其实也有点蒸馏的感觉，把大模型的CoT能力蒸馏到小模型中。\n训练任务包括两个，一个是CoT的NTP loss（对应图中的SFT loss），另一个是q2i的对比学习InfoNCE loss。query塔和item塔共享参数，他们的emb都是最后一个特殊token \u0026lt;emb\u0026gt; 的emb。\nLoss组合： 2.3 RL微调 上一步的SFT主要进行模仿学习，模仿更大的大模型的think能力，小模型本身的reasoning能力受限，接下来需要用GRPO对小模型进行RL微调。RL微调同时对生产CoT和生产emb两个任务都有作用，具体看下面的reward：\nRL微调设计了3个reward：\nFormat Reward：产出的CoT格式符合“\u0026lt;think\u0026gt; Specific CoT \u0026lt;/think\u0026gt;\u0026lt;emb\u0026gt;”就得1分，否则得0分 Length Reward：产出的CoT格式符合长度限制（\u0026lt;=16）就得1分，否则得0分 Retrieval Accuracy Reward：联合原始query和产出的CoT产出的增强query emb，与batch内所有的item emb求相似度，正确item所在的排名为\\(rank(d_i)\\)，再根据公式12计算一个排名的reward。核心思想是：正确的item与query的相似度排名越高则reward越大（即rank值越小则reward越大）。 最后，上述3个reward通过三个β系数组合起来： RL的训练目标，GRPO loss如下： 在RL阶段，除了有GRPO loss，原有的InfoNCE对比学习loss也还在，两个loss通过系数γ组合起来，如公式16所示。\n2.4 训练细节 产CoT的推理模型：Qwen3-30B-A3B-Instruct emb基座模型：Qwen2.5-3B-Instruct 训练资源：128 GPUs 预训练阶段： CoT最大长度：16 loss系数：\\(\\lambda_1=0.1, \\lambda_2=1\\)，也就是InfoNCE loss是主导的，NTP的loss权重很小 batchsize=128 lr=1e-5，cosine scheduler with a warmup ratio of 0.03 训练1个epoch RL阶段： GRPO每次采样8个CoT length reward长度阈值16 三个reward系数：\\(\\beta_1=0.5, \\beta_2=0.2, \\beta_3=1\\)，\\(\\beta_3\\)最大，即准度的reward最重要 loss系数\\(\\gamma_1=1,\\gamma_2=0.1\\)，即GRPO的loss权重最大 batchsize=256，RL阶段的batchsize是预训练阶段的2倍 lr=1e-6，cosine scheduler with a warmup ratio of 0.03，RL阶段的lr比预训练小 训练1个epoch 三、结果 如下图所示，最后一行LREM(Cold Start+RL)效果最好，但是LREM(Cold Start)效果很差啊，比Qwen2.5的好几个base都差。。。这就很奇怪了，理论上LREM(Cold Start)去掉CoT和SFT loss的结构和Qwen2.5 (Uni-Attn. Last)的结构是完全一样的，但是前者的指标比后者差很多，难道是加了CoT和SFT loss有负向影响？ LREM(Cold Start+RL)比LREM(Cold Start)提升非常大，也能说明LREM(Cold Start)效果很差。但是不应该呀，RL的几个reward，理论上在预训练阶段都有训练到，即使对于准度的reward3，其实也相当于positive在in-batch内的相似度要大于其他negative，本质上和InfoNCE loss的目的是一致的，为啥换成RL的形式后提升这么大？ 作者还分析了CoT的作用，把CoT换成空的、随机字符串、或者单纯重复query，效果都下降很多，说明CoT很重要。 四、评价 问题切入点很好，特别是在相关性、召回场景，query更加多样，困难的query也更多，而且本文主要就是解决困难query的场景，但是这种情况在线上的占比应该很小？作者没讲 CoT数据的生产经过了很多步骤，看得出来经过了多轮迭代优化，也说明这个环节有很多坑，直接用更大模型产出的无约束的CoT可能效果不行。。。 本文只针对query进行了CoT扩张，能不能对item也扩展一下呢？ 本文最终效果提升很大，但是这个提升真的来自RL吗，感觉有点怀疑呀。个人感觉CoT的信息可能更重要一点，需要补充一个对比实验，即用emb基座模型直接加入本文产出的CoT进行对比学习预训练，不加后面的RL，看看效果怎么样。就是CoT只作为附加特征，相当于LREM(Cold Start)基础上去掉SFT loss，感觉这样就能取得不错的效果吧。 ","permalink":"http://localhost:1313/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm\u003c/li\u003e\n\u003cli\u003e作者单位：阿里巴巴\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/abs/2510.14321\"\u003ehttps://arxiv.org/abs/2510.14321\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：arxiv\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"一问题\"\u003e一、问题\u003c/h1\u003e\n\u003cp\u003e电商emb召回场景，目前的方法都是直接字面语义上的对比学习训练（direct-embedding methods），即q2i的对比学习训练。对于复杂、困难的query，语义理解能力不足，比如下图Fig1中的query=\u0026ldquo;比茶更提神的饮料\u0026rdquo;，仍然会召回很多茶，因为字面理解没有理解query背后的深层含义。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-Fig1.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"二方法\"\u003e二、方法\u003c/h1\u003e\n\u003cp\u003e使用LLM强大的推理能力（reasoning），先推理出CoT，然后基于CoT再产emb。比如上面的例子中，经过LLM推理之后，推理出咖啡、红牛等关键词，通过这些关键词再去产emb然后召回，效果就好很多。\u003c/p\u003e\n\u003ch2 id=\"21-训练样本构造方法\"\u003e2.1 训练样本构造方法\u003c/h2\u003e\n\u003cp\u003e如下图Fig2中的Data Construction部分：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e收集线上query，尤其是那种困难query，就是在现有direct-embedding表现不好的query\u003c/li\u003e\n\u003cli\u003e把这些query喂给现有召回模型，得到召回商品集合①\u003c/li\u003e\n\u003cli\u003e然后使用强大的Qwen3-30B-A3B-Instruct生产CoT扩展信息\n\u003cul\u003e\n\u003cli\u003eUnconstrained Reasoning：首先不加任何限制地生产CoT，尽可能利用大模型的世界知识和推理能力，生产充分完全的CoT信息\u003c/li\u003e\n\u003cli\u003eInformation Extraction：由于上一步产出的CoT信息太长了，不利于线上推理，因此把上一步产出的CoT和原始query再次输入给大模型，让大模型抽取其中的关键信息，以keyword list形式输出\u003c/li\u003e\n\u003cli\u003ePost Processing：最后对上一步抽取的关键词进行后处理，去除重复词，去除query中已有的词等，得到精简、干净的关键词列表，列表最大长度是16\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e接着把query和CoT喂给已有的向量召回模型，得到扩展的召回商品集合②\n\u003cul\u003e\n\u003cli\u003e由于要训练模型的Reasoning能力，所以只取出集合②-①的差集部分，这部分是CoT带来的增益商品集合\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e最后使用相关性模型对商品集合②-①进行过滤，过滤出相关的商品\u003c/li\u003e\n\u003cli\u003e通过上述步骤，产出约7.5kw的\u0026lt;query, CoT, item\u0026gt;三元组\u003c/li\u003e\n\u003cli\u003e把上述样本划分成两部分，7.1kw的\u0026lt;query, CoT, item\u0026gt;三元组用于Cold start预训练；剩余400w的\u0026lt;query, item\u0026gt;用于RL微调\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-Fig2.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"22-cold-start预训练\"\u003e2.2 Cold Start预训练\u003c/h2\u003e\n\u003cp\u003e对应图Fig2左下角部分，该模块通过大规模的\u0026lt;query, CoT,item\u0026gt;三元组数据预训练，想要达到两个目的：一是让基础模型具备think能力；二是让基础模型产出的emb和下游q2i任务对齐。\u003c/p\u003e\n\u003cp\u003e这里使用的基础模型是Qwen2.5-3B-Instruct，比生产CoT的模型（Qwen3-30B-A3B-Instruct）小，其实也有点蒸馏的感觉，把大模型的CoT能力蒸馏到小模型中。\u003c/p\u003e\n\u003cp\u003e训练任务包括两个，一个是CoT的NTP loss（对应图中的SFT loss），另一个是q2i的对比学习InfoNCE loss。query塔和item塔共享参数，他们的emb都是最后一个特殊token \u0026lt;emb\u0026gt; 的emb。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-formula4.png\"\u003e\n\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-formula5-7.png\"\u003e\u003c/p\u003e\n\u003cp\u003eLoss组合：\n\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-formula8.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"23-rl微调\"\u003e2.3 RL微调\u003c/h2\u003e\n\u003cp\u003e上一步的SFT主要进行模仿学习，模仿更大的大模型的think能力，小模型本身的reasoning能力受限，接下来需要用GRPO对小模型进行RL微调。RL微调同时对生产CoT和生产emb两个任务都有作用，具体看下面的reward：\u003c/p\u003e\n\u003cp\u003eRL微调设计了3个reward：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFormat Reward：产出的CoT格式符合“\u0026lt;think\u0026gt; Specific CoT \u0026lt;/think\u0026gt;\u0026lt;emb\u0026gt;”就得1分，否则得0分\u003c/li\u003e\n\u003cli\u003eLength Reward：产出的CoT格式符合长度限制（\u0026lt;=16）就得1分，否则得0分\u003c/li\u003e\n\u003cli\u003eRetrieval Accuracy Reward：联合原始query和产出的CoT产出的增强query emb，与batch内所有的item emb求相似度，正确item所在的排名为\\(rank(d_i)\\)，再根据公式12计算一个排名的reward。核心思想是：正确的item与query的相似度排名越高则reward越大（即rank值越小则reward越大）。\n\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-formula11-12.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e最后，上述3个reward通过三个β系数组合起来：\n\u003cimg loading=\"lazy\" src=\"/posts/2025-12-13-alibaba-large-reasoning-embedding-model-lrem-paper-reading/LREM-formula13.png\"\u003e\u003c/p\u003e","title":"论文阅读：Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm"},{"content":"本博客使用的评论系统是Giscus，日常使用没啥问题，但是当博客内容很长的时候，就会出现URI_TOO_LONG的问题：\nAn error occurred URI_TOO_LONG hkg1:: 例如下面两篇博客：\n2017年国庆旅行——郑州、杭州 伪·2018届校招面经 正常Giscus评论 异常Giscus评论 针对这个问题，网上搜URI_TOO_LONG说的都是网页请求的URI太长导致的，但都没找到和Giscus相关的内容。\n后来在Giscus的Github Issue里找到一个相同的问题：https://github.com/giscus/giscus/issues/1340，里面一个人提到和Hugo所用的主题有关，另一个人提到和博客的meta name=\u0026quot;description\u0026quot;内容太长有关。但都蜻蜓点水，说的不是很详细，也没有给出一个通用的解决办法。\n后来怀疑这个报错可能是在加载Giscus评论系统的时候，发起的URI请求太长有关。因此针对出问题的博客，通过Chrome右键检查，找到Giscus模块出现的URI链接，如下图所示。\n光这么看看不出来这个URI的长度，可以把这个URI拷贝出来，粘贴到Word中，你会发现这个URI真的非常非常长，而且包含了完整的博客正文！进一步分析发现是URI中的description字段非常长，包含完整的博客正文。\n定位到问题之后，解决办法就很简单了，目标就是如何缩短博客html代码中的description字段长度。这里有很多种方法，最简单的方法就是，在每篇博客的头信息区域，增加自定义配置的description内容，简短一点就行，我就直接复用了博客标题。通过这种方法就缩短了Giscus发起URI中的description字段长度了。\n此外，还可以在主题中搜一下meta name=\u0026quot;description\u0026quot;出现的位置，把里面的代码改掉或者注释掉，都能解决这个问题。\n","permalink":"http://localhost:1313/posts/2025-11-09-resolve-the-uri-too-long-error-of-giscus-in-hugo/","summary":"\u003cp\u003e本博客使用的评论系统是\u003ca href=\"https://github.com/giscus/giscus\"\u003eGiscus\u003c/a\u003e，日常使用没啥问题，但是当博客内容很长的时候，就会出现\u003ccode\u003eURI_TOO_LONG\u003c/code\u003e的问题：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAn error occurred\n\nURI_TOO_LONG\n\nhkg1::\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e例如下面两篇博客：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://bitjoy.net/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/\"\u003e2017年国庆旅行——郑州、杭州\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bitjoy.net/posts/2018-02-04-2018-campus-recruiting/\"\u003e伪·2018届校招面经\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e正常Giscus评论\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e异常Giscus评论\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-11-09-resolve-the-uri-too-long-error-of-giscus-in-hugo/normal-giscus.png\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-11-09-resolve-the-uri-too-long-error-of-giscus-in-hugo/abnormal-giscus.png\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e针对这个问题，网上搜\u003ccode\u003eURI_TOO_LONG\u003c/code\u003e说的都是网页请求的URI太长导致的，但都没找到和Giscus相关的内容。\u003c/p\u003e\n\u003cp\u003e后来在Giscus的Github Issue里找到一个相同的问题：\u003ca href=\"https://github.com/giscus/giscus/issues/1340\"\u003ehttps://github.com/giscus/giscus/issues/1340\u003c/a\u003e，里面一个人提到和Hugo所用的主题有关，另一个人提到和博客的\u003ccode\u003emeta name=\u0026quot;description\u0026quot;\u003c/code\u003e内容太长有关。但都蜻蜓点水，说的不是很详细，也没有给出一个通用的解决办法。\u003c/p\u003e\n\u003cp\u003e后来怀疑这个报错可能是在加载Giscus评论系统的时候，发起的URI请求太长有关。因此针对出问题的博客，通过Chrome右键\u003ccode\u003e检查\u003c/code\u003e，找到Giscus模块出现的URI链接，如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-11-09-resolve-the-uri-too-long-error-of-giscus-in-hugo/giscus-analysis.png\"\u003e\u003c/p\u003e\n\u003cp\u003e光这么看看不出来这个URI的长度，可以把这个URI拷贝出来，粘贴到Word中，你会发现这个URI真的非常非常长，而且包含了完整的博客正文！进一步分析发现是URI中的\u003ccode\u003edescription\u003c/code\u003e字段非常长，包含完整的博客正文。\u003c/p\u003e\n\u003cp\u003e定位到问题之后，解决办法就很简单了，目标就是如何缩短博客html代码中的\u003ccode\u003edescription\u003c/code\u003e字段长度。这里有很多种方法，最简单的方法就是，在每篇博客的头信息区域，增加自定义配置的\u003ccode\u003edescription\u003c/code\u003e内容，简短一点就行，我就直接复用了博客标题。通过这种方法就缩短了Giscus发起URI中的\u003ccode\u003edescription\u003c/code\u003e字段长度了。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-11-09-resolve-the-uri-too-long-error-of-giscus-in-hugo/giscus-solution.png\"\u003e\u003c/p\u003e\n\u003cp\u003e此外，还可以在主题中搜一下\u003ccode\u003emeta name=\u0026quot;description\u0026quot;\u003c/code\u003e出现的位置，把里面的代码改掉或者注释掉，都能解决这个问题。\u003c/p\u003e","title":"解决Hugo的Giscus评论系统出现URI_TOO_LONG的问题"},{"content":"\n基本信息 论文标题：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID 作者单位：Meta 论文链接：https://arxiv.org/pdf/2504.02137 来源：RecSys 2025 Motivation：论文要解决的问题是什么 搜推广的模型严重依赖于item id embedding的表征质量，但在工业场景下，搜推广的id表征存在如下挑战：\nid量级非常大，常常是数十亿甚至是百亿的规模。因此，通常不可能给每个id一个单独的embedding（即文中的individual embedding, IE），IE的成本太高 id分布非常不均匀，马太效应严重。文中统计：0.1%的头部item占据了25%的曝光量；5.5%的腰部item占据了50%的曝光量；94.4%的尾部item只占据了25%的曝光量 id分布漂移严重：搜推广场景中item的变化非常频繁，无时无刻不在发生着新id的产生和旧id的退出，而且不同id存活的时间周期也不尽相同，所以id的准入准出策略很难完美适配所有item 针对上述问题，常见的做法是对item id采用hash然后查emb的方式（即文中的random hash，RH），将所有id hash到一个固定大小的空间，然后查emb。但是RH方式有如下缺点：\n存在hash冲突，把不相关的id hash到一个桶里，导致语义混乱，学习效果不佳 无法解决id分布漂移的问题，比如hash到同一个桶的A、B两个id，如果B出现频率变高，则会带偏A的分布，影响了A的效果 无法进行知识共享，例如新出了商品iphone15，iphone15无法共享到老的iphone14的emb知识，iphone15的id emb必须完全重新学习。针对这种情况，作者做了一个更加极端的AA实验，就是copy一个完全相同的商品，只换item id，如果是IE或者RH策略，则新商品由于id emb是随机初始化的，效果不佳，这是id-based的通病 基于前缀n-gram的semantic id表征方法 针对上述问题，作者沿用了semantic id的思路，首先使用内容理解团队产出的文本、图片等多模态emb，然后基于过去3个月的item多模态emb，训练RQ-VAE模型，并产出所有item的semantic id。\n上述过程都是常规操作，重点在于如何基于semantic id得到item emb表征。假设semantic id是L层，每层的codebook size是K：\n最常规的做法：每层都初始化一个K*d的emb table，每层sid查各自的emb table，然后把L层的sid emb加起来。但是本文完全没有提这种方法，也没有和这种方法比较，非常奇怪。 为了比较，我个人再详细描述下这种常规做法。比如老item A的sid是(c1,c2,c3)；新来一个item B，它的sid是(c1,c2,c4)。用常规方法，A的emb是c1+c2+c3，B的emb是c1+c2+c4。两者c1、c2是可以共享的，所以常规方法也能起到一定的知识共享的效果，共享项有2项：c1、c2。但是因为RQ-VAE的沙漏问题，c2很有可能是沙漏瓶颈，信息量不足。 作者对比了Table 1中的几种方法： Trigram和Fourgram差不多，如果L=3用Trigram、L=4用Fourgram的话，本质上是把L个sid映射成了一个无冲突的int。但是这种方法映射出来的int数量太多了，是\\(K^L\\)。如果K=1024、L=3，则\\(K^L\\)就已经超过10亿了，这和直接无冲突的IE方法一样了，而且存在新id无法共享老id学到的知识的问题 All bigrams，就是所有的sid的2-gram。还是上面的例子，A的emb相当于\\(c_1c_2+c_2c_3\\)，B的emb相当于\\(c_1c_2+c_2c_4\\)，两者可共享\\(c_1c_2\\)项，相比于常规方法，虽然共享项数变少了，但粒度更精细了，孰好孰坏未可知。由Table 2可知，All bigrams的效果至少比Trigram和Fourgram好很多了，而且如果层数L越大，可共享项越多 Prefix-ngram（简称Prefix-SID方法），本文提出的新方法，把所有前缀组合成新id查emb，然后所有emb再求和。还是上面的例子，A的emb相当于\\(c_1+c_1c_2+c_1c_2c_3+c_2+c_2c_3+c_3\\)，B的emb相当于\\(c_1+c_1c_2+c_1c_2c_4+c_2+c_2c_4+c_4\\)，两者可共享\\(c_1, c_1c_2, c_2\\)三项，比之前的所有方法可共享的信息都多，而且如果层数L越大，可共享项越多，因此这种方法的效果最好，训练也最稳定 实验结果很丰富，做了很多分析，Prefix-SID方法有如下优势：\n相比于IE和RH方法，Prefix-SID方法对中长尾item的提升尤其显著，因为新id和老id的表征有了知识共享 对id分布漂移问题更不敏感：由于电商模型训练时消费数据的顺序是和数据的时间一致的，比如一个月的数据，按照1号、2号、\u0026hellip;31号这样的时间先后顺序依次训练，理论上4号的模型在4号的测试集上的效果是最好的。作者做了一个实验，分别用20号和4号的模型都在4号的测试集上进行评测，看看20号的模型指标相比4号降低了多少。作者发现，使用Prefix-SID方法和IE方法，两者的指标降低幅度都差不多，都比较小。首先IE方法由于不存在hash冲突，所以20号的模型仍然能比较好地预测4号的数据；其次，Prefix-SID方法虽然有hash冲突，但是因为冲突的item都是语义相似的，可以进行新老item的知识共享，所以这个冲突反而是好事，对模型效果无影响。但是作者发现RH方法的20号的模型在4号数据上评测指标下降比较多，因为有hash冲突，而且冲突是随机的，20号的分布已经变化很大了，导致在4号数据上效果不佳。Table 4的指标越小越好。 基于Prefix-SID方法虽然也有hash冲突，但是冲突到同一个semantic id的item表征更相似，而RH冲突到同一个桶里的item是完全随机的，相似度差。作者以IE为base，把Prefix-SID和RH都各自都冲突到同一个桶的IE emb提取出来，计算类内相似度和类间相似度，发现基于Prefix-SID的类内相似度方差小，类间距离大，说明Prefix-SID确实能把相似item聚到一起。 评论 可借鉴 基于Prefix-SID方法确实能提高新item和老item的信息共享数量，方法值得借鉴 论文实验分析很丰富 可改进 基于Prefix-SID方法居然没有和最常规的加和方法比较，是本文最大的不足 ","permalink":"http://localhost:1313/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID\u003c/li\u003e\n\u003cli\u003e作者单位：Meta\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2504.02137\"\u003ehttps://arxiv.org/pdf/2504.02137\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：RecSys 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003e搜推广的模型严重依赖于item id embedding的表征质量，但在工业场景下，搜推广的id表征存在如下挑战：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eid量级非常大，常常是数十亿甚至是百亿的规模。因此，通常不可能给每个id一个单独的embedding（即文中的individual embedding, IE），IE的成本太高\u003c/li\u003e\n\u003cli\u003eid分布非常不均匀，马太效应严重。文中统计：0.1%的头部item占据了25%的曝光量；5.5%的腰部item占据了50%的曝光量；94.4%的尾部item只占据了25%的曝光量\u003c/li\u003e\n\u003cli\u003eid分布漂移严重：搜推广场景中item的变化非常频繁，无时无刻不在发生着新id的产生和旧id的退出，而且不同id存活的时间周期也不尽相同，所以id的准入准出策略很难完美适配所有item\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-fig2.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-fig3.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e针对上述问题，常见的做法是对item id采用hash然后查emb的方式（即文中的random hash，RH），将所有id hash到一个固定大小的空间，然后查emb。但是RH方式有如下缺点：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e存在hash冲突，把不相关的id hash到一个桶里，导致语义混乱，学习效果不佳\u003c/li\u003e\n\u003cli\u003e无法解决id分布漂移的问题，比如hash到同一个桶的A、B两个id，如果B出现频率变高，则会带偏A的分布，影响了A的效果\u003c/li\u003e\n\u003cli\u003e无法进行知识共享，例如新出了商品iphone15，iphone15无法共享到老的iphone14的emb知识，iphone15的id emb必须完全重新学习。针对这种情况，作者做了一个更加极端的AA实验，就是copy一个完全相同的商品，只换item id，如果是IE或者RH策略，则新商品由于id emb是随机初始化的，效果不佳，这是id-based的通病\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"基于前缀n-gram的semantic-id表征方法\"\u003e基于前缀n-gram的semantic id表征方法\u003c/h1\u003e\n\u003cp\u003e针对上述问题，作者沿用了semantic id的思路，首先使用内容理解团队产出的文本、图片等多模态emb，然后基于过去3个月的item多模态emb，训练RQ-VAE模型，并产出所有item的semantic id。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-fig1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上述过程都是常规操作，重点在于如何基于semantic id得到item emb表征。假设semantic id是L层，每层的codebook size是K：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e最常规的做法：每层都初始化一个K*d的emb table，每层sid查各自的emb table，然后把L层的sid emb加起来。但是本文完全没有提这种方法，也没有和这种方法比较，非常奇怪。\n\u003cul\u003e\n\u003cli\u003e为了比较，我个人再详细描述下这种常规做法。比如老item A的sid是(c1,c2,c3)；新来一个item B，它的sid是(c1,c2,c4)。用常规方法，A的emb是c1+c2+c3，B的emb是c1+c2+c4。两者c1、c2是可以共享的，所以常规方法也能起到一定的知识共享的效果，共享项有2项：c1、c2。但是因为\u003ca href=\"https://arxiv.org/abs/2407.21488\"\u003eRQ-VAE的沙漏问题\u003c/a\u003e，c2很有可能是沙漏瓶颈，信息量不足。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e作者对比了Table 1中的几种方法：\u003c/li\u003e\n\u003cli\u003eTrigram和Fourgram差不多，如果L=3用Trigram、L=4用Fourgram的话，本质上是把L个sid映射成了一个无冲突的int。但是这种方法映射出来的int数量太多了，是\\(K^L\\)。如果K=1024、L=3，则\\(K^L\\)就已经超过10亿了，这和直接无冲突的IE方法一样了，而且存在新id无法共享老id学到的知识的问题\u003c/li\u003e\n\u003cli\u003eAll bigrams，就是所有的sid的2-gram。还是上面的例子，A的emb相当于\\(c_1c_2+c_2c_3\\)，B的emb相当于\\(c_1c_2+c_2c_4\\)，两者可共享\\(c_1c_2\\)项，相比于常规方法，虽然共享项数变少了，但粒度更精细了，孰好孰坏未可知。由Table 2可知，All bigrams的效果至少比Trigram和Fourgram好很多了，而且如果层数L越大，可共享项越多\u003c/li\u003e\n\u003cli\u003ePrefix-ngram（简称Prefix-SID方法），本文提出的新方法，把所有前缀组合成新id查emb，然后所有emb再求和。还是上面的例子，A的emb相当于\\(c_1+c_1c_2+c_1c_2c_3+c_2+c_2c_3+c_3\\)，B的emb相当于\\(c_1+c_1c_2+c_1c_2c_4+c_2+c_2c_4+c_4\\)，两者可共享\\(c_1, c_1c_2, c_2\\)三项，比之前的所有方法可共享的信息都多，而且如果层数L越大，可共享项越多，因此这种方法的效果最好，训练也最稳定\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-tab1.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-tab2.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e实验结果很丰富，做了很多分析，Prefix-SID方法有如下优势：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e相比于IE和RH方法，Prefix-SID方法对中长尾item的提升尤其显著，因为新id和老id的表征有了知识共享\u003c/li\u003e\n\u003cli\u003e对id分布漂移问题更不敏感：由于电商模型训练时消费数据的顺序是和数据的时间一致的，比如一个月的数据，按照1号、2号、\u0026hellip;31号这样的时间先后顺序依次训练，理论上4号的模型在4号的测试集上的效果是最好的。作者做了一个实验，分别用20号和4号的模型都在4号的测试集上进行评测，看看20号的模型指标相比4号降低了多少。作者发现，使用Prefix-SID方法和IE方法，两者的指标降低幅度都差不多，都比较小。首先IE方法由于不存在hash冲突，所以20号的模型仍然能比较好地预测4号的数据；其次，Prefix-SID方法虽然有hash冲突，但是因为冲突的item都是语义相似的，可以进行新老item的知识共享，所以这个冲突反而是好事，对模型效果无影响。但是作者发现RH方法的20号的模型在4号数据上评测指标下降比较多，因为有hash冲突，而且冲突是随机的，20号的分布已经变化很大了，导致在4号数据上效果不佳。Table 4的指标越小越好。\n\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-tab4.png\"\u003e\u003c/li\u003e\n\u003cli\u003e基于Prefix-SID方法虽然也有hash冲突，但是冲突到同一个semantic id的item表征更相似，而RH冲突到同一个桶里的item是完全随机的，相似度差。作者以IE为base，把Prefix-SID和RH都各自都冲突到同一个桶的IE emb提取出来，计算类内相似度和类间相似度，发现基于Prefix-SID的类内相似度方差小，类间距离大，说明Prefix-SID确实能把相似item聚到一起。\n\u003cimg loading=\"lazy\" src=\"/posts/2025-10-09-meta-prefix-ngram-sid-paper-reading/meta-ngram-sid-tab5.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e可借鉴\n\u003cul\u003e\n\u003cli\u003e基于Prefix-SID方法确实能提高新item和老item的信息共享数量，方法值得借鉴\u003c/li\u003e\n\u003cli\u003e论文实验分析很丰富\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e可改进\n\u003cul\u003e\n\u003cli\u003e基于Prefix-SID方法居然没有和最常规的加和方法比较，是本文最大的不足\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"论文阅读：Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID"},{"content":"\n基本信息 论文标题：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings 作者单位：沃尔玛 论文链接：https://arxiv.org/pdf/2507.17080 来源：RecSys 2025 Motivation：论文要解决的问题是什么 多模态q2i召回通常使用CLIP的对比学习方式进行训练，在电商场景下存在2个问题：\nCLIP这种方式通常是对图片整体的表征，缺乏细粒度的目标检测能力，尤其在电商场景，比如fig1，卖衣服场景，传统CLIP只能识别整张图片是一件T恤，难以关注T恤上的图案等细节特征；另外，电商图片往往存在很多附加背景、道具、模特等元素，会影响主体物体的表征 电商标题、属性等文本描述通常参差不齐，存在错误、堆砌、图文不符等问题，导致CLIP训练时图文对齐效果不佳 VL-CLIP解决方案 针对图片的处理：\n将图片和商品类型（product type）输入到开源模型Grounding DINO中，让模型进行目标检测，将可信度超过某个阈值且可信度最高的区域抠出来，输入到CLIP的图像encoder中。通过这步预处理，相当于对电商图片进行了关键主体识别和提取，只提取和商品最相关的主体进行图像表征。文中使用的图像编码器是ViT-B/32。 针对文本的处理：\n将商品的类型、标题、描述、性别、年龄等文本描述以及图片本身输入到Summarizer多模态大模型，让大模型产出精简、准确的文本描述\\(q_0\\) 将\\(q_0\\)和商品图文信息输入到Evaluator多模态大模型，让大模型对\\(q_0\\)的质量进行评判，如果\\(q_0\\)质量很好，则直接输出\u0026lt;STOP\u0026gt;；否则指出\\(q_0\\)的问题所在，并说明改进方法 如果第2步输出不是\u0026lt;STOP\u0026gt;，则将第2步的输出再输入到Refiner大模型，让大模型根据第2步的结果继续调整并输出更优的文本描述\\(q_i\\) 不断重复第2、3步，直到输出\u0026lt;STOP\u0026gt;，或者最多重复5遍 将产出的精准的文本描述q输入到CLIP的文本encoder中，文中使用的是BERT系列。产出的emb维度是512 上述Summarizer、Evaluator、Refiner都是VLM，文中使用的是GPT-4o，三个任务的prompt设计参考论文附录Table 9 上述对图片和文本的处理本质上是去噪，提取图片的主体物品、让文本描述更加精准。\n产出多模态emb之后，后续的操作就是常规的召回流程了，使用HNSW进行ANN召回。\n评论 可借鉴 使用Grounding DINO对图片进行主体识别，值得借鉴 使用VLM对商品标题、描述等文本信息进行去噪，值得借鉴 但如果商品量级很大的话，这两个步骤估计会很耗时 可改进 如果是q2i场景，直接用query文本是不是更真实，更接近搜索日子的真实数据分布？ ","permalink":"http://localhost:1313/posts/2025-10-08-vl-clip-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-08-vl-clip-paper-reading/VL-CLIP-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings\u003c/li\u003e\n\u003cli\u003e作者单位：沃尔玛\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2507.17080\"\u003ehttps://arxiv.org/pdf/2507.17080\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：RecSys 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003e多模态q2i召回通常使用CLIP的对比学习方式进行训练，在电商场景下存在2个问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCLIP这种方式通常是对图片整体的表征，缺乏细粒度的目标检测能力，尤其在电商场景，比如fig1，卖衣服场景，传统CLIP只能识别整张图片是一件T恤，难以关注T恤上的图案等细节特征；另外，电商图片往往存在很多附加背景、道具、模特等元素，会影响主体物体的表征\u003c/li\u003e\n\u003cli\u003e电商标题、属性等文本描述通常参差不齐，存在错误、堆砌、图文不符等问题，导致CLIP训练时图文对齐效果不佳\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-08-vl-clip-paper-reading/VL-CLIP-fig1.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"vl-clip解决方案\"\u003eVL-CLIP解决方案\u003c/h1\u003e\n\u003cp\u003e针对图片的处理：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将图片和商品类型（product type）输入到开源模型\u003ca href=\"https://github.com/IDEA-Research/GroundingDINO\"\u003eGrounding DINO\u003c/a\u003e中，让模型进行目标检测，将可信度超过某个阈值且可信度最高的区域抠出来，输入到CLIP的图像encoder中。通过这步预处理，相当于对电商图片进行了关键主体识别和提取，只提取和商品最相关的主体进行图像表征。文中使用的图像编码器是ViT-B/32。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-08-vl-clip-paper-reading/VL-CLIP-fig2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e针对文本的处理：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e将商品的类型、标题、描述、性别、年龄等文本描述以及图片本身输入到Summarizer多模态大模型，让大模型产出精简、准确的文本描述\\(q_0\\)\u003c/li\u003e\n\u003cli\u003e将\\(q_0\\)和商品图文信息输入到Evaluator多模态大模型，让大模型对\\(q_0\\)的质量进行评判，如果\\(q_0\\)质量很好，则直接输出\u0026lt;STOP\u0026gt;；否则指出\\(q_0\\)的问题所在，并说明改进方法\u003c/li\u003e\n\u003cli\u003e如果第2步输出不是\u0026lt;STOP\u0026gt;，则将第2步的输出再输入到Refiner大模型，让大模型根据第2步的结果继续调整并输出更优的文本描述\\(q_i\\)\u003c/li\u003e\n\u003cli\u003e不断重复第2、3步，直到输出\u0026lt;STOP\u0026gt;，或者最多重复5遍\u003c/li\u003e\n\u003cli\u003e将产出的精准的文本描述q输入到CLIP的文本encoder中，文中使用的是BERT系列。产出的emb维度是512\u003c/li\u003e\n\u003cli\u003e上述Summarizer、Evaluator、Refiner都是VLM，文中使用的是GPT-4o，三个任务的prompt设计参考论文附录Table 9\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-08-vl-clip-paper-reading/VL-CLIP-fig3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上述对图片和文本的处理本质上是去噪，提取图片的主体物品、让文本描述更加精准。\u003c/p\u003e\n\u003cp\u003e产出多模态emb之后，后续的操作就是常规的召回流程了，使用HNSW进行ANN召回。\u003c/p\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e可借鉴\n\u003cul\u003e\n\u003cli\u003e使用Grounding DINO对图片进行主体识别，值得借鉴\u003c/li\u003e\n\u003cli\u003e使用VLM对商品标题、描述等文本信息进行去噪，值得借鉴\u003c/li\u003e\n\u003cli\u003e但如果商品量级很大的话，这两个步骤估计会很耗时\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e可改进\n\u003cul\u003e\n\u003cli\u003e如果是q2i场景，直接用query文本是不是更真实，更接近搜索日子的真实数据分布？\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"论文阅读：VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings"},{"content":"\n基本信息 论文标题：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook 作者单位：Snap 论文链接：https://arxiv.org/pdf/2507.22224 来源：CIKM 2025 这是CIKM 2025的一篇resource文章，比较简单。核心内容是开源了一个基于semantic id的生成式推荐框架GRID，可以很方便地做各种消融对比实验。\n主要内容 主要结论如下：\n对于semantic id生成算法，简单的RQ-KMeans效果反而是最好的，好于R-VQ和RQ-VAE 生产pretrain emb的LLM模型参数量越大，效果越好，但是提升幅度有限 生产semantic id的codebook size和网络层数并不是越大越好，常规的3层，每层256个id效果反而最好 生成式推荐时，是否需要在用户行为序列基础上增加一个user id，实验发现增加user id效果反而变差，不增加user id效果最好 生成式网络结构encoder-decoder对比decoder-only，发现前者效果更好，因为前者能充分学习到行为序列完整的信息 对行为流进行滑动窗口数据增强能提升模型的泛化能力 当semantic id到item存在映射冲突时，随机选一个item的效果和对冲突item追加一个区分标识（digit），两者效果差不多 在生成式beam search的时候，限制只输出合法semantic id和不增加限制，两者效果差不多 评论 看这篇文章主要是想看看不同semantic id生产方法的对比，发现RQ-KMeans居然比RQ-VAE更好。个人感觉这两个方法效果应该差不多，后者应该更好点才对。首先，RQ-VAE的量化loss本质上和KMeans聚类是一个意思；其次，RQ-VAE还增加了一个重构loss，感觉产出来的semantic id和原始emb的信息损失应该更少。\n此外，本文的所有实验都是基于亚马逊的公开数据集，数据量肯定不能和真正的工业数据集相提并论，所以文中很多结论有可能只适用于本文的设定，换一个场景估计结论就变了，所以看看就好。\n最后，文中很多结论只写了现象，要是能增加原因分析就好了。\n","permalink":"http://localhost:1313/posts/2025-10-07-grid-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-07-grid-paper-reading/GRID-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook\u003c/li\u003e\n\u003cli\u003e作者单位：Snap\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2507.22224\"\u003ehttps://arxiv.org/pdf/2507.22224\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：CIKM 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这是CIKM 2025的一篇resource文章，比较简单。核心内容是开源了一个基于semantic id的生成式推荐框架GRID，可以很方便地做各种消融对比实验。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-07-grid-paper-reading/GRID-fig1.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"主要内容\"\u003e主要内容\u003c/h1\u003e\n\u003cp\u003e主要结论如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e对于semantic id生成算法，简单的RQ-KMeans效果反而是最好的，好于R-VQ和RQ-VAE\u003c/li\u003e\n\u003cli\u003e生产pretrain emb的LLM模型参数量越大，效果越好，但是提升幅度有限\u003c/li\u003e\n\u003cli\u003e生产semantic id的codebook size和网络层数并不是越大越好，常规的3层，每层256个id效果反而最好\u003c/li\u003e\n\u003cli\u003e生成式推荐时，是否需要在用户行为序列基础上增加一个user id，实验发现增加user id效果反而变差，不增加user id效果最好\u003c/li\u003e\n\u003cli\u003e生成式网络结构encoder-decoder对比decoder-only，发现前者效果更好，因为前者能充分学习到行为序列完整的信息\u003c/li\u003e\n\u003cli\u003e对行为流进行滑动窗口数据增强能提升模型的泛化能力\u003c/li\u003e\n\u003cli\u003e当semantic id到item存在映射冲突时，随机选一个item的效果和对冲突item追加一个区分标识（digit），两者效果差不多\u003c/li\u003e\n\u003cli\u003e在生成式beam search的时候，限制只输出合法semantic id和不增加限制，两者效果差不多\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cp\u003e看这篇文章主要是想看看不同semantic id生产方法的对比，发现RQ-KMeans居然比RQ-VAE更好。个人感觉这两个方法效果应该差不多，后者应该更好点才对。首先，RQ-VAE的量化loss本质上和KMeans聚类是一个意思；其次，RQ-VAE还增加了一个重构loss，感觉产出来的semantic id和原始emb的信息损失应该更少。\u003c/p\u003e\n\u003cp\u003e此外，本文的所有实验都是基于亚马逊的公开数据集，数据量肯定不能和真正的工业数据集相提并论，所以文中很多结论有可能只适用于本文的设定，换一个场景估计结论就变了，所以看看就好。\u003c/p\u003e\n\u003cp\u003e最后，文中很多结论只写了现象，要是能增加原因分析就好了。\u003c/p\u003e","title":"论文阅读：Generative Recommendation with Semantic IDs: A Practitioner’s Handbook"},{"content":"\n基本信息 论文标题：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation 作者单位：网易云音乐 论文链接：https://arxiv.org/pdf/2508.20359 来源：CIKM 2025 Motivation：论文要解决的问题是什么 多模态emb在搜推的应用方式，通常是先将多模态emb转换成semantic id，然后把semantic id用到搜推模型中，这种方式有如下两个问题：\n模态内语义退化：多模态emb转换成semantic id通常使用RQ-VAE或者RQ-KMeans的方法，这种方法在不断残差的过程中，后续残差聚类结果已经不能反映初始emb的聚类效果了。其实就是semantic id的沙漏问题，具体可以看这篇文章，后续有空再分享这个问题。 简单来说，如下图所示，初始有DJ、Rock、Lullaby、Choir四个类，但是对残差emb（即RQ-VAE的第二层）聚类的话，初始的四个类的item就打散了，会聚到不同的簇中，也就是RQ-VAE的后续层的聚类效果已经和初始emb的聚类效果很不一样了，这就是文中说的语义退化问题 模态间建模差异：搜推场景的item通常有多种模态特征，比如文本、图像、音频等，传统方法在多模态融合方面比较简单，不能很好地捕捉多模态之间的关系。 PSRQ生产semantic id 本文是音乐推荐场景，主要用到两种模态：text和audio，分别用百川和MERT提取text和audio的模态emb。\n生产semantic id的方法如下图所示：\nfig2a是传统的RQ-KMeans的方法，每一层都用上一层的残差进行聚类。如上文所述，由于沙漏问题，会导致后续层次的semantic id存在语义退化问题 fig2b是本文新提出的PSRQ量化方法，在RQ-KMeans基础上，每一层除了有上一层的残差向量，还会concat上初始emb减去残差emb后的向量。这样就能区分出残差相似，但初始emb不同的item了，也就避免了RQ方法的沙漏问题，后续semantic id也能保留初始emb的语义信息。fig1d能看出来第二层semantic id仍然能够反映初始emb的分类效果。 Semantic id在下游的应用方法 如下图所示：\n每个item有两套多模态emb：text和audio，但是有三套semantic id，除了text和audio各自产一套semantic id之外，还会把text和audio的emb concat起来，再产一套semantic id，相当于多模态融合的semantic id semantic id的emb在排序模型中随机初始化，然后端到端训练 semantic id在用户建模时，使用DIN模型，query用的是多模态融合的semantic id emb，行为流分别用text和audio的semantic id emb。作者说这种方法既能捕捉到单模态细粒度的信息，又能建模跨模态的交互信息 评论 可借鉴 PSRQ的semantic id生产方法确实很有意思，在每一层都用上原始emb，这样不同簇的item在每一层都能分开，不会出现沙漏问题，使得每一层的semantic id都能保留原始emb的语义聚类信息 产了多套semantic id，单模态semantic id是常规操作；多模态emb concat后也产一套semantic id，是个创新点 用户建模时query用多模态semantic id，行为流用单模态semantic id，也是个创新点，虽然论文说这种方法效果最好，但是有点存疑 论文有个实验结果对比了不同semantic id量化方法的效果，结论是：PSRQ \u0026gt; RQ-KMeans = RQ-VAE \u0026gt; VQ \u0026gt; PQ 可改进 pretrain emb和semantic id的生产都没有对齐协同信号 semantic id在下游应用时直接端到端训练，而没有使用codebook初始化，会不会丢失信息比较多？ 产semantic id的过程中，模态内语义退化的问题，描述了现象，但是没有用定量的指标来说明问题，感觉可以借鉴【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】的方法，定量说明后续层的semantic id的聚类效果或者说区分能力相比初始emb已经相差甚远了 fig2b中，第一层的codebook的dim=d，后续层的codebook的dim=2d，那么后续层的残差dim也是2d，那么初始emb怎么和后续的残差emb相减呢，维度对不上啊？我理解可能是这样的，后续层聚类的时候用的是concat的dim=2d的emb，但是算聚类中心的时候只用了残差本身的emb，这样就能解释得通了，但是文中对这部分的细节没有解释。 ","permalink":"http://localhost:1313/posts/2025-10-06-psrq-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-06-psrq-paper-reading/PSRQ-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation\u003c/li\u003e\n\u003cli\u003e作者单位：网易云音乐\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2508.20359\"\u003ehttps://arxiv.org/pdf/2508.20359\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：CIKM 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003e多模态emb在搜推的应用方式，通常是先将多模态emb转换成semantic id，然后把semantic id用到搜推模型中，这种方式有如下两个问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e模态内语义退化\u003c/strong\u003e：多模态emb转换成semantic id通常使用RQ-VAE或者RQ-KMeans的方法，这种方法在不断残差的过程中，后续残差聚类结果已经不能反映初始emb的聚类效果了。其实就是semantic id的沙漏问题，具体可以看\u003ca href=\"https://arxiv.org/abs/2407.21488\"\u003e这篇文章\u003c/a\u003e，后续有空再分享这个问题。\n\u003cul\u003e\n\u003cli\u003e简单来说，如下图所示，初始有DJ、Rock、Lullaby、Choir四个类，但是对残差emb（即RQ-VAE的第二层）聚类的话，初始的四个类的item就打散了，会聚到不同的簇中，也就是RQ-VAE的后续层的聚类效果已经和初始emb的聚类效果很不一样了，这就是文中说的语义退化问题\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e模态间建模差异\u003c/strong\u003e：搜推场景的item通常有多种模态特征，比如文本、图像、音频等，传统方法在多模态融合方面比较简单，不能很好地捕捉多模态之间的关系。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-06-psrq-paper-reading/PSRQ-fig1.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"psrq生产semantic-id\"\u003ePSRQ生产semantic id\u003c/h1\u003e\n\u003cp\u003e本文是音乐推荐场景，主要用到两种模态：text和audio，分别用百川和MERT提取text和audio的模态emb。\u003c/p\u003e\n\u003cp\u003e生产semantic id的方法如下图所示：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efig2a是传统的RQ-KMeans的方法，每一层都用上一层的残差进行聚类。如上文所述，由于沙漏问题，会导致后续层次的semantic id存在语义退化问题\u003c/li\u003e\n\u003cli\u003efig2b是本文新提出的PSRQ量化方法，在RQ-KMeans基础上，每一层除了有上一层的残差向量，还会concat上初始emb减去残差emb后的向量。\u003cstrong\u003e这样就能区分出残差相似，但初始emb不同的item了\u003c/strong\u003e，也就避免了RQ方法的沙漏问题，后续semantic id也能保留初始emb的语义信息。fig1d能看出来第二层semantic id仍然能够反映初始emb的分类效果。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-06-psrq-paper-reading/PSRQ-fig2.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"semantic-id在下游的应用方法\"\u003eSemantic id在下游的应用方法\u003c/h1\u003e\n\u003cp\u003e如下图所示：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e每个item有两套多模态emb：text和audio，但是有三套semantic id，除了text和audio各自产一套semantic id之外，还会把text和audio的emb concat起来，再产一套semantic id，相当于多模态融合的semantic id\u003c/li\u003e\n\u003cli\u003esemantic id的emb在排序模型中随机初始化，然后端到端训练\u003c/li\u003e\n\u003cli\u003esemantic id在用户建模时，使用DIN模型，query用的是多模态融合的semantic id emb，行为流分别用text和audio的semantic id emb。作者说这种方法既能捕捉到单模态细粒度的信息，又能建模跨模态的交互信息\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-06-psrq-paper-reading/PSRQ-fig3.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e可借鉴\n\u003cul\u003e\n\u003cli\u003ePSRQ的semantic id生产方法确实很有意思，在每一层都用上原始emb，这样不同簇的item在每一层都能分开，不会出现沙漏问题，使得每一层的semantic id都能保留原始emb的语义聚类信息\u003c/li\u003e\n\u003cli\u003e产了多套semantic id，单模态semantic id是常规操作；多模态emb concat后也产一套semantic id，是个创新点\u003c/li\u003e\n\u003cli\u003e用户建模时query用多模态semantic id，行为流用单模态semantic id，也是个创新点，虽然论文说这种方法效果最好，但是有点存疑\u003c/li\u003e\n\u003cli\u003e论文有个实验结果对比了不同semantic id量化方法的效果，结论是：PSRQ \u0026gt; RQ-KMeans = RQ-VAE \u0026gt; VQ \u0026gt; PQ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e可改进\n\u003cul\u003e\n\u003cli\u003epretrain emb和semantic id的生产都没有对齐协同信号\u003c/li\u003e\n\u003cli\u003esemantic id在下游应用时直接端到端训练，而没有使用codebook初始化，会不会丢失信息比较多？\u003c/li\u003e\n\u003cli\u003e产semantic id的过程中，模态内语义退化的问题，描述了现象，但是没有用定量的指标来说明问题，感觉可以借鉴\u003ca href=\"https://bitjoy.net/posts/2025-10-04-mme-sid-paper-reading/\"\u003e【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】\u003c/a\u003e的方法，定量说明后续层的semantic id的聚类效果或者说区分能力相比初始emb已经相差甚远了\u003c/li\u003e\n\u003cli\u003efig2b中，第一层的codebook的dim=d，后续层的codebook的dim=2d，那么后续层的残差dim也是2d，那么初始emb怎么和后续的残差emb相减呢，维度对不上啊？我理解可能是这样的，后续层聚类的时候用的是concat的dim=2d的emb，但是算聚类中心的时候只用了残差本身的emb，这样就能解释得通了，但是文中对这部分的细节没有解释。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"论文阅读：Progressive Semantic Residual Quantization for Multimodal-Joint Interest Modeling in Music Recommendation"},{"content":"\n基本信息 论文标题：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System 作者单位：快手 论文链接：https://arxiv.org/pdf/2508.10584 来源：CIKM 2025 Motivation：论文要解决的问题是什么 Semantic id生产时，要么没有和协同信号对齐（fig2(1)），要么是两阶段对齐方式（fig2(2)）：\n例如LETTER先生成协同emb，然后和semantic id对齐 或者例如QARM，先协同对齐emb，再生产semantic id 把协同对齐和生产semantic id分成两个阶段，天然有信息损失，不是最优的。本文的目的就是把生产协同emb，以及semantic id的协同对齐放到一个模型中联合训练完成，尽量减少信息损失（fig2(3)）。\n主模型 主模型如上图所示，中间的ICDM是user和item的双塔模型，用于学习user和item的协同id-based emb；两边分别是生产user和item的semantic id的量化模型。\n中间的ICDM就是经典的召回双塔模型，使用点击样本进行训练，唯一不同的是，在user和item塔都有流行度去偏模块，用于学习user和item的无偏emb，后续user和item的semantic id协同对齐用的也是无偏的emb。\n两边分别是user和item的semantic id量化模型，两者比较类似，以item为例：\n先把item的各种信息，如title、desc、ocr等信息用文本构造成prompt，输入到LLM，借助LLM的summary和reasoning能力，产出item的详细描述 然后把LLM产出的描述再输入到一个预训练的embedding模型PLM，文中用的是bge m3模型，得到item emb 后续就是标准的RQ-VAE过程了 需要注意的是，上述前两步，分别用到了LLM和PLM两个大模型，而且看图上这两个模型都是freeze的，也就是说并不微调这两个大模型。后续协同对齐用的emb是RQ-VAE重构emb的中间层结果，即图中的item quantized emb。\nsemantic id的协同对齐方面，有三大类对齐任务：\nU2I对齐：量化user emb和协同item emb对齐、量化item emb和协同user emb对齐 U2U和I2I对齐：量化user emb和协同user emb对齐、量化item emb和协同item emb对齐 U2U和I2I的共现对齐：点击相同item的两个量化user emb对齐、同一个user点击的两个item的量化item emb对齐 由于fig3中的协同模型和semantic id模型是联合训练的，总共有3大类loss：\n中间的ICDM的双塔召回模型的loss 两边的产semantic id的loss 三个模块的对齐loss 评论 可借鉴 把semantic id的生产和协同信号对齐统一成一阶段的模式，信息损失更少 中间的ICDM模型生产协同emb时进行了去偏，协同对齐的时候用的是去偏的emb，这是其他论文很少提到的 可改进 太复杂了！3个模块，3大类loss，每类loss又有很多个小loss，总loss数量加起来有十多个。。。 任务太多，各种去偏、对齐loss，真的不会互相影响吗？ 中间的ICDM模块有必要吗？我理解ICDM本质是为了训练产出协同emb，但是因为训练样本本身是点击样本，样本本身已经包含了搜推场景的协同信号，也就是ICDM本身没必要存在了，直接用相同的样本训练两边的semantic id量化模型就行了，也能实现在训练semantic id的过程中，完成协同信号的对齐 生产semantic id的emb来自LLM和PLM，但是这两个大模型都是freeze的，如果把这两个模型也sft，效果会不会更好？其实我原本以为的一阶段就是这样的，这也是我在【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】中提到的一阶段方法。 ","permalink":"http://localhost:1313/posts/2025-10-05-das-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-05-das-paper-reading/das-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System\u003c/li\u003e\n\u003cli\u003e作者单位：快手\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2508.10584\"\u003ehttps://arxiv.org/pdf/2508.10584\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：CIKM 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003eSemantic id生产时，要么没有和协同信号对齐（fig2(1)），要么是两阶段对齐方式（fig2(2)）：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e例如\u003ca href=\"https://arxiv.org/abs/2405.07314\"\u003eLETTER\u003c/a\u003e先生成协同emb，然后和semantic id对齐\u003c/li\u003e\n\u003cli\u003e或者例如\u003ca href=\"https://arxiv.org/pdf/2411.11739\"\u003eQARM\u003c/a\u003e，先协同对齐emb，再生产semantic id\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e把协同对齐和生产semantic id分成两个阶段，天然有信息损失，不是最优的。本文的目的就是把生产协同emb，以及semantic id的协同对齐放到一个模型中联合训练完成，尽量减少信息损失（fig2(3)）。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-05-das-paper-reading/das-fig2.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"主模型\"\u003e主模型\u003c/h1\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-05-das-paper-reading/das-fig3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e主模型如上图所示，中间的ICDM是user和item的双塔模型，用于学习user和item的协同id-based emb；两边分别是生产user和item的semantic id的量化模型。\u003c/p\u003e\n\u003cp\u003e中间的ICDM就是经典的召回双塔模型，使用点击样本进行训练，唯一不同的是，在user和item塔都有流行度去偏模块，用于学习user和item的无偏emb，后续user和item的semantic id协同对齐用的也是无偏的emb。\u003c/p\u003e\n\u003cp\u003e两边分别是user和item的semantic id量化模型，两者比较类似，以item为例：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e先把item的各种信息，如title、desc、ocr等信息用文本构造成prompt，输入到LLM，借助LLM的summary和reasoning能力，产出item的详细描述\u003c/li\u003e\n\u003cli\u003e然后把LLM产出的描述再输入到一个预训练的embedding模型PLM，文中用的是bge m3模型，得到item emb\u003c/li\u003e\n\u003cli\u003e后续就是标准的RQ-VAE过程了\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e需要注意的是，上述前两步，分别用到了LLM和PLM两个大模型，而且看图上这两个模型都是freeze的，也就是说并不微调这两个大模型。后续协同对齐用的emb是RQ-VAE重构emb的中间层结果，即图中的item quantized emb。\u003c/p\u003e\n\u003cp\u003esemantic id的协同对齐方面，有三大类对齐任务：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eU2I对齐：量化user emb和协同item emb对齐、量化item emb和协同user emb对齐\u003c/li\u003e\n\u003cli\u003eU2U和I2I对齐：量化user emb和协同user emb对齐、量化item emb和协同item emb对齐\u003c/li\u003e\n\u003cli\u003eU2U和I2I的共现对齐：点击相同item的两个量化user emb对齐、同一个user点击的两个item的量化item emb对齐\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由于fig3中的协同模型和semantic id模型是联合训练的，总共有3大类loss：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e中间的ICDM的双塔召回模型的loss\u003c/li\u003e\n\u003cli\u003e两边的产semantic id的loss\u003c/li\u003e\n\u003cli\u003e三个模块的对齐loss\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e可借鉴\n\u003cul\u003e\n\u003cli\u003e把semantic id的生产和协同信号对齐统一成一阶段的模式，信息损失更少\u003c/li\u003e\n\u003cli\u003e中间的ICDM模型生产协同emb时进行了去偏，协同对齐的时候用的是去偏的emb，这是其他论文很少提到的\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e可改进\n\u003cul\u003e\n\u003cli\u003e太复杂了！3个模块，3大类loss，每类loss又有很多个小loss，总loss数量加起来有十多个。。。\u003c/li\u003e\n\u003cli\u003e任务太多，各种去偏、对齐loss，真的不会互相影响吗？\u003c/li\u003e\n\u003cli\u003e中间的ICDM模块有必要吗？我理解ICDM本质是为了训练产出协同emb，但是因为训练样本本身是点击样本，样本本身已经包含了搜推场景的协同信号，也就是ICDM本身没必要存在了，直接用相同的样本训练两边的semantic id量化模型就行了，也能实现在训练semantic id的过程中，完成协同信号的对齐\u003c/li\u003e\n\u003cli\u003e生产semantic id的emb来自LLM和PLM，但是这两个大模型都是freeze的，如果把这两个模型也sft，效果会不会更好？其实我原本以为的一阶段就是这样的，这也是我在\u003ca href=\"https://bitjoy.net/posts/2025-10-04-mme-sid-paper-reading/\"\u003e【论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs】\u003c/a\u003e中提到的一阶段方法。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"论文阅读：DAS: Dual-Aligned Semantic IDs Empowered Industrial Recommender System"},{"content":"\n基本信息 论文标题：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou 作者单位：快手 论文链接：https://arxiv.org/pdf/2411.11739 来源：CIKM 2025 Motivation：论文要解决的问题是什么 多模态emb在搜推场景应用时通常采用如下图的两阶段方式，先预训练多模态emb，然后作为一个冻结特征放到搜推模型中。这种方式存在2个问题：\n表征不对齐：多模态emb预训练的任务通常是图片分类或者文本的MLM，和下游搜推任务不对齐 表征不更新：多模态emb在搜推任务中作为冻结特征，没有更新 本文的方法就是想要解决上述2个问题。 对齐搜推任务的多模态emb预训练 为了解决多模态emb表征不对齐的问题，本文提出的多模态emb预训练任务直接对齐搜推场景，使用U2I和I2I召回模型，挖掘出相似item pair，然后通过对比学习微调多模态大模型。\n具体来说，通过U2I和I2I模型，能够拿到item emb；然后用每一个target item emb去行为流中检索出最相似的商品，作为trigger item emb。\u0026lt;trigger, target\u0026gt;构成一对正样本，然后进行对比学习训练。\n通过召回模型构造的训练样本，和搜推场景的协同信号对齐了，解决了开头提到的第一个问题，即表征不对齐的问题。\nSemantic id生产方法 Semantic id的生产方法如上图右半部分所示，有两种方式：\nVQ：直接圈定一定数量（如N）的item emb作为底池，编号1~N，然后任意来一个item emb，通过对底池emb进行KNN搜索，找出top-k相似商品，假设是(a,b,\u0026hellip;,k)，则VQ编码的semantic id就是(a,b,\u0026hellip;,k)。文中取k=25，感觉挺大的。。。 RQ-Kmeans：对圈定的N个item emb不断进行Kmeans聚类、求残差、残差继续Kmeans聚类的过程。文中取迭代次数为L=6，但是没说每次聚到多少个类。 注意：文中的RQ-Kmeans方法和RQ-VAE还不一样，RQ-Kmeans没有训练过程，也没有重构loss，纯粹是每次进行聚类，然后选聚类中心作为码本的过程。文中也没有对比过为啥不用RQ-VAE。\n产出两套semantic id之后，直接在下游排序任务中进行端到端更新，解决开头提到的表征不更新的问题。具体建模方法比较常规，不是本文的重点，略讲。\n评论 可借鉴 多模态emb预训练任务是i2i的，直接和下游搜推任务对齐 semantic id有两种产出方式，VQ和RQ-Kmeans，尽可能多地保留原始多模态emb的信息 可改进 多模态emb预训练和下游任务对齐，在2025年不算新鲜事了，常规操作。而且文中i2i的构造过程依赖U2I和I2I召回模型，有外部依赖，不够漂亮 VQ的方法，k=25这也太长了吧，相当于一个小型行为流了，会导致下游任务的特征处理更复杂 为什么用RQ-Kmeans而不是RQ-VAE，没有任何说明与对比 从pretrain emb量化成semantic id的过程中，存在严重的信息丢失，这在Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs论文中有讨论 ","permalink":"http://localhost:1313/posts/2025-10-04-qarm-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-qarm-paper-reading/QARM-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou\u003c/li\u003e\n\u003cli\u003e作者单位：快手\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2411.11739\"\u003ehttps://arxiv.org/pdf/2411.11739\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：CIKM 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003e多模态emb在搜推场景应用时通常采用如下图的两阶段方式，先预训练多模态emb，然后作为一个冻结特征放到搜推模型中。这种方式存在2个问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e表征不对齐\u003c/strong\u003e：多模态emb预训练的任务通常是图片分类或者文本的MLM，和下游搜推任务不对齐\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e表征不更新\u003c/strong\u003e：多模态emb在搜推任务中作为冻结特征，没有更新\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e本文的方法就是想要解决上述2个问题。\n\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-qarm-paper-reading/QARM-fig1.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"对齐搜推任务的多模态emb预训练\"\u003e对齐搜推任务的多模态emb预训练\u003c/h1\u003e\n\u003cp\u003e为了解决多模态emb表征不对齐的问题，本文提出的多模态emb预训练任务直接对齐搜推场景，使用U2I和I2I召回模型，挖掘出相似item pair，然后通过对比学习微调多模态大模型。\u003c/p\u003e\n\u003cp\u003e具体来说，通过U2I和I2I模型，能够拿到item emb；然后用每一个target item emb去行为流中检索出最相似的商品，作为trigger item emb。\u0026lt;trigger, target\u0026gt;构成一对正样本，然后进行对比学习训练。\u003c/p\u003e\n\u003cp\u003e通过召回模型构造的训练样本，和搜推场景的协同信号对齐了，解决了开头提到的第一个问题，即表征不对齐的问题。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-qarm-paper-reading/QARM-fig3.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"semantic-id生产方法\"\u003eSemantic id生产方法\u003c/h1\u003e\n\u003cp\u003eSemantic id的生产方法如上图右半部分所示，有两种方式：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eVQ\u003c/strong\u003e：直接圈定一定数量（如N）的item emb作为底池，编号1~N，然后任意来一个item emb，通过对底池emb进行KNN搜索，找出top-k相似商品，假设是(a,b,\u0026hellip;,k)，则VQ编码的semantic id就是(a,b,\u0026hellip;,k)。文中取k=25，感觉挺大的。。。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRQ-Kmeans\u003c/strong\u003e：对圈定的N个item emb不断进行Kmeans聚类、求残差、残差继续Kmeans聚类的过程。文中取迭代次数为L=6，但是没说每次聚到多少个类。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e注意：文中的RQ-Kmeans方法和RQ-VAE还不一样，RQ-Kmeans没有训练过程，也没有重构loss，纯粹是每次进行聚类，然后选聚类中心作为码本的过程。文中也没有对比过为啥不用RQ-VAE。\u003c/p\u003e\n\u003cp\u003e产出两套semantic id之后，直接在下游排序任务中进行端到端更新，解决开头提到的表征不更新的问题。具体建模方法比较常规，不是本文的重点，略讲。\u003c/p\u003e\n\u003ch1 id=\"评论\"\u003e评论\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e可借鉴\n\u003cul\u003e\n\u003cli\u003e多模态emb预训练任务是i2i的，直接和下游搜推任务对齐\u003c/li\u003e\n\u003cli\u003esemantic id有两种产出方式，VQ和RQ-Kmeans，尽可能多地保留原始多模态emb的信息\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e可改进\n\u003cul\u003e\n\u003cli\u003e多模态emb预训练和下游任务对齐，在2025年不算新鲜事了，常规操作。而且文中i2i的构造过程依赖U2I和I2I召回模型，有外部依赖，不够漂亮\u003c/li\u003e\n\u003cli\u003eVQ的方法，k=25这也太长了吧，相当于一个小型行为流了，会导致下游任务的特征处理更复杂\u003c/li\u003e\n\u003cli\u003e为什么用RQ-Kmeans而不是RQ-VAE，没有任何说明与对比\u003c/li\u003e\n\u003cli\u003e从pretrain emb量化成semantic id的过程中，存在严重的信息丢失，这在\u003ca href=\"https://bitjoy.net/posts/2025-10-04-mme-sid-paper-reading/\"\u003eEmpowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs\u003c/a\u003e论文中有讨论\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"论文阅读：QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"},{"content":"\n基本信息 论文标题：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs 作者单位：香港城市大学\u0026amp;腾讯 论文链接：https://arxiv.org/pdf/2509.02017 来源：CIKM 2025 Motivation：论文要解决的问题是什么 LLM4SR的基本范式如下，即用LLM直接来做搜推的范式（这种方式在学术界常见，但在工业界不常见）。由于LLM的输入词表范围是有限的（通常比较小），因此其token emb dim通常比较大，比如2048或者4096；而搜推场景的item量级很大，而且在不断更新，因此工业界经典的id-based的搜推模型的item emb dim通常比较小，比如64或128。经典的id-based的搜推模型能比较好地学习到搜推场景的协同信号，为了让LLM模型也能感知这种信息，LLM4SR范式通常会先预训练一个id-based的经典搜推模型，然后将其中的item id emb通过下图的Linear Projection的映射层，映射到LLM token emb的空间，让LLM也能感知搜推的协同信号。\n上述LLM4SR范式存在两个问题：\n维度坍缩：id-based训出来的id emb dim比较小（如64），LLM token emb dim比较大（如4096），在由id emb通过Linear Projection映射到toen emb的过程中，虽然64映射到4096空间了，但扩维后的矩阵存在低秩问题，即还是只利用了4096中的64维的空间。\n论文中，作者分两种情况进行了分析，如果Linear Projection只是一个线性层的话，通过公式推导能得出上述结论；如果Linear Projection包含非线性变换，作者通过实验分析也发现了维度坍缩的现象。 灾难遗忘：除了使用id-based模型产出的id emb，LLM4SR也常用多模态模型产出item emb表征，然后转换成semantic id输入到LLM4SR中。在这种情况下，产出的semantic id通过会遗忘多模态item emb的信息，导致下游LLM4SR的效果不佳。\n论文中，作者用公式9来衡量semantic id保留pretrain多模态emb的信息量。具体来说，如果行为流中的商品序列是{A,B,C,D}，target item是E。使用pretrain多模态emb能计算出E和A~D的相似度，例如相似度\u0026lt;E,A\u0026gt; \u0026gt; \u0026lt;E,B\u0026gt;。如果将pretrain多模态emb转换成semantic id，然后由semantic id恢复出新的A~E的emb之后，再计算E和A~D的相似度，如果仍然有\u0026lt;E,A\u0026gt; \u0026gt; \u0026lt;E,B\u0026gt;，则认为一致（concordant），否则不一致（disconcordant）。这个分析方法挺好的，通过这个指标能估算出转换成semantic id之后，仍然保留原有pretrain多模态emb对搜推场景的序关系的保留程度。 作者发现，转换成semantic id之后，信息只保留了37.14%；进一步，如果semantic id是在下游任务中端到端训练的，则信息只保留了5.5%，也就是说94.5%的pretrain emb的序的信息都丢掉了，也就是灾难遗忘。 Semantic id构建方法 3套emb来源，一套id-based经典搜推模型产出的包含协同信号的emb，另外两套是LLM2CLIP产出的多模态文本和图片emb。作者提到传统CLIP对长文本处理能力较弱，所以升级到LLM2CLIP，能更好地处理长文本。 Semantic id构建方法是经典的RQ-VAE的方法，但有如下两个改进点： 将emb的重构loss由MSE升级成MMD (maximum mean discrepancy)，MSE是计算原始emb和重构emb的欧式距离的误差，而MMD是计算两个分布的diff，实验表明能MMD比MSE能保留更多的pretrain多模态emb信息（即上述公式9），保留44.36% 对量化后的emb做了对齐，因为LLM2CLIP本身进行了图文模态的对齐，所以文中只新增了id emb分别和文本、图片模态的对齐 此外，还有一点论文没提但可能和常规RQ-VAE不同之处，就是原始emb在进行RQ-VAE之前，有一个Encoder升维的操作，在重构loss前对应有一个Decoder降维的操作，而semantic id量化恢复emb是Decoder之前的那个。这一升一降，估计也有助于缓解维度坍缩。 主模型 为了缓解维度坍缩，使用3套emb，一套id-based协同信号emb，另外两套是文本和图片的多模态emb 每套emb既包含原始emb过Linear Projection投影之后的表征（低维投影到高维，存在维度坍缩问题）；也包含由原始emb训练产出的semantic id重构回来的emb（天然高维emb） 为了避免灾难遗忘，semantic id使用上述优化的MMD loss训练产出，并且semantic id emb使用在训练semantic id emb产出的codebook emb进行初始化，然后随着LLM4SR finetuning，而不是完全随机初始化然后端到端训练 最后在LLM4SR输出层，有一个Multimodal Frequency-aware Fusion模块，next token prediction任务相当于一个n分类任务。在这个模块中，对target item也会新增一套emb talbe，这样总共就有4套emb table了。然后词表中每个item会根据热度过一个函数得到四种模态的emb的权重，然后4个emb进行融合。通过这种方式也能一定程度上缓解维度坍缩。 评论 可借鉴 论文的分析方法值得借鉴，例如对维度坍缩的推理分析、灾难遗忘的定量分析等 semantic id训练时的MMD loss缓解灾难遗忘 semantic id emb在下游应用时，使用训练的codebook emb进行初始化，而不是随机初始化，能缓解灾难遗忘 使用多套emb及semantic id，缓解维度坍缩 融合多套emb时，考虑item热度信息，动态调整融合权重 可改进 LLM4SR主要用于学术场景，没有考虑工业场景item id数据量巨大，而且不断更新的情况，因此在工业场景不常见 即使用上MMD loss，pretrain emb信息页只保留了44.36%，如果目标是100%的话，这个绝对差距还很大 没有论证semantic id emb遗忘pretrain emb信息对下游任务的影响，虽然遗忘信息了，但端到端训练也学到新知识了，功过相抵，也许效果不一定差？ semantic id通常通过两阶段训练得到，先预训练emb，然后训练semantic id，两阶段过程天然容易使semantic id遗忘预训练emb的信息，如果将两者合并成一阶段的，即把训练semantic id的网络模块加入到预训练emb的网络中，在预训练emb的过程中，就完成semantic id的训练，那么semantic id遗忘的信息会不会更少？类似的思想在召回双塔模型Poeem（https://arxiv.org/abs/2105.03933）中就有过。 ","permalink":"http://localhost:1313/posts/2025-10-04-mme-sid-paper-reading/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-mme-sid-paper-reading/mme-sid-paper-cover.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"基本信息\"\u003e基本信息\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e论文标题：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs\u003c/li\u003e\n\u003cli\u003e作者单位：香港城市大学\u0026amp;腾讯\u003c/li\u003e\n\u003cli\u003e论文链接：\u003ca href=\"https://arxiv.org/pdf/2509.02017\"\u003ehttps://arxiv.org/pdf/2509.02017\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e来源：CIKM 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"motivation论文要解决的问题是什么\"\u003eMotivation：论文要解决的问题是什么\u003c/h1\u003e\n\u003cp\u003eLLM4SR的基本范式如下，即用LLM直接来做搜推的范式（这种方式在学术界常见，但在工业界不常见）。由于LLM的输入词表范围是有限的（通常比较小），因此其token emb dim通常比较大，比如2048或者4096；而搜推场景的item量级很大，而且在不断更新，因此工业界经典的id-based的搜推模型的item emb dim通常比较小，比如64或128。经典的id-based的搜推模型能比较好地学习到搜推场景的协同信号，为了让LLM模型也能感知这种信息，LLM4SR范式通常会先预训练一个id-based的经典搜推模型，然后将其中的item id emb通过下图的Linear Projection的映射层，映射到LLM token emb的空间，让LLM也能感知搜推的协同信号。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-mme-sid-paper-reading/E4SRec.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上述LLM4SR范式存在两个问题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e维度坍缩\u003c/strong\u003e：id-based训出来的id emb dim比较小（如64），LLM token emb dim比较大（如4096），在由id emb通过Linear Projection映射到toen emb的过程中，虽然64映射到4096空间了，但扩维后的矩阵存在低秩问题，即还是只利用了4096中的64维的空间。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e论文中，作者分两种情况进行了分析，如果Linear Projection只是一个线性层的话，通过公式推导能得出上述结论；如果Linear Projection包含非线性变换，作者通过实验分析也发现了维度坍缩的现象。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e灾难遗忘\u003c/strong\u003e：除了使用id-based模型产出的id emb，LLM4SR也常用多模态模型产出item emb表征，然后转换成semantic id输入到LLM4SR中。在这种情况下，产出的semantic id通过会遗忘多模态item emb的信息，导致下游LLM4SR的效果不佳。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e论文中，作者用公式9来衡量semantic id保留pretrain多模态emb的信息量。具体来说，如果行为流中的商品序列是{A,B,C,D}，target item是E。使用pretrain多模态emb能计算出E和A~D的相似度，例如相似度\u0026lt;E,A\u0026gt; \u0026gt; \u0026lt;E,B\u0026gt;。如果将pretrain多模态emb转换成semantic id，然后由semantic id恢复出新的A~E的emb之后，再计算E和A~D的相似度，如果仍然有\u0026lt;E,A\u0026gt; \u0026gt; \u0026lt;E,B\u0026gt;，则认为一致（concordant），否则不一致（disconcordant）。这个分析方法挺好的，通过这个指标能估算出转换成semantic id之后，仍然保留原有pretrain多模态emb对搜推场景的\u003cstrong\u003e序\u003c/strong\u003e关系的保留程度。\u003c/li\u003e\n\u003cli\u003e作者发现，转换成semantic id之后，信息只保留了37.14%；进一步，如果semantic id是在下游任务中端到端训练的，则信息只保留了5.5%，也就是说94.5%的pretrain emb的序的信息都丢掉了，也就是灾难遗忘。\n\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-mme-sid-paper-reading/mme-sid-formula9.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"semantic-id构建方法\"\u003eSemantic id构建方法\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e3套emb来源，一套id-based经典搜推模型产出的包含协同信号的emb，另外两套是LLM2CLIP产出的多模态文本和图片emb。作者提到传统CLIP对长文本处理能力较弱，所以升级到LLM2CLIP，能更好地处理长文本。\u003c/li\u003e\n\u003cli\u003eSemantic id构建方法是经典的RQ-VAE的方法，但有如下两个改进点：\u003c/li\u003e\n\u003cli\u003e将emb的重构loss由MSE升级成MMD (maximum mean discrepancy)，MSE是计算原始emb和重构emb的欧式距离的误差，而MMD是计算两个分布的diff，实验表明能MMD比MSE能保留更多的pretrain多模态emb信息（即上述公式9），保留44.36%\u003c/li\u003e\n\u003cli\u003e对量化后的emb做了对齐，因为LLM2CLIP本身进行了图文模态的对齐，所以文中只新增了id emb分别和文本、图片模态的对齐\u003c/li\u003e\n\u003cli\u003e此外，还有一点论文没提但可能和常规RQ-VAE不同之处，就是原始emb在进行RQ-VAE之前，有一个Encoder升维的操作，在重构loss前对应有一个Decoder降维的操作，而semantic id量化恢复emb是Decoder之前的那个。这一升一降，估计也有助于缓解维度坍缩。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2025-10-04-mme-sid-paper-reading/mme-sid-fig2.png\"\u003e\u003c/p\u003e","title":"论文阅读：Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs"},{"content":"博客数据恢复中，敬请期待！\n测试图片： 测试代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // Necessary header files for input output functions #include \u0026lt;iostream\u0026gt; using namespace std; // main() function: where the execution of // C++ program begins int main() { // This statement prints \u0026#34;Hello World\u0026#34; cout \u0026lt;\u0026lt; \u0026#34;Hello World\u0026#34;; return 0; } 测试数学公式： This is an inline \\(a^*=x-b^*\\) equation.\nThese are block equations:\n\\[a^*=x-b^*\\]\\[ a^*=x-b^* \\]\\[ a^*=x-b^* \\]These are also block equations:\n$$a^*=x-b^*$$$$ a^*=x-b^* $$$$ a^*=x-b^* $$","permalink":"http://localhost:1313/posts/2025-08-15-announcement/","summary":"\u003cp\u003e博客数据恢复中，敬请期待！\u003c/p\u003e\n\u003cp\u003e测试图片：\n\u003cimg alt=\"这是图片\" loading=\"lazy\" src=\"/posts/2025-08-15-announcement/myimg.png\"\u003e\u003c/p\u003e\n\u003cp\u003e测试代码：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Necessary header files for input output functions\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e#include\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e\u0026lt;iostream\u0026gt;\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eusing\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enamespace\u003c/span\u003e std;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// main() function: where the execution of\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// C++ program begins\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// This statement prints \u0026#34;Hello World\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    cout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello World\u0026#34;\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e测试数学公式：\nThis is an inline \\(a^*=x-b^*\\) equation.\u003c/p\u003e","title":"公告"},{"content":"这是我的个人博客，数据恢复中\u0026hellip;\n欢迎评论，如需私信请联系: bitjoy@163.com\n","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003e这是我的个人博客，数据恢复中\u0026hellip;\u003c/p\u003e\n\u003cp\u003e欢迎评论，如需私信请联系: \u003ca href=\"mailto:bitjoy@163.com\"\u003ebitjoy@163.com\u003c/a\u003e\u003c/p\u003e","title":"关于"},{"content":"第九十一回 祭泸水汉相班师 伐中原武侯上表 诸葛亮平定南蛮之地后，准备班师回朝，路过泸水，被水中孤魂野鬼阻挡，不能前进。后来得知要用七七四十九个人头祭奠才能过河。诸葛亮仁慈之人，肯定不会杀人祭河，于是杀了很多牛羊，做成人头的样子，叫做“馒头”，祭奠泸水，这就是馒头的由来。最后孤魂野鬼散去，诸葛亮军队顺利回到成都。\n此时，北方魏国皇帝曹丕病故，享年才40岁，在位仅7年。皇位传给曹丕儿子曹睿。正好雍州、凉州没有太守，司马懿毛遂自荐去当这两个州的太守。诸葛亮利用反间计，散布司马懿要篡位的谣言，离间曹睿和司马懿，最终曹睿罢免司马懿，把司马懿赶回老家种田。\n另一方面，诸葛亮听说曹丕驾崩，幼子曹睿继位，打算北伐魏国。《出师表》就是这个时候写的。于是诸葛亮带着包括赵云在内的一路人马开始北伐。魏国那边就是夏侯渊的儿子夏侯楙前来迎敌。\n第九十二回 赵子龙力斩五将 诸葛亮智取三城 诸葛亮带着赵云等大将来战夏侯楙，夏侯楙那边派出西凉大将韩德及其四个儿子出战，父子一共五个人都被赵云杀了。夏侯楙大败，退守南安城，赵云等三路军马围攻南安城。夏侯楙向周边的天水、安定两个城求解。被诸葛亮使用“诈称魏将”的计策破了南安和安定两个城，捉拿了夏侯楙。只有天水城没有被攻破，而且天水城的谋士姜维（字伯约）识破了诸葛亮的计谋。\n第九十三回 姜伯约归降孔明 武乡侯骂死王朗 接着，诸葛亮带兵来攻打天水城，诸葛亮不知道天水城有姜维这么个文武双全的人才，低估了拿下天水城的难度。同时，姜维也巧妙的使用了兵法，所以在最初的几次战役中，诸葛亮大败。后来诸葛亮摸清了姜维的情况，精心安排了这样一个进攻策略：引一军打姜维老母亲所在地冀县；引一军打囤积粮草之地上邽；引一军打天水城。这个安排一方面把姜维引入到冀县，另一方面分散了天水城的兵力，导致天水城内部大乱。同时，诸葛亮放走夏侯楙，让他去招安姜维；还让人假扮姜维，制造各种姜维已经投降蜀汉的迹象，离间姜维和魏国。最后假戏真做，姜维无路可走，真的投降了蜀汉。在姜维的帮助下，天水城也拿下了。\n诸葛亮拿下汉中三城之后，一路北上，前出祁山，兵临渭水。魏国皇帝曹睿听说后大惊，派出以曹真为大都督，郭淮为副都督，司徒王朗为军师的大军来战诸葛亮。两阵对圆，王朗和诸葛亮出阵，两个人对骂，王朗先出招，骂诸葛亮反贼，不顺应天命；诸葛亮出招，骂王朗汉朝旧臣，都76岁高龄了还出来招摇，助纣为虐，死后有何颜面见二十四帝。最后王朗被诸葛亮骂死。\n这天晚上，曹真算准诸葛亮会乘丧劫寨，诸葛亮也将计就计去劫寨，但是还是诸葛亮老谋深算，劫寨之战以诸葛亮大获全胜。\n第九十四回 诸葛亮乘雪破羌兵 司马懿克日擒孟达 曹真部队失败之后，向西羌求救。西羌自曹操时代开始向曹操进贡，这次听闻魏国求救，于是答应了，派了丞相雅丹和武将越吉共15万大军去偷袭蜀国大本营。诸葛亮听说后，只能分出一部分兵力去抵抗西羌。派出去的关兴和张苞都被勇猛的西羌兵打败了。没办法，诸葛亮留赵云守祁山，自己亲自去西羌前线，巧妙的利用积雪，大破羌兵，杀了越吉，俘虏雅丹。诸葛亮放雅丹回国，西羌事平。\n另一方面，曹真听说诸葛亮退了一部分兵，于是趁机出兵攻打祁山。没想到被镇守的赵云和魏延大败。没办法，曹真求救曹睿皇帝。与此同时，之前关羽败走麦城向孟达求救，孟达没救并投降了魏国。这个时候，孟达又想回到蜀国的怀抱了，诸葛亮同意，让孟达攻打洛阳，诸葛亮攻打长安。\n曹睿如临大敌，太傅钟繇推荐把之前罢免的司马懿请回来。司马懿的军事才能应该不下于诸葛亮，且两个儿子（长子司马师、次子司马昭）素有大志，通晓兵法。司马懿复出的第一战就是大战孟达。结果司马懿获胜，孟达被杀。接着，曹睿正式派司马懿出关破蜀，司马懿带着大将张郃，浩浩荡荡，前来抗诸葛亮。\n第九十五回 马谡拒谏失街亭 武侯弹琴退仲达 诸葛亮算准了司马懿会首先进攻街亭，因为街亭是汉中的咽喉要道，一旦街亭失守，汉中就很危险了。马谡主动请缨去守街亭，于是诸葛亮派马谡主守街亭，同时派魏延、王平等驻扎在街亭附近支援马谡。\n马谡来到街亭之后，觉得这么个小地方，不足为惧，也不听王平的劝诫，非要在山上驻扎。司马懿来了之后，先断了山上游的水源，把山团团围住。马谡军中自乱，最后抵挡不住司马懿的大军，败下阵来。尽管有王平、魏延等的支援，依然不敌司马懿的大军。\n诸葛亮听说马谡大败之后，知道事情很危急，准备从祁山退军回到汉中。同时，为了不制造很大的动静，不让魏军后方的曹真等乘虚而入，诸葛亮各种算计，把关兴、张苞等身边的武将都派出去了。诸葛亮自己带着一帮文官守在西城。司马懿也算到了诸葛亮在西城，于是引大军来打西城。诸葛亮因为实在没有兵了，使出了空城计，把城门大开，军旗尽去，只安排几个士兵扮装老百姓的样子在城门口扫地。司马懿看到后，疑神疑鬼，因为知道诸葛亮向来深思熟虑，这次突然把城门大开，肯定有很大的埋伏，于是退兵了。诸葛亮的空城计成功了。\n在整个混乱的过程中，诸葛亮和司马懿互相算计，总的来说，互有胜负吧。结果就是诸葛亮暂停北伐脚步，退出祁山，回到汉中。司马懿收复陇西诸郡。\n第九十六回 孔明挥泪斩马谡 周鲂断发赚曹休 却说诸葛亮回到汉中之后，清点人马，因马谡之前有军令状，且是因为他不听王平劝诫才导致蜀军大败的，根据军法，挥泪斩马谡。\n话分两头，东吴的周鲂给曹休发了诈降书，且为了让曹休相信自己是真的投降，还把头发割掉了。最后曹休相信周鲂，曹睿派包括曹休在内的三路大军前去攻打东吴。结果很显然，曹休中计，大败而回，气忧成病，回到洛阳就死了。\n第九十七回 讨魏国武侯再上表 破曹兵姜维诈献书 因东吴帮忙击败了曹休，遂发书给诸葛亮让他再次出兵北伐，诸葛亮给后主写了《后出师表》，表明自己鞠躬尽瘁死而后已的精神，再次北伐。就在这时，传来一个坏消息，大将赵云病故，留下两个儿子赵统和赵广。于是诸葛亮北伐只剩几个老将和小将了：魏延、关兴、张苞、王平等。\n魏蜀两军在陈仓口（陈仓口再往北就是街亭）交锋，魏国守陈仓的是郝昭，深沟高垒，诸葛亮使出了各种计谋都没能攻破城池。同时，魏国朝廷派出大将军曹真和身强力壮的王双前来接应。诸葛亮派出的两员大将都被王双打败并杀了。\n最后，姜维使出诈降计，本来想赚曹真的，结果只赚到曹真的一个大将费耀。蜀军大获全胜，虽然没有杀到曹真，杀到费耀也不错。同时曹真损兵折将，蜀军又出祁山（第二次）。\n第九十八回 追汉军王双受诛 袭陈仓武侯取胜 却说诸葛亮第二次出祁山时，由于粮草只够支撑一个月时间，汉中前线只有陈仓这一个地方方便运粮，而陈仓被魏国的郝昭和王双把守着，所以诸葛亮必须速战速决，否则粮草支撑不住。魏国大都督就想了个办法，让部队假装运粮，吸引蜀军来抢粮食，实际粮车里装的都是干柴硫磺等易燃物，等蜀军来抢的时候，把蜀军烧死，同时突袭蜀军营寨。但是这个计策怎么能难倒诸葛亮，诸葛亮算准了，而且将计就计，结果反而把假扮运粮的魏军干掉了。但是蜀军毕竟是没粮了，不能恋战，而且曹睿此时又派了张郃领军来支援，于是，诸葛亮在赢了这一战的情况下，果断撤军。魏军此时刚刚失败，也不敢追诸葛亮。但是魏延军队在陈仓口防御王双，撤离的话，王双肯定是会和魏延干起来的。于是诸葛亮撤军的时候，偷偷给魏延一个锦囊妙计，大部队假装撤军，让王双来追，魏延和一小支部队等王双追出去之后，把王双营寨端了，王双回寨时，魏延又出其不意把王双干掉了。所以诸葛亮第二次出祁山，虽然没有干掉魏国，但是也小挫了一把魏军，同时安全回到汉中。曹真听说王双被杀，忧成疾病。\n此时，吴国孙权称帝了，蜀国派人前去祝贺，同时想联合吴国一起伐魏。吴国假装同意，实则想隔岸观火，坐收渔翁之利。\n又过了一阵子，诸葛亮听到守陈仓的郝昭病危，觉得这是一个进攻陈仓的好机会，于是又率部队去袭击陈仓，没等张郃救兵赶到，诸葛亮大军就把陈仓城攻破了，郝昭本来就病危，被这么一攻，吓死了。等张郃赶到的时候，发现诸葛亮大军把各个路段都占领了，于是退军。随后，诸葛亮又趁胜追击，拿下了散关。这就是诸葛亮第三次出祁山。\n曹睿听说诸葛亮又来了，吓死了，这个时候曹真病又没好，于是就把曹真的大都督职位交给了司马懿。司马懿出兵来与诸葛亮决战。\n第九十九回 诸葛亮大破魏兵 司马懿入寇西蜀 司马懿带着张郃等人前来与诸葛亮决战，但是前几次战争都失败了，于是司马懿决定深沟高垒，不再出战。此时，诸葛亮使用缓兵之计，每隔几天退几十里，给魏军要撤退的错觉。司马懿知道诸葛亮诡计多端，不敢追杀，但是大将张郃不同意，非要去追杀诸葛亮。司马懿没办法，让张郃先去追杀，自己随后就到。诸葛亮算准了司马懿的安排，调兵遣将，不但抵抗住了追杀，还安排部队袭击了司马懿的营寨。司马懿大败而回，虽然诸葛亮没有占领司马懿的营寨，但是歼敌无数。\n在这个过程中，虽然蜀军胜利，但是在一次战争中，张苞（张飞的儿子）不小心骑马掉到河里了，把头磕破了，回成都养病，但是后来死了。诸葛亮悲痛欲绝，决定撤军。诸葛亮第三次出祁山也没有显著成果。\n又过了一段时间，曹真病好了，决定和司马懿率大军进攻蜀国，曹睿同意。于是这两人率40万大军进攻蜀国。诸葛亮算准了接下来一个月都要下雨，蜀国地势又复杂，魏军远道而来，天时地利都不好，所以不用担心。果然这一个月都阴雨连绵，魏军粮草不够，很多士兵都饿死了。没办法，魏军主动撤退。\n第一百回 汉兵劫寨破曹真 武侯斗阵辱仲达 魏军退了之后，诸葛亮并不追杀，而是准备再次出祁山（第四次）。诸葛亮安排兵分两路，一路魏延、张嶷、杜琼、陈式攻打祁山之东箕谷，一路马岱、王平、张冀、马忠攻打祁山之西斜谷，然后会于祁山。司马懿算准了诸葛亮有这个安排，于是也兵分两路，司马懿守箕谷，曹真守斜谷，而且他两还打赌，如果诸葛亮真的来攻打，则司马懿赢，否则曹真赢。\n攻打箕谷的陈式不相信有司马懿的埋伏，不听劝阻，导致被司马懿打败，损兵折将。事后诸葛亮按军法处置，把陈式杀了。而另一方面，曹真不相信诸葛亮会攻打斜谷，消极守军，于是被马岱那路军马打败，诸葛亮顺利出祁山（第四次）。\n有一天，情报说曹真卧病不起，诸葛亮于是写了封信送给曹真看，把曹真骂了一顿，曹真看到信后怀恨而死。魏主曹睿命令司马懿出战。于是两军对峙，司马懿和诸葛亮开始比军事才能（斗阵），司马懿大败。诸葛亮收得胜之兵回祁山，永安城李严派来的送粮官苟安来送粮，但是延误了十天。诸葛亮很生气，按军法该杀，但是手下都求救，诸葛亮就给苟安杖打八十大板放回去了。苟安怀恨在心，投降司马懿。司马懿利用苟安，把苟安放回成都，但是要苟安散布谣言，造谣说诸葛亮要篡后主的位。后主就下令让诸葛亮撤军回成都。\n诸葛亮很伤心，好不容易又出一次祁山，而且战情对自己有利，现在居然被皇上下令撤军，没办法只好撤军了。不过，诸葛亮要杀苟安，后又不杀，杖打八十大板回去，这是很危险的，诸葛亮应该能想到这个小兵会叛变。要不就果断按军法处置杀了，要不就好好安抚。唉，错失良机啊。\n第一百一回 出陇上诸葛妆神 奔剑阁张郃中计 诸葛亮回到成都，查明了事情的真相，后主自己都不好意思了，但是苟安已经逃到魏国去了。\n过了一阵子，诸葛亮又上书要北伐魏国，于是第五次出祁山，兵分两路，王平等四个大将守祁山，诸葛亮带着魏延姜维守卤城。因粮草不够，打算去附近的陇上麦田割点麦子当粮食。司马懿算准了诸葛亮会兵分两路，于是也兵分两路，一路张郃守祁山，司马懿亲自带着人马去陇上。诸葛亮和司马懿在陇上交战，诸葛亮使用“奇门遁甲”之术，“变出”好多个诸葛亮，迷惑司马懿及魏军，然后偷偷安排大部队去陇上割麦。结果，魏军被诸葛亮迷惑得糊里糊涂，还损兵三千，蜀军割麦成功。\n正当诸葛亮优势比较明显时，又出幺蛾子了。诸葛亮突然收到粮草官李严的来信，说东吴接连魏国，打算进攻蜀国大后方。诸葛亮大惊，没办法，决定撤军。退军时，诸葛亮又布下了迷阵，让魏军不敢追杀。但是张郃偏偏要逞能，决定去追杀诸葛亮，在追杀的路上，被诸葛亮的埋伏给杀了。\n诸葛亮安全退回汉中，然后急忙回成都一探究竟，结果吴国并没有要进攻蜀国，而是李严谎报军情，因为李严是粮草官，因凑不齐给诸葛亮北伐的粮草，怕被诸葛亮惩罚，就向诸葛亮撒了这么一个谎，让诸葛亮回来。诸葛亮都快要被气死了，后主也很生气，打算杀了李严，但是朝廷官员说李严是刘备托孤的人之一，还是留一条活路吧，于是后主把李严贬为庶民，用李严的儿子李丰接替为粮草官。\n第一百二回 司马懿占北原渭桥 诸葛亮造木牛流马 却说诸葛亮在蜀国休养了三年，人马雄壮，又准备北伐，有人就劝他，为啥不好好在蜀国待着享受生活呢？诸葛亮说：“臣受先帝知遇之恩，梦寐之间，未尝不设伐魏之策。竭力尽忠，为陛下克复中原，重兴汉室：臣之愿也。”于是第六次出祁山北伐。\n就在诸葛亮出发的时候，听说关羽的儿子关兴病故，诸葛亮痛哭，张飞、关羽的儿子都相继去世，诸葛亮只能带上老将姜维、魏延，和一众小将王平等，共三四十万大军，准备北伐。司马懿方面，带着两个儿子司马师和司马昭，夏侯渊的四个儿子等大将，前来抗蜀。\n此次北伐第一战在渭河附近。诸葛亮谋划好的战略部署，都被司马懿算中了，结果蜀军大败，司马懿守住北原渭桥。诸葛亮写信给东吴孙权，请求支援，孙权同意，起兵三十万北伐。\n诸葛亮在渭水、祁山附近转悠，发现一个上方谷的地方，有点像葫芦，两口小，内部空间大，于是诸葛亮安排人员在里面造木牛流马，造好之后，用木牛流马去剑阁搬运粮草。司马懿知道后，趁蜀军搬运粮草的时候，劫了几个木牛流马回来，依葫芦画瓢，也造了很多木牛流马。诸葛亮知道后，很高兴，他就是要让司马懿学会也用木牛流马来运粮。木牛流马上有一个开关，可以控制木牛流马是否能运行，魏军不知道，只有诸葛亮知道。于是诸葛亮派人去劫魏军的粮草，而且相关的战斗都安排好了。结果蜀军大获全胜，魏军四处逃窜。\n第一百三回 上方谷司马受困 五丈原诸葛禳星 司马懿在渭河，深沟高垒，坚守不出。诸葛亮结合各种计谋，终于引司马懿出战。司马懿兵分两路，一路攻打祁山诸葛亮大本营，一路攻打上方谷。这都是诸葛亮策划好的，两处都有蜀军埋伏。司马懿亲自带着自己的两个儿子攻打上方谷，蜀军魏延引司马懿军队进入上方谷，就谷内放起火来，而且堵住了谷口。本来以为这次司马懿父子三人要被烧死，没想到上天下起雨来，扑灭了大火，救了司马懿。虽然如此，司马懿的两路军队都损失惨重。\n此后，司马懿更是深沟高垒，不敢出战。诸葛亮甚至给司马懿送女人的衣服羞辱司马懿，但是司马懿还是不出战，反倒问起诸葛亮的饮食起居，说诸葛亮吃得少，又日理万机，身体估计要垮掉。诸葛亮听了之后，神思不宁。\n后来，又听说孙权兵分三路攻打魏国，但是都失败了，长叹一声，旧病复发，从此身体就不好了。诸葛亮又夜观天象，发现将星欲坠，担心自己命不久矣。于是每天点蜡烛开始祈禳（祈祷），如果七天之内，蜡烛不灭，则自己的命还可以活很久，否则就要挂了。到第六天的时候，司马懿派人来刺探军情，魏延匆匆忙忙跑进帐篷，本来想向诸葛亮汇报军情的，因为走的太快了，带起来的风把诸葛亮的蜡烛给吹灭了。唉，诸葛亮命不久矣。\n第一百四回 陨大星汉丞相归天 见木像魏都督丧胆 诸葛亮知道自己很快要死了，开始安排后事，丞相一应大事都交给杨仪，用兵秘法各种兵书都传给了姜维，然后告诉杨仪，自己死后，魏延会反叛，你到时候打开我给你的锦囊，自有人收拾魏延的。另一方面，告诉杨仪，自己死后，不要发丧，把自己放在神龛里，这样天上的将星就不会掉下来了，还有当魏军来袭时，把事先准备好的我的木像拿出来，到时候魏军看到肯定要吓死的。没过几天，诸葛亮就死了。\n杨仪、姜维等人，根据诸葛亮的遗嘱，安排徐徐退军，让魏延断后。魏延觉得自己现在被官职比自己小的杨仪管着，不服气，想要联合马岱把杨仪除掉。但是，这时司马懿乘虚而入，以为诸葛亮死了，前来追杀。杨仪抬出事先准备好的诸葛亮的木像，司马懿和魏军吓得半死，拼命撤军，姜维乘势掩杀，魏军被杀自相践踏，死者无数。杨仪、姜维退入栈阁道口。\n第一百五回 武侯预伏锦囊计 魏主拆取承露盘 在杨仪、姜维退入道口的时候，前面遇到魏延反叛，烧断吊桥，挡住去路。魏延和姜维正面交锋，杨仪读了诸葛亮遗留下来的锦囊，对魏延说：“如果你敢在马上连叫三声“谁敢杀我”，我就把汉中地区让给你”。魏延果然在马上大叫“谁敢杀我”，魏延军中的马岱立马把魏延杀了。原来诸葛亮死之前，偷偷跟马岱说了，让马岱跟着魏延假装反叛，实则等到两方交锋时，趁魏延不提防自己，把魏延杀了。\n除掉魏延，杨仪、姜维抬着诸葛亮的灵柩，顺利回到成都。后主、文武百官、平民百姓失声痛哭。根据诸葛亮生前的嘱托，蒋琬为新的丞相，费祎为副丞相，吴懿和姜维守汉中。\n诸葛亮死后几年，魏蜀吴相安无事。魏主曹睿开始骄奢淫逸，大兴土木，听说汉武帝的长安宫有一个铜人，手捧一个承露盘，接天水，汉武帝就是喝这个天水可以长寿、返老返童。曹睿就派人把这个铜人和承露盘拆下来搬到自己建的园林里。另外，曹睿宠幸郭夫人，把原配毛皇后杀了，立郭夫人为新皇后。总之做出各种昏庸残暴的事情。\n第一百六回 公孙渊兵败死襄平 司马懿诈病赚曹爽 正当曹睿各种happy的时候，辽东的公孙渊开始造反了。公孙渊的爸爸是公孙康，当初曹操打袁绍儿子袁尚，袁尚就跑到公孙康那里，然后公孙康把袁尚杀了给了曹操，所以那时候公孙家族还是臣服于曹魏的。传到公孙渊这一代，开始膨胀，居然称帝并主动进攻魏国。曹睿就派司马懿出兵剿灭公孙渊，公孙渊哪是司马懿的对手，交手没几仗，司马懿在襄平把公孙渊及其家族灭了。\n话说有一天半夜三更，魏主曹睿突然看到毛皇后带着几个宫女来要曹睿的命，从那以后，曹睿就病倒了。没过多久，曹睿驾崩，享年36岁，在位13年。皇位传给了曹睿的儿子曹芳，由曹爽和司马懿辅政。曹爽的父亲是曹真，以前和司马懿一起出战防御诸葛亮北伐，那时候曹真和司马懿两人关系不太好。现在曹爽和司马懿一起辅佐曹芳，曹爽看不惯司马懿，于是跟皇上曹芳说可以把司马懿加为太傅。曹芳同意，于是兵权就落到了曹爽的手里。此后，曹爽和门下的幕僚就各种骄奢淫逸。而且曹爽担心司马懿会偷偷算计自己，还派人去太傅府打探司马懿的虚实。司马懿当然知道曹爽心里的小99，于是装聋作哑，装疯卖傻。曹爽信以为真，以为司马懿已经老到不行了，于是不再担心司马懿打击自己，更加得意忘形。\n第一百七回 魏主归政司马氏 姜维兵败牛头山 有一天，魏主曹芳要出城祭祖，曹爽因不再担心司马懿，带着所有手下大将和儿子们，跟着曹芳一起出城潇洒狩猎了。司马懿看准机会，起兵反叛，夺取了京城，并给曹爽送信说我自己想削你的兵权，你如果乖乖交出兵权，我就不追究你的责任。曹爽信以为真，果然交出印绶，徒手回城。司马懿接过印绶之后，立马把曹爽全家老小都抓起来斩首了。从此魏国军权落到了司马懿手里。事实上，如果曹爽那时候不相信司马懿的话，不回城，而是召集其他州的军队，攻打回京城，说不定曹爽能赢呢。Anyway，历史没有如果。\n雍州的夏侯霸和曹爽是亲族，听说曹爽被灭门了，司马懿肯定不会放过自己，就叛乱了，但是打仗又打不赢雍州的郭淮，于是夏侯霸就投降蜀国了。蜀国姜维，听说曹爽被灭门，司马懿掌权，又有夏侯霸来投降，想趁着魏国内乱，去攻打魏国。于是姜维起兵先攻打雍州郭淮，姜维的军事才能较差，在与郭淮的交战中，兵败牛头山。\n第一百八回 丁奉雪中奋短兵 孙峻席间施密计 姜维兵败，回到汉中。后来，司马懿病故，其两个儿子司马师、司马昭子承父业。又东吴陆逊、诸葛瑾接连去世，诸葛恪接替诸葛瑾。孙权也病故，儿子孙亮继位。\n魏国听说孙权死了，想乘机攻打吴国，于是派司马师、司马昭带着三路大军南下攻打吴国。东兴郡是吴国的要害地方，所以魏军先主攻这个地方。诸葛恪安排平北将军丁奉前来应战，当时正是下雪天，丁奉带着三千吴兵，脱掉盔甲，准备使用短兵和魏国互搏，魏军觉得吴军人少，且只用短兵，掉以轻心，结果丁奉带着吴军奋勇杀敌，大败魏军。司马昭转攻为守。\n诸葛恪觉得，既然打了胜仗，不如乘胜追击，北伐魏国。于是一面联系蜀国让联合进攻，一面主攻魏国的要害城市新城。但是呢，诸葛恪对下属太严酷，导致军心涣散，又中了魏军的计谋，最终大败而归。有意思的是，这个诸葛恪打了败仗回来之后，恼羞成怒，把责任都推到手下官兵手里，各种肃清杀替罪羊，还把孙权的亲戚孙峻的掌管御林军的职位给抢了。孙峻不干了，到孙亮面前告状，细数诸葛恪各种罪状，说要把他除掉。孙亮同意，于是孙亮摆了一个鸿门宴，请诸葛恪赴宴，孙峻就席间把诸葛恪杀了，从此吴国军权落到了孙峻手里。\n第一百九回 困司马汉将奇谋 废曹芳魏家果报 话说蜀国姜维接到诸葛恪“联合伐魏”的信号后，起兵出阳平关伐魏。司马师令徐质为先锋，司马昭为大都督，前去抵抗姜维。徐质英勇过人，姜维手下几个人和徐质交手都失败了。后来，姜维使用埋伏计，围困徐质，最后成功杀了徐质，并且把司马昭军队围困在铁笼山。郭淮听说司马昭被困，使用诈降计，骗了姜维的盟友羌人迷当。迷当被俘，投降郭淮。郭淮于是带着迷当的羌人部队，前去救援司马昭。司马昭得救，但是郭淮在和姜维的对战中，被姜维射死。虽然此次战役姜维最终还是失败回汉中，但杀了魏国两员大将：徐质和郭淮，也算是将功补过。\n话说魏主曹芳年幼没有兵权，司马师在魏国，大权在握，越加蛮横无理、目中无人。曹芳忍无可忍，血书衣带诏让人带出宫想召集人马铲除司马师势力。事情败露，被司马师知道了，司马师灭了参与人员家族，并且废了曹芳，另立曹髦为皇。有点像当年曹丕废汉献帝的赶脚，历史总是惊人的相似。\n昔日曹瞒相汉时，其他寡妇与孤儿。 谁知四十余年后，寡妇孤儿亦被欺。\n第一百十回 文鸯单骑退雄兵 姜维背水破大敌 却说司马家族先是杀曹爽，后又废曹芳，引起曹家亲信不满，其中就有镇东将军毋丘俭和扬州刺史文钦，于是他两联合打算讨伐司马师。司马师得了眼疾，刚把眼睛里的瘤子割了，没办法只能亲自出战，留弟弟司马昭掌管朝廷大事。\n在与司马师的对战中，文钦还带着他的儿子文鸯，文鸯骁勇善战，闯入魏军军中，如入无人之境。但是奈何寡不敌众，毋丘俭被杀，文钦知道大势已去，投奔东吴孙峻去了。自此司马师平定淮南叛乱。\n不过这场战争导致司马师眼疾恶化，加上回想起之前自己杀的人，脑子坏掉死了，魏国大权落到了弟弟司马昭的手里。\n蜀国听说司马师死了，司马昭刚刚接任，新旧交替，又打算趁机伐魏。于是姜维又从汉中起兵，攻打魏国。在第一战中，姜维采用背水一战、无路可退的策略，激发士兵战斗意志，打了胜仗。但是在接下来的战争中，毕竟魏国的军事实力还是要强一些，姜维兵败，被迫再次退回汉中。\n第一百十一回 邓士载智取姜伯约 诸葛诞义讨司马昭 姜维回到汉中之后，没过多久，又商议出师伐魏。魏国邓艾、陈泰等人应敌。姜维的计谋被邓艾识破，姜维兵败，再次退回汉中，而且在这次北伐过程中，大将张嶷在救姜维时被魏军杀害。\n魏国司马昭，常怀篡逆之心，想称帝，于是派手下贾充去探访各将军的口风。结果发现镇东将军诸葛诞（和诸葛亮是一个宗族的），不同意司马昭称帝。司马昭于是想除掉诸葛诞，诸葛诞得知之后，揭竿而起，联合东吴讨伐司马昭。司马昭得知之后，想亲自出征讨伐诸葛诞，又担心自己在外时朝廷发生政变，于是要求魏主曹髦和太后和自己一起出征讨伐诸葛诞，没办法，曹髦和太后只能乖乖跟着司马昭出征。\n东吴那边，孙峻病故，从弟孙綝继位，把持朝政。\n第一百十二回 救寿春于诠死节 取长城伯约鏖兵 司马昭和诸葛诞大战寿春城，东吴于诠救援寿春，终于寡不敌众，于诠和诸葛诞兵败被杀，淮南被司马昭平定。\n姜维听说司马昭带着魏主正和诸葛诞东吴大战，觉得这个时候可以乘虚而入，攻打魏国后方。于是再次北伐魏国，这次准备进攻魏国的长城，因魏国的很多粮草都囤积在这里。姜维大败长城将军司马望，司马望坚守不出。后魏将邓艾邓忠父子引兵来救，魏蜀打了一仗，后邓艾父子又深沟高垒，坚守不出，想等司马昭的大部队来救援。后来，姜维听说司马昭打败了诸葛诞，应该会派兵来支援长城，于是退兵回汉中了。\n第一百十三回 丁奉定计斩孙琳 姜维斗阵破邓艾 话说东吴的孙綝把持朝政，吴国皇帝孙亮送出衣带诏想秘密铲除孙綝，结果事发，孙綝杀了所有与之有关的人，罢黜孙亮，另立孙权的第6个儿子孙休为新皇帝。\n孙綝以为只是另立了一个傀儡皇帝，没想到孙休担心自己和孙亮一个下场，也决定铲除孙綝。这次孙休和丁奉秘密布了一个鸿门宴请孙綝赴宴，孙綝自恃把持朝政、掌握兵权，欣然赴约，被孙休灌酒，然后被丁奉、张布等人暗藏的刀斧手杀掉了，铲除了孙綝势力。\n东吴又派使者给蜀国，说司马昭肯定要篡位，到时候一定会入侵吴国、蜀国，希望吴、蜀能早做准备，共同抵御魏国。\n姜维听信后，又出祁山伐魏。姜维因为继承了诸葛亮的阵法，在战场上和魏国的邓艾、司马望等人斗阵法，邓艾、司马望大败。但是司马望想了一个点子，因为蜀国后主刘禅宠信黄皓，日夜以酒色为乐，所以派人去私通黄皓，让她在蜀国散步谣言，说姜维埋怨刘禅，不久要投降魏国了。刘禅听后，把姜维从前线叫回来了。姜维这次北伐又无功而返。哎，刘禅真是猪脑子，昏庸无能还听信谗言，之前把诸葛亮召回来，这次又把姜维召回来。\n第一百十四回 曹髦驱车死南阙 姜维弃粮胜魏兵 却说曹髦看到司马昭把持朝政横行霸道，担心他要篡位，是可忍孰不可忍，曹髦奋起反抗，带着几百个官僮，准备和司马昭大战。可想而知，司马昭掌握了军队，曹髦这是以卵击石。司马昭下令贾允迎战曹髦，贾允命令手下成济动手，成济杀死曹髦。\n赶到现场的司马昭还假惺惺的哭，问到底怎么回事，还下令把刽子手成济全家斩首。真是借刀杀人还卸磨杀驴。皇帝死了，有人就建议司马昭自立为皇帝吧，司马昭说当初曹操篡汉的时候没有称帝，自己篡魏时也不称帝，但是在有意培养自己的接班人儿子司马炎。所以司马昭还是立了一个傀儡皇帝曹操的孙子曹奂。\n姜维听说司马昭弑君，觉得这又是一个师出有名的伐魏的好机会，于是一面给东吴发信，一面起兵出祁山，兵分三路北伐魏国。魏国守祁山的依然是邓艾，邓艾和手下的王瓘商量了一个诈降计，让王瓘去姜维那里诈降，但是被姜维识破了，姜维将计就计，破了邓艾和王瓘，王瓘被蜀兵围困，投江自杀，邓艾也损失惨重。姜维和邓艾各自收兵回国。\n第一百十五回 诏班师后主信谗 托屯田姜维避祸 没过多久，姜维又要北伐魏国。带着夏侯霸、张冀等大将，进攻魏国的洮阳。夏侯霸作为先锋部队，因误入邓艾的空城计，自己和部队都被杀了。后姜维兵分两路，一路继续攻打洮阳，另一路偷袭邓艾的后方祁山寨，大败邓艾部队。就在姜维奋力围攻祁山寨时，收到后主刘禅的诏书，让班师回朝。后主宠幸宦官黄皓，骄奢淫逸。右将军阎宇想夺姜维的大将军职位，巴结黄皓，黄皓就把姜维召回来了。姜维第n次北伐又无功而返。\n姜维回来之后，百般无奈，就跟刘禅说自己打算带兵去沓中屯田，一来让士兵休养生息，二来多种些粮食，准备日后再次伐魏。刘禅同意，于是姜维沿路布下了几十个营寨，防止魏国入侵，自己带着几万部队在沓中屯田。\n司马昭听说蜀国皇帝昏庸无道，姜维也去屯田了，觉得这是个入侵的好机会，于是派邓艾、钟会等大将，准备入侵蜀国。\n第一百十六回 钟会分兵汉中道 武侯显圣定军山 钟会从京城出发，邓艾从祁山出发，浩浩荡荡，进攻蜀国，而且因为兵力充足，各处都安排了进攻路线，比如邓艾攻沓中，诸葛绪断后路等等，安排得妥妥帖帖。姜维赶紧上报朝廷，要求出兵增援，但后主刘禅因听信宦官黄皓，不理姜维。\n没办法，魏军声势浩大，蜀军寡不敌众，最终汉中失守，被钟会夺取。姜维等退入剑阁。\n在这次魏国进攻的战役中，当钟会进攻到定军山这个地方时，出现了好几次灵异事件，搞得钟会军心涣散，担惊受怕，后来问蜀国降将才知道诸葛亮的墓就在定军山，这些灵异事件应该是诸葛武侯显圣了。于是钟会赶紧去祭拜了诸葛亮的墓地，这些灵异事件才消失了。\n第一百十七回 邓士载偷度阴平 诸葛瞻战死绵竹 姜维在撤退的时候使用调虎离山之计成功迷惑了诸葛绪，钟会大怒，本来要杀诸葛绪的，后来众官告免，钟会就让人把诸葛绪押解会京城发落。因为诸葛绪是邓艾的部下，邓艾听到诸葛绪被钟会发落，很生气，决定不和钟会配合，自行从阴平进攻成都。\n阴平的路虽然很险阻，但是邓艾克服困难，最终胜利到达江油城，守城蜀将不战而降。邓艾占据江油，继续进攻绵竹。\n这时刘禅知道事态的严重性，把诸葛亮的儿子诸葛瞻请出来了，诸葛瞻带着儿子诸葛尚前去迎敌。两军在绵竹对战，诸葛瞻终因寡不敌众，父子都战死军中。邓艾大获全胜准备继续进攻成都。\n第一百十八回 哭祖庙一王死孝 入西川二士争功 后主刘禅听说邓艾来势汹汹，和众官商议后决定投降邓艾。刘禅的第五个儿子刘谌听说后，很为刘禅赶到羞愧和愤怒，但又无能为力，于是把自己的妻子儿女都杀了，然后自杀表明自己的气节。\n于是邓艾接受刘禅投降，出榜安民，蜀汉灭亡！\n姜维收到刘禅要求投降的诏书后，军中士兵一片哗然。姜维想了一个点子，也假装投降，但是向钟会投降，而且挑拨离间钟会和邓艾。本来钟会也觉得自己拖住了姜维的军队，邓艾才有机会进攻成都，自己也是有很大的功劳的，现在邓艾反而得了头等功，不服气。于是姜维和钟会一合计，决定向司马昭密告邓艾要盘踞蜀中，密谋造反。司马昭下令让钟会制衡邓艾，同时司马昭自己又怀疑钟会会造反，所以带着魏主曹奂御驾亲征，来视察钟会，这下有好戏看了。\n第一百十九回 假投降巧计成虚话 再受禅依样画葫芦 钟会把邓艾父子监禁，随后准备押回洛阳。后钟会听说司马昭御驾亲征驻扎在长安，准备入蜀，知道自己也要被司马昭杀害，于是和姜维商量计策，姜维说干脆伪造魏主郭太后的命令，讨伐司马昭。但是事情泄露，司马昭派来的监军卫瓘杀了钟会、姜维等人，随后又追杀了邓艾父子。也就是说，灭蜀的两大功臣邓艾和钟会，先是被姜维挑拨离间，后又被上级领导司马昭卸磨杀驴，太惨了。\n从此蜀国彻底灭亡，后主刘禅被押往洛阳，司马昭问刘禅是否思蜀，刘禅说：”此间乐、不思蜀”，悲哀，司马昭就不担心刘禅复国，给他各种好吃好喝，麻痹刘禅。\n没过多久，司马昭突然中风了，然后就死了。临死之前，司马昭立长子司马炎为太子。司马炎继位，废除魏主曹奂，称帝。自此魏国灭亡，晋国开始。\n第一百二十回 荐杜预老将献新谋 降孙皓三分归一统 却说吴主孙休，听说司马炎已经篡位，蜀国灭亡，则晋国一定会南下进攻吴国，忧虑成疾，死了。众官推举孙权的孙子孙皓继位。孙皓荒淫无道，百姓叫苦连天。晋国于是派羊祜南下伐吴，吴国派出陆抗应敌。但是呢，羊祜和陆抗两个奇葩，互相相敬如宾，礼尚往来，大家谁也不进攻，就这样相安无事。\n后来，羊祜都老了，回洛阳见司马炎，跟司马炎说一定要趁孙皓现在荒淫无道，失道寡助的时候，进攻吴国，错过这个机会，等吴国回过神来的时候就晚了。司马炎这才醒悟过来，让羊祜推荐人选，羊祜就推荐了杜预。于是司马炎派杜预出战，杜预率领三军，所向披靡，最后占领东吴。孙皓为了保命，像刘禅一样，投降晋国。自此吴国灭亡，三家归晋！\n有趣的是，魏蜀吴三家最后的皇帝曹奂、刘禅和孙皓，都被司马家族撸到洛阳，最后都善终了。\n","permalink":"http://localhost:1313/posts/2019-07-20-summary-of-the-romance-of-the-three-kingdoms-91-120/","summary":"\u003ch1 id=\"第九十一回-祭泸水汉相班师-伐中原武侯上表\"\u003e第九十一回 祭泸水汉相班师 伐中原武侯上表\u003c/h1\u003e\n\u003cp\u003e诸葛亮平定南蛮之地后，准备班师回朝，路过泸水，被水中孤魂野鬼阻挡，不能前进。后来得知要用七七四十九个人头祭奠才能过河。诸葛亮仁慈之人，肯定不会杀人祭河，于是杀了很多牛羊，做成人头的样子，叫做“馒头”，祭奠泸水，这就是馒头的由来。最后孤魂野鬼散去，诸葛亮军队顺利回到成都。\u003c/p\u003e\n\u003cp\u003e此时，北方魏国皇帝曹丕病故，享年才40岁，在位仅7年。皇位传给曹丕儿子曹睿。正好雍州、凉州没有太守，司马懿毛遂自荐去当这两个州的太守。诸葛亮利用反间计，散布司马懿要篡位的谣言，离间曹睿和司马懿，最终曹睿罢免司马懿，把司马懿赶回老家种田。\u003c/p\u003e\n\u003cp\u003e另一方面，诸葛亮听说曹丕驾崩，幼子曹睿继位，打算北伐魏国。《出师表》就是这个时候写的。于是诸葛亮带着包括赵云在内的一路人马开始北伐。魏国那边就是夏侯渊的儿子夏侯楙前来迎敌。\u003c/p\u003e\n\u003ch1 id=\"第九十二回-赵子龙力斩五将-诸葛亮智取三城\"\u003e第九十二回 赵子龙力斩五将 诸葛亮智取三城\u003c/h1\u003e\n\u003cp\u003e诸葛亮带着赵云等大将来战夏侯楙，夏侯楙那边派出西凉大将韩德及其四个儿子出战，父子一共五个人都被赵云杀了。夏侯楙大败，退守南安城，赵云等三路军马围攻南安城。夏侯楙向周边的天水、安定两个城求解。被诸葛亮使用“诈称魏将”的计策破了南安和安定两个城，捉拿了夏侯楙。只有天水城没有被攻破，而且天水城的谋士姜维（字伯约）识破了诸葛亮的计谋。\u003c/p\u003e\n\u003ch1 id=\"第九十三回-姜伯约归降孔明-武乡侯骂死王朗\"\u003e第九十三回 姜伯约归降孔明 武乡侯骂死王朗\u003c/h1\u003e\n\u003cp\u003e接着，诸葛亮带兵来攻打天水城，诸葛亮不知道天水城有姜维这么个文武双全的人才，低估了拿下天水城的难度。同时，姜维也巧妙的使用了兵法，所以在最初的几次战役中，诸葛亮大败。后来诸葛亮摸清了姜维的情况，精心安排了这样一个进攻策略：引一军打姜维老母亲所在地冀县；引一军打囤积粮草之地上邽；引一军打天水城。这个安排一方面把姜维引入到冀县，另一方面分散了天水城的兵力，导致天水城内部大乱。同时，诸葛亮放走夏侯楙，让他去招安姜维；还让人假扮姜维，制造各种姜维已经投降蜀汉的迹象，离间姜维和魏国。最后假戏真做，姜维无路可走，真的投降了蜀汉。在姜维的帮助下，天水城也拿下了。\u003c/p\u003e\n\u003cp\u003e诸葛亮拿下汉中三城之后，一路北上，前出祁山，兵临渭水。魏国皇帝曹睿听说后大惊，派出以曹真为大都督，郭淮为副都督，司徒王朗为军师的大军来战诸葛亮。两阵对圆，王朗和诸葛亮出阵，两个人对骂，王朗先出招，骂诸葛亮反贼，不顺应天命；诸葛亮出招，骂王朗汉朝旧臣，都76岁高龄了还出来招摇，助纣为虐，死后有何颜面见二十四帝。最后王朗被诸葛亮骂死。\u003c/p\u003e\n\u003cp\u003e这天晚上，曹真算准诸葛亮会乘丧劫寨，诸葛亮也将计就计去劫寨，但是还是诸葛亮老谋深算，劫寨之战以诸葛亮大获全胜。\u003c/p\u003e\n\u003ch1 id=\"第九十四回-诸葛亮乘雪破羌兵-司马懿克日擒孟达\"\u003e第九十四回 诸葛亮乘雪破羌兵 司马懿克日擒孟达\u003c/h1\u003e\n\u003cp\u003e曹真部队失败之后，向西羌求救。西羌自曹操时代开始向曹操进贡，这次听闻魏国求救，于是答应了，派了丞相雅丹和武将越吉共15万大军去偷袭蜀国大本营。诸葛亮听说后，只能分出一部分兵力去抵抗西羌。派出去的关兴和张苞都被勇猛的西羌兵打败了。没办法，诸葛亮留赵云守祁山，自己亲自去西羌前线，巧妙的利用积雪，大破羌兵，杀了越吉，俘虏雅丹。诸葛亮放雅丹回国，西羌事平。\u003c/p\u003e\n\u003cp\u003e另一方面，曹真听说诸葛亮退了一部分兵，于是趁机出兵攻打祁山。没想到被镇守的赵云和魏延大败。没办法，曹真求救曹睿皇帝。与此同时，之前关羽败走麦城向孟达求救，孟达没救并投降了魏国。这个时候，孟达又想回到蜀国的怀抱了，诸葛亮同意，让孟达攻打洛阳，诸葛亮攻打长安。\u003c/p\u003e\n\u003cp\u003e曹睿如临大敌，太傅钟繇推荐把之前罢免的司马懿请回来。司马懿的军事才能应该不下于诸葛亮，且两个儿子（长子司马师、次子司马昭）素有大志，通晓兵法。司马懿复出的第一战就是大战孟达。结果司马懿获胜，孟达被杀。接着，曹睿正式派司马懿出关破蜀，司马懿带着大将张郃，浩浩荡荡，前来抗诸葛亮。\u003c/p\u003e\n\u003ch1 id=\"第九十五回-马谡拒谏失街亭-武侯弹琴退仲达\"\u003e第九十五回 马谡拒谏失街亭 武侯弹琴退仲达\u003c/h1\u003e\n\u003cp\u003e诸葛亮算准了司马懿会首先进攻街亭，因为街亭是汉中的咽喉要道，一旦街亭失守，汉中就很危险了。马谡主动请缨去守街亭，于是诸葛亮派马谡主守街亭，同时派魏延、王平等驻扎在街亭附近支援马谡。\u003c/p\u003e\n\u003cp\u003e马谡来到街亭之后，觉得这么个小地方，不足为惧，也不听王平的劝诫，非要在山上驻扎。司马懿来了之后，先断了山上游的水源，把山团团围住。马谡军中自乱，最后抵挡不住司马懿的大军，败下阵来。尽管有王平、魏延等的支援，依然不敌司马懿的大军。\u003c/p\u003e\n\u003cp\u003e诸葛亮听说马谡大败之后，知道事情很危急，准备从祁山退军回到汉中。同时，为了不制造很大的动静，不让魏军后方的曹真等乘虚而入，诸葛亮各种算计，把关兴、张苞等身边的武将都派出去了。诸葛亮自己带着一帮文官守在西城。司马懿也算到了诸葛亮在西城，于是引大军来打西城。诸葛亮因为实在没有兵了，使出了空城计，把城门大开，军旗尽去，只安排几个士兵扮装老百姓的样子在城门口扫地。司马懿看到后，疑神疑鬼，因为知道诸葛亮向来深思熟虑，这次突然把城门大开，肯定有很大的埋伏，于是退兵了。诸葛亮的空城计成功了。\u003c/p\u003e\n\u003cp\u003e在整个混乱的过程中，诸葛亮和司马懿互相算计，总的来说，互有胜负吧。结果就是诸葛亮暂停北伐脚步，退出祁山，回到汉中。司马懿收复陇西诸郡。\u003c/p\u003e\n\u003ch1 id=\"第九十六回-孔明挥泪斩马谡-周鲂断发赚曹休\"\u003e第九十六回 孔明挥泪斩马谡 周鲂断发赚曹休\u003c/h1\u003e\n\u003cp\u003e却说诸葛亮回到汉中之后，清点人马，因马谡之前有军令状，且是因为他不听王平劝诫才导致蜀军大败的，根据军法，挥泪斩马谡。\u003c/p\u003e\n\u003cp\u003e话分两头，东吴的周鲂给曹休发了诈降书，且为了让曹休相信自己是真的投降，还把头发割掉了。最后曹休相信周鲂，曹睿派包括曹休在内的三路大军前去攻打东吴。结果很显然，曹休中计，大败而回，气忧成病，回到洛阳就死了。\u003c/p\u003e\n\u003ch1 id=\"第九十七回-讨魏国武侯再上表-破曹兵姜维诈献书\"\u003e第九十七回 讨魏国武侯再上表 破曹兵姜维诈献书\u003c/h1\u003e\n\u003cp\u003e因东吴帮忙击败了曹休，遂发书给诸葛亮让他再次出兵北伐，诸葛亮给后主写了《后出师表》，表明自己鞠躬尽瘁死而后已的精神，再次北伐。就在这时，传来一个坏消息，大将赵云病故，留下两个儿子赵统和赵广。于是诸葛亮北伐只剩几个老将和小将了：魏延、关兴、张苞、王平等。\u003c/p\u003e\n\u003cp\u003e魏蜀两军在陈仓口（陈仓口再往北就是街亭）交锋，魏国守陈仓的是郝昭，深沟高垒，诸葛亮使出了各种计谋都没能攻破城池。同时，魏国朝廷派出大将军曹真和身强力壮的王双前来接应。诸葛亮派出的两员大将都被王双打败并杀了。\u003c/p\u003e\n\u003cp\u003e最后，姜维使出诈降计，本来想赚曹真的，结果只赚到曹真的一个大将费耀。蜀军大获全胜，虽然没有杀到曹真，杀到费耀也不错。同时曹真损兵折将，蜀军又出祁山（第二次）。\u003c/p\u003e\n\u003ch1 id=\"第九十八回-追汉军王双受诛-袭陈仓武侯取胜\"\u003e第九十八回 追汉军王双受诛 袭陈仓武侯取胜\u003c/h1\u003e\n\u003cp\u003e却说诸葛亮第二次出祁山时，由于粮草只够支撑一个月时间，汉中前线只有陈仓这一个地方方便运粮，而陈仓被魏国的郝昭和王双把守着，所以诸葛亮必须速战速决，否则粮草支撑不住。魏国大都督就想了个办法，让部队假装运粮，吸引蜀军来抢粮食，实际粮车里装的都是干柴硫磺等易燃物，等蜀军来抢的时候，把蜀军烧死，同时突袭蜀军营寨。但是这个计策怎么能难倒诸葛亮，诸葛亮算准了，而且将计就计，结果反而把假扮运粮的魏军干掉了。但是蜀军毕竟是没粮了，不能恋战，而且曹睿此时又派了张郃领军来支援，于是，诸葛亮在赢了这一战的情况下，果断撤军。魏军此时刚刚失败，也不敢追诸葛亮。但是魏延军队在陈仓口防御王双，撤离的话，王双肯定是会和魏延干起来的。于是诸葛亮撤军的时候，偷偷给魏延一个锦囊妙计，大部队假装撤军，让王双来追，魏延和一小支部队等王双追出去之后，把王双营寨端了，王双回寨时，魏延又出其不意把王双干掉了。所以诸葛亮第二次出祁山，虽然没有干掉魏国，但是也小挫了一把魏军，同时安全回到汉中。曹真听说王双被杀，忧成疾病。\u003c/p\u003e\n\u003cp\u003e此时，吴国孙权称帝了，蜀国派人前去祝贺，同时想联合吴国一起伐魏。吴国假装同意，实则想隔岸观火，坐收渔翁之利。\u003c/p\u003e\n\u003cp\u003e又过了一阵子，诸葛亮听到守陈仓的郝昭病危，觉得这是一个进攻陈仓的好机会，于是又率部队去袭击陈仓，没等张郃救兵赶到，诸葛亮大军就把陈仓城攻破了，郝昭本来就病危，被这么一攻，吓死了。等张郃赶到的时候，发现诸葛亮大军把各个路段都占领了，于是退军。随后，诸葛亮又趁胜追击，拿下了散关。这就是诸葛亮第三次出祁山。\u003c/p\u003e\n\u003cp\u003e曹睿听说诸葛亮又来了，吓死了，这个时候曹真病又没好，于是就把曹真的大都督职位交给了司马懿。司马懿出兵来与诸葛亮决战。\u003c/p\u003e\n\u003ch1 id=\"第九十九回-诸葛亮大破魏兵-司马懿入寇西蜀\"\u003e第九十九回 诸葛亮大破魏兵 司马懿入寇西蜀\u003c/h1\u003e\n\u003cp\u003e司马懿带着张郃等人前来与诸葛亮决战，但是前几次战争都失败了，于是司马懿决定深沟高垒，不再出战。此时，诸葛亮使用缓兵之计，每隔几天退几十里，给魏军要撤退的错觉。司马懿知道诸葛亮诡计多端，不敢追杀，但是大将张郃不同意，非要去追杀诸葛亮。司马懿没办法，让张郃先去追杀，自己随后就到。诸葛亮算准了司马懿的安排，调兵遣将，不但抵抗住了追杀，还安排部队袭击了司马懿的营寨。司马懿大败而回，虽然诸葛亮没有占领司马懿的营寨，但是歼敌无数。\u003c/p\u003e\n\u003cp\u003e在这个过程中，虽然蜀军胜利，但是在一次战争中，张苞（张飞的儿子）不小心骑马掉到河里了，把头磕破了，回成都养病，但是后来死了。诸葛亮悲痛欲绝，决定撤军。诸葛亮第三次出祁山也没有显著成果。\u003c/p\u003e\n\u003cp\u003e又过了一段时间，曹真病好了，决定和司马懿率大军进攻蜀国，曹睿同意。于是这两人率40万大军进攻蜀国。诸葛亮算准了接下来一个月都要下雨，蜀国地势又复杂，魏军远道而来，天时地利都不好，所以不用担心。果然这一个月都阴雨连绵，魏军粮草不够，很多士兵都饿死了。没办法，魏军主动撤退。\u003c/p\u003e\n\u003ch1 id=\"第一百回-汉兵劫寨破曹真-武侯斗阵辱仲达\"\u003e第一百回 汉兵劫寨破曹真 武侯斗阵辱仲达\u003c/h1\u003e\n\u003cp\u003e魏军退了之后，诸葛亮并不追杀，而是准备再次出祁山（第四次）。诸葛亮安排兵分两路，一路魏延、张嶷、杜琼、陈式攻打祁山之东箕谷，一路马岱、王平、张冀、马忠攻打祁山之西斜谷，然后会于祁山。司马懿算准了诸葛亮有这个安排，于是也兵分两路，司马懿守箕谷，曹真守斜谷，而且他两还打赌，如果诸葛亮真的来攻打，则司马懿赢，否则曹真赢。\u003c/p\u003e\n\u003cp\u003e攻打箕谷的陈式不相信有司马懿的埋伏，不听劝阻，导致被司马懿打败，损兵折将。事后诸葛亮按军法处置，把陈式杀了。而另一方面，曹真不相信诸葛亮会攻打斜谷，消极守军，于是被马岱那路军马打败，诸葛亮顺利出祁山（第四次）。\u003c/p\u003e\n\u003cp\u003e有一天，情报说曹真卧病不起，诸葛亮于是写了封信送给曹真看，把曹真骂了一顿，曹真看到信后怀恨而死。魏主曹睿命令司马懿出战。于是两军对峙，司马懿和诸葛亮开始比军事才能（斗阵），司马懿大败。诸葛亮收得胜之兵回祁山，永安城李严派来的送粮官苟安来送粮，但是延误了十天。诸葛亮很生气，按军法该杀，但是手下都求救，诸葛亮就给苟安杖打八十大板放回去了。苟安怀恨在心，投降司马懿。司马懿利用苟安，把苟安放回成都，但是要苟安散布谣言，造谣说诸葛亮要篡后主的位。后主就下令让诸葛亮撤军回成都。\u003c/p\u003e\n\u003cp\u003e诸葛亮很伤心，好不容易又出一次祁山，而且战情对自己有利，现在居然被皇上下令撤军，没办法只好撤军了。不过，诸葛亮要杀苟安，后又不杀，杖打八十大板回去，这是很危险的，诸葛亮应该能想到这个小兵会叛变。要不就果断按军法处置杀了，要不就好好安抚。唉，错失良机啊。\u003c/p\u003e\n\u003ch1 id=\"第一百一回-出陇上诸葛妆神-奔剑阁张郃中计\"\u003e第一百一回 出陇上诸葛妆神 奔剑阁张郃中计\u003c/h1\u003e\n\u003cp\u003e诸葛亮回到成都，查明了事情的真相，后主自己都不好意思了，但是苟安已经逃到魏国去了。\u003c/p\u003e\n\u003cp\u003e过了一阵子，诸葛亮又上书要北伐魏国，于是第五次出祁山，兵分两路，王平等四个大将守祁山，诸葛亮带着魏延姜维守卤城。因粮草不够，打算去附近的陇上麦田割点麦子当粮食。司马懿算准了诸葛亮会兵分两路，于是也兵分两路，一路张郃守祁山，司马懿亲自带着人马去陇上。诸葛亮和司马懿在陇上交战，诸葛亮使用“奇门遁甲”之术，“变出”好多个诸葛亮，迷惑司马懿及魏军，然后偷偷安排大部队去陇上割麦。结果，魏军被诸葛亮迷惑得糊里糊涂，还损兵三千，蜀军割麦成功。\u003c/p\u003e\n\u003cp\u003e正当诸葛亮优势比较明显时，又出幺蛾子了。诸葛亮突然收到粮草官李严的来信，说东吴接连魏国，打算进攻蜀国大后方。诸葛亮大惊，没办法，决定撤军。退军时，诸葛亮又布下了迷阵，让魏军不敢追杀。但是张郃偏偏要逞能，决定去追杀诸葛亮，在追杀的路上，被诸葛亮的埋伏给杀了。\u003c/p\u003e\n\u003cp\u003e诸葛亮安全退回汉中，然后急忙回成都一探究竟，结果吴国并没有要进攻蜀国，而是李严谎报军情，因为李严是粮草官，因凑不齐给诸葛亮北伐的粮草，怕被诸葛亮惩罚，就向诸葛亮撒了这么一个谎，让诸葛亮回来。诸葛亮都快要被气死了，后主也很生气，打算杀了李严，但是朝廷官员说李严是刘备托孤的人之一，还是留一条活路吧，于是后主把李严贬为庶民，用李严的儿子李丰接替为粮草官。\u003c/p\u003e\n\u003ch1 id=\"第一百二回-司马懿占北原渭桥-诸葛亮造木牛流马\"\u003e第一百二回 司马懿占北原渭桥 诸葛亮造木牛流马\u003c/h1\u003e\n\u003cp\u003e却说诸葛亮在蜀国休养了三年，人马雄壮，又准备北伐，有人就劝他，为啥不好好在蜀国待着享受生活呢？诸葛亮说：“臣受先帝知遇之恩，梦寐之间，未尝不设伐魏之策。竭力尽忠，为陛下克复中原，重兴汉室：臣之愿也。”于是第六次出祁山北伐。\u003c/p\u003e","title":"《三国演义》每回内容梗概（91~120）"},{"content":"第六十一回 赵云截江夺阿斗 孙权遗书退老瞒 却说孙权想乘刘备入川的时候，武力讨回荆州，于是想了个法子，派人去荆州找孙权的妹妹（刘备的老婆，孙夫人），说老母亲病危，让她带着阿斗快点回东吴。孙夫人信以为真，火急火燎带着阿斗走了，也没告诉任何人。走的途中，被赵云和张飞追上，夺回了阿斗，孙夫人一个人回了东吴。\n此时，北方的曹操又南下攻打东吴，孙权只能暂时不管荆州，转而抵御曹操。曹操和孙权互相打了几个月，互有胜负，后来春雨连绵，困苦异常，曹操想撤军，又碍于面子。正好孙权写了一封信给曹操，劝曹操快点撤退吧，给了曹操一个台阶下，于是曹操就撤军了。\n第六十二回 取涪关杨高授首 攻雒城黄魏立功 刘备听说孙夫人回东吴了，曹操南下打孙权，曹操和孙权任何一方赢了，都有可能乘势攻打荆州，于是打算回荆州。张松听说之后写信给刘备，叫他当机立断夺取益州，没想到消息泄露，刘璋知道了刘备和张松的计谋，把张松满门抄斩。同时开始对抗刘备。\n庞统给刘备出了一条计策，先取涪城，后取雒城，最后夺成都。在涪城，刘璋手下杨怀、高沛守官，被庞统和刘备杀害，占领了涪城。在雒城，刘璋手下四员大将镇守（刘璝、泠苞、张任、邓贤），刘备手下黄忠和魏延争功，攻打雒城，杀了邓贤，但还是没有拿下雒城。\n第六十三回 诸葛亮痛哭庞统 张翼德义释严颜 庞统和刘备决定再次对雒城发起进攻，但是庞统夜观天象，发现凶兆，诸葛亮也来信说需要提高警惕，可能要出事。庞统的马也发脾气把庞统摔下马来，刘备好心，把自己的马让给庞统骑，刘备骑庞统的马。\n刘备和庞统分兵进攻，庞统进到一个落凤坡的地方，张任发现前面有一个人骑着刘备的马，以为就是刘备，集中放箭，误把庞统杀死了。庞统道号凤雏，正好在落凤坡牺牲。\n刘备和庞统这一仗算是失败收场，退回涪城。刘备写信给诸葛亮求救，诸葛亮安排关羽留守荆州，带着张飞赵云去增援刘备。张飞打先锋，来到巴郡，巴郡太守严颜深沟高垒不出战。最终，张飞用计把严颜骗出城来，俘虏了严颜，但是觉得严颜是条好汉，放了严颜，严颜也归顺了张飞。\n第六十四回 孔明定计捉张任 杨阜借兵破马超 孔明带着张飞和赵云来援助刘备攻打雒城，最后孔明用计活捉并杀死张任，夺取雒城。刘璋见雒城沦陷，派人求解汉中太守张鲁。\n话分两头，马超自从兵败曹操之后，退守羌地，在羌地占领了一大片地方，但是唯独冀城攻打不下。冀城刺史韦康求救夏侯渊也没被曹操同意。于是韦康就投降马超了，但是马超觉得韦康没诚意，把韦康灭门了。韦康手下杨阜先假装投降马超，然后说妻子死了要回老家奔丧，于是溜出冀城，联合几个表兄弟来反攻马超。此时，夏侯渊也得到曹操允许，要打马超。马超就被一堆人围攻，最后惨败，马超妻子和孩子都被杀了。马超最后仅剩下几个人逃到汉中张鲁手下。\n第六十五回 马超大战葭萌关 刘备自领益州牧 却说刘璋求救于张鲁，马超说自己刚来到张鲁手下，打算立个功，就主动请缨去帮刘璋打刘备吧。张鲁同意。\n于是，马超在葭萌关和张飞大战了几百回合，不分胜负。后来，孔明用计离间了马超和张鲁，然后凭三寸不烂之舌说服马超投降刘备。\n刘璋见大势已去，主动打开成都城门，让出益州，投降刘备。自此，刘备占领四川，成为益州牧。至于刘璋，刘备把他赶出四川，派到一个小地方（南郡公安）住了。\n第六十六回 关云长单刀赴会 伏皇后为国捐生 刘备夺取了益州之后，孙权又打算要回荆州。诸葛亮的哥哥诸葛瑾在孙权手下当谋士，于是把诸葛瑾一家老小都软禁了，跟诸葛瑾说诸葛亮要是再不还荆州，就把诸葛瑾一家灭门。于是派诸葛瑾去成都找诸葛亮讨还荆州。诸葛亮和刘备又演了一场戏，诸葛亮大哭，求刘备，刘备最终同意把荆州的三个郡还给孙权。但是实际镇守荆州的是关羽，关羽又耍赖皮，说将在外军令有所不受，不认刘备的书信。这次讨回荆州又失败了。后来鲁肃又想了一点子，请关羽赴鸿门宴，打算把关羽杀了，武力夺取荆州。关羽霸气十足，单刀赴会。在宴席上和手下周仓又演了一出戏。关羽假装喝醉酒了，仅仅握着鲁肃的手，鲁肃手下怕伤了鲁肃，也不敢动手围攻关羽。就这样，关羽实质上拿鲁肃当了人质，直到关羽上船成功逃离鸿门宴。鲁肃的计谋又失败了，不过这关羽也真够抵赖的。\n话分两头，曹操在许昌更加狂妄自大，还打算自立为魏王，谋士荀彧进谏说这样不好，被曹操训了一顿，荀彧忧愤成疾，死了。。。汉献帝和伏皇后也瑟瑟发抖，伏皇后就秘密派人出去给父亲伏完送信，让他铲除曹贼，结果事情泄漏，伏皇后、伏皇后和汉献帝的两个儿子，和伏完一家都被曹操杀死了。然后，曹操还强行把自己的女儿嫁给了汉献帝。\n第六十七回 曹操平定汉中地 张辽威震逍遥津 话说曹操势力越来越大，商议先取汉中，再收吴蜀。于是曹操亲自起大军攻打张鲁，经过几次战役，使了诈降、离间等计谋，最终夺取汉中地盘，原汉中太守张鲁投降曹操，被曹操封为镇南将军。\n此时曹操想得陇望蜀，顺势把蜀国也端了，四川人民听说曹操拿下汉中， 瑟瑟发抖。诸葛亮想到一个计策，曹操大军都在汉中这边，东边合淝等地空虚，可以再次联合东吴，让东吴进攻合淝，则曹操无暇再攻打西川了。于是这一次，刘备真的就把荆州一半地盘还给孙权了，然后和孙权说明了利害关系，孙权和谋士商量了一下，觉得进攻合淝对自己也有利，于是欣然同意了。孙权起大军进攻合淝。\n合淝等地当前由张辽、李典、乐进等人守关。由于敌众我寡，初期的几次战役都失败了，还丢了皖城。在逍遥津一战中，张辽排兵布阵很有讲究，加上使用诈降等计谋，成功重创了孙权部队，此即张辽威震逍遥津。但是孙权也不气馁，不断从东吴大本营调兵来前线。张辽自知敌众我寡，于是也赶紧去汉中找曹操搬救兵。\n第六十八回 甘宁百骑劫魏营 左慈掷杯戏曹操 曹操带大军来救合淝，和孙权在濡须口对峙。孙权手下大将甘宁自告奋勇，趁曹军初到，晚上只带一百个人去曹营劫寨，狠狠的挫了曹操的锐气。但是曹操毕竟势力强大，孙权在某一次战役中被困垓心，差点丧命。互相对峙了几个月，孙权看短期内取胜无望，决定和曹操求和，同时给曹操上贡。曹操同意，于是各自撤军了。\n曹操回到许昌后，建安二十一年夏，正式称王，即魏王。曹操大老婆丁夫人没生儿子；妾刘氏生曹昂，在南征张绣时战死；卞氏生四个儿子：曹丕，曹彰，曹植，曹熊。立长子曹丕为世子。\n曹操称王之后，有一个左慈来到许昌戏弄曹操，制造了各种灵异事件，比如曹操的橘子，拨开之后是空的；变出龙肝；变出牡丹花；变出松江鲈鱼等等。左慈还给曹操敬酒，曹操让左慈先喝，左慈把杯中酒一分为二，自饮一半，另一半给曹操。曹操大怒，左慈将杯变成一只白鸟飞走了。曹操恼羞成怒，但是无论怎样都杀不死左慈。\n第六十九回 卜周易管辂知机 讨汉贼五臣死节 曹操被左慈吓出病来。有人就请来了另一个“算命的”人管辂帮曹操算命，管辂告诉曹操这是幻术不要担心，曹操心宽，没几天病就好了。\n曹操又叫管辂算算东吴和西蜀，管辂说东吴有一员大将要亡，西蜀要出兵进犯汉中。果然没几日，有人来报东吴鲁肃死了，西蜀要出兵进犯曹操。曹操大怒，想要大举进军西蜀，管辂说且慢，来年春天许昌有火灾，于是曹操没进军，只是派几支军队加强西川防守，同时对首都许昌加强巡逻，防止火灾。\n却说许昌有五个汉朝官员，秘密商议，在元宵节放花灯的情况下，起兵讨伐曹贼。于是元宵节那天，许昌出现火灾，京城大乱。幸好管辂帮曹操算过，曹操也早有提防，叛乱被迅速镇压，五个汉臣也牺牲了。\n第七十回 猛张飞智取瓦口隘 老黄忠计夺天荡山 上回说到管辂算到西蜀出兵进犯汉中，即张飞带领部队去夺取瓦口隘（曹操手下张郃守关），张飞使用激将法激怒张郃，两军交战，张郃大败。后又使用前后包围方法，最终大败张郃，占领瓦口隘。\n张郃败走，曹洪又命令张郃去夺取葭萌关。葭萌关守将霍峻和孟达不敌张郃。刘备派手下黄忠和严颜两员老将前去增援。黄忠使用骄兵之计，大胜张郃。张郃败走，退到天荡山，天荡山是夏侯德镇守。黄忠和严颜使用前后包抄的方法，夺取天荡山，杀了夏侯德。张郃大败，又转投定军山夏侯渊处。定军山是汉中囤积粮草的地方，战略意义重大，所以曹操留夏侯渊在此镇守。\n第七十一回 占对山黄忠逸待劳 据汉水赵云寡胜众 张郃败走定军山之后，曹操知事情严重，亲自带兵四十万来战刘备。这个时候，老将黄忠又主动请缨，要求打定军山。诸葛亮拦不住，就派黄忠和法正一起去了，同时安排赵云在后面接应。\n在定军山一战中，黄忠发现定军山的西边有一座山，比定军山还高，而且可以俯瞰定军山内虚实，于是黄忠趁某个夜黑风高的晚上，占领了这座山。夏侯渊气坏了，被迫出战。法正和黄忠，一个在山顶观察敌情，另一个在山腰准备迎战。两队人马的配合，最终大败了夏侯渊，黄忠杀了夏侯渊，曹军大败，定军山失守，曹军移粮草去北山寨中。\n曹操听说自己的心腹大将夏侯渊被杀，很生气，亲自督战来定军山与夏侯渊报仇。在汉水附近，黄忠和赵云商量，黄忠前去烧粮草，赵云在后面接应，如果规定时间黄忠没回来，则赵云去救援。\n黄忠去烧粮草的时候，被曹操大军围困。赵云带队前去救援，赵云一个人冲入重围，如入无人之境，救出黄忠和副将张著。抵抗住了曹操在汉水的进攻。\n第七十二回 诸葛亮智取汉中 曹阿瞒兵退斜谷 刘备军团和曹操军团，以汉水为界，互相进攻。诸葛亮利用曹操多疑的性格，打败曹操。曹操接连丢失汉水，阳平关，退到斜谷界口。期间，曹操还把杨修杀了，因为杨修各种耍小聪明，曹操很讨厌他，在一次“鸡肋”事件中，以扰乱军心为名，杀了杨修。\n第七十三回 玄德进位汉中王 云长攻拔襄阳郡 曹操在汉中和刘备的战争中，接连失败，最后彻底丢掉了汉中。至此，刘备占据四川、汉中和一半的荆州。刘备手下百官和诸葛亮就商议推举刘备为皇帝，刘备哪里肯自立皇帝，连连推辞。但是大家苦苦相劝，最后刘备自立为汉中王。\n曹操听说刘备自立为王之后，非常生气，打算和孙权摒弃前嫌联合起来灭掉刘备。孙权说你家曹仁就在襄阳，你可先进兵攻打荆州关羽，我然后出兵。诸葛亮得知后，吩咐关羽先下手为强，即抢先进攻曹仁，而不等曹仁来打自己。在关羽和曹仁的战斗中，曹仁接连失败，丢了襄阳城，退兵樊城。\n第七十四回 庞令明抬榇决死战 关云长放水淹七军 曹操得知曹仁失败，派七路大军来救樊城。于禁为总指挥，庞德（庞令明）为先锋。因为庞德的哥哥在蜀国当官，庞德之前的老板马超现在也在蜀国，大家都担心庞德这样的身份去打蜀国，有可能也会反叛。庞德为了表明自己誓死效忠魏国的决心，自制一口棺材，奔赴樊城前线。\n在庞德和关羽的对决中，他两大战一百回合，不分胜负。后来庞德假装使用拖刀计，实则放冷箭，射中关羽，马上就要拿下关羽的时候，于禁担心庞德抢了头功，鸣金收兵，同时把七军都移到川口。庞德想要趁关羽受伤进攻关羽时，于禁也不肯，这个于禁真是坏了魏王的好事啊。\n过了几天关羽伤好了，观察敌情，发现于禁七军都屯于串口，这几日又秋雨连绵，于是打算决汉水水淹七军。于禁不听手下谋士的话，按兵不动，最后被关羽水淹七军。关羽活捉于禁和庞德，把于禁囚禁起来了，斩了庞德。\n第七十五回 关云长刮骨疗毒 吕子明白衣渡江 却说关羽灭了于禁七军，接着来围攻樊城，某一天在樊城北门观察敌情的时候，被城楼上的曹仁军放冷箭，射中右臂。箭上有毒，如果不及时医治，关羽右臂就要废掉。当时华佗听说之后，主动来找关羽，帮他治病。治病的方法就是切开右臂，直至看到骨头，然后把骨头上的毒刮下来，也就是刮骨疗毒。整个过程，关羽没说一点疼，还谈笑风生的下着棋。\n曹操听说关羽擒了于禁，又斩了庞德，害怕了，打算双管齐下，一方面派徐晃再去救樊城，另一方面秘密和孙权通信，因为关羽大部队都带去攻打樊城了，希望孙权能率军攻打荆州，让关羽腹背受敌。孙权同意，一方面陆口的吕蒙假装生病了，被孙权调回去江东养病，另一方面，换了一个没有什么才能的陆逊去守陆口。陆逊新官上任，假装对关羽示好，送各种东西麻痹关羽。果然关羽被麻痹了，觉得荆州陆口可以减少防守，把大部分军队都调往樊城。于是，吕蒙把军队假扮成穿白色衣服的商人，过江进攻并夺取了荆州。还剩下两个城，一个是傅士仁镇守的公安，另一个是糜芳镇守的南郡。没多久，傅士仁和糜芳都投降东吴了。至此，东吴全部收回当初借给刘备的荆州之地。\n第七十六回 徐公明大战沔水 关云长败走麦城 另一方面，曹操派遣的徐晃（徐公明）在樊城和关羽大战于沔水，关羽失败。关羽军中听说老家荆州也被东吴夺取了，军心动摇。关羽面临着前有魏军，后有吴军的困难局面。无奈，只有一个小小的麦城还没沦陷，关羽就带着手下仅剩的几百号人退守麦城了。同时，派手下去成都搬救兵。\n关羽失败的直接原因有两个，一个是前线失败，曹操毕竟军事力量强大，前者于禁庞德来打，庞德还把关羽右臂射伤了，后者徐晃和曹操又来，关羽受伤了还不能上战场，只能让义子关平等人上，军事力量消耗太大了；二个是后方沦陷，后方被东吴乘虚而入。所以关羽腹背受敌，很难不失败。\n第七十七回 玉泉山关公显圣 洛阳城曹操感神 却说关羽被围困麦城，见救兵迟迟不来，就带了两百人亲自冲出重围，去四川搬救兵，留下周仓和王甫等一百人守麦城。关羽去西川的路上，被吕蒙算准路线，设下埋伏，最终关羽父子寡不敌众，被吕蒙部下马忠生擒，然后被孙权杀害！周仓和王甫听说这个消息后，跳楼自杀，麦城失守，至此，荆襄之地回归东吴。\n却说关公魂魄不散，在孙权给吕蒙嘉奖赏酒之时，附体吕蒙，痛骂孙权。然后吕蒙莫名其妙就死了。孙权因害怕刘备报仇，把关羽父子首级送给曹操，一方面开始是曹操请孙权帮忙打关羽的，另一方面是想移祸曹操。在洛阳城，曹操看到关羽首级时，关羽显神。曹操既感慨又害怕，厚葬关羽。\n第七十八回 治风疾神医身死 传遗命奸雄数终 刘备听说关羽被孙权杀害，悲痛欲绝，发誓要灭了东吴，在诸葛亮等人的劝阻下，才稍稍平静了下来。\n却说曹操自从葬了关羽之后，心神不宁，老做噩梦，身体每况愈下。有人就说可以请华佗来医治，曹操同意。华佗说曹操得了风寒，必须把头劈开，取出风寒才能好。曹操大怒，以为华佗之前帮关羽刮骨疗毒，和关羽关系好，这次想帮关羽报仇。于是曹操把华佗关入监狱，华佗不久死在了狱中。\n曹操杀了华佗之后，病情加重，经常做噩梦，梦到之前杀过的人。没多久，曹操就病死了，临死之前，立长子曹丕为世子。\n第七十九回 兄逼弟曹植赋诗 侄陷叔刘封伏法 曹丕继位之后，开始铲除兄弟势力。曹丕的母亲卞氏生四个儿子：曹丕，曹彰，曹植，曹熊，曹丕为长子。老二曹彰主动回来奔丧，交出兵权，回自己封地去了，于是幸免于难。老四曹熊听说曹丕要清算了，吓个半死，上吊自杀了。老三曹植比较狂妄，也不回去奔丧，也不交出兵权。于是曹丕派一支军队去曹植那把曹植押回京城，曹丕为了刁难曹植，让曹植在七步之内作一首诗，作出来则免死贬官，作不出来则赐死。曹植于是作《七步诗》，幸免于难。\n却说刘备集团，打算先打东吴，为关羽报仇，然后进攻中原。但是在此之前，打算先除掉刘封和孟达，因为当初关羽败走麦城时，刘封和孟达没有去救关羽。诸葛亮用离间计，先把刘封和孟达分开了，给刘封升官。孟达知道自己马上要被刘备干掉了，连夜投降了魏国。于是刘备命令刘封攻打孟达，曹丕也命令孟达攻打刘封。刘封和孟达大战，刘封大败回成都，被刘备所杀，孟达因投降曹丕捡回一条小命。\n刘备杀了刘封之后，哀痛关羽，生病了。昔日曹操大将夏侯惇也病故了。天下即将大变。\n第八十回 曹丕废帝篡炎刘 汉王正位继大统 没过多久，曹丕及其部下威逼汉献帝禅让，起初汉献帝不愿意也不甘心，但是架不住曹丕的死亡威胁，还是乖乖把皇帝的位置让出来了。曹丕继位，国号大魏，贬汉献帝为山阳公，逐出许昌。曹丕继位之后，担心许昌宫中多妖怪，迁都洛阳。\n诸葛亮等人听说曹丕篡位，打算拥立刘备为汉王，刘备再三推迟，诸葛亮再三劝告。没办法，刘备继位汉帝。至此，魏国和蜀国正式成立。\n第八十一回 急兄仇张飞遇害 雪弟恨先主兴兵 却说刘备想要为关羽报仇，打算起全国之兵，进攻吴国，百官哭劝不止。张飞听到关羽被害的消息后，也非常悲愤，要给二哥报仇。于是刘备和张飞约了个日子，一起起兵进攻吴国。张飞回到自己的地盘后，想让手下做白旗白甲，挂孝伐吴。因急着起兵，要求手下三日之内必须把白旗白甲做好。手下范疆和张达说三天完不成，要宽限几日。张飞大怒，把他两绑在树上鞭打50下。这两个手下寻思，反正三天之内完不成任务也要被杀，还不如先下手为强，杀了张飞。于是，他两乘着张飞晚上喝醉酒深睡的时候，偷偷杀了张飞，带着首级投降东吴了。\n张飞被害的消息传到刘备那里后，刘备又悲痛欲绝，天天哭泣。化悲痛为力量，命令张飞的义子张苞，关羽的义子关兴，护驾，进攻东吴。\n第八十二回 孙权降魏受九锡 先主征吴赏六军 刘备带着几十万大军，浩浩荡荡向东吴进发。东吴听到后很害怕，诸葛瑾自告奋勇去劝刘备，想说服刘备不要打东吴，而是联合起来打曹丕，刘备不为所动。孙权集团遂向曹丕俯首称臣，希望曹丕能帮东吴，但是曹丕也隔岸观火，无动于衷。\n刘备手下张苞、关兴，作为先锋部队，势如破竹，沿路捷报频传，杀了好多东吴的大将。刘备听闻，犒赏六军。\n第八十三回 战猇亭先主得仇人 守江口书生拜大将 刘备军团在猇亭这个地方，取得了重大胜利：一方面关兴杀了潘璋，糜芳和傅士仁怕死，把杀关羽的马忠杀了，投降刘备，刘备哪里肯放过他们，把糜芳和傅士仁杀了，祭奠关羽，至此关羽的仇已报；另一方面，孙权看到吴军节节败退，把杀张飞的范疆、张达两人和张飞首级送给刘备，刘备杀了范疆和张达，祭奠张飞，至此，张飞的仇已报。\n但是，刘备还是不解气，依然向东吴进发。孙权瑟瑟发抖，因东吴之前的军事首领（大都督）周瑜、鲁肃、吕蒙接连去世，现在国难当头，急需一个大都督。当局讨论了好几天，最后立一个书生陆逊为大都督，吴军中多有不服气的。陆逊上任之后，下令深沟高垒，坚守不出，手下军官都觉得陆逊没本事，不敢与刘备打。但是陆逊的策略是刘备率大军而来，锐不可当，不能与之争锋，所以暂避避刘备的锋芒，等刘备过了这阵子，我们再打。\n果然刘备看陆逊不出战，因夏天酷暑难耐，刘备就把大部队移到附近茂密的树林中，靠近小溪安营扎寨，同时把目前排兵布阵的情况回传给诸葛亮商议。\n第八十四回 陆逊营烧七百里 孔明巧布八阵图 陆逊等到刘备在茂密树林中联营扎寨七百余里，知道反攻的机会来了，令部下用火攻。把蜀军所有帐篷都烧了，死伤无数，刘备在一票人马的护送之下，退守白帝城。刘备大败。孙夫人听到部下有人说刘备大败，已经死于军中，遂望西而泣，投江自尽。\n陆逊沿路追击，快要到白帝城时，路过鱼腹浦，感到杀气冲天，误入诸葛亮入川时布下的八阵图，差点出不来。幸好诸葛亮的岳父黄承彦看到后把他救了出来。\n第八十五回 刘先主遗诏托孤儿 诸葛亮安居平五路 话说刘备兵败猇亭彝陵，退守白帝城。没多久就生病了，眼看着病情一天天加重，刘备就把诸葛亮从成都叫到了白帝城，立了遗嘱，交待了后事，把三个儿子托付给诸葛亮，然后就驾崩了。刘备大儿子刘禅即位，称为后主。\n曹丕听说刘备驾崩，决定和周边四个部落联合，加上曹丕本国部队，一共五路大军进攻蜀国。刘禅听说后慌得要死，赶紧请诸葛亮出主意，但请了好几次诸葛亮都不出丞相府，最后刘禅亲自登门拜访，才知道诸葛亮早已有退兵之计了，为了保密才躲在丞相府不出来。唯一担心的是孙权一路军队，诸葛亮就派邓芝出使东吴，想拉拢东吴，联吴抗魏。\n第八十六回 难张温秦宓逞天辩 破曹丕徐盛用火攻 邓芝出使东吴，不辱使命，吴王同意与蜀国联合抗魏，并派使者张温和邓芝一起去蜀国答谢。张温来到蜀国之后，受到后主、诸葛亮的热情款待。张温因此显得傲慢起来，蜀国学士秦宓和张温展开了激烈的辩论，秦宓获胜，灭了张温威风。诸葛亮担心张温感到羞愧不利于蜀吴联合，又派邓芝和张温一起回东吴再次感谢。自此吴蜀联合。\n魏国听说了，怒了，曹丕亲自带兵攻打吴国，吴国手下徐盛大败曹丕，在这一战中还把曹操旧部张辽杀了。同时蜀国也派赵云出兵阳平关，进攻长安。\n却说突然赵云接到诸葛亮通知，南蛮入侵，让赵云赶紧回来南征，让马超坚守阳平关。\n第八十七回 征南寇丞相大兴师 抗天兵蛮王初受执 诸葛亮带着赵云、魏延等大将，起兵五十万，攻打南蛮。\n原蜀国三郡太守雍闿、朱褒、高定在孟获的引诱下反叛蜀国。诸葛亮使用反间计离间了这三人，高定把其余两人都杀了，并归顺诸葛亮。\n诸葛亮继续南下攻打孟获，孟获派三洞元帅分兵三路到来，诸葛亮通过巧妙的调兵遣将，很快制服了这三路兵马。\n接着，孟获亲自出场，也被诸葛亮的排兵布阵生擒了。孟获不服，诸葛亮放孟获回去，孟获说下次要是再失败，就心服。\n第八十八回 渡泸水再缚番王 识诈降三擒孟获 孟获被诸葛亮放回后，决定以泸水为防线，深沟高垒，希望诸葛亮主动退兵。结果诸葛亮侦查出泸水水流较缓的一段，派军偷渡过河，通过反间计，离间孟获和几个洞主，洞主被孟获鞭打，洞主反戈，生擒孟获给诸葛亮。孟获不服，认为是手下反叛导致自己失败的。诸葛亮放走孟获。\n第二次，孟获让亲弟孟优来诸葛亮寨中诈降，被诸葛亮识破，又抓住孟获。孟获又不服，认为是弟弟孟优诈降不当导致自己失败的。诸葛亮又放走孟获。\n第八十九回 武乡侯四番用计 南蛮王五次遭擒 这次孟获躲到某个好朋友的洞中，该洞有两个入口，一南一北，南口很安全，被孟获和洞主堵住了，蜀军无法进入。北口很危险，沿路有4口泉水，但都有毒，而且早上还有瘴气，非常危险。诸葛亮第一次带队从北口进入时，很多士兵都误喝了毒水，后来通过山上的老叟才知道了破解方法。最后诸葛亮带队攻入北口，与此同时，其他洞主有感于诸葛亮的不杀之恩，主动把孟获抓起来给诸葛亮了。孟获觉得这次也是其他洞主反叛导致自己失败的，不服。诸葛亮第五次放走孟获。\n第九十回 驱巨兽六破蛮兵 烧藤甲七擒孟获 孟获第五次失败后，去拉外援了。第六次叫来了另一个洞主木鹿大王，能呼风唤雨，能指挥虎狼豺豹。被诸葛亮用木制巨兽打败。第七次请来了乌戈国帮忙，乌戈国人都穿藤甲能防毒水，于是诸葛亮设计火攻，大败乌戈国人。\n就这样总计七擒七纵，最后孟获心服口服，归顺诸葛亮。\n","permalink":"http://localhost:1313/posts/2019-07-17-summary-of-the-romance-of-the-three-kingdoms-61-90/","summary":"\u003ch1 id=\"第六十一回-赵云截江夺阿斗-孙权遗书退老瞒\"\u003e第六十一回 赵云截江夺阿斗 孙权遗书退老瞒\u003c/h1\u003e\n\u003cp\u003e却说孙权想乘刘备入川的时候，武力讨回荆州，于是想了个法子，派人去荆州找孙权的妹妹（刘备的老婆，孙夫人），说老母亲病危，让她带着阿斗快点回东吴。孙夫人信以为真，火急火燎带着阿斗走了，也没告诉任何人。走的途中，被赵云和张飞追上，夺回了阿斗，孙夫人一个人回了东吴。\u003c/p\u003e\n\u003cp\u003e此时，北方的曹操又南下攻打东吴，孙权只能暂时不管荆州，转而抵御曹操。曹操和孙权互相打了几个月，互有胜负，后来春雨连绵，困苦异常，曹操想撤军，又碍于面子。正好孙权写了一封信给曹操，劝曹操快点撤退吧，给了曹操一个台阶下，于是曹操就撤军了。\u003c/p\u003e\n\u003ch1 id=\"第六十二回-取涪关杨高授首-攻雒城黄魏立功\"\u003e第六十二回 取涪关杨高授首 攻雒城黄魏立功\u003c/h1\u003e\n\u003cp\u003e刘备听说孙夫人回东吴了，曹操南下打孙权，曹操和孙权任何一方赢了，都有可能乘势攻打荆州，于是打算回荆州。张松听说之后写信给刘备，叫他当机立断夺取益州，没想到消息泄露，刘璋知道了刘备和张松的计谋，把张松满门抄斩。同时开始对抗刘备。\u003c/p\u003e\n\u003cp\u003e庞统给刘备出了一条计策，先取涪城，后取雒城，最后夺成都。在涪城，刘璋手下杨怀、高沛守官，被庞统和刘备杀害，占领了涪城。在雒城，刘璋手下四员大将镇守（刘璝、泠苞、张任、邓贤），刘备手下黄忠和魏延争功，攻打雒城，杀了邓贤，但还是没有拿下雒城。\u003c/p\u003e\n\u003ch1 id=\"第六十三回-诸葛亮痛哭庞统-张翼德义释严颜\"\u003e第六十三回 诸葛亮痛哭庞统 张翼德义释严颜\u003c/h1\u003e\n\u003cp\u003e庞统和刘备决定再次对雒城发起进攻，但是庞统夜观天象，发现凶兆，诸葛亮也来信说需要提高警惕，可能要出事。庞统的马也发脾气把庞统摔下马来，刘备好心，把自己的马让给庞统骑，刘备骑庞统的马。\u003c/p\u003e\n\u003cp\u003e刘备和庞统分兵进攻，庞统进到一个落凤坡的地方，张任发现前面有一个人骑着刘备的马，以为就是刘备，集中放箭，误把庞统杀死了。庞统道号凤雏，正好在落凤坡牺牲。\u003c/p\u003e\n\u003cp\u003e刘备和庞统这一仗算是失败收场，退回涪城。刘备写信给诸葛亮求救，诸葛亮安排关羽留守荆州，带着张飞赵云去增援刘备。张飞打先锋，来到巴郡，巴郡太守严颜深沟高垒不出战。最终，张飞用计把严颜骗出城来，俘虏了严颜，但是觉得严颜是条好汉，放了严颜，严颜也归顺了张飞。\u003c/p\u003e\n\u003ch1 id=\"第六十四回-孔明定计捉张任-杨阜借兵破马超\"\u003e第六十四回 孔明定计捉张任 杨阜借兵破马超\u003c/h1\u003e\n\u003cp\u003e孔明带着张飞和赵云来援助刘备攻打雒城，最后孔明用计活捉并杀死张任，夺取雒城。刘璋见雒城沦陷，派人求解汉中太守张鲁。\u003c/p\u003e\n\u003cp\u003e话分两头，马超自从兵败曹操之后，退守羌地，在羌地占领了一大片地方，但是唯独冀城攻打不下。冀城刺史韦康求救夏侯渊也没被曹操同意。于是韦康就投降马超了，但是马超觉得韦康没诚意，把韦康灭门了。韦康手下杨阜先假装投降马超，然后说妻子死了要回老家奔丧，于是溜出冀城，联合几个表兄弟来反攻马超。此时，夏侯渊也得到曹操允许，要打马超。马超就被一堆人围攻，最后惨败，马超妻子和孩子都被杀了。马超最后仅剩下几个人逃到汉中张鲁手下。\u003c/p\u003e\n\u003ch1 id=\"第六十五回-马超大战葭萌关-刘备自领益州牧\"\u003e第六十五回 马超大战葭萌关 刘备自领益州牧\u003c/h1\u003e\n\u003cp\u003e却说刘璋求救于张鲁，马超说自己刚来到张鲁手下，打算立个功，就主动请缨去帮刘璋打刘备吧。张鲁同意。\u003c/p\u003e\n\u003cp\u003e于是，马超在葭萌关和张飞大战了几百回合，不分胜负。后来，孔明用计离间了马超和张鲁，然后凭三寸不烂之舌说服马超投降刘备。\u003c/p\u003e\n\u003cp\u003e刘璋见大势已去，主动打开成都城门，让出益州，投降刘备。自此，刘备占领四川，成为益州牧。至于刘璋，刘备把他赶出四川，派到一个小地方（南郡公安）住了。\u003c/p\u003e\n\u003ch1 id=\"第六十六回-关云长单刀赴会-伏皇后为国捐生\"\u003e第六十六回 关云长单刀赴会 伏皇后为国捐生\u003c/h1\u003e\n\u003cp\u003e刘备夺取了益州之后，孙权又打算要回荆州。诸葛亮的哥哥诸葛瑾在孙权手下当谋士，于是把诸葛瑾一家老小都软禁了，跟诸葛瑾说诸葛亮要是再不还荆州，就把诸葛瑾一家灭门。于是派诸葛瑾去成都找诸葛亮讨还荆州。诸葛亮和刘备又演了一场戏，诸葛亮大哭，求刘备，刘备最终同意把荆州的三个郡还给孙权。但是实际镇守荆州的是关羽，关羽又耍赖皮，说将在外军令有所不受，不认刘备的书信。这次讨回荆州又失败了。后来鲁肃又想了一点子，请关羽赴鸿门宴，打算把关羽杀了，武力夺取荆州。关羽霸气十足，单刀赴会。在宴席上和手下周仓又演了一出戏。关羽假装喝醉酒了，仅仅握着鲁肃的手，鲁肃手下怕伤了鲁肃，也不敢动手围攻关羽。就这样，关羽实质上拿鲁肃当了人质，直到关羽上船成功逃离鸿门宴。鲁肃的计谋又失败了，不过这关羽也真够抵赖的。\u003c/p\u003e\n\u003cp\u003e话分两头，曹操在许昌更加狂妄自大，还打算自立为魏王，谋士荀彧进谏说这样不好，被曹操训了一顿，荀彧忧愤成疾，死了。。。汉献帝和伏皇后也瑟瑟发抖，伏皇后就秘密派人出去给父亲伏完送信，让他铲除曹贼，结果事情泄漏，伏皇后、伏皇后和汉献帝的两个儿子，和伏完一家都被曹操杀死了。然后，曹操还强行把自己的女儿嫁给了汉献帝。\u003c/p\u003e\n\u003ch1 id=\"第六十七回-曹操平定汉中地-张辽威震逍遥津\"\u003e第六十七回 曹操平定汉中地 张辽威震逍遥津\u003c/h1\u003e\n\u003cp\u003e话说曹操势力越来越大，商议先取汉中，再收吴蜀。于是曹操亲自起大军攻打张鲁，经过几次战役，使了诈降、离间等计谋，最终夺取汉中地盘，原汉中太守张鲁投降曹操，被曹操封为镇南将军。\u003c/p\u003e\n\u003cp\u003e此时曹操想得陇望蜀，顺势把蜀国也端了，四川人民听说曹操拿下汉中， 瑟瑟发抖。诸葛亮想到一个计策，曹操大军都在汉中这边，东边合淝等地空虚，可以再次联合东吴，让东吴进攻合淝，则曹操无暇再攻打西川了。于是这一次，刘备真的就把荆州一半地盘还给孙权了，然后和孙权说明了利害关系，孙权和谋士商量了一下，觉得进攻合淝对自己也有利，于是欣然同意了。孙权起大军进攻合淝。\u003c/p\u003e\n\u003cp\u003e合淝等地当前由张辽、李典、乐进等人守关。由于敌众我寡，初期的几次战役都失败了，还丢了皖城。在逍遥津一战中，张辽排兵布阵很有讲究，加上使用诈降等计谋，成功重创了孙权部队，此即张辽威震逍遥津。但是孙权也不气馁，不断从东吴大本营调兵来前线。张辽自知敌众我寡，于是也赶紧去汉中找曹操搬救兵。\u003c/p\u003e\n\u003ch1 id=\"第六十八回-甘宁百骑劫魏营-左慈掷杯戏曹操\"\u003e第六十八回 甘宁百骑劫魏营 左慈掷杯戏曹操\u003c/h1\u003e\n\u003cp\u003e曹操带大军来救合淝，和孙权在濡须口对峙。孙权手下大将甘宁自告奋勇，趁曹军初到，晚上只带一百个人去曹营劫寨，狠狠的挫了曹操的锐气。但是曹操毕竟势力强大，孙权在某一次战役中被困垓心，差点丧命。互相对峙了几个月，孙权看短期内取胜无望，决定和曹操求和，同时给曹操上贡。曹操同意，于是各自撤军了。\u003c/p\u003e\n\u003cp\u003e曹操回到许昌后，建安二十一年夏，正式称王，即魏王。曹操大老婆丁夫人没生儿子；妾刘氏生曹昂，在南征张绣时战死；卞氏生四个儿子：曹丕，曹彰，曹植，曹熊。立长子曹丕为世子。\u003c/p\u003e\n\u003cp\u003e曹操称王之后，有一个左慈来到许昌戏弄曹操，制造了各种灵异事件，比如曹操的橘子，拨开之后是空的；变出龙肝；变出牡丹花；变出松江鲈鱼等等。左慈还给曹操敬酒，曹操让左慈先喝，左慈把杯中酒一分为二，自饮一半，另一半给曹操。曹操大怒，左慈将杯变成一只白鸟飞走了。曹操恼羞成怒，但是无论怎样都杀不死左慈。\u003c/p\u003e\n\u003ch1 id=\"第六十九回-卜周易管辂知机-讨汉贼五臣死节\"\u003e第六十九回 卜周易管辂知机 讨汉贼五臣死节\u003c/h1\u003e\n\u003cp\u003e曹操被左慈吓出病来。有人就请来了另一个“算命的”人管辂帮曹操算命，管辂告诉曹操这是幻术不要担心，曹操心宽，没几天病就好了。\u003c/p\u003e\n\u003cp\u003e曹操又叫管辂算算东吴和西蜀，管辂说东吴有一员大将要亡，西蜀要出兵进犯汉中。果然没几日，有人来报东吴鲁肃死了，西蜀要出兵进犯曹操。曹操大怒，想要大举进军西蜀，管辂说且慢，来年春天许昌有火灾，于是曹操没进军，只是派几支军队加强西川防守，同时对首都许昌加强巡逻，防止火灾。\u003c/p\u003e\n\u003cp\u003e却说许昌有五个汉朝官员，秘密商议，在元宵节放花灯的情况下，起兵讨伐曹贼。于是元宵节那天，许昌出现火灾，京城大乱。幸好管辂帮曹操算过，曹操也早有提防，叛乱被迅速镇压，五个汉臣也牺牲了。\u003c/p\u003e\n\u003ch1 id=\"第七十回-猛张飞智取瓦口隘-老黄忠计夺天荡山\"\u003e第七十回 猛张飞智取瓦口隘 老黄忠计夺天荡山\u003c/h1\u003e\n\u003cp\u003e上回说到管辂算到西蜀出兵进犯汉中，即张飞带领部队去夺取瓦口隘（曹操手下张郃守关），张飞使用激将法激怒张郃，两军交战，张郃大败。后又使用前后包围方法，最终大败张郃，占领瓦口隘。\u003c/p\u003e\n\u003cp\u003e张郃败走，曹洪又命令张郃去夺取葭萌关。葭萌关守将霍峻和孟达不敌张郃。刘备派手下黄忠和严颜两员老将前去增援。黄忠使用骄兵之计，大胜张郃。张郃败走，退到天荡山，天荡山是夏侯德镇守。黄忠和严颜使用前后包抄的方法，夺取天荡山，杀了夏侯德。张郃大败，又转投定军山夏侯渊处。定军山是汉中囤积粮草的地方，战略意义重大，所以曹操留夏侯渊在此镇守。\u003c/p\u003e\n\u003ch1 id=\"第七十一回-占对山黄忠逸待劳-据汉水赵云寡胜众\"\u003e第七十一回 占对山黄忠逸待劳 据汉水赵云寡胜众\u003c/h1\u003e\n\u003cp\u003e张郃败走定军山之后，曹操知事情严重，亲自带兵四十万来战刘备。这个时候，老将黄忠又主动请缨，要求打定军山。诸葛亮拦不住，就派黄忠和法正一起去了，同时安排赵云在后面接应。\u003c/p\u003e\n\u003cp\u003e在定军山一战中，黄忠发现定军山的西边有一座山，比定军山还高，而且可以俯瞰定军山内虚实，于是黄忠趁某个夜黑风高的晚上，占领了这座山。夏侯渊气坏了，被迫出战。法正和黄忠，一个在山顶观察敌情，另一个在山腰准备迎战。两队人马的配合，最终大败了夏侯渊，黄忠杀了夏侯渊，曹军大败，定军山失守，曹军移粮草去北山寨中。\u003c/p\u003e\n\u003cp\u003e曹操听说自己的心腹大将夏侯渊被杀，很生气，亲自督战来定军山与夏侯渊报仇。在汉水附近，黄忠和赵云商量，黄忠前去烧粮草，赵云在后面接应，如果规定时间黄忠没回来，则赵云去救援。\u003c/p\u003e\n\u003cp\u003e黄忠去烧粮草的时候，被曹操大军围困。赵云带队前去救援，赵云一个人冲入重围，如入无人之境，救出黄忠和副将张著。抵抗住了曹操在汉水的进攻。\u003c/p\u003e\n\u003ch1 id=\"第七十二回-诸葛亮智取汉中-曹阿瞒兵退斜谷\"\u003e第七十二回 诸葛亮智取汉中 曹阿瞒兵退斜谷\u003c/h1\u003e\n\u003cp\u003e刘备军团和曹操军团，以汉水为界，互相进攻。诸葛亮利用曹操多疑的性格，打败曹操。曹操接连丢失汉水，阳平关，退到斜谷界口。期间，曹操还把杨修杀了，因为杨修各种耍小聪明，曹操很讨厌他，在一次“鸡肋”事件中，以扰乱军心为名，杀了杨修。\u003c/p\u003e\n\u003ch1 id=\"第七十三回-玄德进位汉中王-云长攻拔襄阳郡\"\u003e第七十三回 玄德进位汉中王 云长攻拔襄阳郡\u003c/h1\u003e\n\u003cp\u003e曹操在汉中和刘备的战争中，接连失败，最后彻底丢掉了汉中。至此，刘备占据四川、汉中和一半的荆州。刘备手下百官和诸葛亮就商议推举刘备为皇帝，刘备哪里肯自立皇帝，连连推辞。但是大家苦苦相劝，最后刘备自立为汉中王。\u003c/p\u003e","title":"《三国演义》每回内容梗概（61~90）"},{"content":"第三十一回 曹操仓亭破本初 玄德荆州依刘表 袁绍手下主要有三股势力：长子袁谭守青州；次子袁熙守幽州；三子袁尚，后妻刘氏所生，绍最爱之，留身边，守冀州；外甥高干守并州。听说袁绍官渡之战败了，都来支援。于是袁绍聚集四州兵马，屯兵仓亭，准备再和曹操干一仗。结果，曹操谋士程昱献十面埋伏之计，大败袁绍，于是袁绍回老巢，转为防守。\n却说刘备势力趁曹操忙于官渡、仓亭之战，偷袭许昌。曹操打败了袁绍之后，赶紧南下收拾刘备。刘备大败，谋士孙乾建议投靠荆州刘表。刘表谋士蔡瑁进谏：不可。刘备先从吕布，后事曹操，近投袁绍，皆不克终，足可见其为人。今若纳之，曹操必加兵于我，枉动干戈。不如斩孙乾之首，以献曹操，操必重待主公也。不过孙乾凭口才和智勇，打动刘表，刘表同意接受刘备。\n第三十二回 夺冀州袁尚争锋 决漳河许攸献计 却说曹操见刘备依附了刘表，打算转而继续攻打袁绍集团。三子袁尚出阵迎敌，大败而走。袁绍听说袁尚大败，旧病复发，吐血斗升而死。临死之前，刘氏问袁尚能继位吗，袁绍点头。\n曹操谋士郭嘉进谏，袁绍立了三子为后，长子袁谭必定会和袁尚争夺继位权，不如先退兵，让他们兄弟两先内斗，等两败俱伤之后，再进军一举歼灭他们。曹操从其言。果然袁谭和袁尚为了继位权干起来了，袁谭败，遂投降曹操。曹操进军攻打冀州城，袁尚败走，谋士审配守冀州城。曹操谋士许攸献计，可决漳河之水淹冀州城。果然冀州城被淹，城里也没粮食，审配大败，曹操占领冀州城。\n第三十三回 曹丕乘乱纳甄氏 郭嘉遗计定辽东 曹操拿下冀州城之后，他的儿子曹丕进城时发现袁熙的老婆甄氏很漂亮，于是没有杀她，把她留下来当老婆了。\n却说袁谭听说曹操打跑了袁尚，竟然想夺回冀州，不听曹操指挥了。于是袁谭和曹操干了一仗，袁谭被杀。高干所在的并州团队也被曹操灭了，高干被杀。袁尚被曹操逼得走投无路，投奔兄长袁熙，两兄弟又逃命到辽西乌桓。\n却说曹操谋士郭嘉，因北伐路途遥远水土不服，生病了，于是曹操就把他留在了易州养病，自己继续北伐追杀袁尚和袁熙。袁氏兄弟被追的又往辽东跑了。但是路上天气寒冷干旱，军队又缺粮。曹操打算暂时回撤，回到易州时，郭嘉已经死了，给曹操留了一个精囊妙计，告诉曹操不要紧逼袁氏兄弟，自然有人提着他两的头来的。果然，辽东太守公孙康担心袁氏兄弟来到辽东之后，鸠占鹊巢，干脆杀了袁氏兄弟，来投降曹操了。自此曹操彻底平了袁绍集团，占领了北方。\n第三十四回 蔡夫人隔屏听密语 刘皇叔跃马过檀溪 却说刘备依附刘表之后，江夏有人造反，刘备主动请缨去平反，平反的过程中缴获一匹的卢马。刘备本来把马送给了刘表，但刘表手下有人谗言说骑着个马妨主，刘表又把这个马还给刘备了。同时，刘表后妻蔡氏不太喜欢刘备，老担心他篡夺荆州，于是刘表暂且把刘备安排到襄阳的新野县了。在新野的时候，甘夫人还替刘备生了刘禅，因甘夫人晚上做梦梦见吞下北斗星而怀孕，给刘禅取小名阿斗。\n后来有一天，刘表请刘备来荆州喝酒，问刘备到底应该立前妻陈氏生的长子刘琦还是后妻蔡氏生的次子刘琼呢。刘备说自古废长立幼，取乱之道，不如慢慢削弱蔡氏的权力，还是要立长子啊。这话正好被蔡夫人偷听到了，于是蔡夫人和外戚蔡瑁怀恨在心，打算除掉刘备势力，以绝后患。\n于是蔡夫人找了个借口，宴请群臣，顺便把刘备也请去了。蔡瑁把吃饭的地方三面围住，只留西门，因为西门口正好有檀溪阻隔。吃饭的时候，蔡瑁把刘备的手下都灌醉了，准备对刘备下手。正好有一个小罗罗给刘备报信，刘备赶紧骑着的卢马往西门跑，的卢马也不是吃素的，纵身一跃居然跳过了檀溪，救了刘备一命。\n第三十五回 玄德南漳逢隐沦 单福新野遇英主 却说刘备越过檀溪之后，一通乱走来到了南漳，遇到了著名的水镜先生司马徽，司马徽告诉刘备，得卧龙、凤雏其中之一，可安天下。但是司马徽自己不肯出山。\n后来赵云他们找到了刘备，一起回了新野。刘备在街上遇到了谋士单福（其实是后面的徐庶）前来投靠。\n却说曹操平了袁绍后，想南下占领荆州，就派曹仁先去新野打刘备了。正好，单福初露头角，献上一计，破了曹仁。\n第三十六回 玄德用计袭樊城 元直走马荐诸葛 后续，刘备用单福的计谋，两破曹仁，同时占领了曹仁的樊城。曹仁败北之后，回见曹操，说刘备肯定有高人相助，后来程昱调查发现这个单福真名其实是徐庶，因之前杀人了，现在隐姓埋名改成单福，程昱说这个人的才能是自己的十倍。但是徐庶特别孝顺，家里只有一个老母亲。曹操为了把徐庶骗到许都，为自己效力，把徐庶的老母亲骗到许都，同时模仿其母的字迹，给徐庶写了一封信，说自己被曹操软禁，希望徐庶能来许昌救自己。因为徐庶很孝顺，所以泪别刘备，刘备也很不舍。徐庶为了感谢刘备，临走之前，给刘备推荐了襄阳隆中的诸葛亮，同时担心诸葛亮不出山帮刘备，还亲自去隆中跟诸葛亮说了刘备这个人。\n第三十七回 司马徽再荐名士 刘玄德三顾草庐 司马徽听说徐庶在刘备这，特来刘备这找徐庶聊天，没曾想徐庶已经去曹操那了。刘备就顺便和司马徽聊起了卧龙，司马徽特别夸赞和推荐了诸葛亮。于是就出现了著名的刘备三顾茅庐请诸葛亮出山的故事。这三顾，第一次只见到门童，诸葛亮不在；第二次只见到诸葛亮的弟弟诸葛均，诸葛亮排行第二，大哥诸葛瑾，在孙权那当谋士。\n第三十八回 定三分隆中决策 战长江孙氏报仇 刘备第三次顾茅庐终于见到了诸葛亮，诸葛亮虽在隆中，但尽知天下事，和刘备指点江山，分析当前天下局势，然后给刘备出了一个大政方针：将军欲成霸业，北让曹操占天时，南让孙权占地利，将军可占人和。先取荆州为家，后即取西川建基业，以成鼎足之势，然后可图中原也。\n话分两头，吴国第三代领导人孙权，为了给父亲孙坚报仇（孙坚在第七回中被刘表手下杀害），进攻江夏，江夏当时是由刘表的部下黄祖镇守。黄祖大败，被杀，孙权拿下江夏。但是孙权手下谋士张昭说江夏是个孤城，不可守，不如回江东，于是孙权报完仇之后就回江东了。\n第三十九回 荆州城公子三求计 博望坡军师初用兵 黄祖被杀，江夏失守，刘表慌了，请刘备共商御敌大计。这个时候，刘表前妻的儿子，也就是大儿子刘琦来找刘备帮忙，说继母容不下自己，自己的处境很危险。刘备说这个事情你可以找诸葛亮来处理。第二天，刘琦找诸葛亮帮忙，诸葛亮只推刘表自己的家事不便插手，推了三次，刘琦也求了三次。最后诸葛亮终于给了刘琦一计，说现在孙权走了之后，江夏急需人防守，你可以请求调去江夏，这样不在刘表和继母的身边，反倒安全。刘琦照做，刘表果然同意。\n却说曹操听说刘备在新野招兵买马，久留必有后患，可早图之。于是曹操派出以夏侯惇为首的十万大军，直抵博望坡，以窥新野。诸葛亮作为刘备的军师，第一次排兵布阵，安排大家用火攻，大败夏侯惇。这是诸葛亮初出茅庐的第一功。\n第四十回 蔡夫人议献荆州 诸葛亮火烧新野 夏侯惇打了败仗，回报曹操，曹操大怒，亲自带领五十万大军南下攻打荆州，孔融进谏曹操说刘表刘备都是汉室宗亲，不可兴无义之师。曹操大怒，把孔融一家老小全杀了。此孔融即孔融让梨的孔融。\n此时刘表病危，商议要立长子刘琦为荆州之主，让刘备辅佐。蔡夫人听说之后，刘表死后，篡改遗嘱，立次子（自己生的儿子）刘琮为荆州之主，而且不发丧。更可恨的是，听说曹操大军压境，刘琮手下和蔡夫人等人商议投降曹操，保全荆州之主的位置。当派使者去给曹操投降时，被刘备手下的人抓到，刘备这才知道刘表已死，刘琮把荆州卖了。这个时候，诸葛亮献上一计，曹操那么多兵来了，新野是守不住了，不如弃新野，走樊城。于是诸葛亮像上次博望坡一样，提前把新野的老百姓撤离，新野成了一个空城，诱敌入内，然后火烧新野。曹军大败。\n第四十一回 刘玄德携民渡江 赵子龙单骑救主 虽然曹军大败，但毕竟人数众多，曹军一路追击，刘备一路逃难，前后去了樊城→襄阳→江陵，即使情况再危险，刘备也不肯放弃新野的老百姓，所以逃难的速度很慢。一路上被曹军冲杀，大家伙都走散了。刘备安排赵云护送甘夫人、糜夫人和阿斗，但是路上二位夫人也走散了，赵云就返回去，冲入敌军，找他们。路上经过长坂坡等地，找到了甘夫人，后来也找到人糜夫人和阿斗，但是糜夫人受伤了，不肯走，怕耽误赵云和阿斗，于是投井自杀了。于是赵云怀抱着阿斗，不断冲杀，突出重围。\n第四十二回 张翼德大闹长坂桥 刘豫州败走汉津口 赵云救下阿斗之后，开始撤退，回到长坂坡，已经筋疲力尽了，这个时候正好张飞在长坂坡等着，于是请张飞支援。张飞站在长坂桥上，后面曹军追上，张飞厉声大喝：燕人张翼德在此！谁敢来决死战？，连叫了好几次，把曹军吓破胆了，曹军灰溜溜逃走了。\n曹军走之后，张飞把长坂桥拆了，曹军知道后，知道刘备那边没什么兵力，之前是被吓唬到了，所以又开始追击刘备。刘备他们就退走到汉津，关羽守夏口，刘备刘琦去江夏，以成掎角之势。\n第四十三回 诸葛亮舌战群儒 鲁子敬力排众议 这个时候，孙权听说刘表已死，刘备新败，想接连刘备共抗曹操。于是派鲁肃假借给刘表吊丧的名义，想接连刘备刘琦。而刘备也担心仅凭自己的力量难以抵御曹操的进攻，想要向孙权借兵，联合孙权一起抵抗曹操。于是派诸葛亮和鲁肃前往吴地柴桑，向孙权搬救兵。\n而孙权的手下又分为两股势力，张昭等大部分认为为了保全吴国，必须投降曹操；而鲁肃认为，投降曹操之后，对其他大臣没有任何影响，他们是什么官还是什么官，但是对孙权就不同了，孙权就要听命于曹操，相当于势力被削弱了，所以他认为孙权应该起兵共同抵抗曹操。\n在孙权处，诸葛亮和孙权的手下众多谋士进行辩论，凭三寸不烂之舌，把一个个谋士都辩论下去了。而且鲁肃也力排众议，建议孙权联合刘备一起抵抗曹操。\n第四十四回 孔明用智激周瑜 孙权决计破曹操 孙权纠结的时候，吴国太提醒说当年孙策临死之前有言：内事不决问张昭，外事不决问周瑜，为什么不问问周瑜的意见呢。于是孙权把在外的周瑜召回来问建议。周瑜到了之后，主降派和主战派轮番来见周瑜，叫周瑜劝孙权降或战。周瑜是打算降的，轮到诸葛亮和鲁肃来说的时候，诸葛亮用了一个计谋来激将周瑜，说曹操来攻打吴国，目的是为了得到大乔和小乔，好锁到铜雀台欢愉。这可把周瑜气炸了，因为大乔是孙策的老婆，小乔是周瑜的老婆。周瑜一听，自己的老婆可能要被曹操抢走，马上改变主意主战了。\n经过周瑜的劝说，孙权终于决定主战，抵抗曹操了。\n第四十五回 三江口曹操折兵 群英会蒋干中计 有一天曹操派人给周瑜送信，但信封上写着“汉大丞相付周都督开拆”，周瑜大怒，撕信斩来使。曹操知道后大怒，起军攻打周瑜。两军交战于三江口，曹操军事实力：刘表旧将蔡瑁和张允部下，以及青州、徐州的部队，但是由于蔡、张部队久不操练，青、徐不习水性，三江口大战，曹操大败。回去之后，曹操命令蔡、张操练水军。\n周瑜得胜登高的时候，发现北边曹操水寨中灯火通明，于是偷偷跑到曹操水寨窥探，发现他们的水军训练营很有章法，得知是荆州刘表部下的蔡瑁和张允指导训练。因为荆州也多是湖泊，蔡瑁和张允就是很好的水军教练。周瑜就想想办法除掉蔡瑁和张允。\n曹操本来三江口输了，今天又被周瑜窥探水寨，很生气。此时，曹操帐下蒋干自告奋勇，说可以去江东劝降周瑜。蒋干来到周瑜处之后，反中了周瑜的计谋。周瑜知道蒋干要来劝降，于是故意和蒋干喝得酩酊大醉，晚上还要和蒋干一起睡觉。周瑜事先在房间里放了一张伪造的书信，伪造蔡瑁和张允私通周瑜。蒋干趁周瑜睡着的时候，看到了这封信，然后偷偷把信偷走告诉曹操。曹操本来就因为三江口失败有点怪罪蔡瑁和张允，现在蒋干又发现了他两私通周瑜的信，一怒之下，杀了蔡瑁和张允，正中了周瑜的计，除掉了这两个懂水军的指导。不过事后曹操发现自己中计了。\n群英会是因为周瑜为了欢迎蒋干来叙旧，安排了很多精英来陪酒，故名群英会。\n第四十六回 用奇计孔明借箭 献密计黄盖受刑 诸葛亮识破了周瑜的计策，周瑜嫉妒诸葛亮，想借机杀掉诸葛亮。一日，周瑜问水战，用什么武器最好，诸葛亮说用箭。周瑜说军中正好缺少箭，限你十日之内造好十万支箭。诸葛亮说不消十日，三日即可。诸葛亮算好了第三天会有大雾，所以将很多草船连起来，逼近曹操水寨，曹操本来疑心重，又是大雾，不敢出兵，只敢向诸葛亮的草船射箭。就这样，诸葛亮轻松收集到了曹操的十万支箭，完成了周瑜的任务。\n却说曹操丢了十多万支箭，很郁闷，荀攸说必须要派人诈降打入周瑜内部。因曹操之前误杀了蔡瑁，于是派蔡瑁的弟弟蔡中、蔡和去周瑜处诈降。周瑜识破但不说破。周瑜和诸葛亮都认为应该用火攻，但是正愁没人去曹操寨中诈降通报消息。此时，黄盖出场，周瑜使出苦肉计，随便找个借口痛打黄盖，整个过程都让蔡中、蔡和知道。以便日后让曹操相信黄盖是真降而不是诈降。\n第四十七回 阚泽密献诈降书 庞统巧授连环计 阚泽和黄盖是老朋友，决定帮黄盖送诈降书，本来曹操已经识破了黄盖的苦肉计，但是凭借阚泽的口才和镇定，曹操相信了黄盖是真投降。\n但是曹操还是心慌啊，这个时候，蒋干又主动请缨，想去周瑜寨中一探虚实。正好，周瑜安排蒋干和庞统相遇，庞统说自己怀才不遇，被周瑜冷落，蒋干顺势把庞统引荐给曹操。庞统给曹操献上连环计，就是把曹操寨中的所有的船都用锁链连起来，这样船不会颠簸，士兵在船上也不会晕船，方便打仗。周瑜+庞统这个连环计其实是为了日后方便火攻曹操。\n第四十八回 宴长江曹操赋诗 锁战船北军用武 曹操没有识破庞统的连环计，为了庆祝得到庞统的计策和黄盖的投降，在长江口大宴群臣，而且把酒吟诗，作《短歌行》（就是对酒当歌，人生几何那个）。\n曹操军队整装待发，全都准备好要大干一场了。因为曹操担心北军不习水性，才把所有大船连起来了，此时，北军两个中将焦触、张南不服气，说自己虽来自北方，但是也能乘船，主动请缨，要乘小船打前锋。曹操同意，他们乘船打前锋，周瑜派出韩当和周泰御敌，曹军大败，焦触和张南被杀。\n第四十九回 七星坛诸葛祭风 三江口周瑜纵火 周瑜得知曹操的船都已经连起来了，请诸葛亮借东风。诸葛亮于是驻坛借东南风，因为曹军在西北方向，吴军在东南方向，所以需要用东南风火攻曹军。\n诸葛亮借到了东风之后，算到了周瑜小心眼要杀自己，赶紧回江夏和刘备回合，并安排张飞、赵云等人劫住曹操退路，同时安排关羽劫华容道。周瑜那边，也是调军有方，全都安排好了，东风起了之后，黄盖乘船带着各种易燃物，赶往曹军诈降。接近曹寨之后，开始放火，曹军大败，各种逃命。\n第五十回 诸葛亮智算华容 关云长义释曹操 曹军被周瑜放火烧杀了一大半军队，剩下的跟着曹操一路逃命。所到之处，均被诸葛亮算中，提前安排了赵云、张飞等在指定地点截杀曹军。最后，诸葛亮算准了曹军会经过华容道，但可能死不了，于是安排关羽在华容道截杀。曹操遇到关羽，说起之前对关羽的恩情，以及之后没有追究他关羽过五关斩六将的事情，关羽是个重情义的人，最后放了曹操。曹操虽逃过一劫，但最后能跟上的军队只剩27个人了。\n曹操兵败之后，灰溜溜回许昌了，安排曹仁、曹洪、夏侯惇等镇守荆州、合肥、襄阳等南郡的地方。\n第五十一回 曹仁大战东吴兵 孔明一气周公瑾 这个时候，刘备想把荆州、襄阳、南郡等地抢到手，但是赤壁之战毕竟是周瑜赢得，周瑜要拿下南郡易如反掌，肯定不会让刘备得了这个便宜。刘备就和周瑜商量，如果周瑜能打下南郡，就归周瑜，否则，刘备去打。于是，周瑜和曹仁干了几仗，期间有一仗曹仁打开了曹操留给他的锦囊妙计，类似于空城计吧，周瑜被骗，中了毒箭。后来周瑜诈死，反击曹仁，曹仁大败，本来南郡就要到手了，但是这个时候，刘备和诸葛亮窃取了周瑜的胜利果实，吩咐关张赵分别夺取了南郡、荆州、襄阳等地。此即孔明一气周瑜。\n第五十二回 诸葛亮智辞鲁肃 赵子龙计取桂阳 刘备诸葛亮抢了周瑜的胜利果实，鲁肃就去刘备那说理，想要要回南郡等地。但是，诸葛亮说南郡等地本来是刘表的，刘备是皇叔，和刘表也是亲戚，当初刘表就说要把荆州等地给刘备，所以这些地方，本不属于吴国，属于刘表的。鲁肃说刘表死了，他的儿子们也不在，应该归吴国。此时，诸葛亮请出事先请来的刘表的儿子刘琦。这个时候，鲁肃没话说了，刘表儿子在这，荆州这地方就暂且继续让刘琦管着吧。但是，鲁肃说如果刘琦死了，荆州必须给吴国，诸葛亮答应。\n此时，伊籍推荐了荆州有名的马氏兄弟，马谡和马良。马良建议刘备，荆州四面受敌，不可久守，可继续南下先后夺取零陵、武陵、桂阳和长沙。于是，张飞赵云拿下零陵，赵云拿下桂阳。\n第五十三回 关云长义释黄汉升 孙仲谋大战张文远 后张飞主动请缨，拿下武陵。关羽听说张飞赵云都立功了，也主动请缨要去攻打长沙。长沙太守韩玄微不足道，但其手下黄忠很厉害。关羽和黄忠大战一百回合，难分胜负。第一仗，黄忠马失前蹄，关羽放过了黄忠，第二仗，黄忠为了感恩，故意没有射杀关羽。韩玄看出黄忠故意没杀关羽，怒了要斩黄忠，此时，魏延觉得韩玄傲慢无才，带了一帮人把韩玄杀了，和黄忠投降了关羽。至此，平了南方4郡。\n话分两头，孙权和在合肥的张辽干起来了，张辽破了孙权的诈降计，歼灭孙权部将太史慈，孙权兵败。\n在此期间，公子刘琦病故，东吴要来讨回荆州了。\n第五十四回 吴国太佛寺看新郎 刘皇叔洞房续佳偶 东吴派鲁肃来讨要荆州，又被诸葛亮三寸不烂之舌赖皮不给，不过立下字据，说等夺取西川之后再还荆州，诸葛亮和鲁肃都签字了。\n又过了一段时间，甘夫人去世，刘备没了夫人。这个时候，周瑜想出一个鬼点子，打算假意把孙权的妹妹嫁给刘备，但要求刘备入赘，这样在刘备入赘的时候，乘机抓住刘备，威胁使其归还荆州。说办就办，东吴派吕范说媒。诸葛亮将计就计，答应了这桩婚事，派孙乾说媒，同时和赵云一起去东吴办婚事。\n刘备在见孙权之前，先去见了乔国老（二乔的父亲），说明了来意。这乔国老马上把事情告诉了吴国太（即孙权的母亲），这下吴国太可闹翻了天，说只有一个女儿，被孙权拿来使美人计，万一真如孙权计划，假结婚，又杀了刘备，岂不是让女儿守望门寡。骂得孙权狗血淋头。这时乔国老在旁边解围，说刘备好歹也是刘皇叔，一表人才，可以先实际考察一下。于是，吴国太就在甘露寺设宴招待刘备。考察的结果是，吴国太很喜欢刘备，于是这门亲事假戏真做，刘备不但安然无恙，反而还娶到一个妙龄少女当老婆。而周瑜的计谋没有得逞，孙权设置的鸿门宴反被吴国太训斥一番，真是“周郎妙计安天下，赔了夫人又折兵”啊。\n第五十五回 玄德智激孙夫人 孔明二气周公瑾 却说周瑜的计谋失败之后，又想出一招，说既然已经假戏真做，不如把刘备软禁在吴国，香车宝马荣华富贵给刘备伺候着，慢慢的离间刘和关张诸葛亮等人的关系，然后逐个击破。说办就办，这刘备还真每天沉迷酒色，不思荆州了。这时，赵云打开了诸葛亮事先给他的第二个锦囊。赵云就跑去见刘备，谎称曹操为报赤壁之战的仇，率精兵攻打荆州，要刘备快点回去。刘备就跑去和孙夫人商量，说想先回趟荆州，这孙夫人可能也真喜欢刘备，说要跟刘备一块走。于是假装元旦的时候，和刘备面北祭祖，趁机一起逃走了。孙权和周瑜知道后，先后派四路人马追赶，都被孙夫人和赵云骂回去了。于是刘备和孙夫人顺利逃离吴国，回到荆州大本营。这事气得周瑜大叫一声，金疮迸裂，倒于船上。此即孔明二气周瑜。\n第五十六回 曹操大宴铜雀台 孔明三气周公瑾 话分两头，赤壁之战失败之后的曹操回到北方老家，正好铜雀台建好了，于是曹操就在铜雀台大宴文武百官，武官比射箭，文官比作诗，好不热闹。\n回到周瑜这里，周瑜让刘备跑了之后，又派鲁肃去找刘备讨要荆州，被刘备痛哭一通。鲁肃也是个仁慈的人，就回去见周瑜了。周瑜又出了个鬼点子，说既然刘备答应打下四川才还荆州，那我们假装帮他出兵打四川，用打下的四川换回荆州，而实际上当经过荆州的时候，乘机把刘备灭了。但是这条计谋被诸葛亮识破，当周瑜部队经过荆州时，发现荆州城都空了，只留下赵云守城，突然间，关羽、张飞、黄忠、魏延等部分从四面八方杀来，周瑜傻眼了，又收到诸葛亮的恐吓羞辱的信，长叹一声“既生瑜，何生亮”，就这样被诸葛亮气死了。\n第五十七回 柴桑口卧龙吊丧 耒阳县凤雏理事 周瑜被诸葛亮气死之后，诸葛亮又假心假意跑去柴桑口给周瑜吊丧。无论是悼词还是哭泣，诸葛亮都表演得非常成功，反而让人觉得周瑜气量狭窄而诸葛亮多情了。\n在柴桑口的时候，诸葛亮遇到了凤雏庞统，诸葛亮对庞统说如果仕途不顺的话，可以来投奔刘备，还给他写了封推荐信。此时的庞统，被鲁肃引荐给了孙权，但是孙权觉得庞统太狂妄了，没用他。鲁肃又把他推荐给了刘备。刘备看庞统这个人面容丑陋，也不喜欢，随便把他打发到小县城耒阳县当县官（此时庞统故意没有拿出诸葛亮和鲁肃的推荐信）。庞统觉得被怠慢了，在耒阳县天天喝酒不理正事。刘备就派张飞去耒阳县调查情况，结果发现庞统半天就把上百天的正事处理完了，才发现庞统是个人才。最后，张飞把庞统带回刘备的身边了。也就是刘备集齐了卧龙、凤雏两大谋士。\n话分两头，曹操得知刘备羽翼丰满，觉得是个威胁，想攻打刘备，又担心西凉军马腾乘虚而入。于是想出一个计谋，假装把马腾封为征南将军，去讨伐孙权，把他召入京城，乘机除掉。马腾没办法，就把大儿子马超留在西凉，带着另外两个小儿子马休、马铁和侄子马岱进京。曹操派手下黄奎接待，没想到黄奎也受了衣带诏要除曹操的，于是黄奎把曹操的计谋告诉马腾，并打算一起密谋除掉曹操。又没想到计谋被黄奎的妾得知，告诉了曹操，结果曹操把黄奎、马腾和他两个小儿子都灭了。马家只剩留守西凉的马超一个人。\n第五十八回 马孟起兴兵雪恨 曹阿瞒割须弃袍 曹操灭了马腾之后，打算南下攻打孙权。孙权求助于刘备，诸葛亮让刘备写信给马超，让马超攻打曹操，报杀父之仇。于是，马超和逃回来的叔叔马岱兴兵攻打曹操。由于马超武功高强，又有杀父之仇在身，士兵士气很足。进攻很猛，势如破竹。曹操前线的部队节节溃败，没办法，曹操只能暂停南下，转而北上抵御马超。但是，曹操一众武将，还是敌不过马超，被马超打得落花流水。马超可能不认识曹操，只知道穿红袍、留长胡子的人是曹操，曹操为了保命，赶紧把红袍也脱了，长胡子也割了。这就是曹阿瞒割须弃袍的故事。\n第五十九回 许褚裸衣斗马超 曹操抹书间韩遂 在马超和曹操的对抗中，有一仗，曹操手下猛将许褚和马超大战一百回合，不分胜负，许褚在兴头上，干脆脱了衣服和马超打，结果还是输给马超了。\n后来，曹操使用反间计，成功离间马超和韩遂，韩遂被马超砍了左手，但是马超最终也输给了曹操，和马岱等人逃去陇西了。\n此时，汉中太守张鲁听说曹操干掉了西凉，担心曹操南下打汉中，商量着先把旁边益州刘璋灭了，然后抵御曹操。\n第六十回 张永年反难杨修 庞士元议取西蜀 益州刘璋的部下张松（字永年），献计说可以去找曹操帮忙打张鲁，这样张鲁自顾不暇，可解益州之危。于是刘璋就派张松去曹操那搬救兵，张松临走之前还秘密带上了益州的地图。不过张松一来长得太难看，二来态度傲慢，曹操不喜欢他。后来张松还和曹操手下的杨修干起来了，开始互喷。总而言之，张松此行不但没有搬来曹操的救兵，反而惹恼了曹操，曹操也准备南下夺取益州。\n张松从许昌回益州的路上，经过了荆州，刘备热情款待了张松几天几夜。张松被刘备的宽厚仁慈感动了，临走之前，把益州的地图给了刘备，而且还劝刘备来夺取益州，自己可以作为内应。张松回去之后，跟刘璋说了曹操的坏话，然后说可以把同是姓刘的刘备请入益州，抵御张鲁和曹操。傻傻的刘璋果然派人去请刘备入川，刘备也在庞统的极力推荐下，决定入川。于是，刘备就入川了，但是留下诸葛亮、关张赵重兵把守大后方荆州。\n","permalink":"http://localhost:1313/posts/2019-07-11-summary-of-the-romance-of-the-three-kingdoms-31-60/","summary":"\u003ch1 id=\"第三十一回-曹操仓亭破本初-玄德荆州依刘表\"\u003e第三十一回 曹操仓亭破本初 玄德荆州依刘表\u003c/h1\u003e\n\u003cp\u003e袁绍手下主要有三股势力：长子袁谭守青州；次子袁熙守幽州；三子袁尚，后妻刘氏所生，绍最爱之，留身边，守冀州；外甥高干守并州。听说袁绍官渡之战败了，都来支援。于是袁绍聚集四州兵马，屯兵仓亭，准备再和曹操干一仗。结果，曹操谋士程昱献十面埋伏之计，大败袁绍，于是袁绍回老巢，转为防守。\u003c/p\u003e\n\u003cp\u003e却说刘备势力趁曹操忙于官渡、仓亭之战，偷袭许昌。曹操打败了袁绍之后，赶紧南下收拾刘备。刘备大败，谋士孙乾建议投靠荆州刘表。刘表谋士蔡瑁进谏：不可。刘备先从吕布，后事曹操，近投袁绍，皆不克终，足可见其为人。今若纳之，曹操必加兵于我，枉动干戈。不如斩孙乾之首，以献曹操，操必重待主公也。不过孙乾凭口才和智勇，打动刘表，刘表同意接受刘备。\u003c/p\u003e\n\u003ch1 id=\"第三十二回-夺冀州袁尚争锋-决漳河许攸献计\"\u003e第三十二回 夺冀州袁尚争锋 决漳河许攸献计\u003c/h1\u003e\n\u003cp\u003e却说曹操见刘备依附了刘表，打算转而继续攻打袁绍集团。三子袁尚出阵迎敌，大败而走。袁绍听说袁尚大败，旧病复发，吐血斗升而死。临死之前，刘氏问袁尚能继位吗，袁绍点头。\u003c/p\u003e\n\u003cp\u003e曹操谋士郭嘉进谏，袁绍立了三子为后，长子袁谭必定会和袁尚争夺继位权，不如先退兵，让他们兄弟两先内斗，等两败俱伤之后，再进军一举歼灭他们。曹操从其言。果然袁谭和袁尚为了继位权干起来了，袁谭败，遂投降曹操。曹操进军攻打冀州城，袁尚败走，谋士审配守冀州城。曹操谋士许攸献计，可决漳河之水淹冀州城。果然冀州城被淹，城里也没粮食，审配大败，曹操占领冀州城。\u003c/p\u003e\n\u003ch1 id=\"第三十三回-曹丕乘乱纳甄氏-郭嘉遗计定辽东\"\u003e第三十三回 曹丕乘乱纳甄氏 郭嘉遗计定辽东\u003c/h1\u003e\n\u003cp\u003e曹操拿下冀州城之后，他的儿子曹丕进城时发现袁熙的老婆甄氏很漂亮，于是没有杀她，把她留下来当老婆了。\u003c/p\u003e\n\u003cp\u003e却说袁谭听说曹操打跑了袁尚，竟然想夺回冀州，不听曹操指挥了。于是袁谭和曹操干了一仗，袁谭被杀。高干所在的并州团队也被曹操灭了，高干被杀。袁尚被曹操逼得走投无路，投奔兄长袁熙，两兄弟又逃命到辽西乌桓。\u003c/p\u003e\n\u003cp\u003e却说曹操谋士郭嘉，因北伐路途遥远水土不服，生病了，于是曹操就把他留在了易州养病，自己继续北伐追杀袁尚和袁熙。袁氏兄弟被追的又往辽东跑了。但是路上天气寒冷干旱，军队又缺粮。曹操打算暂时回撤，回到易州时，郭嘉已经死了，给曹操留了一个精囊妙计，告诉曹操不要紧逼袁氏兄弟，自然有人提着他两的头来的。果然，辽东太守公孙康担心袁氏兄弟来到辽东之后，鸠占鹊巢，干脆杀了袁氏兄弟，来投降曹操了。自此曹操彻底平了袁绍集团，占领了北方。\u003c/p\u003e\n\u003ch1 id=\"第三十四回-蔡夫人隔屏听密语-刘皇叔跃马过檀溪\"\u003e第三十四回 蔡夫人隔屏听密语 刘皇叔跃马过檀溪\u003c/h1\u003e\n\u003cp\u003e却说刘备依附刘表之后，江夏有人造反，刘备主动请缨去平反，平反的过程中缴获一匹的卢马。刘备本来把马送给了刘表，但刘表手下有人谗言说骑着个马妨主，刘表又把这个马还给刘备了。同时，刘表后妻蔡氏不太喜欢刘备，老担心他篡夺荆州，于是刘表暂且把刘备安排到襄阳的新野县了。在新野的时候，甘夫人还替刘备生了刘禅，因甘夫人晚上做梦梦见吞下北斗星而怀孕，给刘禅取小名阿斗。\u003c/p\u003e\n\u003cp\u003e后来有一天，刘表请刘备来荆州喝酒，问刘备到底应该立前妻陈氏生的长子刘琦还是后妻蔡氏生的次子刘琼呢。刘备说自古废长立幼，取乱之道，不如慢慢削弱蔡氏的权力，还是要立长子啊。这话正好被蔡夫人偷听到了，于是蔡夫人和外戚蔡瑁怀恨在心，打算除掉刘备势力，以绝后患。\u003c/p\u003e\n\u003cp\u003e于是蔡夫人找了个借口，宴请群臣，顺便把刘备也请去了。蔡瑁把吃饭的地方三面围住，只留西门，因为西门口正好有檀溪阻隔。吃饭的时候，蔡瑁把刘备的手下都灌醉了，准备对刘备下手。正好有一个小罗罗给刘备报信，刘备赶紧骑着的卢马往西门跑，的卢马也不是吃素的，纵身一跃居然跳过了檀溪，救了刘备一命。\u003c/p\u003e\n\u003ch1 id=\"第三十五回-玄德南漳逢隐沦-单福新野遇英主\"\u003e第三十五回 玄德南漳逢隐沦 单福新野遇英主\u003c/h1\u003e\n\u003cp\u003e却说刘备越过檀溪之后，一通乱走来到了南漳，遇到了著名的水镜先生司马徽，司马徽告诉刘备，得卧龙、凤雏其中之一，可安天下。但是司马徽自己不肯出山。\u003c/p\u003e\n\u003cp\u003e后来赵云他们找到了刘备，一起回了新野。刘备在街上遇到了谋士单福（其实是后面的徐庶）前来投靠。\u003c/p\u003e\n\u003cp\u003e却说曹操平了袁绍后，想南下占领荆州，就派曹仁先去新野打刘备了。正好，单福初露头角，献上一计，破了曹仁。\u003c/p\u003e\n\u003ch1 id=\"第三十六回-玄德用计袭樊城-元直走马荐诸葛\"\u003e第三十六回 玄德用计袭樊城 元直走马荐诸葛\u003c/h1\u003e\n\u003cp\u003e后续，刘备用单福的计谋，两破曹仁，同时占领了曹仁的樊城。曹仁败北之后，回见曹操，说刘备肯定有高人相助，后来程昱调查发现这个单福真名其实是徐庶，因之前杀人了，现在隐姓埋名改成单福，程昱说这个人的才能是自己的十倍。但是徐庶特别孝顺，家里只有一个老母亲。曹操为了把徐庶骗到许都，为自己效力，把徐庶的老母亲骗到许都，同时模仿其母的字迹，给徐庶写了一封信，说自己被曹操软禁，希望徐庶能来许昌救自己。因为徐庶很孝顺，所以泪别刘备，刘备也很不舍。徐庶为了感谢刘备，临走之前，给刘备推荐了襄阳隆中的诸葛亮，同时担心诸葛亮不出山帮刘备，还亲自去隆中跟诸葛亮说了刘备这个人。\u003c/p\u003e\n\u003ch1 id=\"第三十七回-司马徽再荐名士-刘玄德三顾草庐\"\u003e第三十七回 司马徽再荐名士 刘玄德三顾草庐\u003c/h1\u003e\n\u003cp\u003e司马徽听说徐庶在刘备这，特来刘备这找徐庶聊天，没曾想徐庶已经去曹操那了。刘备就顺便和司马徽聊起了卧龙，司马徽特别夸赞和推荐了诸葛亮。于是就出现了著名的刘备三顾茅庐请诸葛亮出山的故事。这三顾，第一次只见到门童，诸葛亮不在；第二次只见到诸葛亮的弟弟诸葛均，诸葛亮排行第二，大哥诸葛瑾，在孙权那当谋士。\u003c/p\u003e\n\u003ch1 id=\"第三十八回-定三分隆中决策-战长江孙氏报仇\"\u003e第三十八回 定三分隆中决策 战长江孙氏报仇\u003c/h1\u003e\n\u003cp\u003e刘备第三次顾茅庐终于见到了诸葛亮，诸葛亮虽在隆中，但尽知天下事，和刘备指点江山，分析当前天下局势，然后给刘备出了一个大政方针：将军欲成霸业，北让曹操占天时，南让孙权占地利，将军可占人和。先取荆州为家，后即取西川建基业，以成鼎足之势，然后可图中原也。\u003c/p\u003e\n\u003cp\u003e话分两头，吴国第三代领导人孙权，为了给父亲孙坚报仇（孙坚在第七回中被刘表手下杀害），进攻江夏，江夏当时是由刘表的部下黄祖镇守。黄祖大败，被杀，孙权拿下江夏。但是孙权手下谋士张昭说江夏是个孤城，不可守，不如回江东，于是孙权报完仇之后就回江东了。\u003c/p\u003e\n\u003ch1 id=\"第三十九回-荆州城公子三求计-博望坡军师初用兵\"\u003e第三十九回 荆州城公子三求计 博望坡军师初用兵\u003c/h1\u003e\n\u003cp\u003e黄祖被杀，江夏失守，刘表慌了，请刘备共商御敌大计。这个时候，刘表前妻的儿子，也就是大儿子刘琦来找刘备帮忙，说继母容不下自己，自己的处境很危险。刘备说这个事情你可以找诸葛亮来处理。第二天，刘琦找诸葛亮帮忙，诸葛亮只推刘表自己的家事不便插手，推了三次，刘琦也求了三次。最后诸葛亮终于给了刘琦一计，说现在孙权走了之后，江夏急需人防守，你可以请求调去江夏，这样不在刘表和继母的身边，反倒安全。刘琦照做，刘表果然同意。\u003c/p\u003e\n\u003cp\u003e却说曹操听说刘备在新野招兵买马，久留必有后患，可早图之。于是曹操派出以夏侯惇为首的十万大军，直抵博望坡，以窥新野。诸葛亮作为刘备的军师，第一次排兵布阵，安排大家用火攻，大败夏侯惇。这是诸葛亮初出茅庐的第一功。\u003c/p\u003e\n\u003ch1 id=\"第四十回-蔡夫人议献荆州-诸葛亮火烧新野\"\u003e第四十回 蔡夫人议献荆州 诸葛亮火烧新野\u003c/h1\u003e\n\u003cp\u003e夏侯惇打了败仗，回报曹操，曹操大怒，亲自带领五十万大军南下攻打荆州，孔融进谏曹操说刘表刘备都是汉室宗亲，不可兴无义之师。曹操大怒，把孔融一家老小全杀了。此孔融即孔融让梨的孔融。\u003c/p\u003e\n\u003cp\u003e此时刘表病危，商议要立长子刘琦为荆州之主，让刘备辅佐。蔡夫人听说之后，刘表死后，篡改遗嘱，立次子（自己生的儿子）刘琮为荆州之主，而且不发丧。更可恨的是，听说曹操大军压境，刘琮手下和蔡夫人等人商议投降曹操，保全荆州之主的位置。当派使者去给曹操投降时，被刘备手下的人抓到，刘备这才知道刘表已死，刘琮把荆州卖了。这个时候，诸葛亮献上一计，曹操那么多兵来了，新野是守不住了，不如弃新野，走樊城。于是诸葛亮像上次博望坡一样，提前把新野的老百姓撤离，新野成了一个空城，诱敌入内，然后火烧新野。曹军大败。\u003c/p\u003e\n\u003ch1 id=\"第四十一回-刘玄德携民渡江-赵子龙单骑救主\"\u003e第四十一回 刘玄德携民渡江 赵子龙单骑救主\u003c/h1\u003e\n\u003cp\u003e虽然曹军大败，但毕竟人数众多，曹军一路追击，刘备一路逃难，前后去了樊城→襄阳→江陵，即使情况再危险，刘备也不肯放弃新野的老百姓，所以逃难的速度很慢。一路上被曹军冲杀，大家伙都走散了。刘备安排赵云护送甘夫人、糜夫人和阿斗，但是路上二位夫人也走散了，赵云就返回去，冲入敌军，找他们。路上经过长坂坡等地，找到了甘夫人，后来也找到人糜夫人和阿斗，但是糜夫人受伤了，不肯走，怕耽误赵云和阿斗，于是投井自杀了。于是赵云怀抱着阿斗，不断冲杀，突出重围。\u003c/p\u003e\n\u003ch1 id=\"第四十二回-张翼德大闹长坂桥-刘豫州败走汉津口\"\u003e第四十二回 张翼德大闹长坂桥 刘豫州败走汉津口\u003c/h1\u003e\n\u003cp\u003e赵云救下阿斗之后，开始撤退，回到长坂坡，已经筋疲力尽了，这个时候正好张飞在长坂坡等着，于是请张飞支援。张飞站在长坂桥上，后面曹军追上，张飞厉声大喝：燕人张翼德在此！谁敢来决死战？，连叫了好几次，把曹军吓破胆了，曹军灰溜溜逃走了。\u003c/p\u003e\n\u003cp\u003e曹军走之后，张飞把长坂桥拆了，曹军知道后，知道刘备那边没什么兵力，之前是被吓唬到了，所以又开始追击刘备。刘备他们就退走到汉津，关羽守夏口，刘备刘琦去江夏，以成掎角之势。\u003c/p\u003e\n\u003ch1 id=\"第四十三回-诸葛亮舌战群儒-鲁子敬力排众议\"\u003e第四十三回 诸葛亮舌战群儒 鲁子敬力排众议\u003c/h1\u003e\n\u003cp\u003e这个时候，孙权听说刘表已死，刘备新败，想接连刘备共抗曹操。于是派鲁肃假借给刘表吊丧的名义，想接连刘备刘琦。而刘备也担心仅凭自己的力量难以抵御曹操的进攻，想要向孙权借兵，联合孙权一起抵抗曹操。于是派诸葛亮和鲁肃前往吴地柴桑，向孙权搬救兵。\u003c/p\u003e\n\u003cp\u003e而孙权的手下又分为两股势力，张昭等大部分认为为了保全吴国，必须投降曹操；而鲁肃认为，投降曹操之后，对其他大臣没有任何影响，他们是什么官还是什么官，但是对孙权就不同了，孙权就要听命于曹操，相当于势力被削弱了，所以他认为孙权应该起兵共同抵抗曹操。\u003c/p\u003e\n\u003cp\u003e在孙权处，诸葛亮和孙权的手下众多谋士进行辩论，凭三寸不烂之舌，把一个个谋士都辩论下去了。而且鲁肃也力排众议，建议孙权联合刘备一起抵抗曹操。\u003c/p\u003e\n\u003ch1 id=\"第四十四回-孔明用智激周瑜-孙权决计破曹操\"\u003e第四十四回 孔明用智激周瑜 孙权决计破曹操\u003c/h1\u003e\n\u003cp\u003e孙权纠结的时候，吴国太提醒说当年孙策临死之前有言：内事不决问张昭，外事不决问周瑜，为什么不问问周瑜的意见呢。于是孙权把在外的周瑜召回来问建议。周瑜到了之后，主降派和主战派轮番来见周瑜，叫周瑜劝孙权降或战。周瑜是打算降的，轮到诸葛亮和鲁肃来说的时候，诸葛亮用了一个计谋来激将周瑜，说曹操来攻打吴国，目的是为了得到大乔和小乔，好锁到铜雀台欢愉。这可把周瑜气炸了，因为大乔是孙策的老婆，小乔是周瑜的老婆。周瑜一听，自己的老婆可能要被曹操抢走，马上改变主意主战了。\u003c/p\u003e","title":"《三国演义》每回内容梗概（31~60）"},{"content":"第一回 宴桃园豪杰三结义 斩黄巾英雄首立功 黄巾起义，刘关张结为兄弟，大小顺序为刘关张，张飞是卖猪肉的，很有钱。刘备：双股剑；关羽：青龙偃月刀；张飞：丈八蛇矛。\n第二回 张翼德怒鞭督邮 何国舅谋诛宦竖 刘关张镇压黄巾起义有功，但只得到一个很小的县令官，与民秋毫无犯，但被上面下来检查的督邮视察时，因没有贿赂督邮，被督邮穿小鞋，张飞怒不可遏，鞭打督邮。\n此时，朝廷内，十常侍专权，把持朝政。汉灵帝驾崩之后，何进拥立刘辩为皇，因为刘辩是灵帝和何进的姐姐的儿子。同时，把董后鸩杀。同时，何进为了除掉十常侍，引董卓入宫。\n第三回 议温明董卓叱丁原 馈金珠李肃说吕布 十常侍压力山大，为了保命，先下手为强，请何皇后把何进单独召进宫，进宫的路上，把何进杀了。宫内大乱，十常侍被何进部下杀掉。同时，董卓入宫，为彰显威严，欲废少帝辩，立刘协为新皇帝，在温明园讨论废立之事时，忠臣丁原挺身反对，董卓仗势欺人，斥责丁原。无奈丁原背后站着义子吕布，董卓奈何不了丁原。\n次日，丁原携吕布向董卓宣战，吕布骁勇善战，无人能破。此时，董卓部下李肃是吕布的老朋友，带着金银珠宝和董卓的赤兔马来劝说吕布，凭着李肃的三寸不烂之舌和吕布的头脑简单，吕布被说服，杀掉义父丁原，同时投奔董卓麾下。\n第四回 废汉帝陈留践位 谋董贼孟德献刀 董卓既得吕布，态度更加强硬，9月，废少帝，立陈留王刘协为新皇帝。把刘辩打入冷宫，某日，刘辩发牢骚写了首诗，董卓抓住机会，赐鸩酒把刘辩和何太后都杀了。\n曹操为了谋杀奸臣董卓，偷偷带着司徒王允的七宝刀，找了一个机会靠近董卓，本来要刺杀董卓，没成想被董卓从衣镜里发现了，曹操马上改口说有一口很好的宝刀，要献给董卓，然后开溜。董卓后来才get到曹操是要来刺杀自己的。\n第五回 发矫诏诸镇应曹公 破关兵三英战吕布 曹操逃出来之后，被董卓通缉，所以只能发布通告，尽书董卓恶行，招兵买马，准备讨伐董卓。各大诸侯太守都来响应，组成了一个十八路诸侯联盟，盟主是原朝廷重臣袁绍。但是袁绍统领诸侯，调度大军的能力不够，各诸侯也各怀鬼胎，没有凝聚力。\n前锋部队孙坚在进军汜水关时被华雄击败，华雄不可一世，在潘凤等大将接连被华雄斩杀之时，关羽主动请缨前去战华雄，在温酒未冷却的极短时间内斩杀华雄，关羽从此名震诸侯。此即温酒斩华（huà）雄的故事。\n董卓折了华雄，起兵二十万，兵分两路，其中一路由董卓亲自带队，和吕布等人，守住虎牢关，就是标题中的关兵中的关。在虎牢关处，吕布骑着赤兔马，不可一世，盟军无人能敌。最后，刘关张三人亲自出马，大战吕布，吕布败走。\n第六回 焚金阙董卓行凶 匿玉玺孙坚背约 吕布新败，董卓引兵回洛阳，迁都长安，同时把洛阳的宫殿烧毁。孙坚飞奔洛阳，救火的同时，在井中发现了传国玉玺，并私藏起来。后来被盟主袁绍发现，孙坚感到被羞耻了，拔寨离洛阳而去。袁绍大怒，写信给荆州刘表，因为孙坚回老家江东——扬州（？）要经过刘表家，所以袁绍写信给刘表，叫他半路截住孙坚。果然在半路上，孙坚和刘表来了一场恶战，亏孙坚部下三员大将程普、黄盖、韩当死救得脱。自此，孙坚和刘表结怨。\n第七回 袁绍磐河战公孙 孙坚跨江击刘表 袁绍屯兵河内，缺少粮草，向冀州太守韩馥借粮，谋士逢纪说大丈夫落到向别人借粮，可耻啊，冀州乃钱粮广盛之地，不如取而代之。袁绍于是和公孙瓒密谋，让公孙瓒出兵冀州，则韩馥必向袁绍求救，袁绍乘虚而入，占领冀州，然后和公孙瓒平分冀州。\n公孙瓒照做，但是当袁绍占领冀州之后，并没有和公孙瓒平分冀州，而是独占了。公孙瓒派弟弟公孙越去袁绍处，想要分点油水，没想到反被袁绍杀害。于是，公孙瓒大怒，举兵攻打袁绍。两军交战于磐河。此战互有胜负，公孙瓒手下赵云出场，和同在公孙瓒手下的刘关张相见，一见如故，分别时泪如雨下。\n袁绍的弟弟袁术，在南阳，听说哥哥新得冀州，想要哥哥赏赐点马匹，袁绍不给，自此兄弟不睦。袁术又向荆州刘表借粮，刘表也不给。袁术怒了，写信给孙坚，说昔日孙坚私藏玉玺回老家的路上，被刘表伏击，今日，我袁术愿与你结盟，攻打刘表。于是孙坚果然起兵，跨过汉水（长江），攻打刘表。没成想中了刘表部下蒯良的计谋，被杀了，可惜啊。孙坚部下黄盖生擒刘表部下黄祖，于是和刘表交换回孙坚尸体。孙坚大儿子孙策，字伯符；二儿子孙权，字仲谋。\n第八回 王司徒巧使连环计 董太师大闹凤仪亭 董卓在长安听说孙坚死了，更加骄奢淫逸。此时，司徒王允为了江山社稷愁死了，王允府上的歌伎貂蝉，特别漂亮，允以亲女待之。貂蝉想帮王允分担忧愁，于是王允想出了一个连环计。因董卓和其干儿子吕布都是有勇无谋，贪财好色之徒，王允先把貂蝉许配给吕布，然后又悄悄把貂蝉送给董卓。貂蝉从中挑不离间，致使父子二人翻脸。\n因为貂蝉已经被送到董卓府上，有一天，吕布趁着董卓和汉献帝聊天，偷偷来到董卓府上，在府上的凤仪亭看到了貂蝉，和貂蝉搂搂抱抱，被董卓赶回来发现了，董卓大闹凤仪亭，追着吕布打。自此父子二人结怨。\n第九回 除暴徒吕布助司徒 犯长安李傕听贾诩 经过了上面的事情，貂蝉劝董卓搬家，于是董卓和貂蝉搬到郿坞去了。司徒王允和吕布想了一个计策，说汉献帝病刚好，想召集文武百官吃个饭，于是派昔日董卓心腹李肃（就是第三回的李肃，因为董卓没有给李肃升官，李肃对董卓也有怨念），去郿坞宣旨。董卓傻乎乎兴高采烈来到宫内，被早就在此埋伏的王允、吕布、李肃等人杀死，同时去郿坞把董卓全家灭口，包括杀了董卓谋士李儒。吕布得到貂蝉。\n董卓手下四员大将李傕、郭汜、张济、樊稠，听说董卓被杀，打算吃个散伙饭，各自逃命。谋士贾诩说，我们都被通缉，既然自首也要死，不如来个你死我活。四人听了之后认为有道理，于是在西凉起兵，杀奔长安。而且他们制定的军事政策是，其中两个人在山外前后诱杀吕布，但又不恋战，另两个人偷偷起兵直接攻打长安城，让吕布和长安城首尾不能接应。此计果然奏效，董卓余党在长安城内为内应，打开城门，四员大将进入城中，烧杀抢掠，把王允也杀了。吕布弃了家小，投袁术去了。\n第十回 勤王室马腾举义 报父仇曹操兴师 李傕、郭汜、张济、樊稠占领宫内之后，骄奢蛮横，残虐百姓。西凉太守马腾和并州刺史韩遂，密谋贼党，但是都失败了。此时，马腾之子马超出场。樊稠因放过同乡人韩遂，被李傕郭汜杀掉。\n因朝廷昏庸无能，青州黄巾起义又起，太傅朱儁推荐派曹操去剿灭黄巾起义，李傕郭汜同意。东郡太守曹操和济北相鲍信一同破贼。\n曹操在兖州，招贤纳士，群贤毕至。文官：荀彧、荀攸，叔侄二人；程昱；郭嘉；刘晔；满宠；吕虔；毛玠。武官：于禁；典韦。自是曹操部下文有谋臣，武有猛将，威镇山东。\n曹操一高兴，打算把琅琊郡的老父亲曹嵩接过来享天伦之乐，于是，曹嵩和弟弟曹德并一家老小准备赶往兖州。途径徐州，徐州太守陶谦，想讨好曹操，大设宴席款待曹嵩等人，并派部下张闿护送曹嵩。没曾想，张闿原是黄巾余党，在陶谦处没有得到重用，今贼心不改，在路上把曹嵩一家老小全杀了。曹操听到消息，大怒，亲自起兵杀奔徐州。\n第十一回 刘皇叔北海救孔融 吕温侯濮阳破曹操 于是，徐州陶谦，向北海孔融求救，正商议间，黄巾余党管亥来北海攻打孔融。幸好孔融平时人品好，善待城外的一个老奶奶，老奶奶听说孔融有难，叫回来省亲的儿子太史慈去救孔融。太史慈虽然很厉害，但毕竟只有一个人，孔融就叫太史慈杀出重围，请刘备来救援。刘备于是向公孙瓒借了赵云，带着关张来救孔融。\n刘备来了之后，先给曹操写了封信，好言相劝，劝和。正好，这个时候，从宫中逃出来的吕布，攻陷了曹操的老巢兖州和濮阳，于是曹操送刘备一个人情，撤兵回老巢了。\n曹操经过商议之后，准备亲自领兵，去夺回濮阳，在濮阳和吕布进行了恶战，战败，差点被吕布围剿。\n第十二回 陶恭祖三让徐州 曹孟德大战吕布 吕布的谋士陈宫，诡计多端，出了一个点子，诱使曹操进濮阳城，待曹操进城之后，关门放火，差点把曹操灭了，众将死救得脱。\n陶谦在徐州，已经63岁了，儿子又无才，于是，想把徐州送给刘备接管，刘备死活不肯要。就这样来来回回三次，陶谦都要死了，刘备才肯接管徐州。\n曹操自从被吕布打了个败仗，回老家待着。谋士荀彧说，现在收成不好，可以去陈地、汝南、颍川抢占地盘，这些地方都是黄巾余党，乌合之众，轻易可破，又可得粮草。曹操听之，果然占领了这些地方，顺带还收了一员武将许褚。\n某天听说兖州吕布手下大将薛兰、李封都出去劫掠了，可以乘虚而入，夺回兖州。曹操听之，果然夺回兖州，同时六员大将齐战吕布，吕布败走。\n第十三回 李傕郭汜大交兵 杨奉董承双救驾 吕布败走之后，来徐州投奔刘备，屯兵小沛。\n却说李傕郭汜在宫廷横行无忌，太尉杨彪和大司农朱儁密谋诛杀李傕郭汜。杨彪献一反间计：郭汜的妻子妒忌心很强，可派人秘密告诉郭汜妻子，郭汜在和李傕夫人偷情。反间计成功，李傕郭汜反目成仇，李傕劫了天子，郭汜劫了文武百官，每日厮杀。\n因李傕喜欢旁门左道等妖术，谋士贾诩屡谏不止，有怨言。李傕赏罚不分，部下杨奉也有怨言，起兵倒戈，李傕军势渐微。正好张济要来讲和，请天子移驾弘农，李傕卖个人情，开始移驾弘农。路上遇到杨奉劫驾，同时国舅董承也来帮助杨奉，劫驾成功。此即杨奉董承双救驾。\n败走的李傕后来又和郭汜合兵一处，对这两个人无语了。\n第十四回 曹孟德移驾幸许都 吕奉先乘夜袭徐郡 皇帝到了洛阳之后，看到昔日的首都已经变成了断壁残垣，满目疮痍，感叹国运之衰。改年号为建安。此时，李傕郭汜又跑来进犯，太尉杨彪建议把山东的曹操召过来护驾。曹操大喜，起大军来护驾，干翻了李傕郭汜。杨奉、韩暹知道既然曹操来了，肯定容不下自己了，于是借口追击李傕郭汜，引兵囤大梁去了。\n此时，曹操谋士董昭建议，既然已经护驾成功，洛阳一片荒废，可以移驾许都，曹操同意。曹操经过护驾移驾等一系列事迹之后，在朝廷（中原）奠定了地位，手下文武百官很多。文官：荀彧、荀攸、郭嘉、刘晔、曹掾、毛玠、任峻。武官：程昱、范成、董昭、满宠、夏侯惇、夏侯渊、曹仁、曹洪、吕虔、李典、乐进、于禁、徐晃、许褚、典韦。自此，朝廷大事，先禀曹操，后奏天子。这大概就是曹操挟天子以令诸侯的开始吧。\n却说曹操既定大事，担心吕布和刘备联合，恐成心腹大患，于是想离间吕布和刘备，但是失败了。于是离间刘备和袁术，要刘备攻打袁术。刘备没办法，只能起兵攻打袁术，留张飞守徐州。张飞喝酒之后，脾气暴躁，硬要部下曹豹也喝酒，曹豹不喝酒，而且说看在我女婿吕布的面上，不要为难自己。张飞本来就不喜欢吕布，听到曹豹把吕布搬出来，很生气，就下令鞭打曹豹。曹豹很生气，写信给在小沛的吕布，说今晚张飞他们都喝醉了，可以来攻城。吕布果然晚上乘着张飞他们喝醉了，来攻城，而且成功了。张飞败走，逃到刘备和袁术对战的前线盱眙。\n第十五回 太史慈酣斗小霸王 孙伯符大战严白虎 袁术见吕布夺得徐州，叫吕布助攻自己攻打刘备，事成之后，给很多钱粮，吕布照做，但是袁术却没给钱粮，吕布大怒，想攻打袁术，谋士陈宫进谏，袁术兵多粮广，不可攻打，可请刘备回徐州，住小沛。刘备照做，相当于刘备和吕布的位置互换了。\n却说自从孙策父亲孙坚被杀之后，孙策就投奔袁术了，虽然立了不少功，但还是闷闷不乐，想重振父业，于是想借袁术的兵东征刘繇，为了借兵，把孙坚留下来的玉玺都先压在袁术那了，袁术也同意借兵了。\n于是孙策东征刘繇，一路上遇到各种文臣武将，其中就有周瑜。此时，太史慈在刘繇的部下，和孙策大战。孙策武功很高啊，一路上挟死一将，喝死一将，被称为“小霸王”。刘繇军大败，投豫章刘表去了。太史慈后来也归顺孙策了。\n当时东吴归一个叫严白虎的人管，孙策领兵准备顺带把东吴这块地也抢下来。严白虎和会稽太守王朗一起抵抗孙策。但是孙策太厉害了，打败严白虎和王朗，余姚人董袭杀了严白虎来投孙策，王朗去海隅了。自此，江南皆平，孙策奠定了在江南的霸主地位，同时也吸纳了很多文臣武将。\n第十六回 吕奉先射戟辕门 曹孟德败师育水 孙策既然在江南站稳脚跟，想要袁术还自己玉玺，袁术大怒，不给，反而想攻打孙策。谋士杨大将说不可，倒是可以攻打位于小沛的刘备，拿下刘备，再拿下徐州的吕布。于是，袁术进兵刘备，同时想让吕布夹攻刘备，吕布觉得刘备是好人，帮过自己，而且袁术之前答应给吕布的粮草都没给，现在又要帮他打刘备，吕布不肯。于是去前线给袁术和刘备讲和，说如果我能射中辕门外的画戟，你们各自停战收兵，刘备军事实力弱，自然答应，袁术手下纪灵觉得这么远，肯定射不中，所以也答应。没想到吕布弦无虚发，射中了。纪灵只好退兵，于是吕布相当于帮了刘备很大一个忙。\n搞笑的是，刘备在小沛招兵买马时，张飞抢了吕布手下的马匹，吕布大怒，来攻打刘备。刘备没办法，只能弃小沛，投许都曹操。吕布让手下高顺守小沛。\n刘备投曹操之后，曹操给刘备申请了一个豫州牧官位。同时，曹操听说张济张绣来犯首都，曹操大怒，兴兵讨伐张绣。张绣谋士贾诩说曹操势不可挡，不如投降，于是张绣真的就投降曹操了，每天曹操和张绣喝大酒。有一天，曹操发现张济的妻子很漂亮，于是曹操把张济妻子睡了。张绣知道之后，大怒，密谋暗算曹操，趁晚上，张绣部下胡车儿偷了典韦的武器双铁戟，同时大败曹操军队，还把曹操的长子曹昂，曹操的爱将典韦杀了，此即曹操因贪恋美色，败师育水。后来曹操缓过劲来，反败为胜，大败张绣，张绣败走，投奔刘表。\n第十七回 袁公路大起七军 曹孟德会合三将 袁公路，即袁术。却说袁术在淮南，地广粮多，又有孙策抵押的传国玉玺，于是称帝了。同时，催吕布把他的女儿送给袁术的儿子当老婆。但是听说吕布反而把使者送到许都，被曹操杀了，很生气，起七路大军杀奔徐州。吕布谋士陈登献计，破了袁术大军，袁术败回淮南。\n却说这个时候袁术想向孙策借粮，但是孙策因为袁术没有还自己玉玺，怀恨在心，不给，同时上表曹操，劝他南征，共击袁术。袁术因为缺粮，劫掠陈留。曹操也想乘虚攻打袁术，于是联合孙策、刘备和吕布，攻打袁术，此即会合三将。袁术败，丢了寿春城。曹操平了寿春，安排刘备又回到徐州旁边的小沛，叫刘备和吕布不要再闹矛盾了，和睦相处。真实意图是想一起除掉吕布，同时也和吕布的手下陈珪陈登父子私通好了。\n这个时候，张绣接连刘表，又开始兴风作浪了，曹操又来讨伐张绣。\n第十八回 贾文和料敌决胜 夏侯惇拔矢啖睛 张绣谋士贾诩神机妙算，大破曹操军队，曹操回府。\n一日，吕布谋士陈宫在小沛抓到一个使者，搜身发现此人给刘备向曹操送信，密谋杀吕布。吕布知道之后，围攻刘备，刘备赶紧求救于曹操。曹操派夏侯惇、夏侯渊、吕虔、李典等领兵五万来助刘备。夏侯惇和吕布手下高顺对阵时，被高顺手下曹性暗算，射中左眼，夏侯惇拔箭时，把眼珠子带出来了，他认为父精母血，不可弃也，把眼珠子吃了，此即夏侯惇拔矢啖睛。\n第十九回 下邳城曹操鏖兵 白门楼吕布殒命 却说吕布带着张辽、高顺围攻刘关张，形势危急，刘关张只能弃了小沛，开始逃难到许都。曹操率领大军进攻徐州。吕布手下陈登献计（陈珪陈登父子其实暗地里已经归顺曹操了）：徐州被围，下邳有粮可救，不如把家小和钱粮移到下邳。吕布从之。在吕布和曹军厮杀的时候，原吕布谋士糜竺（其实是最开始徐州陶谦的谋士），偷偷夺取徐州大权，不让吕布回徐州城了。吕布没办法，只能去下邳。\n曹操军队又围着下邳城一顿乱打，但是两个月都没攻下。这时，曹操谋士郭嘉和荀彧献计：决沂、泗之水，淹下邳城。曹操从之。另一方面，吕布自己作死，不善待部下将领，只听信两个老婆。搞得手下宋宪、魏续、侯成等人倒戈，偷赤兔马、打开城门、投画戟等，把吕布给卖了。而且还乘吕布睡觉的时候，把吕布绑起来了。吕布被擒，下邳城攻破。吕布和谋士陈宫都被曹操下令杀害。一代枭雄吕布，就此殒命。\n白门楼是下邳城的南大门，因此城门楼使用白色建筑物建造，故得名“白门楼“。\n第二十回 曹阿瞒许田打围 董国舅内阁受诏 曹操灭了吕布势力之后，占领徐州，令车骑将军车胄当徐州首领，带着刘备等人回许都。汉献帝见了刘备，论起辈分，发现刘备是自己叔叔，自此刘备被称为刘皇叔。曹操的谋士都劝曹操乘刚立功拿下徐州，乘机称帝得了，曹操说朝廷老臣众多，未可轻动，不过倒是可以请皇帝出城打猎，借机观察一下皇帝的动向。于是曹操就和皇帝一起在许田打猎，皇帝射一只鹿，连射三箭不中，于是叫曹操试射。曹操向皇帝讨要了皇帝专用的宝雕弓、金鈚箭，一箭射中大鹿。群臣远远的只看见是金鈚箭射中了，以为是皇帝射中的，大呼“万岁”。曹操也正好乘机一马当先，挡在皇帝的前面，接受群臣的祝贺，相当于变相当皇帝。\n皇帝受了这等屈辱，敢怒不敢言，伏皇后父亲伏完推荐国舅董承来帮忙缉拿奸臣曹操。但是朝廷之中，曹操的耳目众多，为了不被曹操发现，皇帝咬破手指，血书密诏，藏在玉带（应该就是腰带？），然后缝起来。第二天，召见董承进宫，明说感谢董承之前护驾有功（第13回），实际上是想送他玉带和锦袍，并告诉他领了玉带和锦袍后，要仔细观察。董承出宫之后，发现了玉带，陆陆续续的，董承召集了一些忠臣，并在白娟上签名，誓死忠于汉室，铲除奸臣曹操，这些人包括：董承、马腾、刘备等。\n第二十一回 曹操煮酒论英雄 关公赚城斩车胄 刘备为了不露锋芒，天天跑到后院种菜。有一天，曹操请刘备喝酒，席间，问：玄德久历四方，必知当世英雄。请试指言之。刘备本想谦虚过去不做评论，但是曹操不肯，非要刘备说。刘备把当世几大势力都说了一遍，曹操都不同意。最后，曹操说：今天下英雄，惟使君与操耳。刘备听到后吃了一惊，筷子掉地上了，正好这个时候下雨天上打雷，刘备就谎称被雷惊吓到了。\n刘备事后和关、张说：“吾之学圃，正欲使操知我无大志；不意操竟指我为英雄，我故失惊落箸。又恐操生疑，故借惧雷以掩饰之耳。”\n又有一天，人报袁绍破了公孙瓒，公孙瓒已自杀身亡。淮南袁术骄奢过度，不恤军民，众皆背反，术使人归帝号于袁绍。绍欲取玉玺，术约亲自送至，见今弃淮南欲归河北。刘备听到这个消息后，一方面寻思不知道公孙瓒手下的赵云现在何处，另一方面想借这个机会脱离曹操，所以对曹操说，袁术从淮南去河北，必经过徐州，我想带兵去徐州半路截住袁术。曹操同意。于是刘关张带着人马去徐州了。在徐州，刘关张和袁术大战，袁术大败，吐血斗余而死。袁术本来是三国势力中比较大的一支，兵粮足备，又有传国玉玺，算是第一个称帝的势力，居然落到这个下场，真是凄惨啊。\n袁术死后，手下徐璆夺得玉玺，赴许昌献于曹操，曹操大喜，封徐璆为高陵太守。\n曹操知道刘备不会回来，就叫徐州太守车胄把刘备势力灭了，关羽献计说可以半夜回城时，假扮曹操军队，引车胄出城，然后灭掉车胄。事情果然和关羽预计的一样，骗车胄开城门之后，刘关张灭了车胄全家，占领徐州。\n此时的中国大概是这样的：http://www.sdmzh.com.cn/map/sgst.html，199年。\n第二十二回 袁曹各起马步三军 关张共擒王刘二将 刘备违抗曹操命令，又杀了车胄占了徐州，担心曹操大军来讨伐，于是陈登献计联合袁绍共抗曹操。虽然刘备刚杀了袁绍的弟弟袁术，但刘备请袁绍的三世通家郑玄帮忙通融，袁绍考虑再三，决定出兵帮助刘备攻打曹操，于是起兵马、步、水三军。曹操闻讯，也起马、步、水三军进行抵抗，同时派刘岱、王忠去徐州攻打刘备。\n袁绍军中，谋臣不和：原来许攸不乐审配领兵，沮授又恨绍不用其谋，各不相和，不图进取。袁绍心怀疑惑，不思进兵。所以两军在前线驻扎，也不开火，就这样耗着，曹操干脆就回许都了。\n刘岱、王忠两个草包部队，跑去徐州，一个被关羽生擒，一个被张飞生擒，不过刘备都把他们放回去了。刘备担心曹操来犯，于是令关羽守下邳，孙乾、简雍、糜竺、糜芳守徐州，刘备和张飞守小沛，以为掎角之势。注意，这个时候，刘关张不在同一个地方了。\n第二十三回 祢正平裸衣骂贼 吉太医下毒遭刑 话说曹操想起兵打刘备，孔融建议可先使人招安张绣、刘表。张绣被招安，曹操令张绣招安刘表，但刘景升好结纳名流，今必得一有文名之士往说之，方可降耳。孔融推荐才子祢衡，字正平。但是曹操怠慢了祢衡，祢衡嘲笑曹操手下无能人，曹操为了羞辱祢衡，每天上朝的时候让祢衡击鼓。\n但是祢衡击鼓的时候没有更衣，被训斥了，祢衡干脆把衣服全脱了，辱骂曹贼。曹操后派祢衡去说刘表，刘表被祢衡羞辱，刘表推荐去说黄祖，黄祖也被羞辱，黄祖遂杀了祢衡。刘表暂未归顺曹操。\n却说国舅董承病了，名医吉太来帮董承治病时，听到董承说梦话要杀曹贼，于是和董承商量，可以在帮曹操治病的时候，乘机下毒杀了曹操。但是，某天董承发现家奴和侍妾在窃窃私语，不知是怀疑他们偷情还是怎么了，把家奴打了四十大板，家奴怀恨在心，半夜逃出董承家，去给曹操报信了。曹操得知此事之后，诈称自己有病，破了吉太和董承的阴谋，杀了吉太，同时搜出了皇帝的衣带诏，一举杀了董承、王子服等有关的所有人。城中官民见者，无不下泪。\n第二十四回 国贼行凶杀贵妃 皇叔败走投袁绍 曹操杀了董承等人还不解气，连带杀了董贵妃，董贵妃是董承的妹妹，当时已经有身孕了，依然被曹操杀害。\n此时，在衣带诏上签名的只剩下西凉太守马腾和徐州刘备了。曹操决定起兵打刘备，刘备求救于袁绍，本来袁绍这个时候如果乘机袭击曹操老巢许都的话，曹操腹背受敌肯定招架不住，但是袁绍因自己最爱的小儿子生病了，没心思打仗，不打算出兵帮刘备，只说如果刘备走投无路了可以来投袁绍。\n却说刘关张这个时候肯定不是曹操大军的对手，三人带三个小部队，都被打散了，张飞被迫去了芒砀山，刘备投了袁绍，关羽带着刘备的妻小死守下邳，徐州被曹操攻破。\n第二十五回 屯土山关公约三事 救白马曹操解重围 却说曹操毕竟是大规模正规军，攻势很猛，关羽被迫退到了一座土山包上。这个时候，曹操想招安关羽，派张辽去说关羽，张辽口才还是不错，最后，关羽和曹操定了三个约定，才愿意暂时归顺曹操账下，这三个约定分别是：1. 只降汉帝，不降曹操；2. 二嫂处请给皇叔俸禄养赡，一应上下人等，皆不许到门；3. 但知刘皇叔去向，不管千里万里，便当辞去。曹操答应。\n曹操为了感化笼络关羽，对关羽非常好，甚至把从吕布那缴获的赤兔马送给了关羽。\n这个时候，袁绍突然想攻打曹操了，真是可笑，谋士田丰进谏说之前曹操去打徐州时，许都空虚，那个时候不打许都，现在跑去打许都，不是送死吗。但是袁绍不听，甚至还把田丰囚禁起来了。\n袁绍带着颜良、文丑等大将，首先进攻白马。颜良和文丑武力高超啊，曹操连派几元大将，都被颜良和文丑杀了。曹操最后让关羽出马，关羽胜出，杀了颜良，帮曹操解了白马之围。\n第二十六回 袁本初败兵折将 关云长挂印封金 袁绍见颜良被杀，又派出文丑出战，又被关羽杀了。两次败军回报都说是被一个美髯公杀了，袁绍猜是关羽，于是怀疑刘备和关羽串通好了，两次要杀刘备，都被刘备解释过去了，说徐州一战，兄弟三人都打散了，也不知道对方是不是关羽，我可以写封信让人送给关羽，这样关羽就可以归到袁绍账下了，一个关羽可远远强于被杀的颜良文丑啊。袁绍真是傻啊。\n这个时候正在逃难的孙乾偷偷跑到关羽寨下，跟关羽说刘备好像就在袁绍那里。又有一天，袁绍部下南阳陈震送来了刘备的信，跟关羽说他现在在袁绍处，可速来。于是，关羽挂印封金（曹操之前给关羽一个汉寿亭侯的官当），不要任何金银珠宝和官爵，带着甘、糜二夫人，准备去袁绍处找刘备。\n第二十七回 美髯公千里走单骑 汉寿侯五关斩六将 话说关羽离开曹操的时候，多次想向曹操当面辞别，但曹操为了留住关羽，都没有见关羽。关羽只得留下一封信给曹操，独自上路。这也就导致关羽没有得到的曹操的书面授权的通关文书。关羽在去往袁绍的路上，虽然骑着赤兔马，但要护送甘、糜二夫人，速度也不是很快。沿路遇到了五处关卡，守关之人皆以关羽没有得到曹操的授权而阻拦，结果是，这些人都被关羽杀了，斩将六员，此即过五关斩六将。\n第二十八回 斩蔡阳兄弟释疑 会古城主臣聚义 话说张飞逃难的一个叫古城的地方，并且占领了这个山城。关羽一行人也正好到这个地方。两人见面，张飞气不打一处来，和关羽干起来了，因为他觉得关羽不顾桃园结义，降了曹操，又被封侯赐爵，是忘恩负义之人，不论关羽怎么解释，张飞就是不听。\n突然秦琪的舅舅蔡阳来为秦琪报仇，因为关羽过五关斩六将的时候杀了秦琪。关羽为了告诉张飞，自己是清白的，亲自来战蔡阳，并把蔡阳杀了，同时抓了一个小兵，让他跟张飞解释自己在曹操处时是怎样不忘刘备的。最后，张飞信了，兄弟两冰释前嫌，握手言和。\n于是，他们商量好，张飞依然守着古城，关羽和孙乾去袁绍处找刘备。刘备和他们见面之后，找了个借口要摆脱袁绍，说：”刘表镇守荆州，我可以去诏安他，因为我刘备和刘表都是汉室后代，他应该会听我的话。同时，可以派孙乾再去诏安关羽（这其实是幌子，为了让孙乾也和刘备一起离开）。”这时候，简雍说我也可以和刘备一起走，可以监视刘备，防止他跑了（其实简雍这样说也是为了一块跑）。\n这样，刘备同伙一帮人就这样顺利的脱离了袁绍，袁绍真是太傻了。刘备等人在去古城的路上，正好遇到了赵云，原来公孙瓒死了之后，赵云无处可走，听说张飞在古城，就来古城找张飞，正好遇到了刘备等人。就这样，刘关张和赵云等人顺利在古城再次会师。\n第二十九回 小霸王怒斩于吉 碧眼儿坐领江东 却说孙策自霸江东，兵精粮足，还和曹操结了亲家，曹仁之女许配给孙策幼弟孙匡。但是孙策向曹操要个大司马的官当当，曹操不同意，孙策怀恨在心，常有袭许都之心。吴郡太守许贡偷偷想向曹操报信，被孙策发现并截住了。孙策杀了许贡，但是某天，许贡的三个家客给许贡报仇，毒箭射中了孙策。医生说孙策必须静养百日才能好，而且不能动怒。\n有一天，孙策发现一个仙人于吉在街上走着，老百姓都焚香伏道而拜，孙策觉得这是妖魔鬼怪，抓住于吉并杀了他。但是于吉真是仙人，被无辜杀害之后，又一直出现在孙策面前，不过别人看不见，只有孙策看得见。就这样，孙策不断被于吉激怒，又干不掉于吉。最终，毒性发作，金仓迸裂，死了。孙策死前，把江山交给了弟弟孙权，告诉他内事不决问张昭，外事不决问周瑜。孙策的老婆是大乔。\n孙权继承父兄业绩之后，吸纳了很多人才，比如鲁肃、诸葛瑾、顾雍等人。从此，孙权威震江东，深得民心，孙权的时代开始了。\n第三十回 战官渡本初败绩 劫乌巢孟德烧粮 却说袁绍兴兵往官渡进发，原本曹操的兵力是要少于袁绍的，但是袁绍听谋士建议，不体恤部下官兵，导致谋士许攸叛变，投奔曹操；手下大将张邰、高览也倒戈，投奔曹操。袁绍的粮草都囤在乌巢，许攸建议曹操可以派一小支精锐部队，把乌巢的粮草都烧了，这样袁军大乱，曹操可以乘势掩杀大败袁绍。\n曹操采纳了许攸的建议，果然烧了袁绍的粮草，大败袁绍，曹操大获全胜，这是三国上有名的三大战役之一，也是著名的以弱胜强的战役之一。袁绍太不爱惜人才了，而且不体恤属下；自己没智慧无计谋，而且还不听取采纳谋士的建议，不失败才怪。\n百度百科上有一段胜败分析：\n官渡之战是袁曹双方力量转变，使当时中国北部由分裂走向统一的一次关键性战役，对于三国历史的发展有着极其重要的影响。此战曹军的胜利不是偶然的，袁曹间的兼并战争，虽属于封建割据势力之间的争斗，但它实现了地区统一，客观上符合人民的愿望。 官渡之战是汉末乃至中国史上有名的以少胜多的战役，也是曹操与袁绍争夺北方霸权的转折点。官渡一战之后，曹操终于一反之前对袁绍的劣势，为自己统一北方奠定了基础。曹操在战事初期处于劣势，当中全赖三人为曹操扭转困局——荀彧、荀攸、许攸。\n1、曹操于黎阳与袁绍相持，本欲还兵再作打算，荀攸献计：“今兵少不敌，分其势乃可。公到延津，若将渡兵向其后者，绍必西应之，然后轻兵袭白马，掩其不备，颜良可擒也。”曹操依计行事，果然大破袁军，斩杀颜良。\n2、建安五年八月始，两军再次相持于官渡，双方互有胜负。其后曹操军中缺粮，适逢袁绍谋士许攸与营中将士不和，投奔曹操。许攸献计烧袁绍军粮，使袁绍不战自败。\n3、曹操曾经在交战之时想过放弃，写信给许都的荀彧。而荀彧却提醒了曹操：“在战争双方都疲惫不堪时，谁后退谁被动，谁放弃谁灭亡。战机就在这时出现。”最后帮助曹操寻回信心，继续坚持。\n曹操能接纳能人之言，取得最终的胜利，这全在于用人之道。荀攸、许攸皆是人才，献上计谋，有化险为夷之功；荀彧则具备长远的战略眼光，能够鼓励和帮助曹操在关键时期坚持战斗，这是更高层次的人才。由此观之，人才的妥善任用应该可说是“一计敌万人”。至于曹操，他是一个懂得运用人才的人才，能接纳他人之言，故袁绍兵多也不足为惧，正所谓兵不在多，在乎能否调遣。\n","permalink":"http://localhost:1313/posts/2019-07-10-summary-of-the-romance-of-the-three-kingdoms-1-30/","summary":"\u003ch1 id=\"第一回-宴桃园豪杰三结义-斩黄巾英雄首立功\"\u003e第一回 宴桃园豪杰三结义 斩黄巾英雄首立功\u003c/h1\u003e\n\u003cp\u003e黄巾起义，刘关张结为兄弟，大小顺序为刘关张，张飞是卖猪肉的，很有钱。刘备：双股剑；关羽：青龙偃月刀；张飞：丈八蛇矛。\u003c/p\u003e\n\u003ch1 id=\"第二回-张翼德怒鞭督邮-何国舅谋诛宦竖\"\u003e第二回 张翼德怒鞭督邮 何国舅谋诛宦竖\u003c/h1\u003e\n\u003cp\u003e刘关张镇压黄巾起义有功，但只得到一个很小的县令官，与民秋毫无犯，但被上面下来检查的督邮视察时，因没有贿赂督邮，被督邮穿小鞋，张飞怒不可遏，鞭打督邮。\u003c/p\u003e\n\u003cp\u003e此时，朝廷内，十常侍专权，把持朝政。汉灵帝驾崩之后，何进拥立刘辩为皇，因为刘辩是灵帝和何进的姐姐的儿子。同时，把董后鸩杀。同时，何进为了除掉十常侍，引董卓入宫。\u003c/p\u003e\n\u003ch1 id=\"第三回-议温明董卓叱丁原-馈金珠李肃说吕布\"\u003e第三回 议温明董卓叱丁原 馈金珠李肃说吕布\u003c/h1\u003e\n\u003cp\u003e十常侍压力山大，为了保命，先下手为强，请何皇后把何进单独召进宫，进宫的路上，把何进杀了。宫内大乱，十常侍被何进部下杀掉。同时，董卓入宫，为彰显威严，欲废少帝辩，立刘协为新皇帝，在温明园讨论废立之事时，忠臣丁原挺身反对，董卓仗势欺人，斥责丁原。无奈丁原背后站着义子吕布，董卓奈何不了丁原。\u003c/p\u003e\n\u003cp\u003e次日，丁原携吕布向董卓宣战，吕布骁勇善战，无人能破。此时，董卓部下李肃是吕布的老朋友，带着金银珠宝和董卓的赤兔马来劝说吕布，凭着李肃的三寸不烂之舌和吕布的头脑简单，吕布被说服，杀掉义父丁原，同时投奔董卓麾下。\u003c/p\u003e\n\u003ch1 id=\"第四回-废汉帝陈留践位-谋董贼孟德献刀\"\u003e第四回 废汉帝陈留践位 谋董贼孟德献刀\u003c/h1\u003e\n\u003cp\u003e董卓既得吕布，态度更加强硬，9月，废少帝，立陈留王刘协为新皇帝。把刘辩打入冷宫，某日，刘辩发牢骚写了首诗，董卓抓住机会，赐鸩酒把刘辩和何太后都杀了。\u003c/p\u003e\n\u003cp\u003e曹操为了谋杀奸臣董卓，偷偷带着司徒王允的七宝刀，找了一个机会靠近董卓，本来要刺杀董卓，没成想被董卓从衣镜里发现了，曹操马上改口说有一口很好的宝刀，要献给董卓，然后开溜。董卓后来才get到曹操是要来刺杀自己的。\u003c/p\u003e\n\u003ch1 id=\"第五回-发矫诏诸镇应曹公-破关兵三英战吕布\"\u003e第五回 发矫诏诸镇应曹公 破关兵三英战吕布\u003c/h1\u003e\n\u003cp\u003e曹操逃出来之后，被董卓通缉，所以只能发布通告，尽书董卓恶行，招兵买马，准备讨伐董卓。各大诸侯太守都来响应，组成了一个十八路诸侯联盟，盟主是原朝廷重臣袁绍。但是袁绍统领诸侯，调度大军的能力不够，各诸侯也各怀鬼胎，没有凝聚力。\u003c/p\u003e\n\u003cp\u003e前锋部队孙坚在进军汜水关时被华雄击败，华雄不可一世，在潘凤等大将接连被华雄斩杀之时，关羽主动请缨前去战华雄，在温酒未冷却的极短时间内斩杀华雄，关羽从此名震诸侯。此即温酒斩华（huà）雄的故事。\u003c/p\u003e\n\u003cp\u003e董卓折了华雄，起兵二十万，兵分两路，其中一路由董卓亲自带队，和吕布等人，守住虎牢关，就是标题中的关兵中的关。在虎牢关处，吕布骑着赤兔马，不可一世，盟军无人能敌。最后，刘关张三人亲自出马，大战吕布，吕布败走。\u003c/p\u003e\n\u003ch1 id=\"第六回-焚金阙董卓行凶-匿玉玺孙坚背约\"\u003e第六回 焚金阙董卓行凶 匿玉玺孙坚背约\u003c/h1\u003e\n\u003cp\u003e吕布新败，董卓引兵回洛阳，迁都长安，同时把洛阳的宫殿烧毁。孙坚飞奔洛阳，救火的同时，在井中发现了传国玉玺，并私藏起来。后来被盟主袁绍发现，孙坚感到被羞耻了，拔寨离洛阳而去。袁绍大怒，写信给荆州刘表，因为孙坚回老家江东——扬州（？）要经过刘表家，所以袁绍写信给刘表，叫他半路截住孙坚。果然在半路上，孙坚和刘表来了一场恶战，亏孙坚部下三员大将程普、黄盖、韩当死救得脱。自此，孙坚和刘表结怨。\u003c/p\u003e\n\u003ch1 id=\"第七回-袁绍磐河战公孙-孙坚跨江击刘表\"\u003e第七回 袁绍磐河战公孙 孙坚跨江击刘表\u003c/h1\u003e\n\u003cp\u003e袁绍屯兵河内，缺少粮草，向冀州太守韩馥借粮，谋士逢纪说大丈夫落到向别人借粮，可耻啊，冀州乃钱粮广盛之地，不如取而代之。袁绍于是和公孙瓒密谋，让公孙瓒出兵冀州，则韩馥必向袁绍求救，袁绍乘虚而入，占领冀州，然后和公孙瓒平分冀州。\u003c/p\u003e\n\u003cp\u003e公孙瓒照做，但是当袁绍占领冀州之后，并没有和公孙瓒平分冀州，而是独占了。公孙瓒派弟弟公孙越去袁绍处，想要分点油水，没想到反被袁绍杀害。于是，公孙瓒大怒，举兵攻打袁绍。两军交战于磐河。此战互有胜负，公孙瓒手下赵云出场，和同在公孙瓒手下的刘关张相见，一见如故，分别时泪如雨下。\u003c/p\u003e\n\u003cp\u003e袁绍的弟弟袁术，在南阳，听说哥哥新得冀州，想要哥哥赏赐点马匹，袁绍不给，自此兄弟不睦。袁术又向荆州刘表借粮，刘表也不给。袁术怒了，写信给孙坚，说昔日孙坚私藏玉玺回老家的路上，被刘表伏击，今日，我袁术愿与你结盟，攻打刘表。于是孙坚果然起兵，跨过汉水（长江），攻打刘表。没成想中了刘表部下蒯良的计谋，被杀了，可惜啊。孙坚部下黄盖生擒刘表部下黄祖，于是和刘表交换回孙坚尸体。孙坚大儿子孙策，字伯符；二儿子孙权，字仲谋。\u003c/p\u003e\n\u003ch1 id=\"第八回-王司徒巧使连环计-董太师大闹凤仪亭\"\u003e第八回 王司徒巧使连环计 董太师大闹凤仪亭\u003c/h1\u003e\n\u003cp\u003e董卓在长安听说孙坚死了，更加骄奢淫逸。此时，司徒王允为了江山社稷愁死了，王允府上的歌伎貂蝉，特别漂亮，允以亲女待之。貂蝉想帮王允分担忧愁，于是王允想出了一个连环计。因董卓和其干儿子吕布都是有勇无谋，贪财好色之徒，王允先把貂蝉许配给吕布，然后又悄悄把貂蝉送给董卓。貂蝉从中挑不离间，致使父子二人翻脸。\u003c/p\u003e\n\u003cp\u003e因为貂蝉已经被送到董卓府上，有一天，吕布趁着董卓和汉献帝聊天，偷偷来到董卓府上，在府上的凤仪亭看到了貂蝉，和貂蝉搂搂抱抱，被董卓赶回来发现了，董卓大闹凤仪亭，追着吕布打。自此父子二人结怨。\u003c/p\u003e\n\u003ch1 id=\"第九回-除暴徒吕布助司徒-犯长安李傕听贾诩\"\u003e第九回 除暴徒吕布助司徒 犯长安李傕听贾诩\u003c/h1\u003e\n\u003cp\u003e经过了上面的事情，貂蝉劝董卓搬家，于是董卓和貂蝉搬到郿坞去了。司徒王允和吕布想了一个计策，说汉献帝病刚好，想召集文武百官吃个饭，于是派昔日董卓心腹李肃（就是第三回的李肃，因为董卓没有给李肃升官，李肃对董卓也有怨念），去郿坞宣旨。董卓傻乎乎兴高采烈来到宫内，被早就在此埋伏的王允、吕布、李肃等人杀死，同时去郿坞把董卓全家灭口，包括杀了董卓谋士李儒。吕布得到貂蝉。\u003c/p\u003e\n\u003cp\u003e董卓手下四员大将李傕、郭汜、张济、樊稠，听说董卓被杀，打算吃个散伙饭，各自逃命。谋士贾诩说，我们都被通缉，既然自首也要死，不如来个你死我活。四人听了之后认为有道理，于是在西凉起兵，杀奔长安。而且他们制定的军事政策是，其中两个人在山外前后诱杀吕布，但又不恋战，另两个人偷偷起兵直接攻打长安城，让吕布和长安城首尾不能接应。此计果然奏效，董卓余党在长安城内为内应，打开城门，四员大将进入城中，烧杀抢掠，把王允也杀了。吕布弃了家小，投袁术去了。\u003c/p\u003e\n\u003ch1 id=\"第十回-勤王室马腾举义-报父仇曹操兴师\"\u003e第十回 勤王室马腾举义 报父仇曹操兴师\u003c/h1\u003e\n\u003cp\u003e李傕、郭汜、张济、樊稠占领宫内之后，骄奢蛮横，残虐百姓。西凉太守马腾和并州刺史韩遂，密谋贼党，但是都失败了。此时，马腾之子马超出场。樊稠因放过同乡人韩遂，被李傕郭汜杀掉。\u003c/p\u003e\n\u003cp\u003e因朝廷昏庸无能，青州黄巾起义又起，太傅朱儁推荐派曹操去剿灭黄巾起义，李傕郭汜同意。东郡太守曹操和济北相鲍信一同破贼。\u003c/p\u003e\n\u003cp\u003e曹操在兖州，招贤纳士，群贤毕至。文官：荀彧、荀攸，叔侄二人；程昱；郭嘉；刘晔；满宠；吕虔；毛玠。武官：于禁；典韦。自是曹操部下文有谋臣，武有猛将，威镇山东。\u003c/p\u003e\n\u003cp\u003e曹操一高兴，打算把琅琊郡的老父亲曹嵩接过来享天伦之乐，于是，曹嵩和弟弟曹德并一家老小准备赶往兖州。途径徐州，徐州太守陶谦，想讨好曹操，大设宴席款待曹嵩等人，并派部下张闿护送曹嵩。没曾想，张闿原是黄巾余党，在陶谦处没有得到重用，今贼心不改，在路上把曹嵩一家老小全杀了。曹操听到消息，大怒，亲自起兵杀奔徐州。\u003c/p\u003e\n\u003ch1 id=\"第十一回-刘皇叔北海救孔融-吕温侯濮阳破曹操\"\u003e第十一回 刘皇叔北海救孔融 吕温侯濮阳破曹操\u003c/h1\u003e\n\u003cp\u003e于是，徐州陶谦，向北海孔融求救，正商议间，黄巾余党管亥来北海攻打孔融。幸好孔融平时人品好，善待城外的一个老奶奶，老奶奶听说孔融有难，叫回来省亲的儿子太史慈去救孔融。太史慈虽然很厉害，但毕竟只有一个人，孔融就叫太史慈杀出重围，请刘备来救援。刘备于是向公孙瓒借了赵云，带着关张来救孔融。\u003c/p\u003e\n\u003cp\u003e刘备来了之后，先给曹操写了封信，好言相劝，劝和。正好，这个时候，从宫中逃出来的吕布，攻陷了曹操的老巢兖州和濮阳，于是曹操送刘备一个人情，撤兵回老巢了。\u003c/p\u003e\n\u003cp\u003e曹操经过商议之后，准备亲自领兵，去夺回濮阳，在濮阳和吕布进行了恶战，战败，差点被吕布围剿。\u003c/p\u003e\n\u003ch1 id=\"第十二回-陶恭祖三让徐州-曹孟德大战吕布\"\u003e第十二回 陶恭祖三让徐州 曹孟德大战吕布\u003c/h1\u003e\n\u003cp\u003e吕布的谋士陈宫，诡计多端，出了一个点子，诱使曹操进濮阳城，待曹操进城之后，关门放火，差点把曹操灭了，众将死救得脱。\u003c/p\u003e\n\u003cp\u003e陶谦在徐州，已经63岁了，儿子又无才，于是，想把徐州送给刘备接管，刘备死活不肯要。就这样来来回回三次，陶谦都要死了，刘备才肯接管徐州。\u003c/p\u003e\n\u003cp\u003e曹操自从被吕布打了个败仗，回老家待着。谋士荀彧说，现在收成不好，可以去陈地、汝南、颍川抢占地盘，这些地方都是黄巾余党，乌合之众，轻易可破，又可得粮草。曹操听之，果然占领了这些地方，顺带还收了一员武将许褚。\u003c/p\u003e\n\u003cp\u003e某天听说兖州吕布手下大将薛兰、李封都出去劫掠了，可以乘虚而入，夺回兖州。曹操听之，果然夺回兖州，同时六员大将齐战吕布，吕布败走。\u003c/p\u003e\n\u003ch1 id=\"第十三回-李傕郭汜大交兵-杨奉董承双救驾\"\u003e第十三回 李傕郭汜大交兵 杨奉董承双救驾\u003c/h1\u003e\n\u003cp\u003e吕布败走之后，来徐州投奔刘备，屯兵小沛。\u003c/p\u003e\n\u003cp\u003e却说李傕郭汜在宫廷横行无忌，太尉杨彪和大司农朱儁密谋诛杀李傕郭汜。杨彪献一反间计：郭汜的妻子妒忌心很强，可派人秘密告诉郭汜妻子，郭汜在和李傕夫人偷情。反间计成功，李傕郭汜反目成仇，李傕劫了天子，郭汜劫了文武百官，每日厮杀。\u003c/p\u003e","title":"《三国演义》每回内容梗概（1~30）"},{"content":"这一讲是上一讲的补充，内容比较零碎，包括：Word2vec回顾、优化、基于统计的词向量、GloVe、词向量评价、词义等，前两个内容没必要再介绍了，下面逐一介绍后四个内容。\n基于统计的词向量 词向量的目的就是希望通过低维稠密向量来表示词的含义，而词的分布式语义表示方法认为词的含义由其上下文语境决定。Word2vec把中心词和临近词抽取出来，通过预测的方式训练得到词向量。在Word2vec之前，传统的方式通过统计词的共现性来得到词向量，即一个词的词向量表示为其临近词出现的频率，如果两个词的含义很相近，则其临近词分布会比较像，得到的词向量也比较像。其具体计算过程在第一次作业中有详细的描述，这里再简单回顾如下。\n假设一个语料库中包含三个句子，共有8个特异词（包括点号），对于每个词，统计其前后一个词的词频（临近窗口为1），由此能得到一个8×8的对称矩阵，其每一行（或每一列）表示该词的词向量。比如对于like这个词，在三个句子中，其左右共出现2次I，1次deep和1次NLP，所以like对应的词向量中，I、deep和NLP维的值分别为2,1,1。\n这种基于词频统计的方法很简单，但是它有如下不足：\n特异的词很多，所以矩阵很大，维度很高，需要的存储空间也很大 特异词的数目是在不断增长的，则词向量的维度也在不断增长 矩阵很稀疏，即词向量很稀疏，会导致很多NLP任务会遇到稀疏计算的问题 所以需要把上述计数矩阵转换为一个低维稠密的矩阵，方法就是SVD分解。上述矩阵原本是一个\\(n\\times n\\)的矩阵，SVD分解后能得到一个\\(n\\times k\\)的矩阵，其中\\(k\\ll n\\)。即原本的词向量是一个\\(n\\)维的高维稀疏向量，变成了\\(k\\)维的低维稠密向量，而且还不会损失太多的信息。\n2005年的一篇文章对上述简单的计数方法进行了改进，包括去掉停用词、使用倾斜窗口、使用皮尔逊相关系数等，提出了COALS模型，该模型得到的词向量效果也不错，也具有句法特征和语义特征。使用统计的方法和使用预测的方法训练词向量，两者的对比如下。基于统计计数的方法的主要特点是：训练速度快，能充分利用统计信息，主要用来捕获词的相似性。基于预测的方法的主要特点是：对语料库的大小可扩展，没有充分利用统计信息，能捕获除了词的相似性之外的其他复杂特征。\nGloVe GloVe的全称是GloVe: Global Vectors for Word Representation，正是这门课的老师Christopher D. Manning的研究成果。有关GloVe论文的详细解读，可以看这篇博客。GloVe希望能综合上述基于统计和基于预测的两种方法的优点。\nGloVe的基本思想依然是基于统计的方法，当统计得到共现矩阵X之后，可以计算得到词\\(k\\)是词\\(i\\)的临近词的概率：\n$$P_{i,k}=\\dfrac{X_{i,k}}{X_{i}}$$再定义两个\\(P\\)的比值：\n$$ratio_{i,j,k}=\\dfrac{P_{i,k}}{P_{j,k}}$$如果词\\(k\\)在两个词\\(i\\)和\\(j\\)的临近概率相同，无论是同样大（water）还是同样小（fashion），经过比值计算后，\\(ratio_{i,j,k}\\)都约等于1了，说明在维度water和fashion上，无法区分ice和steam。而在维度solid和gash上，由于概率\\(P\\)的差异，导致\\(ratio_{i,j,k}\\)很大或者很小，这是有意义的，说明在solid和gas维度上，可以区分ice和steam的语义。\n基于这样的观察，GloVe首先统计语料库中三元组\\(i,j,k\\)的\\(ratio_{i,j,k}\\)，然后初始化词向量\\(v\\)，构造函数\\(g\\)，使得利用词向量计算得到的\\(g(v_{i},v_{j},v_{k})\\)和真实\\(ratio_{i,j,k}\\)尽量接近。\n$$\\dfrac{P_{i,k}}{P_{j,k}}=ratio_{i,j,k}=g(v_{i},v_{j},v_{k})$$$$J=\\sum_{i,j,k}^N(\\dfrac{P_{i,k}}{P_{j,k}}-g(v_{i},v_{j},v_{k}))^2$$但是上述方法的复杂度太高了，对于一个\\(N\\times N\\)的共现矩阵，上述算法需要计算所有的三元组，复杂度是\\(N\\times N\\times N\\)。GloVe文章通过各种转换技巧，把复杂度降为了一个\\(N\\times N\\)的问题，具体过程可以看上面提到的博客或者paper原文。\n总的来说，基于共现矩阵这种统计的方法，能捕获整个语料库全局的信息；而类似word2vec的预测的方法，则主要捕获局部的滑动窗口内的共现信息，两种方法训练得到的词向量效果都不错。\n词向量评价 评价词向量的好坏主要有两个尺度，一是内部任务评价（intrinsic），一是外部任务评价（extrinsic），两者的主要特点如下。大概意思是内部任务是根据词本身具有的性质，比如近义、反义等，评价词向量本身的性能。外部任务是指词向量对NLP下游任务的性能的影响，比如同样是一个文本分类问题，换不同的词向量，对文本分类任务的性能的影响能反映出词向量的性能。\n常见的内部任务评价是词的类比推理（Word Vector Analogies），就是类似man:woman :: king:queen这种，word2vec还专门整理出了这样的测试数据：word2vec/trunk/questions-words.txt。另一个内部任务评价是使用训练得到的词向量计算的词相似度，和人类认为的相似度做比较，有团队专门整理出了人类对两个词的相似度打分，具体可以看这里。\n对词向量的外部任务评价就很多了，几乎所有的NLP任务都可以用来作为词向量的外部任务评价，比如命名实体识别、文本分类等等，这里不再展开。\n词义 一个词往往具有多个含义（word senses），特别是对于常用的词或者存在很久的词。那么一个词向量能同时包含这个词的多个语义吗？有文章把一个词的多个语义通过线性加权的方式叠加到一个词向量中，然后还能通过稀疏编码的方式求解出每个语义的词向量，具体可以看下图中的参考文献。\n","permalink":"http://localhost:1313/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/","summary":"\u003cp\u003e这一讲是上一讲的补充，内容比较零碎，包括：Word2vec回顾、优化、基于统计的词向量、GloVe、词向量评价、词义等，前两个内容没必要再介绍了，下面逐一介绍后四个内容。\u003c/p\u003e\n\u003ch1 id=\"基于统计的词向量\"\u003e基于统计的词向量\u003c/h1\u003e\n\u003cp\u003e词向量的目的就是希望通过低维稠密向量来表示词的含义，而词的分布式语义表示方法认为词的含义由其上下文语境决定。Word2vec把中心词和临近词抽取出来，通过预测的方式训练得到词向量。在Word2vec之前，传统的方式通过统计词的共现性来得到词向量，即一个词的词向量表示为其临近词出现的频率，如果两个词的含义很相近，则其临近词分布会比较像，得到的词向量也比较像。其具体计算过程在\u003ca href=\"https://github.com/01joy/stanford-cs224n-winter-2019/blob/master/1.8/assignment/a1/exploring_word_vectors.ipynb\"\u003e第一次作业\u003c/a\u003e中有详细的描述，这里再简单回顾如下。\u003c/p\u003e\n\u003cp\u003e假设一个语料库中包含三个句子，共有8个特异词（包括点号），对于每个词，统计其前后一个词的词频（临近窗口为1），由此能得到一个8×8的对称矩阵，其每一行（或每一列）表示该词的词向量。比如对于like这个词，在三个句子中，其左右共出现2次I，1次deep和1次NLP，所以like对应的词向量中，I、deep和NLP维的值分别为2,1,1。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/co-occurrence_matrix.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这种基于词频统计的方法很简单，但是它有如下不足：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特异的词很多，所以矩阵很大，维度很高，需要的存储空间也很大\u003c/li\u003e\n\u003cli\u003e特异词的数目是在不断增长的，则词向量的维度也在不断增长\u003c/li\u003e\n\u003cli\u003e矩阵很稀疏，即词向量很稀疏，会导致很多NLP任务会遇到稀疏计算的问题\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e所以需要把上述计数矩阵转换为一个低维稠密的矩阵，方法就是SVD分解。上述矩阵原本是一个\\(n\\times n\\)的矩阵，SVD分解后能得到一个\\(n\\times k\\)的矩阵，其中\\(k\\ll n\\)。即原本的词向量是一个\\(n\\)维的高维稀疏向量，变成了\\(k\\)维的低维稠密向量，而且还不会损失太多的信息。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://pdfs.semanticscholar.org/73e6/351a8fb61afc810a8bb3feaa44c41e5c5d7b.pdf\"\u003e2005年的一篇文章\u003c/a\u003e对上述简单的计数方法进行了改进，包括去掉停用词、使用倾斜窗口、使用皮尔逊相关系数等，提出了COALS模型，该模型得到的词向量效果也不错，也具有句法特征和语义特征。使用统计的方法和使用预测的方法训练词向量，两者的对比如下。基于统计计数的方法的主要特点是：训练速度快，能充分利用统计信息，主要用来捕获词的相似性。基于预测的方法的主要特点是：对语料库的大小可扩展，没有充分利用统计信息，能捕获除了词的相似性之外的其他复杂特征。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/count_based_vs_direct_prediction.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"glove\"\u003eGloVe\u003c/h1\u003e\n\u003cp\u003eGloVe的全称是\u003ca href=\"https://nlp.stanford.edu/projects/glove/\"\u003eGloVe: Global Vectors for Word Representation\u003c/a\u003e，正是这门课的老师Christopher D. Manning的研究成果。有关GloVe论文的详细解读，可以看\u003ca href=\"https://blog.csdn.net/coderTC/article/details/73864097\"\u003e这篇博客\u003c/a\u003e。GloVe希望能综合上述基于统计和基于预测的两种方法的优点。\u003c/p\u003e\n\u003cp\u003eGloVe的基本思想依然是基于统计的方法，当统计得到共现矩阵X之后，可以计算得到词\\(k\\)是词\\(i\\)的临近词的概率：\u003c/p\u003e\n$$P_{i,k}=\\dfrac{X_{i,k}}{X_{i}}$$\u003cp\u003e再定义两个\\(P\\)的比值：\u003c/p\u003e\n$$ratio_{i,j,k}=\\dfrac{P_{i,k}}{P_{j,k}}$$\u003cp\u003e如果词\\(k\\)在两个词\\(i\\)和\\(j\\)的临近概率相同，无论是同样大（water）还是同样小（fashion），经过比值计算后，\\(ratio_{i,j,k}\\)都约等于1了，说明在维度water和fashion上，无法区分ice和steam。而在维度solid和gash上，由于概率\\(P\\)的差异，导致\\(ratio_{i,j,k}\\)很大或者很小，这是有意义的，说明在solid和gas维度上，可以区分ice和steam的语义。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/GloVe.png\"\u003e\u003c/p\u003e\n\u003cp\u003e基于这样的观察，GloVe首先统计语料库中三元组\\(i,j,k\\)的\\(ratio_{i,j,k}\\)，然后初始化词向量\\(v\\)，构造函数\\(g\\)，使得利用词向量计算得到的\\(g(v_{i},v_{j},v_{k})\\)和真实\\(ratio_{i,j,k}\\)尽量接近。\u003c/p\u003e\n$$\\dfrac{P_{i,k}}{P_{j,k}}=ratio_{i,j,k}=g(v_{i},v_{j},v_{k})$$$$J=\\sum_{i,j,k}^N(\\dfrac{P_{i,k}}{P_{j,k}}-g(v_{i},v_{j},v_{k}))^2$$\u003cp\u003e但是上述方法的复杂度太高了，对于一个\\(N\\times N\\)的共现矩阵，上述算法需要计算所有的三元组，复杂度是\\(N\\times N\\times N\\)。GloVe文章通过各种转换技巧，把复杂度降为了一个\\(N\\times N\\)的问题，具体过程可以看上面提到的博客或者paper原文。\u003c/p\u003e\n\u003cp\u003e总的来说，基于共现矩阵这种统计的方法，能捕获整个语料库全局的信息；而类似word2vec的预测的方法，则主要捕获局部的滑动窗口内的共现信息，两种方法训练得到的词向量效果都不错。\u003c/p\u003e\n\u003ch1 id=\"词向量评价\"\u003e词向量评价\u003c/h1\u003e\n\u003cp\u003e评价词向量的好坏主要有两个尺度，一是内部任务评价（intrinsic），一是外部任务评价（extrinsic），两者的主要特点如下。大概意思是内部任务是根据词本身具有的性质，比如近义、反义等，评价词向量本身的性能。外部任务是指词向量对NLP下游任务的性能的影响，比如同样是一个文本分类问题，换不同的词向量，对文本分类任务的性能的影响能反映出词向量的性能。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/word_vector_evaluation.png\"\u003e\u003c/p\u003e\n\u003cp\u003e常见的内部任务评价是词的类比推理（Word Vector Analogies），就是类似man:woman :: king:queen这种，word2vec还专门整理出了这样的测试数据：\u003ca href=\"https://code.google.com/archive/p/word2vec/source/default/source\"\u003eword2vec/trunk/questions-words.txt\u003c/a\u003e。另一个内部任务评价是使用训练得到的词向量计算的词相似度，和人类认为的相似度做比较，有团队专门整理出了人类对两个词的相似度打分，具体可以看\u003ca href=\"http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e对词向量的外部任务评价就很多了，几乎所有的NLP任务都可以用来作为词向量的外部任务评价，比如命名实体识别、文本分类等等，这里不再展开。\u003c/p\u003e\n\u003ch1 id=\"词义\"\u003e词义\u003c/h1\u003e\n\u003cp\u003e一个词往往具有多个含义（word senses），特别是对于常用的词或者存在很久的词。那么一个词向量能同时包含这个词的多个语义吗？有文章把一个词的多个语义通过线性加权的方式叠加到一个词向量中，然后还能通过稀疏编码的方式求解出每个语义的词向量，具体可以看下图中的参考文献。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-23-cs224n-1-10-word-vectors-2-and-word-senses/word_senses.png\"\u003e\u003c/p\u003e","title":"CS224N（1.10）Word Vectors 2 and Word Senses"},{"content":"今天开始介绍大名鼎鼎的NLP网课Stanford-CS224N。第一讲内容为课程简介和词向量。\n词向量即用来表示这个词的含义的向量。早期的NLP常用one-hot编码来表示词向量，假如词典中共有10000个词，则这个one-hot向量长度就是10000，该词在词典中所处位置对应的值为1，其他值为0。\none-hot表示方法虽然简单，但其有诸多缺点：1. 词典中的词是不断增多的，比如英语，通过对原有的词增加前缀和后缀，可以变换出很多不同的词，one-hot编码会导致向量维度非常大，且每个向量是稀疏的；2. 不同词的one-hot编码向量是垂直的，在向量空间中无法表示近似关系，即使两个含义相近的词，它们的词向量点积也为0。\n既然one-hot编码有这么多缺点，那我们就换一种编码，one-hot是高维稀疏向量，那新的编码就改用低维稠密向量，这样就解决了上述问题，那么怎样得到一个词的低维稠密的词向量呢？这就是word2vec算法。\nword2vec采用了分布式语义的方法来表示一个词的含义。本质上，一个词的含义就是这个词所处的上下文语境。回想一下我们高中做英语完形填空时，一篇短文，挖了好多空，让我们根据空缺词的上下文语境选择合适的词。也就是说上下文语境已经能够确定这个词的含义了，如果选词正确，也就意味着我们理解了这个空缺词的含义。\nword2vec算法发表于2013年，包括两种训练算法Skip-grams (SG)和Continuous Bag of Words (CBOW)，这两种方法很类似，其中CBOW和上述介绍到的英语完形填空几乎是一样的，由上下文词预测中心词；而SG则和CBOW正好相反，由中心词预测上下文词，本文主要介绍SG算法。\n给定一个语料库，这个语料库包含了很多文章，每篇文章又包含很多句子，每个句子又包含很多词语。所以一个语料库是一个天然的标注集，因为对于每一个选定的中心词，我们都知道其临近的词是什么。这样一个（中心词，临近词）对就构成了一个标注集。SG算法的中心思想就是对于每个选定的中心词，尽量准确的预测其周围可能出现的词的概率分布。具体来说，SG算法首先随机初始化每个词的词向量；然后预测不同临近词出现的概率，最后最大化实际临近词出现的概率。\n形式化来说，就是用极大似然估计的方法，求解每个词的词向量。其目标函数如下，其中\\(\\theta\\)是待求解的参数；\\(t\\)为选定的中心词位置；对于每个\\(t\\)（外层\\(\\prod\\)），估计其邻域\\(\\pm m\\)个词出现的概率（内层\\(\\prod\\)）。\n求解极大似然估计的方法比较成熟，一般先把极大似然转换为最小化-log似然，然后用梯度下降求解。所以核心问题就变成了如何求解\\(P(w_{t+j}|w_t;\\theta)\\)。\n对于每个词\\(w\\)，定义其两个词向量：\\(v_w\\)表示当\\(w\\)为中心词时的词向量，\\(u_w\\)表示当\\(w\\)为其他词的临近词时的词向量。则对于一个中心词\\(c\\)和其临近词\\(o\\)，有：\n上式本质是一个softmax函数，因为给定\\(c\\)，\\(o\\)相当于是标注结果，所以把它们的点积作为分子，希望分子越大越好；而分母则是所有可能的\\(u_w\\)和\\(v_c\\)的点积之和，起到归一化作用。\n题外话：讲这张幻灯片时，还提到softmax的一个形象解释。softmax包括max和soft两层含义。假设对于一个数组[1,2,3,4]，直接max也就是hard max的结果是保留最大值，其他全变为0，即[0,0,0,4]。但是softmax对他们求\\(\\frac{exp(x_i)}{\\sum_{j=1}^nexp(x_j)}\\)，变成了[0.03, 0.09, 0.24, 0.64]，最大的还是第4个数，但第四个数的优势被放大了，原来4只是1的4倍，现在0.64是0.03的21倍。所以softmax不但保留了max的属性，还变得更soft了，原来小的数不会被抹为0，只不过拉大了差异。\n使用梯度下降还需要求解\\(P\\)对参数\\(\\theta\\)的梯度，在这里\\(\\theta\\)代表了所有词的中心词向量和临近词向量。对于上式，\\(u_o\\)、\\(v_c\\)等就是\\(\\theta\\)的一部分。不断利用求导的链式法则，容易得到：\n$$\\begin{eqnarray}\\frac{\\partial P(o|c)}{\\partial v_c}=u_o-\\sum_{w\\in V}P(w|c)u_w.\\tag{1}\\end{eqnarray}$$最后算出来的梯度很有意思，\\(u_o\\)表示观察到的上下文词向量（o表示observed），减号后面的是这个位置的期望的词向量，期望=概率*值。差值就是真实观察词向量减去期望词向量，这就是梯度。当它们的差值很小时，说明给定\\(c\\)能很好的预测其临近词的概率分布。\nOK，当以上内容都准备妥当之后，我们就可以开始求解词向量了。首先随机初始化每个词\\(w\\)的中心词向量\\(v_w\\)和临近词向量\\(u_w\\)；然后求解-log损失函数\\(J(\\theta)\\)；最后根据梯度下降更新所有参数\\(\\theta\\)。\n上述word2vec算法简单，直观，但写代码实现比较复杂。在实际应用场景中，人们往往使用神经网络的方法来求解词向量，具体教程请看这里： http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ 。\n我们把训练词向量的问题转换为端到端的文本分类问题。如下图所示，对于语料库中的一个句子，假设临近窗口为前后两个词，则可以抽取出如下图右边所示的训练样本，每个训练样本是一个（中心词，临近词）的pair。比如对于（the, quick）训练样本，我们希望神经网络在输入the这个中心词时，能以较高的概率预测出quick这个词。\n网络的结构如下图所示，也非常简单，是仅含一个隐藏层的全连接网络。比如上图的一组训练数据是（the, quick），表示输入是the的one-hot编码，输出是quick的one-hot编码。假设词典里有10,000个不同的词，则one-hot编码长度为10,000。有一个隐藏层的全连接网络，对应权重构成两个权重矩阵，和输入层连接的矩阵为\\(V\\)，其每一行表示词作为中心词时的词向量。输入行向量乘以\\(V\\)正好得到输入词的词向量，这对应课上讲的作为中心词的词向量\\(v_c\\)。\n隐层和输出层连接的权重矩阵为\\(U\\)，其每一列表示输出层的词的临近词词向量。隐层的行向量\\(v_c\\)乘以矩阵\\(U\\)，得到词\\(c\\)的临近词的概率分布，再经过softmax激活，进行归一化。其实反过来看，从输出往隐层看，相当于输出层的行向量乘以\\(U\\)的转置，得到隐层词向量。这其实就是另一种训练词向量的方法CBOW，即英语完形填空，用临近词来预测中心词。\n对于下图的神经网络，输出用softmax激活，损失函数使用-log损失，训练网络时使用梯度下降，其效果正好是课上讲的使用极大似然估计的方法！\n另一方面，上图的这种结构是skip-gram模型，如果把对应的箭头反一下，输入变输出，输出变输入，其实就变成了CBOW模型了。\n上述全连接网络虽然能很方便的计算词向量，但存在两个问题：1. 网络过于庞大，参数量太多；2. 训练样本太多，每个样本都更新所有参数，训练速度慢。针对这两个问题，作者分别提出了 subsampling 和 negative sampling 的技巧，具体请看教程： http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ 。\n第一个问题，网络参数量太多。假设有1w个特异的词，词向量长度为300，整个网络就有两个300w的矩阵（上图的V和U）参数需要优化。另一方面，训练语料库往往是很大的，随随便便就是成百上千万的文章，由此拆分得到的训练词组对就更大了，很容易到上亿的级别。几百万的参数，几亿的训练数据， 导致网络太过庞大，训练不动。\nsubsampling技巧是指，每个词有一个保留概率p，以这个概率p保留其在训练数据中，以1-p删掉这个词。比如上面的例子，删掉了fox，则fox对应的4个训练数据也一并删掉了，所以能减少较多的训练数据。对于词\\(w_i\\)，其概率\\(P(w_i)\\)公式如下，其中\\(z(w_i)\\)是词\\(w\\)的词频。概率p和这个词在语料库中的词频有关，词频越大，保留概率越低，即被删掉的概率越大，所以subsampling之后应该能较大的减少训练数据。\n$$\\begin{eqnarray}P(w_i) = (\\sqrt{\\frac{z(w_i)}{0.001}} + 1) \\cdot \\frac{0.001}{z(w_i)} .\\tag{2}\\end{eqnarray}$$\n第二个问题，原来的网络在训练时，对于每个输入中心词，都会对两个很大的参数矩阵V和U（和上面假设一样，300w）进行轻微的更新，更新操作太多了。\nnegative sampling技巧，只更新一小部分参数。比如对于(“fox”, “quick”)，期望的输出是quick的one-hot，即只有quick对应位为1，其他为0。但网络的softmax输出肯定不可能是完完全全的quick为1，其他为0；有可能是quick为0.8，其他位有些0.001，0.03之类的非0值，这就导致输出层的所有神经元都有误差。按照传统的方法，输出层所有神经元对应的U矩阵的权重都要更新。negative sampling技巧是，只更新和quick连接的U权重以及随机选5个输出神经元的连接权重进行更新，这样一下把需要更新的U权重个数从300w降到了6*300=1800，只需要更新0.06%的参数，大大减小了参数更新量！\n5个随机选中的神经元（输出位置，即对应1w维的某个词）被称为negative sample，被选中的概率和词频成正比，词频越大的词被选中的概率越大，和上面subsampling类似。概率公式如下，其中\\(f(w_i)\\)应该和(2)中的\\(z(w_i)\\)一样，都表示词频。\n$$\\begin{eqnarray}P(w_i) = \\frac{ {f(w_i)}^{3/4} }{\\sum_{j=0}^{n}\\left( {f(w_j)}^{3/4} \\right) }.\\tag{3}\\end{eqnarray}$$ 作业请见GitHub： https://github.com/01joy/stanford-cs224n-winter-2019/blob/master/1.8/assignment/a1/exploring_word_vectors.ipynb\n复习SVD分解请看： https://blog.csdn.net/u010099080/article/details/68060274 以及 https://www.zhihu.com/question/34143886/answer/131046490\n","permalink":"http://localhost:1313/posts/2019-06-22-cs224n-1-8-introduction-and-word-vectors/","summary":"\u003cp\u003e今天开始介绍大名鼎鼎的NLP网课Stanford-CS224N。第一讲内容为课程简介和词向量。\u003c/p\u003e\n\u003cp\u003e词向量即用来表示这个词的含义的向量。早期的NLP常用one-hot编码来表示词向量，假如词典中共有10000个词，则这个one-hot向量长度就是10000，该词在词典中所处位置对应的值为1，其他值为0。\u003c/p\u003e\n\u003cp\u003eone-hot表示方法虽然简单，但其有诸多缺点：1. 词典中的词是不断增多的，比如英语，通过对原有的词增加前缀和后缀，可以变换出很多不同的词，one-hot编码会导致向量维度非常大，且每个向量是稀疏的；2. 不同词的one-hot编码向量是垂直的，在向量空间中无法表示近似关系，即使两个含义相近的词，它们的词向量点积也为0。\u003c/p\u003e\n\u003cp\u003e既然one-hot编码有这么多缺点，那我们就换一种编码，one-hot是高维稀疏向量，那新的编码就改用低维稠密向量，这样就解决了上述问题，那么怎样得到一个词的低维稠密的词向量呢？这就是word2vec算法。\u003c/p\u003e\n\u003cp\u003eword2vec采用了分布式语义的方法来表示一个词的含义。本质上，一个词的含义就是这个词所处的上下文语境。回想一下我们高中做英语完形填空时，一篇短文，挖了好多空，让我们根据空缺词的上下文语境选择合适的词。也就是说上下文语境已经能够确定这个词的含义了，如果选词正确，也就意味着我们理解了这个空缺词的含义。\u003c/p\u003e\n\u003cp\u003eword2vec算法发表于2013年，包括两种训练算法Skip-grams (SG)和Continuous Bag of Words (CBOW)，这两种方法很类似，其中CBOW和上述介绍到的英语完形填空几乎是一样的，由上下文词预测中心词；而SG则和CBOW正好相反，由中心词预测上下文词，本文主要介绍SG算法。\u003c/p\u003e\n\u003cp\u003e给定一个语料库，这个语料库包含了很多文章，每篇文章又包含很多句子，每个句子又包含很多词语。所以一个语料库是一个天然的标注集，因为对于每一个选定的中心词，我们都知道其临近的词是什么。这样一个（中心词，临近词）对就构成了一个标注集。SG算法的中心思想就是对于每个选定的中心词，尽量准确的预测其周围可能出现的词的概率分布。具体来说，SG算法首先随机初始化每个词的词向量；然后预测不同临近词出现的概率，最后最大化实际临近词出现的概率。\u003c/p\u003e\n\u003cp\u003e形式化来说，就是用极大似然估计的方法，求解每个词的词向量。其目标函数如下，其中\\(\\theta\\)是待求解的参数；\\(t\\)为选定的中心词位置；对于每个\\(t\\)（外层\\(\\prod\\)），估计其邻域\\(\\pm m\\)个词出现的概率（内层\\(\\prod\\)）。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-22-cs224n-1-8-introduction-and-word-vectors/word2vec_objective_function.png\"\u003e\u003c/p\u003e\n\u003cp\u003e求解极大似然估计的方法比较成熟，一般先把极大似然转换为最小化-log似然，然后用梯度下降求解。所以核心问题就变成了如何求解\\(P(w_{t+j}|w_t;\\theta)\\)。\u003c/p\u003e\n\u003cp\u003e对于每个词\\(w\\)，定义其两个词向量：\\(v_w\\)表示当\\(w\\)为中心词时的词向量，\\(u_w\\)表示当\\(w\\)为其他词的临近词时的词向量。则对于一个中心词\\(c\\)和其临近词\\(o\\)，有：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-22-cs224n-1-8-introduction-and-word-vectors/word2vec_prediction_function.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上式本质是一个softmax函数，因为给定\\(c\\)，\\(o\\)相当于是标注结果，所以把它们的点积作为分子，希望分子越大越好；而分母则是所有可能的\\(u_w\\)和\\(v_c\\)的点积之和，起到归一化作用。\u003c/p\u003e\n\u003cp\u003e题外话：讲这张幻灯片时，还提到softmax的一个形象解释。softmax包括max和soft两层含义。假设对于一个数组[1,2,3,4]，直接max也就是hard max的结果是保留最大值，其他全变为0，即[0,0,0,4]。但是softmax对他们求\\(\\frac{exp(x_i)}{\\sum_{j=1}^nexp(x_j)}\\)，变成了[0.03, 0.09, 0.24, 0.64]，最大的还是第4个数，但第四个数的优势被放大了，原来4只是1的4倍，现在0.64是0.03的21倍。所以softmax不但保留了max的属性，还变得更soft了，原来小的数不会被抹为0，只不过拉大了差异。\u003c/p\u003e\n\u003cp\u003e使用梯度下降还需要求解\\(P\\)对参数\\(\\theta\\)的梯度，在这里\\(\\theta\\)代表了所有词的中心词向量和临近词向量。对于上式，\\(u_o\\)、\\(v_c\\)等就是\\(\\theta\\)的一部分。不断利用求导的链式法则，容易得到：\u003c/p\u003e\n$$\\begin{eqnarray}\\frac{\\partial P(o|c)}{\\partial v_c}=u_o-\\sum_{w\\in V}P(w|c)u_w.\\tag{1}\\end{eqnarray}$$\u003cp\u003e最后算出来的梯度很有意思，\\(u_o\\)表示观察到的上下文词向量（o表示observed），减号后面的是这个位置的期望的词向量，期望=概率*值。差值就是真实观察词向量减去期望词向量，这就是梯度。当它们的差值很小时，说明给定\\(c\\)能很好的预测其临近词的概率分布。\u003c/p\u003e\n\u003cp\u003eOK，当以上内容都准备妥当之后，我们就可以开始求解词向量了。首先随机初始化每个词\\(w\\)的中心词向量\\(v_w\\)和临近词向量\\(u_w\\)；然后求解-log损失函数\\(J(\\theta)\\)；最后根据梯度下降更新所有参数\\(\\theta\\)。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e上述word2vec算法简单，直观，但写代码实现比较复杂。在实际应用场景中，人们往往使用神经网络的方法来求解词向量，具体教程请看这里： \u003ca href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\"\u003ehttp://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\u003c/a\u003e 。\u003c/p\u003e\n\u003cp\u003e我们把训练词向量的问题转换为端到端的文本分类问题。如下图所示，对于语料库中的一个句子，假设临近窗口为前后两个词，则可以抽取出如下图右边所示的训练样本，每个训练样本是一个（中心词，临近词）的pair。比如对于（the, quick）训练样本，我们希望神经网络在输入the这个中心词时，能以较高的概率预测出quick这个词。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-22-cs224n-1-8-introduction-and-word-vectors/training_data.png\"\u003e\u003c/p\u003e\n\u003cp\u003e网络的结构如下图所示，也非常简单，是仅含一个隐藏层的全连接网络。比如上图的一组训练数据是（the, quick），表示输入是the的one-hot编码，输出是quick的one-hot编码。假设词典里有10,000个不同的词，则one-hot编码长度为10,000。有一个隐藏层的全连接网络，对应权重构成两个权重矩阵，和输入层连接的矩阵为\\(V\\)，其每一行表示词作为中心词时的词向量。输入行向量乘以\\(V\\)正好得到输入词的词向量，这对应课上讲的作为中心词的词向量\\(v_c\\)。\u003c/p\u003e\n\u003cp\u003e隐层和输出层连接的权重矩阵为\\(U\\)，其每一列表示输出层的词的临近词词向量。隐层的行向量\\(v_c\\)乘以矩阵\\(U\\)，得到词\\(c\\)的临近词的概率分布，再经过softmax激活，进行归一化。其实反过来看，从输出往隐层看，相当于输出层的行向量乘以\\(U\\)的转置，得到隐层词向量。这其实就是另一种训练词向量的方法CBOW，即英语完形填空，用临近词来预测中心词。\u003c/p\u003e\n\u003cp\u003e对于下图的神经网络，输出用softmax激活，损失函数使用-log损失，训练网络时使用梯度下降，其效果正好是课上讲的使用极大似然估计的方法！\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-06-22-cs224n-1-8-introduction-and-word-vectors/skip_gram_net_arch2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e另一方面，上图的这种结构是skip-gram模型，如果把对应的箭头反一下，输入变输出，输出变输入，其实就变成了CBOW模型了。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e上述全连接网络虽然能很方便的计算词向量，但存在两个问题：1. 网络过于庞大，参数量太多；2. 训练样本太多，每个样本都更新所有参数，训练速度慢。针对这两个问题，作者分别提出了 subsampling 和 negative sampling 的技巧，具体请看教程：\n\u003ca href=\"http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\"\u003ehttp://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\u003c/a\u003e 。\u003c/p\u003e\n\u003cp\u003e第一个问题，网络参数量太多。假设有1w个特异的词，词向量长度为300，整个网络就有两个300w的矩阵（上图的V和U）参数需要优化。另一方面，训练语料库往往是很大的，随随便便就是成百上千万的文章，由此拆分得到的训练词组对就更大了，很容易到上亿的级别。几百万的参数，几亿的训练数据， 导致网络太过庞大，训练不动。\u003c/p\u003e\n\u003cp\u003esubsampling技巧是指，每个词有一个保留概率p，以这个概率p保留其在训练数据中，以1-p删掉这个词。比如上面的例子，删掉了fox，则fox对应的4个训练数据也一并删掉了，所以能减少较多的训练数据。对于词\\(w_i\\)，其概率\\(P(w_i)\\)公式如下，其中\\(z(w_i)\\)是词\\(w\\)的词频。概率p和这个词在语料库中的词频有关，词频越大，保留概率越低，即被删掉的概率越大，所以subsampling之后应该能较大的减少训练数据。\u003c/p\u003e\n$$\\begin{eqnarray}P(w_i) = (\\sqrt{\\frac{z(w_i)}{0.001}} + 1) \\cdot \\frac{0.001}{z(w_i)} .\\tag{2}\\end{eqnarray}$$\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/mccormickml.com/assets/word2vec/subsample_func_plot.png\"\u003e\u003c/p\u003e\n\u003cp\u003e第二个问题，原来的网络在训练时，对于每个输入中心词，都会对两个很大的参数矩阵V和U（和上面假设一样，300w）进行轻微的更新，更新操作太多了。\u003c/p\u003e\n\u003cp\u003enegative sampling技巧，只更新一小部分参数。比如对于(“fox”, “quick”)，期望的输出是quick的one-hot，即只有quick对应位为1，其他为0。但网络的softmax输出肯定不可能是完完全全的quick为1，其他为0；有可能是quick为0.8，其他位有些0.001，0.03之类的非0值，这就导致输出层的所有神经元都有误差。按照传统的方法，输出层所有神经元对应的U矩阵的权重都要更新。negative sampling技巧是，只更新和quick连接的U权重以及随机选5个输出神经元的连接权重进行更新，这样一下把需要更新的U权重个数从300w降到了6*300=1800，只需要更新0.06%的参数，大大减小了参数更新量！\u003c/p\u003e\n\u003cp\u003e5个随机选中的神经元（输出位置，即对应1w维的某个词）被称为negative sample，被选中的概率和词频成正比，词频越大的词被选中的概率越大，和上面subsampling类似。概率公式如下，其中\\(f(w_i)\\)应该和(2)中的\\(z(w_i)\\)一样，都表示词频。\u003c/p\u003e\n$$\\begin{eqnarray}P(w_i) = \\frac{ {f(w_i)}^{3/4} }{\\sum_{j=0}^{n}\\left( {f(w_j)}^{3/4} \\right) }.\\tag{3}\\end{eqnarray}$$\u003chr\u003e\n\u003cp\u003e作业请见GitHub：\n\u003ca href=\"https://github.com/01joy/stanford-cs224n-winter-2019/blob/master/1.8/assignment/a1/exploring_word_vectors.ipynb\"\u003ehttps://github.com/01joy/stanford-cs224n-winter-2019/blob/master/1.8/assignment/a1/exploring_word_vectors.ipynb\u003c/a\u003e\u003c/p\u003e","title":"CS224N（1.8）Introduction and Word Vectors"},{"content":"相信很多学CS的同学之前都没听说过“蛋白质结构预测”这个问题，直到2018年12月初，一则劲爆消息瞬间引爆了CSer的朋友圈，那就是Google Deepmind团队开发的AlphaFold一举拿下当年的CASP比赛冠军，而且远远甩开了第二名。我当时就转载过类似的公众号文章，大家可以阅读并想象当时朋友圈的欢呼声：阿尔法狗再下一城 | 蛋白结构预测AlphaFold大胜传统人类模型。\n当时，很多同学也转载过类似的文章，但其实很少有人真正明白“蛋白质结构预测”这个问题是什么，它的难度有多大，CASP是个什么比赛，以及AlphaFold的内部原理是什么。当然，对于这一连串的问题，我当时也是懵逼的。不过自己好歹也是个跟蛋白质有关的PhD，如此热点事件，自然是要关注的。不过之后一直没时间，直到今年相关顶级文章再次爆出，我就借着准备文献讲评的机会了解了相关的知识，在这里跟大家分享一下。\nhttps://upload.wikimedia.org/wikipedia/commons/a/a9/Protein_folding.png\n蛋白质结构分为四级，分别是一级结构、二级结构、三级结构和四级结构，下面分别描述。\n一级结构 蛋白质的一级结构可以理解为一条线性的字符串，比如MSFIKTFSGKHFYYDKINKDDIVINDIAVSLSNICR。其基本组成单元是一个个的氨基酸，即一个个的字母。氨基酸有单字母表示和三字母表示，为了简洁，本文使用单字母表示，下图的例子是三字母表示。常见的氨基酸只有20种，所以一级结构的字符串通常只包含20种字母，不包含的6种字母是BJOUXZ。\nhttp://oregonstate.edu/instruct/bb450/450material/schedule450s17e.html 本文大部分蛋白质基础知识都来源于此\n20种氨基酸的结构符合一个通式，如下图所示，中间的碳原子称为Cα碳原子，表示它处在α位；左边连了一个氨基-NH2，称为N端；右边连了一个羧基-COOH，称为C端。20种不同氨基酸的差别就在于Cα上连接的侧链基团R，具体的差别网上一搜就能查到。 https://upload.wikimedia.org/wikipedia/commons/c/ce/AminoAcidball.svg\n20种氨基酸连接的方式为脱水缩合，即一个氨基酸的羧基-COOH和另一个氨基酸的氨基-NH2反应，丢掉一个H2O，形成一个肽键-CO-NH-，如下图所示。丢掉了羧基和氨基的氨基酸被称为氨基酸残基，这个名词很形象，氨基酸缺胳膊少腿，所以变成了“残”基。 二级结构 二级结构就是在一级结构的字符串的基础上，肽链怎样进行盘旋、折叠等变换，形成一种局部的三维结构，这种局部的三维结构通常由氢键支撑。常见的二级结构有α螺旋和β折叠，如下图所示。其中α螺旋的每个残基的-NH的H和临近的第4个残基的-CO的O形成氢键，由此支撑α螺旋的结构稳定性，如下图的箭头所指虚线。β折叠则是两条肽链，平行排列，对应残基的-NH的H和-CO的O形成氢键，由此形成两股β折叠的结构，多股β折叠形成类似手风琴的样子。β折叠分为平行和反平行排列，我们前面介绍到肽段分为N端和C端，如果形成β折叠的两股链都是从N到C（或从C到N），则称为平行排列，否则是反平行排列。每股β折叠都有一个大箭头表示其方向。\n细分的话，蛋白质的二级结构总共有8种，包括转角、无规则卷曲等。目前常采用DSSP的分类方法，有些文献会把8种结构粗分为α螺旋、β折叠和转角这三种结构。\n由上图可知，蛋白质的二级结构极大的决定了其三级结构（下面介绍），所以有很多工作是研究怎样准确预测蛋白质的二级结构的，即预测每个氨基酸残基处于哪一种二级结构中。形式化表示就是，对于一个蛋白质一级结构字符串\\(A_1A_2A_3A_4A_5…\\)，输出\\(a_1a_2a_3a_4a_5…\\)，其中\\(a_i\\)∈{α螺旋，β折叠，转角}。所以，蛋白质的二级结构是一个端到端的问题，很像机器翻译，目前很多文章都会用深度学习NLP的方法来预测蛋白质的二级结构。\n三级结构 简单理解，三级结构就是把多个二级结构拼接到一起，折叠成一个完整的蛋白质三维结构，如下图所示。维持蛋白质三级结构的力比较多样，除了氢键之外，还有二硫键、金属键等。\n四级结构 简单理解，四级结构就是多个三级结构分子组合成一个复合物，就是四级结构。\nhttps://en.wikipedia.org/wiki/Protein_quaternary_structure\n对于CSer来说，由于四级结构仅仅是多个三级结构组合到一起，我们常说的蛋白质三维结构预测问题，通常是指预测蛋白质的三级结构。问题是，构成蛋白质链的原子非常多，我们怎样形式化描述一条蛋白质的三维结构呢？这还要从最原始的一级结构说起。\n蛋白质结构预测问题 前面提到，两个氨基酸通过脱水缩合的方式形成肽键从而连接到一起形成一级结构（本文图四），肽键虽然是单键，但它具有类似双键的特点，即难以旋转（比如羧基中的-C=O键就是双键，无法旋转）。所以，由肽键及周围的6个原子形成了一个固定的肽键平面，这6个原子分别是-C-CO-NH-C-，如下图所示，箭头所指的红色键就是肽键，它周围画出了一个平面，就是肽键平面。\n肽键平面的存在极大的简化了蛋白质结构，可以认为这6个原子的相对位置是固定的了！另一方面，跟这个平面相连的左右两个C原子的两个键是单键，所以他们可以旋转，旋转的角度称为扭转角ϕ和ψ，为了更直观的感受肽链的肽键平面和两个扭转角，可以看下面的动画：K0045879-Rotation_around_amide_bonds_in_protein.mp4（来自https://www.sciencephoto.com/media/639617/view）\n事实上，扭转角ϕ和ψ并不是在360°范围内随机均匀分布的，1963年就有科学家统计过扭转角ϕ和ψ的分布，他们发现稳定的蛋白质结构的ϕ和ψ通常只分布在一小部分区域，如下图的拉氏图所示，这些区域正好对应了常见的α螺旋和β折叠的结构。\n最后，我们还需要介绍一个角度，那就是ω。前面提到，虽然肽键具有双键的特点，难以旋转，但它在少数情况下还是可以旋转的。假设通常情况下，肽键的角度定义为ω=0°，如下图所示，红色的键即为肽键，这种结构的好处是它能让形成肽键的两个残基的侧链R（图中黑色基团）离得尽量的远，这样能保持比较稳定的结构。如果肽键旋转为ω=180°，变为下图的样子，则两个侧链R很靠近，就产生位阻效应，就不稳定，所以这种情况比较少见。但不管怎么说，肽键的扭转角ω也是一个变量因素。\n综上所述，对于一条肽链，如果知道每个残基的三个扭转角ϕ、ψ和ω，则可以重构出肽链的主干部分的三维结构，这就像将极坐标转换为直角坐标一样容易。需要提醒的是，本文提到的蛋白质三维结构预测问题，对蛋白质的结构进行了简化，包括：1. 仅预测蛋白质或肽链的主干结构，不考虑侧链R的结构；2. 假设肽链主干中每个键的长度是固定的；3. 不考虑键的角度，比如对于上图的肽键，仅考虑肽键绕肽键轴本身的旋转，不考虑肽键绕着某一端原子的旋转，比如固定左边的蓝色小球，肽键和右边的红色小球旋转出平面了。 下图的肽键平面，详细的标识出了各个相对固定的值。\nFigure 8-1 from Fundamentals of Biochemistry\n所以，对于CSer来说，蛋白质的三维结构预测问题，就可以看成一个端到端的学习问题，输入是一个字符串，输出是每个字符（残基）对应的三个扭转角ϕ、ψ和ω，问题看起来非常的简洁漂亮。而且，这个问题和NLP中的序列标注、机器翻译等问题很像，所以很多NLP的技术可以用来预测蛋白质的三维结构。下图的插画就是最近发表在Cell Sytems上的一篇用LSTM预测蛋白质三维结构的文章，我会在下一篇博客中和大家分享这篇文章。\nhttps://www.sciencedirect.com/science/article/pii/S2405471219300766?via%3Dihub\n有关“蛋白质结构预测”本身的最后一个问题是，为什么能仅仅通过一级结构的序列信息，预测得到其三级结构呢？也就是说蛋白质结构预测这个问题是否可解，如果蛋白质的三级结构还由其他因素决定，那么即使Deeplearning玩出花了，在生物上也是不可行的。所以，每遇到一个新问题，都要自问一下，这个问题从原理上是否可解。对于“蛋白质结构预测”这个问题，最开始也有人进行了类似的自问，得到的答案是可行的：\n1965年，安芬森（Anfinsen）基于还原变性的牛胰RNase在不需其他任何物质帮助下，仅通过去除变性剂和还原剂就使其恢复天然结构的实验结果，提出了“多肽链的氨基酸序列包含了形成其热力学上稳定的天然构象所必需的全部信息”的“自组装学说”，随后这个学说又得到一些补充。这些学说表明：氨基酸序列确定其空间构象，从而为蛋白质结构预测提供了可行性。\nhttp://chinaxiv.org/user/download.htm?id=6478\nCASP比赛 提到蛋白质三级结构预测，不得不提的是CASP这个比赛。CASP的全称是The Critical Assessment of protein Structure Prediction (CASP)，即蛋白质结构预测的关键评估，被誉为蛋白质结构预测的奥林匹克竞赛。CASP从1994年开始举办，每两年一届，最近的一届是2018年的CASP13。\n每一届CASP比赛，都会提供大约100条未知结构的蛋白质序列，让所有参赛者进行结构预测，比赛结束之后，主办方会通过生化方法测定这些蛋白质的三维结构，然后和参赛者预测的结果进行比对，然后给出预测得分。提供的蛋白质序列分为两类：一类序列和PDB数据库中已有结构的序列有相似性，由此可以基于模板预测，准确度比较高，这类算法称为Template-Based Modeling；另一类序列和PDB库已知结构的序列相似度很低，可以认为是全新的蛋白质，因为无法利用已有模板信息，需要进行从头测序（De novo或ab initio或Free Modeling），目前的准确率比较低。参赛选手也分为两组，一组是servers only，即仅允许算法参赛，给定3天的时间；另一组是human and servers，即允许人和算法合作，共同预测蛋白质结构，给定3周的时间。\nCASP同时提供多种比赛项目，比如常规的结构预测（Regular targets）、数据辅助预测（Data-Assisted targets）和蛋白质接触面预测（Contact predictions）等，其中数据辅助预测中提供了核磁数据（NMR）、交联数据（XLMS）等，对的，交联数据就是我目前研究的pLink处理的数据。\nhttp://predictioncenter.org/casp13/index.cgi\nAlphaFold参加了CASP13的humans and servers组，第一次参赛就一鸣惊人，获得冠军并甩开第二名好多。\nhttp://predictioncenter.org/casp13/zscores_final.cgi\n在蛋白质结构预测领域，活跃着很多华人学者，比如密西根大学张阳团队在servers only组获得七连冠CASP7~CASP13，14年霸主地位！ （I-TASSER (as Zhang-Server) and QUARK from Zhang Lab）。大家可以围观一下张老师实验室的机房以及张老师在清华的一个访谈。另外，芝加哥大学的许锦波团队开发的RaptorX在每届的CASP上也取得了不错的成绩。国内方面，中科院计算所的卜东波老师和上海交大的沈红斌老师也有相关的研究。\nhttp://www.predictioncenter.org/casp13/zscores_final.cgi?model_type=first\u0026gr_type=server_only\nOK，以上就是有关“蛋白质结构预测”这个问题的入门介绍，由于本人并非研究这个领域，理解有误的地方还请大家指出。最后，在入门这个领域时，我查了很多资料，上面的图片也都来自网络并注明了出处，下面汇总列出供大家参考：\n蛋白质结构基础知识： 维基百科：https://en.wikipedia.org/wiki/Protein_structure 俄勒冈州立大学课程General Biochemistry BB 450/550：http://oregonstate.edu/instruct/bb450/450material/schedule450s17e.html csbsju课程Biochemistry Online：http://employees.csbsju.edu/hjakubowski/classes/ch331/protstructure/olunderstandconfo.html 蛋白质：http://refer.biovip.com/doc-view-334.html 肽键平面动画： https://www.sciencephoto.com/media/639617/view https://employees.csbsju.edu/hjakubowski/classes/ch331/protstructure/pp180to0.gif https://employees.csbsju.edu/hjakubowski/classes/ch331/protstructure/pp0to180.gif https://www.youtube.com/watch?v=Kewhg5spUjs https://www.youtube.com/watch?v=meNEUTn9Atg 蛋白质结构预测入门综述： 蛋白质二级结构预测方法的评价（唐一源，计算机与应用化学）：http://www.cnki.com.cn/Article/CJFDTotal-JSYH200306003.htm 蛋白质结构预测（张阳，物理学报）：http://www.cnki.com.cn/Article/CJFDTOTAL-WLXB201617012.htm 蛋白质三级结构预测算法综述（卜东波，计算机学报）：http://www.cnki.com.cn/Article/CJFDTOTAL-JSJX201804002.htm 基于深度学习的八类蛋白质二级结构预测算法（杨伟，计算机应用）：http://www.cnki.com.cn/Article/CJFDTOTAL-JSJY201705054.htm 蛋白质结构预测：梦想与现实（卜东波，信息技术快报）：http://chinaxiv.org/user/download.htm?id=6478 蛋白质结构预测团队： DeepMind：https://deepmind.com/blog/alphafold/ 密西根大学张阳团队：https://zhanglab.ccmb.med.umich.edu/ 芝加哥大学许锦波团队：https://ttic.uchicago.edu/~jinbo/ 哈佛医学院Mohammed AlQuraishi：https://scholar.harvard.edu/alquraishi 中科院计算所卜东波团队：http://bioinfo.ict.ac.cn/~dbu/ 上海交大沈红斌团队：http://www.csbio.sjtu.edu.cn/ ","permalink":"http://localhost:1313/posts/2019-05-25-introduction-to-protein-structure-prediction/","summary":"\u003cp\u003e相信很多学CS的同学之前都没听说过“蛋白质结构预测”这个问题，直到2018年12月初，一则劲爆消息瞬间引爆了CSer的朋友圈，那就是Google Deepmind团队开发的AlphaFold一举拿下当年的CASP比赛冠军，而且远远甩开了第二名。我当时就转载过类似的公众号文章，大家可以阅读并想象当时朋友圈的欢呼声：\u003ca href=\"https://mp.weixin.qq.com/s?__biz=MzUyOTcxNDA2MA==\u0026amp;mid=2247484072\u0026amp;idx=1\u0026amp;sn=4ced43b28439e193e2a88f402c81cb2f\u0026amp;chksm=fa5d9c4bcd2a155df9c5d7450b7d6f0424af39fcfc8634c2345ff0b01300192b4ea6c0565341\u0026amp;mpshare=1\u0026amp;scene=1\u0026amp;srcid=1203qx87iYulfkOp7jvZx5uJ\u0026amp;key=e93dd65619c9c8dbcde90e46cec14857b94dda95e137fa444083b63060f0711ed2b7bd7e3b247f8c8ab36b68388bad7974855e5725ecc112fdd999179387a498599090c13da887f08290f51de8576a8d\u0026amp;ascene=1\u0026amp;uin=MTgzOTAyODQw\u0026amp;devicetype=Windows\u0026#43;10\u0026amp;version=62060739\u0026amp;lang=zh_CN\u0026amp;pass_ticket=HhfIKFFBQhij05w1DGJ%2BS23fCqJLztSSaonfMvhvStM%3D\"\u003e阿尔法狗再下一城 | 蛋白结构预测AlphaFold大胜传统人类模型\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e当时，很多同学也转载过类似的文章，但其实很少有人真正明白“蛋白质结构预测”这个问题是什么，它的难度有多大，CASP是个什么比赛，以及AlphaFold的内部原理是什么。当然，对于这一连串的问题，我当时也是懵逼的。不过自己好歹也是个跟蛋白质有关的PhD，如此热点事件，自然是要关注的。不过之后一直没时间，直到今年相关顶级文章再次爆出，我就借着准备文献讲评的机会了解了相关的知识，在这里跟大家分享一下。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/a/a9/Protein_folding.png\"\u003e\n\u003ca href=\"https://upload.wikimedia.org/wikipedia/commons/a/a9/Protein_folding.png\"\u003ehttps://upload.wikimedia.org/wikipedia/commons/a/a9/Protein_folding.png\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e蛋白质结构分为四级，分别是一级结构、二级结构、三级结构和四级结构，下面分别描述。\u003c/p\u003e\n\u003ch1 id=\"一级结构\"\u003e一级结构\u003c/h1\u003e\n\u003cp\u003e蛋白质的一级结构可以理解为一条线性的字符串，比如MSFIKTFSGKHFYYDKINKDDIVINDIAVSLSNICR。其基本组成单元是一个个的氨基酸，即一个个的字母。氨基酸有单字母表示和三字母表示，为了简洁，本文使用单字母表示，下图的例子是三字母表示。常见的氨基酸只有20种，所以一级结构的字符串通常只包含20种字母，不包含的6种字母是BJOUXZ。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/PrimaryProteinStructure.png\"\u003e\n\u003ca href=\"http://oregonstate.edu/instruct/bb450/450material/schedule450s17e.html\"\u003ehttp://oregonstate.edu/instruct/bb450/450material/schedule450s17e.html\u003c/a\u003e 本文大部分蛋白质基础知识都来源于此\u003c/p\u003e\n\u003cp\u003e20种氨基酸的结构符合一个通式，如下图所示，中间的碳原子称为Cα碳原子，表示它处在α位；左边连了一个氨基-NH2，称为N端；右边连了一个羧基-COOH，称为C端。20种不同氨基酸的差别就在于Cα上连接的侧链基团R，具体的差别网上一搜就能查到。\n\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/c/ce/AminoAcidball.svg\"\u003e\n\u003ca href=\"https://upload.wikimedia.org/wikipedia/commons/c/ce/AminoAcidball.svg\"\u003ehttps://upload.wikimedia.org/wikipedia/commons/c/ce/AminoAcidball.svg\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e20种氨基酸连接的方式为脱水缩合，即一个氨基酸的羧基-COOH和另一个氨基酸的氨基-NH2反应，丢掉一个H2O，形成一个肽键-CO-NH-，如下图所示。丢掉了羧基和氨基的氨基酸被称为氨基酸残基，这个名词很形象，氨基酸缺胳膊少腿，所以变成了“残”基。\n\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/PeptideBonds.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"二级结构\"\u003e二级结构\u003c/h1\u003e\n\u003cp\u003e二级结构就是在一级结构的字符串的基础上，肽链怎样进行盘旋、折叠等变换，形成一种\u003cstrong\u003e局部\u003c/strong\u003e的三维结构，这种局部的三维结构通常由氢键支撑。常见的二级结构有α螺旋和β折叠，如下图所示。其中α螺旋的每个残基的-NH的H和临近的第4个残基的-CO的O形成氢键，由此支撑α螺旋的结构稳定性，如下图的箭头所指虚线。β折叠则是两条肽链，平行排列，对应残基的-NH的H和-CO的O形成氢键，由此形成两股β折叠的结构，多股β折叠形成类似手风琴的样子。β折叠分为平行和反平行排列，我们前面介绍到肽段分为N端和C端，如果形成β折叠的两股链都是从N到C（或从C到N），则称为平行排列，否则是反平行排列。每股β折叠都有一个大箭头表示其方向。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/AlphaHelix.png\"\u003e\n\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/BetaSheets.png\"\u003e\u003c/p\u003e\n\u003cp\u003e细分的话，蛋白质的二级结构总共有8种，包括转角、无规则卷曲等。目前常采用\u003ca href=\"https://en.wikipedia.org/wiki/Protein_secondary_structure#DSSP_classification\"\u003eDSSP的分类方法\u003c/a\u003e，有些文献会把8种结构粗分为α螺旋、β折叠和转角这三种结构。\u003c/p\u003e\n\u003cp\u003e由上图可知，蛋白质的二级结构极大的决定了其三级结构（下面介绍），所以有很多工作是研究怎样准确预测蛋白质的二级结构的，即预测每个氨基酸残基处于哪一种二级结构中。形式化表示就是，对于一个蛋白质一级结构字符串\\(A_1A_2A_3A_4A_5…\\)，输出\\(a_1a_2a_3a_4a_5…\\)，其中\\(a_i\\)∈{α螺旋，β折叠，转角}。所以，蛋白质的二级结构是一个端到端的问题，很像机器翻译，目前很多文章都会用深度学习NLP的方法来预测蛋白质的二级结构。\u003c/p\u003e\n\u003ch1 id=\"三级结构\"\u003e三级结构\u003c/h1\u003e\n\u003cp\u003e简单理解，三级结构就是把多个二级结构拼接到一起，折叠成一个完整的蛋白质三维结构，如下图所示。维持蛋白质三级结构的力比较多样，除了氢键之外，还有二硫键、金属键等。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/TertiaryStructure.png\"\u003e\n\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/ForcesStabilizingTertiaryStructure.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"四级结构\"\u003e四级结构\u003c/h1\u003e\n\u003cp\u003e简单理解，四级结构就是多个三级结构分子组合成一个复合物，就是四级结构。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/ProteinQuaternaryStructure.png\"\u003e\n\u003ca href=\"https://en.wikipedia.org/wiki/Protein_quaternary_structure\"\u003ehttps://en.wikipedia.org/wiki/Protein_quaternary_structure\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e对于CSer来说，由于四级结构仅仅是多个三级结构组合到一起，我们常说的蛋白质三维结构预测问题，通常是指预测蛋白质的三级结构。问题是，构成蛋白质链的原子非常多，我们怎样形式化描述一条蛋白质的三维结构呢？这还要从最原始的一级结构说起。\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"蛋白质结构预测问题\"\u003e蛋白质结构预测问题\u003c/h1\u003e\n\u003cp\u003e前面提到，两个氨基酸通过脱水缩合的方式形成肽键从而连接到一起形成一级结构（本文图四），肽键虽然是单键，但它具有类似双键的特点，即难以旋转（比如羧基中的-C=O键就是双键，无法旋转）。所以，由肽键及周围的6个原子形成了一个固定的肽键平面，这6个原子分别是-C-CO-NH-C-，如下图所示，箭头所指的红色键就是肽键，它周围画出了一个平面，就是肽键平面。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/MultiplePeptideBondPlanes.png\"\u003e\n\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/PhiandPsiAngles.png\"\u003e\u003c/p\u003e\n\u003cp\u003e肽键平面的存在极大的简化了蛋白质结构，可以认为这6个原子的相对位置是固定的了！另一方面，跟这个平面相连的左右两个C原子的两个键是单键，所以他们可以旋转，旋转的角度称为扭转角ϕ和ψ，为了更直观的感受肽链的肽键平面和两个扭转角，可以看下面的动画：\u003ca href=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/K0045879-Rotation_around_amide_bonds_in_protein.mp4\"\u003eK0045879-Rotation_around_amide_bonds_in_protein.mp4\u003c/a\u003e（来自\u003ca href=\"https://www.sciencephoto.com/media/639617/view\"\u003ehttps://www.sciencephoto.com/media/639617/view\u003c/a\u003e）\u003c/p\u003e\n\u003cp\u003e事实上，扭转角ϕ和ψ并不是在360°范围内随机均匀分布的，1963年就有科学家统计过扭转角ϕ和ψ的分布，他们发现稳定的蛋白质结构的ϕ和ψ通常只分布在一小部分区域，如下图的拉氏图所示，这些区域正好对应了常见的α螺旋和β折叠的结构。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/RamachandranPlotLabeled.png\"\u003e\u003c/p\u003e\n\u003cp\u003e最后，我们还需要介绍一个角度，那就是ω。前面提到，虽然肽键具有双键的特点，难以旋转，但它在少数情况下还是可以旋转的。假设通常情况下，肽键的角度定义为ω=0°，如下图所示，红色的键即为肽键，这种结构的好处是它能让形成肽键的两个残基的侧链R（图中黑色基团）离得尽量的远，这样能保持比较稳定的结构。如果肽键旋转为ω=180°，变为下图的样子，则两个侧链R很靠近，就产生\u003ca href=\"https://baike.baidu.com/item/%E7%A9%BA%E9%97%B4%E4%BD%8D%E9%98%BB%E6%95%88%E5%BA%94\"\u003e位阻效应\u003c/a\u003e，就不稳定，所以这种情况比较少见。但不管怎么说，肽键的扭转角ω也是一个变量因素。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/StericHindrance.png\"\u003e\u003c/p\u003e\n\u003cp\u003e综上所述，对于一条肽链，如果知道每个残基的三个扭转角ϕ、ψ和ω，则可以重构出肽链的主干部分的三维结构，这就像将极坐标转换为直角坐标一样容易。需要提醒的是，本文提到的蛋白质三维结构预测问题，对蛋白质的结构进行了简化，包括：1. 仅预测蛋白质或肽链的主干结构，不考虑侧链R的结构；2. 假设肽链主干中每个键的长度是固定的；3. 不考虑键的角度，比如对于上图的肽键，仅考虑肽键绕肽键轴本身的旋转，不考虑肽键绕着某一端原子的旋转，比如固定左边的蓝色小球，肽键和右边的红色小球旋转出平面了。 下图的肽键平面，详细的标识出了各个相对固定的值。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://github.com/01joy/bitjoy.net/raw/master/blog/2019/05/AmidePlane.jpg\"\u003e\nFigure 8-1 from Fundamentals of Biochemistry\u003c/p\u003e\n\u003cp\u003e所以，对于CSer来说，蛋白质的三维结构预测问题，就可以看成一个端到端的学习问题，输入是一个字符串，输出是每个字符（残基）对应的三个扭转角ϕ、ψ和ω，问题看起来非常的简洁漂亮。而且，这个问题和NLP中的序列标注、机器翻译等问题很像，所以很多NLP的技术可以用来预测蛋白质的三维结构。下图的插画就是最近发表在Cell Sytems上的一篇用LSTM预测蛋白质三维结构的文章，我会在下一篇博客中和大家分享这篇文章。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-25-introduction-to-protein-structure-prediction/ProteinStructurePrediction.png\"\u003e\n\u003ca href=\"https://www.sciencedirect.com/science/article/pii/S2405471219300766?via%3Dihub\"\u003ehttps://www.sciencedirect.com/science/article/pii/S2405471219300766?via%3Dihub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e有关“蛋白质结构预测”本身的最后一个问题是，为什么能仅仅通过一级结构的序列信息，预测得到其三级结构呢？也就是说蛋白质结构预测这个问题是否可解，如果蛋白质的三级结构还由其他因素决定，那么即使Deeplearning玩出花了，在生物上也是不可行的。所以，每遇到一个新问题，都要自问一下，这个问题从原理上是否可解。对于“蛋白质结构预测”这个问题，最开始也有人进行了类似的自问，得到的答案是可行的：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e1965年，安芬森（Anfinsen）基于还原变性的牛胰RNase在不需其他任何物质帮助下，仅通过去除变性剂和还原剂就使其恢复天然结构的实验结果，提出了“多肽链的氨基酸序列包含了形成其热力学上稳定的天然构象所必需的全部信息”的“自组装学说”，随后这个学说又得到一些补充。这些学说表明：氨基酸序列确定其空间构象，从而为蛋白质结构预测提供了可行性。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"http://chinaxiv.org/user/download.htm?id=6478\"\u003ehttp://chinaxiv.org/user/download.htm?id=6478\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"casp比赛\"\u003eCASP比赛\u003c/h1\u003e\n\u003cp\u003e提到蛋白质三级结构预测，不得不提的是CASP这个比赛。CASP的全称是The Critical Assessment of protein Structure Prediction (CASP)，即蛋白质结构预测的关键评估，被誉为蛋白质结构预测的奥林匹克竞赛。CASP从1994年开始举办，每两年一届，最近的一届是2018年的CASP13。\u003c/p\u003e\n\u003cp\u003e每一届CASP比赛，都会提供大约100条未知结构的蛋白质序列，让所有参赛者进行结构预测，比赛结束之后，主办方会通过生化方法测定这些蛋白质的三维结构，然后和参赛者预测的结果进行比对，然后给出预测得分。提供的蛋白质序列分为两类：一类序列和PDB数据库中已有结构的序列有相似性，由此可以基于模板预测，准确度比较高，这类算法称为Template-Based Modeling；另一类序列和PDB库已知结构的序列相似度很低，可以认为是全新的蛋白质，因为无法利用已有模板信息，需要进行从头测序（De novo或ab initio或Free Modeling），目前的准确率比较低。参赛选手也分为两组，一组是servers only，即仅允许算法参赛，给定3天的时间；另一组是human and servers，即允许人和算法合作，共同预测蛋白质结构，给定3周的时间。\u003c/p\u003e\n\u003cp\u003eCASP同时提供多种比赛项目，比如常规的结构预测（Regular targets）、数据辅助预测（Data-Assisted targets）和蛋白质接触面预测（Contact predictions）等，其中数据辅助预测中提供了核磁数据（NMR）、交联数据（XLMS）等，对的，交联数据就是我目前研究的pLink处理的数据。\u003c/p\u003e","title":"“蛋白质结构预测”问题描述"},{"content":"由于本书成书较早（2015），作者当时使用的是Theano，但Theano已不再维护，所以本博客使用当下流行的Pytorch框架讲解MNIST图片分类的代码实现，具体就是Pytorch官方给出的MNIST代码：https://github.com/pytorch/examples/tree/master/mnist。\n使用该工具在线制作：http://alexlenail.me/NN-SVG/LeNet.html\n下面，我首先贴出经过我注释的Pytorch MNIST代码，然后对一些关键问题进行解释。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 from __future__ import print_function import argparse import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms # 所有网络类要继承nn.Module class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 调用父类构造函数 self.conv1 = nn.Conv2d(1, 20, 5, 1) # (in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) self.conv2 = nn.Conv2d(20, 50, 5, 1) # 这一层的in_channels正好是上一层的out_channels self.fc1 = nn.Linear(4*4*50, 500) self.fc2 = nn.Linear(500, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 2) # kernel_size=2, stride=2，pooling之后的大小除以2 x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 2) x = x.view(-1, 4*4*50) # 展开成 (z, 4*4*50)，其中z是通过自动推导得到的，所以这里设置为-1，这里相当于展开成行向量，便于后续全连接 x = F.relu(self.fc1(x)) x = self.fc2(x) return F.log_softmax(x, dim=1) # log_softmax 即 log(softmax(x))；dim=1对行进行softmax，因为上面x.view展开成行向量了，log_softmax速度和数值稳定性都比softmax好一些 def train(args, model, device, train_loader, optimizer, epoch): model.train() # 告诉pytorch，这是训练阶段 https://stackoverflow.com/a/51433411/2468587 for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() # 每个batch的梯度重新累加 output = model(data) loss = F.nll_loss(output, target) # 这里的nll_loss就是Michael Nielsen在ch3提到的log-likelihood cost function，配合softmax使用，batch的梯度/loss要求均值mean loss.backward() # 求loss对参数的梯度dw optimizer.step() # 梯度下降，w\u0026#39;=w-η*dw if batch_idx % args.log_interval == 0: print(\u0026#39;Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\u0026#39;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) def test(args, model, device, test_loader): model.eval() # 告诉pytorch，这是预测（评价）阶段 test_loss = 0 correct = 0 with torch.no_grad(): # 预测时不需要误差反传，https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/2 for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += F.nll_loss(output, target, reduction=\u0026#39;sum\u0026#39;).item() # sum up batch loss，预测时的loss求sum，L54再求均值 pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print(\u0026#39;\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\u0026#39;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) def plot1digit(data_loader): import numpy as np import matplotlib.pyplot as plt examples = enumerate(data_loader) batch_idx, (Xs, ys) = next(examples) # 读取到的是一个batch的所有数据 X=Xs[0].numpy()[0] # Xs[0]取出batch中的第一个数据，由tensor转换为numpy，因为pytorch tensor的格式是[channel, height, width]，所以最后[0]取出其第一个通道的[h,w] y=ys[0].numpy() # y没有通道，就一个标量值 np.savetxt(\u0026#39;../../../fig/%d.csv\u0026#39;%y, X, delimiter=\u0026#39;,\u0026#39;) plt.imshow(X, cmap=\u0026#39;Greys\u0026#39;) # or \u0026#39;Greys_r\u0026#39; plt.savefig(\u0026#39;../../../fig/%d.png\u0026#39;%y) plt.show() def main(): # Training settings parser = argparse.ArgumentParser(description=\u0026#39;PyTorch MNIST Example\u0026#39;) parser.add_argument(\u0026#39;--batch-size\u0026#39;, type=int, default=64, metavar=\u0026#39;N\u0026#39;, help=\u0026#39;input batch size for training (default: 64)\u0026#39;) parser.add_argument(\u0026#39;--test-batch-size\u0026#39;, type=int, default=1000, metavar=\u0026#39;N\u0026#39;, help=\u0026#39;input batch size for testing (default: 1000)\u0026#39;) parser.add_argument(\u0026#39;--epochs\u0026#39;, type=int, default=10, metavar=\u0026#39;N\u0026#39;, help=\u0026#39;number of epochs to train (default: 10)\u0026#39;) parser.add_argument(\u0026#39;--lr\u0026#39;, type=float, default=0.01, metavar=\u0026#39;LR\u0026#39;, help=\u0026#39;learning rate (default: 0.01)\u0026#39;) parser.add_argument(\u0026#39;--momentum\u0026#39;, type=float, default=0.5, metavar=\u0026#39;M\u0026#39;, help=\u0026#39;SGD momentum (default: 0.5)\u0026#39;) parser.add_argument(\u0026#39;--no-cuda\u0026#39;, action=\u0026#39;store_true\u0026#39;, default=False, help=\u0026#39;disables CUDA training\u0026#39;) parser.add_argument(\u0026#39;--seed\u0026#39;, type=int, default=1, metavar=\u0026#39;S\u0026#39;, help=\u0026#39;random seed (default: 1)\u0026#39;) parser.add_argument(\u0026#39;--log-interval\u0026#39;, type=int, default=10, metavar=\u0026#39;N\u0026#39;, help=\u0026#39;how many batches to wait before logging training status\u0026#39;) parser.add_argument(\u0026#39;--save-model\u0026#39;, action=\u0026#39;store_true\u0026#39;, default=False, help=\u0026#39;For Saving the current Model\u0026#39;) args = parser.parse_args() use_cuda = not args.no_cuda and torch.cuda.is_available() torch.manual_seed(args.seed) device = torch.device(\u0026#34;cuda\u0026#34; if use_cuda else \u0026#34;cpu\u0026#34;) kwargs = {\u0026#39;num_workers\u0026#39;: 1, \u0026#39;pin_memory\u0026#39;: True} if use_cuda else {} train_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;../data\u0026#39;, train=True, download=True, transform=transforms.Compose([ # https://discuss.pytorch.org/t/can-some-please-explain-how-the-transforms-work-and-why-normalize-the-data/2461/3 transforms.ToTensor(), # 把[0,255]的(H,W,C)的图片转换为[0,1]的(channel,height,width)的图片 transforms.Normalize((0.1307,), (0.3081,)) # 进行z-score标准化，这两个数分别是MNIST的均值和标准差 ])), batch_size=args.batch_size, shuffle=True, **kwargs) test_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;../data\u0026#39;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=args.test_batch_size, shuffle=True, **kwargs) # plot1digit(train_loader) model = Net().to(device) optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) for epoch in range(1, args.epochs + 1): train(args, model, device, train_loader, optimizer, epoch) test(args, model, device, test_loader) if (args.save_model): torch.save(model.state_dict(),\u0026#34;mnist_cnn.pt\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: main() 首先是MNIST数据格式的问题，在L108~L120，我们使用Pytorch的DataLoader载入了训练和测试数据，数据格式本质上和本系列博客的第一篇博客介绍的是一致的，即每张图片都是28*28的灰度图片，因为是灰度图片，所以只有一个通道数，默认格式是(H,W,C)，且值域范围是[0,255]。但上述代码对原始图片进行了两个变换，分别是ToTensor和Normalize。ToTensor将[0,255]的灰度图片(H,W,C)转换为[0,1]的灰度图片(C,H,W)，即Pytorch对2D图片的格式要求都是channel在前。所以经过这一转换，一张图片的shape是(1,28,28)，是一个三维矩阵；如果是彩色图片的话，有R,G,B三个通道，C=3。Normalize对图片数据进行z-score标准化，即减去均值再除以标准差；L112的两个值就是预先计算的MNIST数据集的均值和标准差。这些操作的好处是能让模型更加平稳快速收敛。同第一篇博客一样，我们可以把Pytorch格式的图片打印出来以便直观理解，L61的plot1digit函数就是这个作用。\nPytorch中自定义的网络类都要继承nn.Module这个基类（L10），自定义的网络类需要实现两个函数，分别是构造函数__init__和前向传播函数forward。在__init__中，定义需要用到的成员变量，往往是一些layer，比如上述代码中定义了两个卷积层和两个全连接层。在forward中，完成了实际的网络搭建过程，传入图片x，x流过整个网络，最后返回网络的输出。\n上述代码的网络结构如本博客开篇的图片所示，依次是：输入层、卷积层（包含ReLU激活）、池化层、卷积层（包含ReLU激活）、池化层、全连接层（包含ReLU激活）、全连接层（包含logsoftmax输出）。网络的构建过程很像搭积木，在forward函数和开篇的图片中都能很直观的看出来。\n需要稍微解释一下的是Conv2d类的初始化参数，前4个参数分别是in_channels, out_channels, kernel_size, stride，分别表示传入通道数、传出通道数、卷积核大小、卷积步长，对于第一个卷积核（L13），由于直接和输入层相连，MNIST图片是28*28的单通道图片，所以in_channels=1；out_channels的大小表示所用卷积核的数目，比如这里设置为20就表示有20个卷积核；kernel_size=5表示5*5的卷积核；stride=1表示卷积移动的步长为1。(1,28,28)的图片，经过上述卷积之后，得到的feature map大小变为了(20,24,24)，即有20个大小为24*24的feature map。经过max_pooling之后变为(20,12,12)，只改变了feature map大小，没有改变其channels数。所以第二个卷积核的in_channels等于第一个卷积核的out_channels，等于20（L14）。以此类推，第二个max_pooling输出的feature map就是(50,4,4)，所以第一个全连接层的输入维度是4*4*50。需要稍微注意的是在由feature map和下一层进行全连接时，需要先展开成一个行向量（L23），变成类似于BP网络的输入格式。\nPytorch的官方文档对每个函数都有详细的解释，甚至还给出了公式说明怎样计算卷积层和池化层之后的feature map的size，非常良心，所以遇到任何问题，一定要先仔细看官方文档。如果你仔细看文档的话，会发现nn和nn.functional下会有很多同名的类和函数，比如nn.Conv2d和nn.functional.conv2d同时存在，有关它们的区别，简单来说，前者表示类，后者表示函数，像卷积层、全连接层等需要保存学习参数的layer，建议使用nn；而像ReLU和max_pooling等不需要保存学习参数或功能比较简单的layer，建议直接用nn.functional，具体的区别和建议请看：https://www.zhihu.com/question/66782101/answer/579393790。\n其他还有一些小细节，比如Pytorch模型在训练和预测时，需要分别调用model.train()和model.eval()告诉Pytorch此时是训练和预测阶段了，因为在train阶段，会使用dropout、batchnorm等技术，而在预测时不会调用，所以需要显式告诉Pytorch现在是训练还是预测。还有就是训练阶段，每个batch需要梯度清零（optimizer.zero_grad）、求梯度（loss.backward）、梯度下降更新参数（optimizer.step）等步骤。在预测阶段，不用求梯度（torch.no_grad）等。具体可以看上述代码注释。\n最后总结，使用Pytorch构建深度学习模型是非常简单的，注意Pytorch的基本规则，仿照MNIST例子依葫芦画瓢就可以搭建自己的模型了。建议大家看过官方代码之后，自己重写一遍，检查一下是否学习到位。\n至此，Neural Networks and Deep Learning这本书的学习笔记到此结束，算是入门了神经网络和深度学习，接下来请开启深度学习实战的炼丹之旅。\n","permalink":"http://localhost:1313/posts/2019-05-19-neural-networks-and-deep-learning-appendix-pytorch-mnist-tutorial/","summary":"\u003cp\u003e由于本书成书较早（2015），作者当时使用的是Theano，但Theano已不再维护，所以本博客使用当下流行的Pytorch框架讲解MNIST图片分类的代码实现，具体就是Pytorch官方给出的MNIST代码：\u003ca href=\"https://github.com/pytorch/examples/tree/master/mnist\"\u003ehttps://github.com/pytorch/examples/tree/master/mnist\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-19-neural-networks-and-deep-learning-appendix-pytorch-mnist-tutorial/nn.png\"\u003e\n使用该工具在线制作：\u003ca href=\"http://alexlenail.me/NN-SVG/LeNet.html\"\u003ehttp://alexlenail.me/NN-SVG/LeNet.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e下面，我首先贴出经过我注释的Pytorch MNIST代码，然后对一些关键问题进行解释。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 46\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 47\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 48\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 49\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 50\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 51\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 52\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 53\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 54\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 55\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 56\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 57\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 58\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 59\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 60\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 61\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 62\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 63\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 64\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 65\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 66\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 67\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 68\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 69\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 70\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 71\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 72\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 73\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 74\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 75\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 76\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 77\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 78\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 79\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 80\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 81\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 82\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 83\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 84\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 85\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 86\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 87\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 88\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 89\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 90\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 91\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 92\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 93\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 94\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 95\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 96\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 97\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 98\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 99\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e100\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e101\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e102\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e103\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e104\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e105\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e106\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e107\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e108\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e109\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e110\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e111\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e112\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e113\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e114\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e115\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e116\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e117\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e118\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e119\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e120\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e121\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e122\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e123\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e124\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e125\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e126\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e127\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e128\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e129\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e130\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e131\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e132\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e133\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e134\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e135\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e __future__ \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e print_function\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e argparse\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch.nn \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e nn\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch.nn.functional \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e F\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch.optim \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e optim\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e torchvision \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e datasets, transforms\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 所有网络类要继承nn.Module\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eNet\u003c/span\u003e(nn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eModule):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        super(Net, self)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e() \u003cspan style=\"color:#75715e\"\u003e# 调用父类构造函数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econv1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e nn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConv2d(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# (in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econv2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e nn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConv2d(\u003cspan style=\"color:#ae81ff\"\u003e20\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# 这一层的in_channels正好是上一层的out_channels\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efc1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e nn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eLinear(\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e500\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efc2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e nn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eLinear(\u003cspan style=\"color:#ae81ff\"\u003e500\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eforward\u003c/span\u003e(self, x):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erelu(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econv1(x))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax_pool2d(x, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# kernel_size=2, stride=2，pooling之后的大小除以2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erelu(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econv2(x))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax_pool2d(x, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e x\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eview(\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# 展开成 (z, 4*4*50)，其中z是通过自动推导得到的，所以这里设置为-1，这里相当于展开成行向量，便于后续全连接\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erelu(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efc1(x))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efc2(x)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elog_softmax(x, dim\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# log_softmax 即 log(softmax(x))；dim=1对行进行softmax，因为上面x.view展开成行向量了，log_softmax速度和数值稳定性都比softmax好一些\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e     \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etrain\u003c/span\u003e(args, model, device, train_loader, optimizer, epoch):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etrain() \u003cspan style=\"color:#75715e\"\u003e# 告诉pytorch，这是训练阶段 https://stackoverflow.com/a/51433411/2468587\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e batch_idx, (data, target) \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e enumerate(train_loader):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        data, target \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device), target\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        optimizer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ezero_grad() \u003cspan style=\"color:#75715e\"\u003e# 每个batch的梯度重新累加\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        output \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(data)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        loss \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enll_loss(output, target) \u003cspan style=\"color:#75715e\"\u003e# 这里的nll_loss就是Michael Nielsen在ch3提到的log-likelihood cost function，配合softmax使用，batch的梯度/loss要求均值mean\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        loss\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebackward() \u003cspan style=\"color:#75715e\"\u003e# 求loss对参数的梯度dw\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        optimizer\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estep() \u003cspan style=\"color:#75715e\"\u003e# 梯度下降，w\u0026#39;=w-η*dw\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e batch_idx \u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e args\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elog_interval \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Train Epoch: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e [\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e (\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{:.0f}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%)]\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003eLoss: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{:.6f}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eformat(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                epoch, batch_idx \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e len(data), len(train_loader\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edataset),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#ae81ff\"\u003e100.\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e batch_idx \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e len(train_loader), loss\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitem()))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etest\u003c/span\u003e(args, model, device, test_loader):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eeval()  \u003cspan style=\"color:#75715e\"\u003e# 告诉pytorch，这是预测（评价）阶段\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    test_loss \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    correct \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad(): \u003cspan style=\"color:#75715e\"\u003e# 预测时不需要误差反传，https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e data, target \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e test_loader:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            data, target \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device), target\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            output \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(data)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            test_loss \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e F\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enll_loss(output, target, reduction\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;sum\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitem() \u003cspan style=\"color:#75715e\"\u003e# sum up batch loss，预测时的loss求sum，L54再求均值\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            pred \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e output\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eargmax(dim\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, keepdim\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# get the index of the max log-probability\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            correct \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e pred\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eeq(target\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eview_as(pred))\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esum()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitem()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    test_loss \u003cspan style=\"color:#f92672\"\u003e/=\u003c/span\u003e len(test_loader\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edataset)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003eTest set: Average loss: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{:.4f}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e, Accuracy: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e (\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{:.0f}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%)\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eformat(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        test_loss, correct, len(test_loader\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edataset),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ae81ff\"\u003e100.\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e correct \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e len(test_loader\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edataset)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eplot1digit\u003c/span\u003e(data_loader):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e plt\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    examples \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e enumerate(data_loader)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    batch_idx, (Xs, ys) \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e next(examples) \u003cspan style=\"color:#75715e\"\u003e# 读取到的是一个batch的所有数据\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    X\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eXs[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enumpy()[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#75715e\"\u003e# Xs[0]取出batch中的第一个数据，由tensor转换为numpy，因为pytorch tensor的格式是[channel, height, width]，所以最后[0]取出其第一个通道的[h,w]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    y\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eys[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enumpy() \u003cspan style=\"color:#75715e\"\u003e# y没有通道，就一个标量值\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esavetxt(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../../../fig/\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e.csv\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003ey, X, delimiter\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;,\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e     \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eimshow(X, cmap\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Greys\u0026#39;\u003c/span\u003e) \u003cspan style=\"color:#75715e\"\u003e# or \u0026#39;Greys_r\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esavefig(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../../../fig/\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e.png\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003ey)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Training settings\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e argparse\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eArgumentParser(description\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;PyTorch MNIST Example\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--batch-size\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eint, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e64\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;N\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;input batch size for training (default: 64)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--test-batch-size\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eint, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1000\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;N\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;input batch size for testing (default: 1000)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--epochs\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eint, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;N\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;number of epochs to train (default: 10)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--lr\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003efloat, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.01\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;LR\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;learning rate (default: 0.01)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--momentum\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003efloat, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;M\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;SGD momentum (default: 0.5)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--no-cuda\u0026#39;\u003c/span\u003e, action\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;store_true\u0026#39;\u003c/span\u003e, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;disables CUDA training\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--seed\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eint, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;S\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;random seed (default: 1)\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--log-interval\u0026#39;\u003c/span\u003e, type\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eint, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e, metavar\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;N\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;how many batches to wait before logging training status\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e     \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd_argument(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;--save-model\u0026#39;\u003c/span\u003e, action\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;store_true\u0026#39;\u003c/span\u003e, default\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                        help\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;For Saving the current Model\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    args \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e parser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eparse_args()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    use_cuda \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e args\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_cuda \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecuda\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eis_available()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emanual_seed(args\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eseed)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    device \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edevice(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e use_cuda \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;cpu\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    kwargs \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;num_workers\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;pin_memory\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e} \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e use_cuda \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    train_loader \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eutils\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataLoader(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        datasets\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eMNIST(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../data\u0026#39;\u003c/span\u003e, train\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, download\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                       transform\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003etransforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eCompose([ \u003cspan style=\"color:#75715e\"\u003e# https://discuss.pytorch.org/t/can-some-please-explain-how-the-transforms-work-and-why-normalize-the-data/2461/3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                           transforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eToTensor(), \u003cspan style=\"color:#75715e\"\u003e# 把[0,255]的(H,W,C)的图片转换为[0,1]的(channel,height,width)的图片\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                           transforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eNormalize((\u003cspan style=\"color:#ae81ff\"\u003e0.1307\u003c/span\u003e,), (\u003cspan style=\"color:#ae81ff\"\u003e0.3081\u003c/span\u003e,)) \u003cspan style=\"color:#75715e\"\u003e# 进行z-score标准化，这两个数分别是MNIST的均值和标准差\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                       ])),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        batch_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eargs\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebatch_size, shuffle\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003ekwargs)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    test_loader \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eutils\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataLoader(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        datasets\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eMNIST(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../data\u0026#39;\u003c/span\u003e, train\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e, transform\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003etransforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eCompose([\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                           transforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eToTensor(),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                           transforms\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eNormalize((\u003cspan style=\"color:#ae81ff\"\u003e0.1307\u003c/span\u003e,), (\u003cspan style=\"color:#ae81ff\"\u003e0.3081\u003c/span\u003e,))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                       ])),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        batch_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eargs\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etest_batch_size, shuffle\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003ekwargs)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# plot1digit(train_loader)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Net()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(device)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    optimizer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e optim\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSGD(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eparameters(), lr\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eargs\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elr, momentum\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eargs\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emomentum)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e epoch \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, args\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eepochs \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        train(args, model, device, train_loader, optimizer, epoch)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        test(args, model, device, test_loader)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (args\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esave_model):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esave(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estate_dict(),\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mnist_cnn.pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;__main__\u0026#39;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    main()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e首先是MNIST数据格式的问题，在L108~L120，我们使用Pytorch的DataLoader载入了训练和测试数据，数据格式本质上和本系列博客的\u003ca href=\"https://bitjoy.net/posts/2018-11-25-neutral-networks-and-deep-learning-1-mnist-dataset/\"\u003e第一篇博客\u003c/a\u003e介绍的是一致的，即每张图片都是28*28的灰度图片，因为是灰度图片，所以只有一个通道数，默认格式是(H,W,C)，且值域范围是[0,255]。但上述代码对原始图片进行了两个变换，分别是ToTensor和Normalize。ToTensor将[0,255]的灰度图片(H,W,C)转换为[0,1]的灰度图片(C,H,W)，即Pytorch对2D图片的格式要求都是channel在前。所以经过这一转换，一张图片的shape是(1,28,28)，是一个三维矩阵；如果是彩色图片的话，有R,G,B三个通道，C=3。Normalize对图片数据进行z-score标准化，即减去均值再除以标准差；L112的两个值就是预先计算的MNIST数据集的均值和标准差。这些操作的好处是能让模型更加平稳快速收敛。同第一篇博客一样，我们可以把Pytorch格式的图片打印出来以便直观理解，L61的plot1digit函数就是这个作用。\u003c/p\u003e","title":"Neural Networks and Deep Learning（七）番外篇·Pytorch MNIST教程"},{"content":"简化版 叶文洁向宇宙发射了一个信号 三体人接收到了这个信号 三体人计划逃离水深火热的三体星系，殖民太阳系 地球人在保卫太阳系的末日之战中，被三体舰队团灭，太阳系岌岌可危 罗辑参透了黑暗森林法则，并假借雪地工程实现了对三体星系的威慑，三体撤军，太阳系幸存 罗辑年老体衰，程心接替罗辑成为新的执剑人 三体人预料到程心心慈手软，不敢实施黑暗森林打击 三体人果断进军太阳系，程心果然没有实施黑暗森林打击，地球沦为三体人的殖民地 在太空执行任务的地球飞船发射了三体坐标，三体再次撤军，并随后遭到黑暗森林打击，三体星系灭亡 发射三体坐标也暴露了太阳系坐标，太阳系遭到更高级的打击——降维打击，太阳系被二维化 程心借助光速飞船逃离太阳系来到了云天明送给她的类地行星蓝星上 程心又进入了云天明送给她的平行小宇宙，企图躲避大宇宙的归零大爆炸 太多的小宇宙导致大宇宙质量流失，无法归零 大宇宙向所有宇宙发布回归运动声明，请求小宇宙归还质量 程心最终归还质量，回到了大宇宙 大宇宙坍缩成奇点，完成大爆炸，宇宙开始了新的轮回 扩充版 《三体I·地球往事》 叶文洁经历了父亲在文革中被迫害致死、在大兴安岭被出卖等种种事件之后，对人类的恶彻底绝望了，她决定在红岸基地度过一生。在红岸基地，她意外发现可以利用太阳作为放大器把信号发往其他星球，于是她向宇宙发出了地球的第一个信号，希望外星文明来拯救罪恶的人类。隔壁的三体文明正处于水生火热之中，收到地球信号后，决定冲出三体星球，占领地球。三体人首先派出了两个质子（智子），封锁地球的基础研究，然后派出大型三体舰队进军地球。在地球上，分两个大阵营：一个是以叶文洁为领袖的地球三体组织，可以认为是地球的叛军；另一个是各国组织的政府军，准备消灭地球叛军并对战三体人。地球三体组织又分为三个派系，降临派、拯救派和幸存派。政府军能打败地球叛军并阻止三体人的进攻吗，请看下回分解。\n《三体II·黑暗森林》 地球人为了抵抗三体舰队的入侵，利用三体人思维透明的弱点，选定了四个人开展面壁者计划，其中三人相继失败。200年后，三体星球派来的水滴团灭了地球舰队的舰队方阵，足足有两千多艘几个足球场大的战舰，在一个小时内团灭。侥幸逃离的几艘战舰之间为了维持自身的生存，开始自相残杀，地球文明面临灭顶之灾。罗辑，唯一没有被识破的面壁人，参透了黑暗森林法则：\n宇宙就是一座黑暗森林，每个文明都是带枪的猎人，像幽灵般潜行于林间，轻轻拨开挡路的树枝，竭力不让脚步发出一点儿声音，连呼吸都小心翼翼……他必须小心，因为林中到处都有与他一样潜行的猎人。如果他发现了别的生命，不管是不是猎人，不管是天使还是魔鬼，不管是娇嫩的婴儿还是步履蹒跚的老人，也不管是天仙般的少女还是天神般的男孩，能做的只有一件事：开枪消灭之。在这片森林中，他人就是地狱，就是永恒的威胁，任何暴露自己存在的生命都将很快被消灭。这就是宇宙文明的图景，这就是对费米悖论的解释。”\n罗辑假借雪地工程，制造了一个和三体文明同归于尽的方案，即在太阳周围精心安排一层油膜，使得从宇宙其他文明的视角来看，透过油膜的点点亮光，表示三体星系的坐标。只要这个坐标发射，三体文明就会遭到黑暗森林打击。罗辑通过雪地工程，使地球文明第一次获得了和三体文明谈判的资格，在此之前，地球文明卑微如一只蚂蚁。罗辑成功了，三体文明接受了罗辑的谈判条件，地球文明幸存了下来，并且三体智子解除了对地球的科技封锁。接下来地球文明和三体文明又会发生怎样惊心动魄的故事呢，请听下回分解。\n《三体III·死神永生》 《三体II》之后，罗辑拯救了地球文明，地球和三体处在互相制衡的状态，地球处于威慑纪元。由于罗辑掌握发射三体坐标的开关，决定着两个文明的生死存亡，罗辑被称为执剑人。渐渐的，罗辑老了，需要新人接替罗辑成为执剑人，程心最终竞选成功，成为新的执剑人。在这期间三体文明和地球文明交流密切，关系融洽，似乎一切都那么的平静和美好。\n突然，意想不到的事情发生了，三体派出大批舰队进攻地球，而此时的执剑人程心却没能说服自己发射三体坐标（因为如果发射三体坐标，也会同时暴露地球坐标，导致地球遭受打击，作为圣母心的程心自然是受不了的）。就这样，地球沦陷，三体舰队全面占领地球，把地球人圈养在澳大利亚。\n就在地球文明生死存亡之际，在外太空执行任务的“万有引力”号飞船广播了三体坐标，三体文明自知死路一条，撤离地球，地球再一次得救，处于广播纪元。不久，三体遭受黑暗森林打击，三体文明毁灭。广播三体坐标也暴露了地球的坐标，所以地球人开始探索拯救地球免于黑暗森林打击的方案。\n三体文明虽然被毁灭，但由于文明发达，仍有三体人得以逃往外太空。在三体智子和地球告别之际，智子安排程心和云天明会面，云天明是程心的大学同学，暗恋程心，买下一颗遥远的恒星并送给程心，程心却在不知情的情况下把云天明的大脑发射到三体人手中。云天明被三体人复活，并被安排和程心会面，在和程心会面过程中，云天明给程心讲了三个故事，通过多重隐喻的方式传达了拯救地球的方案。 地球人通过对三个故事的研究，总结出拯救地球的三个方案：\n安全声明，降低太阳系的光速，使太阳系变成一个低光速黑域，地球人把自己锁死在太阳系，永远也无法逃出。通过这种方案，让地外文明觉得太阳系不是威胁，打消进攻的念头。 超光速飞船，制造超光速飞船，飞离被暴露的太阳系，寻找新的家园。 掩体计划，将地球人迁移到类木行星的背阳面，由于类木行星距离太阳较远，当黑暗森林打击到来时，用类木行星作为盾牌，抵挡太阳爆炸发射的冲击波。 经过不断的争论和调整，地球人最终选定掩体计划，因为安全声明方案需要降低光速，难度太大，而超光速飞船即使研制出来，肯定只能让少数人逃生，由此会引发普通阶层的不满，导致地球内乱。于是，地球进入掩体纪元。\n随着掩体计划的实施，地球人陆续搬迁到类木行星背阳面的太空近地轨道居住，地球人又过上了幸福的生活。可好景不长，太阳系的坐标终究是暴露了，被高级得多的歌者文明发现，他们自然知道使用常规的黑暗森林打击无法消灭躲在类木行星后面的地球人，于是他们启用了更高级的武器——降维打击！他们向太阳系发射了一张小纸条，不久这张小纸条扩大成一张二维平面，这张二维平面就像一个超级黑洞一样，把周围的三维物体吸到它的平面上，压扁，变成一张静态的二维图片。就这样，太阳系的行星包括太阳本身不断被吸到这个二维平面，坍缩成一张死去了的二维图片。要想逃躲被二维化的命运，必须以超光速飞离太阳系，但是之前的超光速飞船计划已经被明令禁止了。通常被公开禁止的东西，都有人在私底下偷偷流通，超光速飞船也不例外。程心的公司，因为各种原因，私底下偷偷研制成功了超光速的曲率驱动飞船。于是，程心和她的助理艾AA乘坐超光速飞船逃离了太阳系，来到了云天明送给她的那颗恒星的一个类地行星蓝星上，程心等人进入了银河纪元。\n没想到，蓝星上有人！是之前逃离太阳系的万有引力号上的成员关一帆。在蓝星上，关一帆检测到旁边的行星灰星有飞船迹象，以为是云天明，于是和程心乘坐飞船前往灰星，艾AA就留在了蓝星。在前往灰星的路上，关一帆告诉程心，太阳系向二维平面的跌落会永远进行下去，直到整个宇宙都跌入到二维。实际上，宇宙原本是十维空间，但是由于星际战争，不断有文明使用降维打击，慢慢的，宇宙的维度就被打成了三维，现在又将被打成二维。当宇宙被星际战争打成零维之后，宇宙重启，就像把时针拨过12点一样。比起降维打击，之前人类参透的黑暗森林打击不值一提，在星际战争中，黑暗森林打击就像狙击手之间的阵地战，对于整个战争来说是件小事，而最有威力的武器是利用宇宙规律，比如降低维度用来攻击，降低光速用来防御，真是太可怕了。\n关一帆和程心来到灰星之后，发现了曲率驱动飞船留下的尾迹——死线，这五根死线非常粗非常黑，只有很高级的飞船才能产生如此粗和黑的死线，关一帆猜测是归零者的飞船留下来的，归零者是一群智慧个体，想重启宇宙回到田园时代。这些死线（很粗的圆柱体）是绝对的光速为零的黑域，任何东西只要进去了，就逃不出来，必死无疑。这些死线还有一个特点是如果周围有其他曲率驱动飞船，则产生的死线会和已有的死线发生干扰，使得黑域扩散。\n所以非常不巧的是，归零者来到了灰星，而云天明来到了蓝星，而程心他们却去了灰星。更可怕的是，云天明的曲率驱动飞船产生的尾迹和归零者的死线产生了干扰，导致黑域扩散，关一帆和程心的飞船跌入黑域，光速变慢。在黑域里，电子计算机和量子计算机失效，关一帆启动了神经元计算机，同时，由于氧气不足，他们两进入了冬眠。经过几天的航行，他们的飞船终于回到了蓝星，但因为他们的光速变慢了，所以他们的几天，对于处在蓝星上的艾AA和云天明来说已经是几千万年之后了。关一帆和程心在蓝星上找到了艾AA和云天明留给他们的礼物，一扇门，一扇通往另一个平行小宇宙的门，当然，这个小宇宙也是云天明送给他们的。关一帆和程心来到了这个小宇宙，很巧的是，智子也在这个小宇宙里，作为该小宇宙的管家。智子告诉两位，这个小宇宙是时间之外的宇宙，和之前的宇宙是平行的，能躲过之前大宇宙的坍缩。当大宇宙坍缩到奇点然后大爆炸形成新的大宇宙之后，他们就可以从这个小宇宙回到新的大宇宙，开始新的田园生活了。\n原本以为关一帆和程心会在小宇宙中幸福的生活下去，没想到，他们突然收到了大宇宙的超膜广播，用一百多万种语言写成的广播，广播内容是回归运动声明：\n回归运动声明：我们宇宙的总质量减少至临界值以下，宇宙将由封闭转变为开放，宇宙将在永恒的膨胀中死去，所有的生命和记忆都将死去。请归还你们拿走的质量，只把记忆体送往新宇宙。\n即有太多的文明发现了可以制造小宇宙来躲避大宇宙的坍缩，导致大宇宙的质量减小到临界值而无法完成归零的大爆炸，大宇宙将由封闭转变为开放，在永恒的膨胀中死去。该声明请求所有小宇宙归还他们拿走的质量，以完成大宇宙的归零。\n在经历了几百年的星际战争，在亲眼目睹了太阳系母亲的坍缩和宇宙的黑暗之后，程心和关一帆内心平静，他们决定响应回归运动，将小宇宙的所有质量，包括天、地、太阳、飞船等等一切质量，都拆卸下来归还给了大宇宙。最后，关一帆、程心和智子，手拉手，离开了小宇宙，进入了大宇宙，开始了宇宙新一轮轮回。死神永生！\n读后感：佩服大刘巨大的脑洞！全书看完，完全不觉得是科幻小说，所有物理、生物、计算机的知识，运用得天衣无缝，毫无破绽，觉得这就是地球、太阳系、宇宙的未来。科幻作家首先要是一名合格的作家，本文的文学性毫不弱于其科幻性，我贫乏的语言已经不足以表达这部作品的伟大了。《三体》系列完全可以拍成一部不输于冰与火之歌的史诗巨作！推荐看完全书的同学去B站看文曰小强的速读视频，这个up主也是厉害，如此硬核的小说，用84分钟就讲完了。如果没看过原书就不推荐看了，因为小说本身的信息密度就很高，再经过小强加工压缩到84分钟，信息密度就更高了，很可能会看得一头雾水。总之，膜拜大刘，一举把中国的科幻水平提高到世界水准。\n","permalink":"http://localhost:1313/posts/2019-05-18-introduction-of-the-three-body-problem/","summary":"\u003ch1 id=\"简化版\"\u003e简化版\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e叶文洁向宇宙发射了一个信号\u003c/li\u003e\n\u003cli\u003e三体人接收到了这个信号\u003c/li\u003e\n\u003cli\u003e三体人计划逃离水深火热的三体星系，殖民太阳系\u003c/li\u003e\n\u003cli\u003e地球人在保卫太阳系的末日之战中，被三体舰队团灭，太阳系岌岌可危\u003c/li\u003e\n\u003cli\u003e罗辑参透了黑暗森林法则，并假借雪地工程实现了对三体星系的威慑，三体撤军，太阳系幸存\u003c/li\u003e\n\u003cli\u003e罗辑年老体衰，程心接替罗辑成为新的执剑人\u003c/li\u003e\n\u003cli\u003e三体人预料到程心心慈手软，不敢实施黑暗森林打击\u003c/li\u003e\n\u003cli\u003e三体人果断进军太阳系，程心果然没有实施黑暗森林打击，地球沦为三体人的殖民地\u003c/li\u003e\n\u003cli\u003e在太空执行任务的地球飞船发射了三体坐标，三体再次撤军，并随后遭到黑暗森林打击，三体星系灭亡\u003c/li\u003e\n\u003cli\u003e发射三体坐标也暴露了太阳系坐标，太阳系遭到更高级的打击——降维打击，太阳系被二维化\u003c/li\u003e\n\u003cli\u003e程心借助光速飞船逃离太阳系来到了云天明送给她的类地行星蓝星上\u003c/li\u003e\n\u003cli\u003e程心又进入了云天明送给她的平行小宇宙，企图躲避大宇宙的归零大爆炸\u003c/li\u003e\n\u003cli\u003e太多的小宇宙导致大宇宙质量流失，无法归零\u003c/li\u003e\n\u003cli\u003e大宇宙向所有宇宙发布回归运动声明，请求小宇宙归还质量\u003c/li\u003e\n\u003cli\u003e程心最终归还质量，回到了大宇宙\u003c/li\u003e\n\u003cli\u003e大宇宙坍缩成奇点，完成大爆炸，宇宙开始了新的轮回\u003c/li\u003e\n\u003c/ol\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://images.weserv.nl/?url=https://img1.doubanio.com/view/subject/l/public/s2768378.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://images.weserv.nl/?url=https://img3.doubanio.com/view/subject/l/public/s3078482.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://images.weserv.nl/?url=https://img3.doubanio.com/view/subject/l/public/s26012674.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"扩充版\"\u003e扩充版\u003c/h1\u003e\n\u003ch2 id=\"三体i地球往事\"\u003e《三体I·地球往事》\u003c/h2\u003e\n\u003cp\u003e叶文洁经历了父亲在文革中被迫害致死、在大兴安岭被出卖等种种事件之后，对人类的恶彻底绝望了，她决定在红岸基地度过一生。在红岸基地，她意外发现可以利用太阳作为放大器把信号发往其他星球，于是她向宇宙发出了地球的第一个信号，希望外星文明来拯救罪恶的人类。隔壁的三体文明正处于水生火热之中，收到地球信号后，决定冲出三体星球，占领地球。三体人首先派出了两个质子（智子），封锁地球的基础研究，然后派出大型三体舰队进军地球。在地球上，分两个大阵营：一个是以叶文洁为领袖的地球三体组织，可以认为是地球的叛军；另一个是各国组织的政府军，准备消灭地球叛军并对战三体人。地球三体组织又分为三个派系，降临派、拯救派和幸存派。政府军能打败地球叛军并阻止三体人的进攻吗，请看下回分解。\u003c/p\u003e\n\u003ch2 id=\"三体ii黑暗森林\"\u003e《三体II·黑暗森林》\u003c/h2\u003e\n\u003cp\u003e地球人为了抵抗三体舰队的入侵，利用三体人思维透明的弱点，选定了四个人开展面壁者计划，其中三人相继失败。200年后，三体星球派来的水滴团灭了地球舰队的舰队方阵，足足有两千多艘几个足球场大的战舰，在一个小时内团灭。侥幸逃离的几艘战舰之间为了维持自身的生存，开始自相残杀，地球文明面临灭顶之灾。罗辑，唯一没有被识破的面壁人，参透了黑暗森林法则：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e宇宙就是一座黑暗森林，每个文明都是带枪的猎人，像幽灵般潜行于林间，轻轻拨开挡路的树枝，竭力不让脚步发出一点儿声音，连呼吸都小心翼翼……他必须小心，因为林中到处都有与他一样潜行的猎人。如果他发现了别的生命，不管是不是猎人，不管是天使还是魔鬼，不管是娇嫩的婴儿还是步履蹒跚的老人，也不管是天仙般的少女还是天神般的男孩，能做的只有一件事：开枪消灭之。在这片森林中，他人就是地狱，就是永恒的威胁，任何暴露自己存在的生命都将很快被消灭。这就是宇宙文明的图景，这就是对费米悖论的解释。”\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e罗辑假借雪地工程，制造了一个和三体文明同归于尽的方案，即在太阳周围精心安排一层油膜，使得从宇宙其他文明的视角来看，透过油膜的点点亮光，表示三体星系的坐标。只要这个坐标发射，三体文明就会遭到黑暗森林打击。罗辑通过雪地工程，使地球文明第一次获得了和三体文明谈判的资格，在此之前，地球文明卑微如一只蚂蚁。罗辑成功了，三体文明接受了罗辑的谈判条件，地球文明幸存了下来，并且三体智子解除了对地球的科技封锁。接下来地球文明和三体文明又会发生怎样惊心动魄的故事呢，请听下回分解。\u003c/p\u003e\n\u003ch2 id=\"三体iii死神永生\"\u003e《三体III·死神永生》\u003c/h2\u003e\n\u003cp\u003e《三体II》之后，罗辑拯救了地球文明，地球和三体处在互相制衡的状态，地球处于威慑纪元。由于罗辑掌握发射三体坐标的开关，决定着两个文明的生死存亡，罗辑被称为执剑人。渐渐的，罗辑老了，需要新人接替罗辑成为执剑人，程心最终竞选成功，成为新的执剑人。在这期间三体文明和地球文明交流密切，关系融洽，似乎一切都那么的平静和美好。\u003c/p\u003e\n\u003cp\u003e突然，意想不到的事情发生了，三体派出大批舰队进攻地球，而此时的执剑人程心却没能说服自己发射三体坐标（因为如果发射三体坐标，也会同时暴露地球坐标，导致地球遭受打击，作为圣母心的程心自然是受不了的）。就这样，地球沦陷，三体舰队全面占领地球，把地球人圈养在澳大利亚。\u003c/p\u003e\n\u003cp\u003e就在地球文明生死存亡之际，在外太空执行任务的“万有引力”号飞船广播了三体坐标，三体文明自知死路一条，撤离地球，地球再一次得救，处于广播纪元。不久，三体遭受黑暗森林打击，三体文明毁灭。广播三体坐标也暴露了地球的坐标，所以地球人开始探索拯救地球免于黑暗森林打击的方案。\u003c/p\u003e\n\u003cp\u003e三体文明虽然被毁灭，但由于文明发达，仍有三体人得以逃往外太空。在三体智子和地球告别之际，智子安排程心和云天明会面，云天明是程心的大学同学，暗恋程心，买下一颗遥远的恒星并送给程心，程心却在不知情的情况下把云天明的大脑发射到三体人手中。云天明被三体人复活，并被安排和程心会面，在和程心会面过程中，云天明给程心讲了三个故事，通过多重隐喻的方式传达了拯救地球的方案。 地球人通过对三个故事的研究，总结出拯救地球的三个方案：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e安全声明，降低太阳系的光速，使太阳系变成一个低光速黑域，地球人把自己锁死在太阳系，永远也无法逃出。通过这种方案，让地外文明觉得太阳系不是威胁，打消进攻的念头。\u003c/li\u003e\n\u003cli\u003e超光速飞船，制造超光速飞船，飞离被暴露的太阳系，寻找新的家园。\u003c/li\u003e\n\u003cli\u003e掩体计划，将地球人迁移到类木行星的背阳面，由于类木行星距离太阳较远，当黑暗森林打击到来时，用类木行星作为盾牌，抵挡太阳爆炸发射的冲击波。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e经过不断的争论和调整，地球人最终选定掩体计划，因为安全声明方案需要降低光速，难度太大，而超光速飞船即使研制出来，肯定只能让少数人逃生，由此会引发普通阶层的不满，导致地球内乱。于是，地球进入掩体纪元。\u003c/p\u003e\n\u003cp\u003e随着掩体计划的实施，地球人陆续搬迁到类木行星背阳面的太空近地轨道居住，地球人又过上了幸福的生活。可好景不长，太阳系的坐标终究是暴露了，被高级得多的歌者文明发现，他们自然知道使用常规的黑暗森林打击无法消灭躲在类木行星后面的地球人，于是他们启用了更高级的武器——降维打击！他们向太阳系发射了一张小纸条，不久这张小纸条扩大成一张二维平面，这张二维平面就像一个超级黑洞一样，把周围的三维物体吸到它的平面上，压扁，变成一张静态的二维图片。就这样，太阳系的行星包括太阳本身不断被吸到这个二维平面，坍缩成一张死去了的二维图片。要想逃躲被二维化的命运，必须以超光速飞离太阳系，但是之前的超光速飞船计划已经被明令禁止了。通常被公开禁止的东西，都有人在私底下偷偷流通，超光速飞船也不例外。程心的公司，因为各种原因，私底下偷偷研制成功了超光速的曲率驱动飞船。于是，程心和她的助理艾AA乘坐超光速飞船逃离了太阳系，来到了云天明送给她的那颗恒星的一个类地行星蓝星上，程心等人进入了银河纪元。\u003c/p\u003e\n\u003cp\u003e没想到，蓝星上有人！是之前逃离太阳系的万有引力号上的成员关一帆。在蓝星上，关一帆检测到旁边的行星灰星有飞船迹象，以为是云天明，于是和程心乘坐飞船前往灰星，艾AA就留在了蓝星。在前往灰星的路上，关一帆告诉程心，太阳系向二维平面的跌落会永远进行下去，直到整个宇宙都跌入到二维。实际上，宇宙原本是十维空间，但是由于星际战争，不断有文明使用降维打击，慢慢的，宇宙的维度就被打成了三维，现在又将被打成二维。当宇宙被星际战争打成零维之后，宇宙重启，就像把时针拨过12点一样。比起降维打击，之前人类参透的黑暗森林打击不值一提，在星际战争中，黑暗森林打击就像狙击手之间的阵地战，对于整个战争来说是件小事，而最有威力的武器是利用宇宙规律，比如降低维度用来攻击，降低光速用来防御，真是太可怕了。\u003c/p\u003e\n\u003cp\u003e关一帆和程心来到灰星之后，发现了曲率驱动飞船留下的尾迹——死线，这五根死线非常粗非常黑，只有很高级的飞船才能产生如此粗和黑的死线，关一帆猜测是归零者的飞船留下来的，归零者是一群智慧个体，想重启宇宙回到田园时代。这些死线（很粗的圆柱体）是绝对的光速为零的黑域，任何东西只要进去了，就逃不出来，必死无疑。这些死线还有一个特点是如果周围有其他曲率驱动飞船，则产生的死线会和已有的死线发生干扰，使得黑域扩散。\u003c/p\u003e\n\u003cp\u003e所以非常不巧的是，归零者来到了灰星，而云天明来到了蓝星，而程心他们却去了灰星。更可怕的是，云天明的曲率驱动飞船产生的尾迹和归零者的死线产生了干扰，导致黑域扩散，关一帆和程心的飞船跌入黑域，光速变慢。在黑域里，电子计算机和量子计算机失效，关一帆启动了神经元计算机，同时，由于氧气不足，他们两进入了冬眠。经过几天的航行，他们的飞船终于回到了蓝星，但因为他们的光速变慢了，所以他们的几天，对于处在蓝星上的艾AA和云天明来说已经是几千万年之后了。关一帆和程心在蓝星上找到了艾AA和云天明留给他们的礼物，一扇门，一扇通往另一个平行小宇宙的门，当然，这个小宇宙也是云天明送给他们的。关一帆和程心来到了这个小宇宙，很巧的是，智子也在这个小宇宙里，作为该小宇宙的管家。智子告诉两位，这个小宇宙是时间之外的宇宙，和之前的宇宙是平行的，能躲过之前大宇宙的坍缩。当大宇宙坍缩到奇点然后大爆炸形成新的大宇宙之后，他们就可以从这个小宇宙回到新的大宇宙，开始新的田园生活了。\u003c/p\u003e\n\u003cp\u003e原本以为关一帆和程心会在小宇宙中幸福的生活下去，没想到，他们突然收到了大宇宙的超膜广播，用一百多万种语言写成的广播，广播内容是回归运动声明：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e回归运动声明：我们宇宙的总质量减少至临界值以下，宇宙将由封闭转变为开放，宇宙将在永恒的膨胀中死去，所有的生命和记忆都将死去。请归还你们拿走的质量，只把记忆体送往新宇宙。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e即有太多的文明发现了可以制造小宇宙来躲避大宇宙的坍缩，导致大宇宙的质量减小到临界值而无法完成归零的大爆炸，大宇宙将由封闭转变为开放，在永恒的膨胀中死去。该声明请求所有小宇宙归还他们拿走的质量，以完成大宇宙的归零。\u003c/p\u003e\n\u003cp\u003e在经历了几百年的星际战争，在亲眼目睹了太阳系母亲的坍缩和宇宙的黑暗之后，程心和关一帆内心平静，他们决定响应回归运动，将小宇宙的所有质量，包括天、地、太阳、飞船等等一切质量，都拆卸下来归还给了大宇宙。最后，关一帆、程心和智子，手拉手，离开了小宇宙，进入了大宇宙，开始了宇宙新一轮轮回。死神永生！\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e读后感：佩服大刘巨大的脑洞！全书看完，完全不觉得是科幻小说，所有物理、生物、计算机的知识，运用得天衣无缝，毫无破绽，觉得这就是地球、太阳系、宇宙的未来。科幻作家首先要是一名合格的作家，本文的文学性毫不弱于其科幻性，我贫乏的语言已经不足以表达这部作品的伟大了。《三体》系列完全可以拍成一部不输于冰与火之歌的史诗巨作！推荐看完全书的同学去B站看文曰小强的速读视频，这个up主也是厉害，如此硬核的小说，用84分钟就讲完了。如果没看过原书就不推荐看了，因为小说本身的信息密度就很高，再经过小强加工压缩到84分钟，信息密度就更高了，很可能会看得一头雾水。总之，膜拜大刘，一举把中国的科幻水平提高到世界水准。\u003c/p\u003e","title":"《三体》始末"},{"content":"今天我们终于进入到了本书的重头戏——深度学习。其实，这一章的深度学习主要介绍的是卷积神经网络，即CNN。\n本书之前的章节介绍的都是如下图的全连接网络，虽然全连接网络已经能够在MNIST数据集上取得98%以上的测试准确率，但有两个比较大的缺点：1. 训练参数太多，容易过拟合；2. 难以捕捉图片的局部信息。第一点很好理解，参数一多，网络就难以训练，难以加深。对于第二点，因为全连接的每个神经元都和上一层的所有神经元相连，无论距离远近，也就是说网络不会捕捉图片的局部信息和空间结构信息。\n本章要介绍的卷积神经网络，相对于全连接网络，有如下三个特点：1. 局部感知local receptive fields 2. 权值共享shared weights 3. 池化pooling，下面分别介绍这三部分内容。\n局部感知 对于MNIST的一张28*28灰度图片，全连接网络的输入把图片展开成一个维度为784的向量，这就天然丢失了图片的空间结构信息。而CNN的输入保持了图片28*28的二维空间结构信息，相应的，CNN的中间层也是二维的。这就涉及到输入层的二维图片和隐藏层的二维图片如何对应的问题。\nCNN使用一个被称为“卷积核”的东西，把输入图片转换为隐藏层的特征图（feature map），如下图所示，假设卷积核大小为5*5，则输入图片每5*5的一个小区域被转换为隐藏层的一个神经元（像素），这个小区域就称为局部感受野。\n当卷积核不断的在输入图片中移动时，假设每次移动一格（stride=1），则原来28*28的图片，经过一次卷积后，得到的feature map大小为24*24，相比输入图片小了一圈。\n权值共享 那么，这个卷积操作具体是怎样执行的呢，非常简单。5*5的卷积核本质是一个5*5的矩阵，矩阵中的每个值相当于这个卷积核的参数，或者说权值w。每次卷积时，5*5的矩阵和输入图片中5*5的感受野对应位相乘再相加得到隐藏层的一个值。\n下图是一个缩小版的动图例子，左图的绿色大图相当于输入的5*5图片，移动的黄色小图相当于当前卷积的感受野，大小为3*3。在这个3*3的感受野中，每个单元格居中的数字是输入图片的像素值，右下角的红色小字表示卷积核的权值。每次卷积操作，感受野内的图片像素和卷积核权值相乘再相加，得到右图红色小图中的一个单元格的值，这就完成了一次卷积。当黄色感受野不断在输入图片中移动时，右边的feature map也不断被填充，直到一轮卷积完成。整个过程进行了9次卷积，feature map的大小为3*3=9卷积次数。\nhttps://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59\n这里又涉及到CNN的第二个特点——权值共享。注意到，对于上图的一轮卷积操作，不同感受野内右下角的权值矩阵是一样的，也就是说9次卷积的卷积核权值是一样的。权值共享有两个好处，一是特征位置无关，二是参数量大大下降。\n对于特征位置无关 。这个3*3的卷积核相当于一个特征提取器或者说滤波器，比如这个特征提取器能够提取“猫”这个特征，则无论猫在输入图片的左上角还是右下角，“猫”这个特征都能被提取出来，因为卷积核在小范围移动，无论“猫”位于图片的哪个区域，当卷积核移动到这个区域时，卷积得到的输出比较大，被激活，得到“猫”这个特征。所以CNN对位置不敏感，这对图像处理尤其有利。正因为这个特点，经过卷积核卷积操作之后的小图片（上图右边的红色图片）被称为特征图（feature map），因为它就是用卷积核提取出来的符合这个卷积核描述的一个特征。\n对于参数量大大下降。事实上，一次卷积操作除了上面动图显示的卷积核与感受野内的图片相乘再相加之外，还会对加和之后的值做一个激活输出。回到我们的MNIST例子，一次卷积操作用公式来表示就是：\n$$\\begin{eqnarray}\\sigma\\left(b + \\sum_{l=0}^4 \\sum_{m=0}^4 w_{l,m} a_{j+l, k+m} \\right).\\tag{1}\\end{eqnarray}$$\\(w\\)表示卷积核权值矩阵，\\(a\\)表示感受野内的输入图片，两个累加\\(\\sum\\)就是上面动图显示的相乘相加过程，得到和之后，还会加上一个偏移量\\(b\\)，最后进行激活输出\\(\\sigma\\)。所以一个5*5的卷积核，参数量为5*5+1=26。如果有20个卷积核，参数总量为20*26=520。但如果是全连接网络，假设隐藏层有30个，则参数量为784*30+30=23550。所以仅考虑隐藏层的参数量，CNN就比全连接网络少了45倍的参数，参数量少了，就能加快训练，网络也有可能加深。\n池化 池化就很好理解了，对于卷积得到的feature map，再画一个框（类似于卷积层的感受野），把框内的最大值取出来作为池化之后的值，这就是max-pooling。池化的目的是用来简化信息的，相当于降维。池化的框也可以称为核kernel，如果kernel的大小是2*2的，则一个24*24的feature map，经过max-pooling之后就变成了12*12了，维度瞬间降了一半， 把原来的feature map变成了一个紧凑的feature map。\n池化层往往跟在卷积层的后面，下图表示一张28*28的图片，使用3个5*5的卷积核之后，得到了3个24*24的feature map，再经过2*2的max-pooling，得到3个12*12的feature map。\n到这里，CNN的三大特点就介绍完毕了。对于上图，三个卷积核相当于提取了三种特征，我们还需要完成最终的分类任务，这时候还得把全连接网络请过来。经过max-pooling之后，我们再接一个包含10个神经元的全连接层，作为输出层，完整的网络结果如下：\n最后的全连接层和我们前面介绍的全连接网络是完全一样的，只不过全连接的输入是3个经过max-pooling之后的feature map，再和输出层相连时，可以想象成先把3个12*12的feature map展开并首尾相连，得到一个3*12*12=432的向量，再和输出层的10个神经元进行全连接。这就是一个非常简单的CNN网络，包含一个输入层、一个卷积层、一个池化层和一个输出层。\n本文的代码示例network3.py中，构建了一个和上图类似的简单的CNN网络，如下图所示，使用了20个卷积核，相当于提取了20种特征；max-pooling之后使用了两个全连接层，前一层包含100个隐藏神经元，使用sigmoid激活；后一层包含10个神经元，使用softmax激活，作为输出层。就是这么一个简单的CNN网络，其在测试集上的准确率达到了98.78%，超过了本文之前构建的所有的全连接网络。\n由于原文使用的是已经不再维护的Theano，本博客不打算详细介绍其代码实现，我将在稍后的博文中分享Pytorch的CNN代码。不过我还是把原文对CNN的优化过程总结如下，用测试集的准确率作为性能指标：\n上图简单的CNN网络，98.78% 增加一个卷积层，且把激活函数换成ReLU，99.23% 数据增强，把原有的5000张图片，上下左右各平移一个像素，增加了4倍数据，99.37% 增加一个全连接层，且全连接层神经元增加为1000个，使用dropout=0.5，epoch相应减少到40个，99.6%。因为卷积层有权值共享，天然参数少防止过拟合，所以dropout一般只用于全连接层 模型融合ensemble，5个上述模型，采用majority vote，99.67%，已接近人类水平 虽然经过上述5步，准确率没有达到100%，但那些分类错误的图片，真的很难说分错了，因为图片看起来就不是它标注的结果（右上角），就应该是分错的结果（右下角）。总的来说，我觉得已经非常不错了。\n稍微解释两个问题。\n一、为什么ReLU比Sigmoid性能好，目前还没有一个令人信服的答案，只是很多人在很多标准数据集上测试，发现ReLU性能比Sigmoid好，这个消息传开之后，大家都开始用ReLU了。通常的解释是ReLU的max(0,z)在z很大时，梯度依然为1，不存在梯度消失的问题；而Sigmoid在z很大时，\\(\\sigma'(z)\\)梯度几乎为0，存在梯度消失的问题。\n二、为什么CNN可以做深？怎样解决上一章提到的梯度消失或者梯度爆炸问？事实上，CNN并没有解决上一章的问题，而是绕过了这个问题：1) 卷积层大大减少了模型参数，使得网络更加简单，可以训练得更快；2）强大的正则减少过拟合：dropout和卷积层；3）使用ReLU而不是sigmoid，可以加速训练过程3~5倍；4）使用GPU，并且愿意训练更长的时间，因为sigmoid的梯度消失只不过是梯度很小更新很慢，如果训练时间足够长的话，应该也能收敛。而如果用GPU的话，训练速度加快了，相同时间就可以训练更多的epoch。组合3和4，相当于比之前的网络训练时长增加了30倍，收敛效果自然会更好。\n除了CNN，文章最后还简单提到了图像识别的进展、深度学习的其他应用以及神经网络的未来等内容，本博客就不详细介绍了，有兴趣的可以去看原文。\n附录：LeNet-5和多通道卷积 这部分内容是我自己加的，书中提到network3.py是模仿著名的LeNet-5，我于是就查了查LeNet-5的文章和相关介绍。\nhttps://blog.csdn.net/saw009/article/details/80590245\n上图就是非常经典的LeNet-5网络结构图，包含2个卷积层、2个池化层、2个全连接层和1个Gaussian connections。目前比较流行的做法中已经把最后一个Gaussian connections换成了全连接层。本博客前面介绍的卷积层输入只有一张图片，比较好理解，但是LeNet-5的C3即第二个卷积层，面对的是S2得到的6张feature map，相当于有6张输入图片，如何做卷积。\n这就涉及到“多通道”卷积的概念，可以把多通道理解为S2的6个feature map之于C3，一个feature map理解为一个通道。事实上，如果输入图片是彩色的，则输入图片就包含R、G、B三个通道，相当于输入了3张feature map，此时在C1中就会遇到“多通道”卷积的问题。\n假设输入有R、G、B三个通道，则每个通道都需要有一个不同的卷积核进行卷积，最后把三个通道的卷积结果加起来，进行激活，如下面动图所示。\nhttps://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\nhttps://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n回到LeNet-5，S2有6个通道，C3得到了16个feature map，说明C3有16个卷积核。但是，C3每个卷积核并不只包含一个卷积矩阵，而是包含三个卷积矩阵，即每个卷积核也是三通道的，而且这三个通道的卷积矩阵参数是不同的，即不共享，它们只共享一个偏移量\\(b\\)，如下图所示。\nhttps://blog.csdn.net/saw009/article/details/80590245\n但是S2有6个feature map，三个通道不够用，所以LeNet-5的论文中还规定了每个卷积核的三通道卷积S2中的哪3个feature map，如下图所示。\nhttps://blog.csdn.net/saw009/article/details/80590245\n如果还不理解的话，CS231N还有一个更直观的例子： https://cs231n.github.io/assets/conv-demo/index.html 可以点击按钮暂停，然后手动计算看看是否和动图的结果一致。\nhttps://cs231n.github.io/assets/conv-demo/index.html\n有关CNN和LeNet-5的参考资料：\nCNN直观解释：https://www.zhihu.com/question/39022858/answer/224446917 可视化卷积：https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59 可视化“多通道”卷积：https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1 动图展示“多通道”卷积：https://cs231n.github.io/convolutional-networks/ LeNet-5原文：http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf LeNet-5网络解读：https://blog.csdn.net/saw009/article/details/80590245 ","permalink":"http://localhost:1313/posts/2019-05-04-neural-networks-and-deep-learning-6-dl/","summary":"\u003cp\u003e今天我们终于进入到了本书的重头戏——深度学习。其实，这一章的深度学习主要介绍的是卷积神经网络，即CNN。\u003c/p\u003e\n\u003cp\u003e本书之前的章节介绍的都是如下图的全连接网络，虽然全连接网络已经能够在MNIST数据集上取得98%以上的测试准确率，但有两个比较大的缺点：1. 训练参数太多，容易过拟合；2. 难以捕捉图片的局部信息。第一点很好理解，参数一多，网络就难以训练，难以加深。对于第二点，因为全连接的每个神经元都和上一层的所有神经元相连，无论距离远近，也就是说网络不会捕捉图片的局部信息和空间结构信息。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz41.png\"\u003e\u003c/p\u003e\n\u003cp\u003e本章要介绍的卷积神经网络，相对于全连接网络，有如下三个特点：1. 局部感知local receptive fields 2. 权值共享shared weights 3. 池化pooling，下面分别介绍这三部分内容。\u003c/p\u003e\n\u003ch1 id=\"局部感知\"\u003e局部感知\u003c/h1\u003e\n\u003cp\u003e对于MNIST的一张28*28灰度图片，全连接网络的输入把图片展开成一个维度为784的向量，这就天然丢失了图片的空间结构信息。而CNN的输入保持了图片28*28的二维空间结构信息，相应的，CNN的中间层也是二维的。这就涉及到输入层的二维图片和隐藏层的二维图片如何对应的问题。\u003c/p\u003e\n\u003cp\u003eCNN使用一个被称为“卷积核”的东西，把输入图片转换为隐藏层的特征图（feature map），如下图所示，假设卷积核大小为5*5，则输入图片每5*5的一个小区域被转换为隐藏层的一个神经元（像素），这个小区域就称为局部感受野。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz44.png\"\u003e\u003c/p\u003e\n\u003cp\u003e当卷积核不断的在输入图片中移动时，假设每次移动一格（stride=1），则原来28*28的图片，经过一次卷积后，得到的feature map大小为24*24，相比输入图片小了一圈。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz45.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"权值共享\"\u003e权值共享\u003c/h1\u003e\n\u003cp\u003e那么，这个卷积操作具体是怎样执行的呢，非常简单。5*5的卷积核本质是一个5*5的矩阵，矩阵中的每个值相当于这个卷积核的参数，或者说权值w。每次卷积时，5*5的矩阵和输入图片中5*5的感受野对应位相乘再相加得到隐藏层的一个值。\u003c/p\u003e\n\u003cp\u003e下图是一个缩小版的动图例子，左图的绿色大图相当于输入的5*5图片，移动的黄色小图相当于当前卷积的感受野，大小为3*3。在这个3*3的感受野中，每个单元格居中的数字是输入图片的像素值，右下角的红色小字表示卷积核的权值。每次卷积操作，感受野内的图片像素和卷积核权值相乘再相加，得到右图红色小图中的一个单元格的值，这就完成了一次卷积。当黄色感受野不断在输入图片中移动时，右边的feature map也不断被填充，直到一轮卷积完成。整个过程进行了9次卷积，feature map的大小为3*3=9卷积次数。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-05-04-neural-networks-and-deep-learning-6-dl/e7ff1-1zcjpufrb6ehpri4eyp6aaa.gif\"\u003e\n\u003ca href=\"https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59\"\u003ehttps://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e这里又涉及到CNN的第二个特点——权值共享。注意到，对于上图的一轮卷积操作，不同感受野内右下角的权值矩阵是一样的，也就是说9次卷积的卷积核权值是一样的。权值共享有两个好处，一是特征位置无关，二是参数量大大下降。\u003c/p\u003e\n\u003cp\u003e对于特征位置无关 。这个3*3的卷积核相当于一个特征提取器或者说\u003ca href=\"https://www.zhihu.com/question/39022858/answer/224446917\"\u003e滤波器\u003c/a\u003e，比如这个特征提取器能够提取“猫”这个特征，则无论猫在输入图片的左上角还是右下角，“猫”这个特征都能被提取出来，因为卷积核在小范围移动，无论“猫”位于图片的哪个区域，当卷积核移动到这个区域时，卷积得到的输出比较大，被激活，得到“猫”这个特征。所以CNN对位置不敏感，这对图像处理尤其有利。正因为这个特点，经过卷积核卷积操作之后的小图片（上图右边的红色图片）被称为特征图（feature map），因为它就是用卷积核提取出来的符合这个卷积核描述的一个特征。\u003c/p\u003e\n\u003cp\u003e对于参数量大大下降。事实上，一次卷积操作除了上面动图显示的卷积核与感受野内的图片相乘再相加之外，还会对加和之后的值做一个激活输出。回到我们的MNIST例子，一次卷积操作用公式来表示就是：\u003c/p\u003e\n$$\\begin{eqnarray}\\sigma\\left(b + \\sum_{l=0}^4 \\sum_{m=0}^4 w_{l,m} a_{j+l, k+m} \\right).\\tag{1}\\end{eqnarray}$$\u003cp\u003e\\(w\\)表示卷积核权值矩阵，\\(a\\)表示感受野内的输入图片，两个累加\\(\\sum\\)就是上面动图显示的相乘相加过程，得到和之后，还会加上一个偏移量\\(b\\)，最后进行激活输出\\(\\sigma\\)。所以一个5*5的卷积核，参数量为5*5+1=26。如果有20个卷积核，参数总量为20*26=520。但如果是全连接网络，假设隐藏层有30个，则参数量为784*30+30=23550。所以仅考虑隐藏层的参数量，CNN就比全连接网络少了45倍的参数，参数量少了，就能加快训练，网络也有可能加深。\u003c/p\u003e\n\u003ch1 id=\"池化\"\u003e池化\u003c/h1\u003e\n\u003cp\u003e池化就很好理解了，对于卷积得到的feature map，再画一个框（类似于卷积层的感受野），把框内的最大值取出来作为池化之后的值，这就是max-pooling。池化的目的是用来简化信息的，相当于降维。池化的框也可以称为核kernel，如果kernel的大小是2*2的，则一个24*24的feature map，经过max-pooling之后就变成了12*12了，维度瞬间降了一半， 把原来的feature map变成了一个紧凑的feature map。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz47.png\"\u003e\u003c/p\u003e\n\u003cp\u003e池化层往往跟在卷积层的后面，下图表示一张28*28的图片，使用3个5*5的卷积核之后，得到了3个24*24的feature map，再经过2*2的max-pooling，得到3个12*12的feature map。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz48.png\"\u003e\u003c/p\u003e\n\u003cp\u003e到这里，CNN的三大特点就介绍完毕了。对于上图，三个卷积核相当于提取了三种特征，我们还需要完成最终的分类任务，这时候还得把全连接网络请过来。经过max-pooling之后，我们再接一个包含10个神经元的全连接层，作为输出层，完整的网络结果如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz49.png\"\u003e\u003c/p\u003e\n\u003cp\u003e最后的全连接层和我们前面介绍的全连接网络是完全一样的，只不过全连接的输入是3个经过max-pooling之后的feature map，再和输出层相连时，可以想象成先把3个12*12的feature map展开并首尾相连，得到一个3*12*12=432的向量，再和输出层的10个神经元进行全连接。这就是一个非常简单的CNN网络，包含一个输入层、一个卷积层、一个池化层和一个输出层。\u003c/p\u003e\n\u003cp\u003e本文的代码示例network3.py中，构建了一个和上图类似的简单的CNN网络，如下图所示，使用了20个卷积核，相当于提取了20种特征；max-pooling之后使用了两个全连接层，前一层包含100个隐藏神经元，使用sigmoid激活；后一层包含10个神经元，使用softmax激活，作为输出层。就是这么一个简单的CNN网络，其在测试集上的准确率达到了98.78%，超过了本文之前构建的所有的全连接网络。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/simple_conv.png\"\u003e\u003c/p\u003e\n\u003cp\u003e由于原文使用的是已经不再维护的Theano，本博客不打算详细介绍其代码实现，我将在稍后的博文中分享Pytorch的CNN代码。不过我还是把原文对CNN的优化过程总结如下，用测试集的准确率作为性能指标：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e上图简单的CNN网络，98.78%\u003c/li\u003e\n\u003cli\u003e增加一个卷积层，且把激活函数换成ReLU，99.23%\u003c/li\u003e\n\u003cli\u003e数据增强，把原有的5000张图片，上下左右各平移一个像素，增加了4倍数据，99.37%\u003c/li\u003e\n\u003cli\u003e增加一个全连接层，且全连接层神经元增加为1000个，使用dropout=0.5，epoch相应减少到40个，99.6%。因为卷积层有权值共享，天然参数少防止过拟合，所以dropout一般只用于全连接层\u003c/li\u003e\n\u003cli\u003e模型融合ensemble，5个上述模型，采用majority vote，99.67%，已接近人类水平\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e虽然经过上述5步，准确率没有达到100%，但那些分类错误的图片，真的很难说分错了，因为图片看起来就不是它标注的结果（右上角），就应该是分错的结果（右下角）。总的来说，我觉得已经非常不错了。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/ensemble_errors.png\"\u003e\u003c/p\u003e\n\u003cp\u003e稍微解释两个问题。\u003c/p\u003e","title":"Neural Networks and Deep Learning（六）深度学习"},{"content":"本章我们将分析一下为什么深度神经网络难以训练的问题。\n首先来看问题：如果神经网络的层次不断加深，则在BP误差反向传播的过程中，网络前几层的梯度更新会非常慢，导致前几层的权重无法学习到比较好的值，这就是梯度消失问题（The vanishing gradient problem）。\n以我们在第三章学习的network2.py为例（交叉熵损失函数+Sigmoid激活函数），我们可以计算每个神经元中误差对偏移量\\(b\\)的偏导\\(\\partial C/ \\partial b\\)，根据第二章BP网络的知识，\\(\\partial C/ \\partial b\\)也是\\(\\partial C/ \\partial w\\)的一部分（BP3和BP4的关系），所以如果\\(\\partial C/ \\partial b\\)的绝对值大，则说明梯度大，在误差反向传播的时候，\\(b\\)和\\(w\\)更新就快。\n假设network2的网络结构是[784,30,30,10]，即有两个隐藏层，则我们可以画出在误差反向传播过程中，隐藏层每个神经元的\\(\\partial C/ \\partial b\\)的大小，用柱子长度表示。由下图可知，我们发现第二个隐藏层的梯度普遍大于第一个隐藏层的梯度，这会是一般现象吗，还是偶然现象？\n既然梯度出现了层与层的差异，则可以定义第\\(l\\)层的梯度（如不加说明，则默认是误差\\(C\\)对偏移量\\(b\\)的梯度）向量的长度为\\(\\| \\delta^l \\|\\)，比如\\(\\| \\delta^1 \\|\\)表示第一个隐藏层中每个神经元的\\(\\partial C/ \\partial b\\)的绝对值之和，就是一范数，如果\\(\\| \\delta^l \\|\\)越大，则说明这一层权重的更新越快。\n由此，我们可以画出当有两个隐藏层时，\\(\\| \\delta^l \\|\\)随epoch的变化情况：\n当有三个隐藏层时：\n当有四个隐藏层时：\n我们发现，规律是惊人的一致，即越靠近输出层的隐藏层，\\(\\| \\delta^l \\|\\)越大，即梯度更新越快；越靠近输入层的隐藏层，\\(\\| \\delta^l \\|\\)越小，即梯度更新越慢。\n这就会导致梯度消失的问题（The vanishing gradient problem）：即在误差反向传播过程中，刚开始权重更新比较快，越到后面（越靠近输入层），则权重更新变得很慢，无法搜索到比较优的值。\n所以，对于同样的network2，其他参数都不变，只是单纯增加网络层数，验证集上的准确率反而会下降！按理说网络层数增加，验证集上的准确率会上升，或者不变，至少不应该下降啊，因为最不济增加的网络层什么都不做，准确率应该一样才对，为什么反而下降了呢。虽然层数增加了，但因为上述梯度消失问题，靠近输入层的权重反而没学好，因为权重是随机初始化的，所以验证集上的准确率反而下降了。\n那么，为什么层数增加会导致梯度消失问题呢，我们可以从BP的更新公式中一探究竟。\n为了简化问题，假设我们的网络每一层只有一个神经元：\n则根据BP的更新公式，可以计算得到\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b_1} = \\sigma'(z_1) \\, w_2 \\sigma'(z_2) \\,w_3 \\sigma'(z_3) \\, w_4 \\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}.\\tag{1}\\end{eqnarray}$$计算过程其实很简单，对照本博客开头的那张图，\\(\\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}\\)就是(BP1)，把(BP1)带入(BP2)，就是不断乘以\\(w^{l+1} \\sigma'(z^l)\\)，然后就能得到下图的公式。\n我们在第三章时也曾介绍到梯度消失问题，当时提出的应对策略是采用交叉熵损失函数+Sigmoid，在求解梯度时可以把\\(\\sigma’\\)抵消掉，以此来解决梯度消失的问题。但是请注意，当时求解的仅仅是误差对输出层的梯度，可以通过交叉熵损失函数+Sigmoid抵消\\(\\sigma’\\)，对应到上图就是仅仅最后两项的\\(\\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}\\)可以抵消\\(\\sigma’\\)，即只有BP公式中的(BP1)可以抵消\\(\\sigma’\\)。而如果误差继续反向传播，则其他层的梯度依然包含\\(\\sigma’\\)项，由上图就可以看到，\\(\\frac{\\partial C}{\\partial b_1}\\)除了最后的\\(\\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}\\)抵消了一个\\(\\sigma’\\)，前面还有3个\\(\\sigma’\\)连乘。如果\\(\\sigma\\)是Sigmoid激活函数的话，很容易就导致在Sigmoid两端，梯度更新缓慢的问题，然后又通过乘法放大了梯度消失的问题。\n观察公式(1)，我们发现梯度中包含很多\\(w_j \\sigma'(z_j)\\)项相乘，如果\\(\\sigma\\)是Sigmoid，则\\(\\sigma'(z_j) \\leq 1/4\\)，等号在\\(z_j=0\\)时取得；又因为权重随机初始化自\\(w_j\\sim N(0,1)\\)，所以很容易有\\(|w_j| \u003c 1\\)，这就导致\\(|w_j \\sigma'(z_j)| \u003c 1/4\\)，多个\\(w_j \\sigma'(z_j)\\)项相乘，越乘越小，导致梯度消失。\n由下图可知，不同层的梯度，有部分是相同的，区别就在于\\(w_j \\sigma'(z_j)\\)项乘的多少，所以这就能说明为什么反向传播得越多（越靠近输入层），梯度更新越慢。\n上述\\(w_j \\sigma'(z_j)\\)项连乘的问题不但会导致梯度消失问题，有时候还可能导致梯度爆炸（The exploding gradient problem）。比如假设令\\(w_1 = w_2 = w_3 = w_4 = 100\\)，令\\(b_i = -100 * a_{i-1}\\)，使得\\(z_i = w_i a_{i-1} + b_i = 0\\)，这就有\\(\\sigma'(z_j) = 1/4\\)，进而有\\(w_j\\sigma'(z_j)=100 * \\frac{1}{4} = 25\u003e1\\)。多个\\(w_j \\sigma'(z_j)\\)项相乘就会越乘越大，导致梯度爆炸。\n所以根源还是出现在多个\\(w_j \\sigma'(z_j)\\)项相乘的问题上，导致BP梯度更新不稳定，有时候可能梯度消失，有时候可能梯度爆炸，这就导致深度神经网络训练起来有难度。也许把Sigmoid激活函数换成ReLU可以解决这个问题？\n","permalink":"http://localhost:1313/posts/2019-04-14-neural-networks-and-deep-learning-5-why-are-nn-hard-to-train/","summary":"\u003cp\u003e本章我们将分析一下为什么深度神经网络难以训练的问题。\u003c/p\u003e\n\u003cp\u003e首先来看问题：如果神经网络的层次不断加深，则在BP误差反向传播的过程中，网络前几层的梯度更新会非常慢，导致前几层的权重无法学习到比较好的值，这就是梯度消失问题（The vanishing gradient problem）。\u003c/p\u003e\n\u003cp\u003e以我们在\u003ca href=\"https://bitjoy.net/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/\"\u003e第三章学习的network2.py\u003c/a\u003e为例（交叉熵损失函数+Sigmoid激活函数），我们可以计算每个神经元中误差对偏移量\\(b\\)的偏导\\(\\partial C/ \\partial b\\)，根据\u003ca href=\"https://bitjoy.net/posts/2018-12-14-neutral-networks-and-deep-learning-2-bp/\"\u003e第二章BP网络\u003c/a\u003e的知识，\\(\\partial C/ \\partial b\\)也是\\(\\partial C/ \\partial w\\)的一部分（BP3和BP4的关系），所以如果\\(\\partial C/ \\partial b\\)的绝对值大，则说明梯度大，在误差反向传播的时候，\\(b\\)和\\(w\\)更新就快。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz21.png\"\u003e\u003c/p\u003e\n\u003cp\u003e假设network2的网络结构是[784,30,30,10]，即有两个隐藏层，则我们可以画出在误差反向传播过程中，隐藏层每个神经元的\\(\\partial C/ \\partial b\\)的大小，用柱子长度表示。由下图可知，我们发现第二个隐藏层的梯度普遍大于第一个隐藏层的梯度，这会是一般现象吗，还是偶然现象？\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-14-neural-networks-and-deep-learning-5-why-are-nn-hard-to-train/ch5.1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e既然梯度出现了层与层的差异，则可以定义第\\(l\\)层的梯度（如不加说明，则默认是误差\\(C\\)对偏移量\\(b\\)的梯度）向量的长度为\\(\\| \\delta^l \\|\\)，比如\\(\\| \\delta^1 \\|\\)表示第一个隐藏层中每个神经元的\\(\\partial C/ \\partial b\\)的绝对值之和，就是一范数，如果\\(\\| \\delta^l \\|\\)越大，则说明这一层权重的更新越快。\u003c/p\u003e\n\u003cp\u003e由此，我们可以画出当有两个隐藏层时，\\(\\| \\delta^l \\|\\)随epoch的变化情况：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/training_speed_2_layers.png\"\u003e\u003c/p\u003e\n\u003cp\u003e当有三个隐藏层时：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/training_speed_3_layers.png\"\u003e\u003c/p\u003e\n\u003cp\u003e当有四个隐藏层时：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/training_speed_4_layers.png\"\u003e\u003c/p\u003e\n\u003cp\u003e我们发现，规律是惊人的一致，即越靠近输出层的隐藏层，\\(\\| \\delta^l \\|\\)越大，即梯度更新越快；越靠近输入层的隐藏层，\\(\\| \\delta^l \\|\\)越小，即梯度更新越慢。\u003c/p\u003e\n\u003cp\u003e这就会导致梯度消失的问题（The vanishing gradient problem）：即在误差\u003cstrong\u003e反向\u003c/strong\u003e传播过程中，刚开始权重更新比较快，越到后面（越靠近输入层），则权重更新变得很慢，无法搜索到比较优的值。\u003c/p\u003e\n\u003cp\u003e所以，对于同样的network2，其他参数都不变，只是单纯增加网络层数，验证集上的准确率反而会下降！按理说网络层数增加，验证集上的准确率会上升，或者不变，至少不应该下降啊，因为最不济增加的网络层什么都不做，准确率应该一样才对，为什么反而下降了呢。虽然层数增加了，但因为上述梯度消失问题，靠近输入层的权重反而没学好，因为权重是随机初始化的，所以验证集上的准确率反而下降了。\u003c/p\u003e\n\u003cp\u003e那么，为什么层数增加会导致梯度消失问题呢，我们可以从BP的更新公式中一探究竟。\u003c/p\u003e\n\u003cp\u003e为了简化问题，假设我们的网络每一层只有一个神经元：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz37.png\"\u003e\u003c/p\u003e\n\u003cp\u003e则根据BP的更新公式，可以计算得到\u003c/p\u003e\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b_1} = \\sigma'(z_1) \\, w_2 \\sigma'(z_2) \\,w_3 \\sigma'(z_3) \\, w_4 \\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}.\\tag{1}\\end{eqnarray}$$\u003cp\u003e计算过程其实很简单，对照本博客开头的那张图，\\(\\sigma'(z_4) \\, \\frac{\\partial C}{\\partial a_4}\\)就是(BP1)，把(BP1)带入(BP2)，就是不断乘以\\(w^{l+1} \\sigma'(z^l)\\)，然后就能得到下图的公式。\u003c/p\u003e","title":"Neural Networks and Deep Learning（五）为什么深度神经网络难以训练"},{"content":"我们应该都听说过神经网络强大到能拟合任意一个函数，但细究起来很少有人能论证这个观点，这一章就用通俗易懂的图解方式来证明神经网络为什么能拟合任意一个函数。\n开始介绍之前，有两点需要注意：\n并不是说神经网络可以精确计算任意一个函数\\(f(x)\\)，而是说当隐藏层神经元增加时，可以无限逼近\\(f(x)\\)，比如对于任何一个输入\\(x\\)，网络的输出\\(g(x)\\)和正确值\\(f(x)\\)的差小于某个阈值，\\(|g(x) – f(x)| \u003c \\epsilon\\)； 神经网络拟合的是连续函数，而不是那种不连续、离散、急剧变化的函数。 假设给定一个下图的连续函数，函数形式未知，本章将用图解的方式来证明，一个单隐层的神经网络就可以很好的拟合这个未知函数。\n首先，假设我们的隐藏层只有两个神经元，激活函数使用Sigmoid，并且我们暂时只关注上面那个神经元的参数和输出。则通过调整该神经元的\\(w\\)和\\(b\\)，可以得到不同形状的Sigmoid函数形式。\n极端情况下，如果\\(w\\)很大而\\(b\\)很小，则可以用Sigmoid函数模拟阶梯函数：\n如果令\\(s = -b/w\\)，则只用一个\\(s\\)就可以确定Sigmoid的函数图像：\n如果把隐藏层下面那个神经元也考虑进来，并且令隐藏层的两个神经元和输出层的神经元的连接权重互为相反数，则输出层未激活值\\(z=w_1 a_1 + w_2 a_2\\)的函数图像变成了一个神奇的鼓包，这个鼓包就是我们后续拟合任意函数的基本单元。根据严格的函数形式，还可以知道\\(w_1\\)和\\(w_2\\)的绝对值控制着鼓包的高度，\\(s_1\\)和\\(s_2\\)的值控制着鼓包的位置和宽度。大家可以去原始网页上体验一下作者给出的可交互版本，很有意思。\n有了这个基本单元之后，我们可以通过增加隐藏层神经元的个数来增加鼓包的个数，比如再增加一对隐层神经元，可增加一个鼓包。虽然下图的例子中两个鼓包相互独立，但通过调整4个\\(s\\)，可以让两个鼓包相连甚至交错，大家可以去原网页试一试。\n继续增加隐层神经元个数，则可以继续增加鼓包的数量，如下图所示。\n到这里想必大家马上知道了为什么神经网络能拟合任何一个函数了，如果隐层神经元足够多，则右图的小鼓包可以足够密，通过调整每个鼓包的高度，则无穷多个鼓包的顶点连线可以拟合任意一个函数。这和我们求函数积分（函数下方面积）时使用多个小矩形近似是一个道理！\n所以对于本章开头的未知函数，我们通过调整不同鼓包的高度，可以使得小矩形面积之和与真实积分的差在\\( \\epsilon=0.4\\)以内。如果无限增加隐层神经元个数，则可以无限逼近真实值。这就说明神经网络确实可以拟合任意一个函数。\n上述推导稍微需要注意的一点是，右图的输出是未激活函数值\\(\\sum_j w_j a_j\\)，而网络真正的输出是激活值\\(\\sigma(\\sum_j w_j a_j + b)\\)。这没有太大的关系，因为上面已经说明未激活输出能拟合任意函数，激活函数也是一个函数。增加激活函数就要求右图需要拟合激活函数和真实函数的嵌套函数。既然未激活输出能拟合任意函数，肯定能拟合这个嵌套函数\\(\\sigma^{-1} \\circ f(x)\\)，再用激活函数作用一下\\(\\sigma\\circ\\sigma^{-1} \\circ f(x)\\)，激活函数抵消了，正好得到\\(f(x)\\)。\n如果输入是多维，或者输出是多维，都是类似的道理。这就说明神经网络确实可以拟合任意函数，真的很强大哦。\n","permalink":"http://localhost:1313/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/","summary":"\u003cp\u003e我们应该都听说过神经网络强大到能拟合任意一个函数，但细究起来很少有人能论证这个观点，这一章就用通俗易懂的图解方式来证明神经网络为什么能拟合任意一个函数。\u003c/p\u003e\n\u003cp\u003e开始介绍之前，有两点需要注意：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e并不是说神经网络可以精确计算任意一个函数\\(f(x)\\)，而是说当隐藏层神经元增加时，可以无限逼近\\(f(x)\\)，比如对于任何一个输入\\(x\\)，网络的输出\\(g(x)\\)和正确值\\(f(x)\\)的差小于某个阈值，\\(|g(x) – f(x)| \u003c \\epsilon\\)；\u003c/li\u003e\n\u003cli\u003e神经网络拟合的是连续函数，而不是那种不连续、离散、急剧变化的函数。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e假设给定一个下图的连续函数，函数形式未知，本章将用图解的方式来证明，一个单隐层的神经网络就可以很好的拟合这个未知函数。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e首先，假设我们的隐藏层只有两个神经元，激活函数使用Sigmoid，并且我们暂时只关注上面那个神经元的参数和输出。则通过调整该神经元的\\(w\\)和\\(b\\)，可以得到不同形状的Sigmoid函数形式。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e极端情况下，如果\\(w\\)很大而\\(b\\)很小，则可以用Sigmoid函数模拟阶梯函数：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/high_weight_function.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e如果令\\(s = -b/w\\)，则只用一个\\(s\\)就可以确定Sigmoid的函数图像：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e如果把隐藏层下面那个神经元也考虑进来，并且令隐藏层的两个神经元和输出层的神经元的连接权重互为相反数，则输出层未激活值\\(z=w_1 a_1 + w_2 a_2\\)的函数图像变成了一个神奇的鼓包，这个鼓包就是我们后续拟合任意函数的基本单元。根据严格的函数形式，还可以知道\\(w_1\\)和\\(w_2\\)的绝对值控制着鼓包的高度，\\(s_1\\)和\\(s_2\\)的值控制着鼓包的位置和宽度。\u003ca href=\"http://neuralnetworksanddeeplearning.com/chap4.html#bump_fn\"\u003e大家可以去原始网页上体验一下作者给出的可交互版本，很有意思\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/bump_function.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e有了这个基本单元之后，我们可以通过增加隐藏层神经元的个数来增加鼓包的个数，比如再增加一对隐层神经元，可增加一个鼓包。虽然下图的例子中两个鼓包相互独立，但通过调整4个\\(s\\)，可以让两个鼓包相连甚至交错，\u003ca href=\"http://neuralnetworksanddeeplearning.com/chap4.html#double_bump\"\u003e大家可以去原网页试一试\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.4.png\"\u003e\u003c/p\u003e\n\u003cp\u003e继续增加隐层神经元个数，则可以继续增加鼓包的数量，如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.5.png\"\u003e\u003c/p\u003e\n\u003cp\u003e到这里想必大家马上知道了为什么神经网络能拟合任何一个函数了，如果隐层神经元足够多，则右图的小鼓包可以足够密，通过调整每个鼓包的高度，则无穷多个鼓包的顶点连线可以拟合任意一个函数。这和我们求函数积分（函数下方面积）时使用多个小矩形近似是一个道理！\u003c/p\u003e\n\u003cp\u003e所以对于本章开头的未知函数，我们通过调整不同鼓包的高度，可以使得小矩形面积之和与真实积分的差在\\( \\epsilon=0.4\\)以内。如果无限增加隐层神经元个数，则可以无限逼近真实值。这就说明神经网络确实可以拟合任意一个函数。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-07-neural-networks-and-deep-learning-4-why-nn-can-compute-any-function/ch4.6.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上述推导稍微需要注意的一点是，右图的输出是未激活函数值\\(\\sum_j w_j a_j\\)，而网络真正的输出是激活值\\(\\sigma(\\sum_j w_j a_j + b)\\)。这没有太大的关系，因为上面已经说明未激活输出能拟合任意函数，激活函数也是一个函数。增加激活函数就要求右图需要拟合激活函数和真实函数的嵌套函数。既然未激活输出能拟合任意函数，肯定能拟合这个嵌套函数\\(\\sigma^{-1} \\circ f(x)\\)，再用激活函数作用一下\\(\\sigma\\circ\\sigma^{-1} \\circ f(x)\\)，激活函数抵消了，正好得到\\(f(x)\\)。\u003c/p\u003e\n\u003cp\u003e如果输入是多维，或者输出是多维，都是类似的道理。这就说明神经网络确实可以拟合任意函数，真的很强大哦。\u003c/p\u003e","title":"Neural Networks and Deep Learning（四）图解神经网络为什么能拟合任意函数"},{"content":"权重初始化 在之前的章节中，我们都是用一个标准正态分布\\(N(0,1^2)\\)初始化所有的参数\\(w\\)和\\(b\\)，但是当神经元数量比较多时，会出现意想不到的问题。\n假设一个神经网络的输入层有1000个神经元，且某个样本的1000维输入中，恰好有500维是0，另500维是1。我们目前考察隐藏层的第一个神经元，则该神经元为激活的输出为\\(z = \\sum_j w_j x_j+b\\)，因为输入中的500维是0，所以\\(z\\)相当于有501个来自\\(N(0,1^2)\\)的随机变量相加。因为\\(w_j\\)和\\(b\\)的初始化都是独立同分布的，所以\\(z\\)也是一个正态分布，均值为0，但方差变成了\\(\\sqrt{501} \\approx 22.4\\)，即\\(z\\sim N(0,\\sqrt{501}^2)\\)。我们知道对于正态分布，如果方差越小，则分布的形状是高廋型的；如果方差越大，则分布的形状是矮胖型的。所以\\(z\\)有很大的概率取值会远大于1或远小于-1。又因为激活函数是sigmoid，当\\(z\\)远大于1或远小于-1时，\\(\\sigma (z)\\)趋近于1或者0，且导数趋于0，变化缓慢，导致梯度消失。\n请注意，这里的梯度消失和之前介绍得梯度消失稍有不同，之前是说在误差反向传播过程中，损失函数对权重的导数中包含梯度消失项，所以可以通过更换损失函数来解决。但是这里的梯度消失并不是在误差反向传播过程中产生的，而是在正向传播产生的，跟损失函数没关系。\n解决这个问题的方法很简单，根据上面的分析，如果输入\\(x_j\\)全为1，\\(w\\)和\\(b\\)都来自\\(N(0,1^2)\\)，则\\(z\\sim N(0, \\sqrt{n+1}^2)\\)，其中\\(n\\)为输入样本的维度。要减小\\(z\\)的方差，减小\\(w\\)和\\(b\\)的方差就可以了。因为\\(b\\)只有一个，对整体的影响不大，可以不修改\\(b\\)的分布，\\(b\\)依然来自\\(N(0,1^2)\\)。把\\(w_j\\)的分布修改为\\(N(0, (\\frac{1}{\\sqrt{n}})^2)\\)，此时\\(z\\sim N(0, \\sqrt{2}^2)\\)，\\(\\sqrt{2}=1.414\\)就非常接近1了，\\(z\\)的分布也变成了一个高廋型的，梯度消失问题也就不存在了。\n如果是开头的例子，输入维度为1000，其中500为0，500为1，\\(w_j\\sim N(0, (\\frac{1}{\\sqrt{1000}})^2)\\)，\\(b\\sim N(0,1^2)\\)，则\\(z\\sim N(0, \\sqrt{\\frac{3}{2}}^2)\\)，\\(\\sqrt{3/2} = 1.22\\ldots\\)也是高廋型的，不会有梯度消失的问题。\n由下图可知，在新的权重初始化策略下，网络很快就收敛了，比之前的方法快很多。\n怎样选择超参数 大原则：在网络优化的前期，尽量使网络结构、问题简单，以便快速得实验结果，不断尝试超参数取值，当找到正确的优化方向后，再慢慢把网络和问题变复杂，精细调整超参数。比如MNIST问题，开始可以减少训练数据，只取0和1的图片，做二分类；同时可以减少网络层数，验证集大小等，以便快速得到网络输出，判断网络性能变化。这样可以快速尝试新的超参数。\n学习率\\(\\eta\\) 在误差反向传播中，学习率太大，虽然可以加速学习，但在后期可能导致网络震荡，无法收敛；学习率太小，导致学习速度太慢，训练时间过长。\n确定学习率的方法是：首先随便选定一个值，比如0.01，然后不断增大10倍：0.1, 1, 10, 100…如果发现cost曲线在震荡，说明选大了，要降低，直到找到一个比较合适的值。这个过程只要找到合适的数量级就可以了，不一定要非常精确。比如发现0.1是比较合适的，那么可以再尝试0.2,0.3…，如果发现0.5不错，可以设学习率为0.5的一半0.25，这样可以使得在后续epoch中，不容易发生震荡。最好的方法是可变学习率，即前期学习率稍大（0.5），后期学习率稍小（0.1）之类的。\nepoch no-improvement-in-ten rule，就是说如果模型在最近的10个epoch中，验证集的accuracy都没有提高，则可以stop了。在早期实验中可以这么做，后续精细优化时可以改变ten，比如no-improvement-in-20/30等。\n正则化参数\\(\\lambda\\) 首先不要正则（\\(\\lambda=0\\)），使用上面提到的方法确定学习率\\(\\eta\\)，在确定的学习率情况下，正则\\(\\lambda=1\\)开始进行优化，比如每次乘以10或者除以10，观察验证集上的accuracy指标，找到正则化所在的合适的数量级，然后再fine-tune。\nMini-batch size 太小了，无法利用现有软件包的矩阵操作的优势，速度会很慢。极端情况下，如果mini-batch size=1，就是说每次只用一个sample做BP，则100次mini-batch=1会比一次mini-batch=100操作慢很多，因为很多软件包对矩阵操作有优化，而没有对for循环优化。太大了，则一次BP要很久，参数更新的次数也比较少。\n其他技术 随机梯度下降SGD的变种 海森矩阵法 SGD优化的目标就是最小化损失函数\\(C\\)，\\(C\\)是所有参数\\(w = w_1, w_2, \\ldots\\)的函数，即\\(C=C(w)\\)。希望能够通过改变\\(w\\)，不断最小化\\(C\\)，即找一个\\(\\Delta w\\)，使得\\(C(w+\\Delta w)\\)最小化。把\\(C(w+\\Delta w)\\)泰勒展开得到：\n$$\\begin{eqnarray}C(w+\\Delta w) \u0026 = \u0026 C(w) + \\sum_j \\frac{\\partial C}{\\partial w_j} \\Delta w_j\\nonumber \\\\ \u0026 \u0026 + \\frac{1}{2} \\sum_{jk} \\Delta w_j \\frac{\\partial^2 C}{\\partial w_j\\partial w_k} \\Delta w_k + \\ldots\\tag{1}\\end{eqnarray}$$写成矩阵形式就是：\n$$\\begin{eqnarray}C(w+\\Delta w) = C(w) + \\nabla C \\cdot \\Delta w +\\frac{1}{2} \\Delta w^T H \\Delta w + \\ldots,\\tag{2}\\end{eqnarray}$$其中的\\(\\nabla C\\)就是常规的梯度向量，\\(H\\)就是著名的海森矩阵，其中\\(H_{jk}=\\partial^2 C / \\partial w_j \\partial w_k\\)。如果把(2)中的高阶项扔掉，得到如下的近似等式：\n$$\\begin{eqnarray} C(w+\\Delta w) \\approx C(w) + \\nabla C \\cdot \\Delta w +\\frac{1}{2} \\Delta w^T H \\Delta w.\\tag{3}\\end{eqnarray}$$最小化(3)的右边，得：\n$$\\begin{eqnarray}\\Delta w = -H^{-1} \\nabla C.\\tag{4}\\end{eqnarray}$$所以我们可以通过如下方式更新\\(w\\)已达到最小化\\(C\\)的目的，当然也可以给\\(\\Delta w\\)乘上学习率\\(\\eta\\)。\n海森矩阵法有点像通过解析的方式最小化\\(C\\)，感觉上比SGD方法更精确靠谱。事实上，海森矩阵法确实比SGD方法收敛速度更快，但是因为在公式(4)中需要求解海森矩阵\\(H\\)的逆矩阵\\(H^{-1}\\)，当网络的参数量很大时，求解过程会非常慢，导致海森矩阵法不实用。\n基于动量的梯度下降 增加速度这个变量，个人不是太理解：\n$$\\begin{eqnarray} v \u0026 \\rightarrow \u0026 v’ = \\mu v – \\eta \\nabla C \\tag{5}\\\\w \u0026 \\rightarrow \u0026 w’ = w+v’.\\tag{6}\\end{eqnarray}$$不同的激活函数 tanh是和sigmoid很像的一个激活函数，其函数形式为：\n$$\\begin{eqnarray}\\tanh(z) \\equiv \\frac{e^z-e^{-z}}{e^z+e^{-z}}.\\tag{7}\\end{eqnarray}$$事实上，sigmoid函数\\(\\sigma(z)\\)和\\(\\tanh(z)\\)有线性关系：\n$$\\begin{eqnarray} \\sigma(z) = \\frac{1+\\tanh(z/2)}{2},\\tag{8}\\end{eqnarray}$$\\(\\tanh(z)\\)的函数图像如下，和sigmoid非常类似：\n\\(\\tanh(z)\\)和sigmoid的主要区别就是值域不一样，前者值域为[-1,1]，后者值域为[0,1]。这会导致什么差异呢？观察(BP4)这个公式，对于第\\(l\\)层的第\\(j\\)个神经元和第\\(l-1\\)层的所有神经元的连接权重\\(w_{jk}^l\\)，如果使用sigmoid激活，则\\(a_k^{l-1}\\)都是非负的，而这些梯度共用一个\\(\\delta_j^l\\)，所以对于固定的\\(j\\)，不同的\\(k\\)，所有的梯度\\(\\frac{\\partial C}{\\partial w^l_{jk}}\\)方向是一样的！这在无形中就减小了搜索空间。而如果用\\(\\tanh(z)\\)激活的话，不同的\\(k\\)的\\(a^{l-1}_k\\)正负号可能就不一样，搜索空间更大，更容易收敛。\n另一个比较常见的激活函数是ReLU激活函数，其函数形式如下：\n$$ReLU(z)=max(0, z)\\tag{9}$$函数图像如下：\nReLU和Sigmoid、tanh很不一样，ReLU在\\(z\u003e0\\)的方向上不会有梯度消失的问题。\n好了，第三章的内容就全部介绍完毕了，这一章介绍了很多调试神经网络的经验法则，没有太多的理论基础，相信随着这个领域的发展，神经网络的黑盒子会被慢慢打开。\n","permalink":"http://localhost:1313/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/","summary":"\u003ch1 id=\"权重初始化\"\u003e权重初始化\u003c/h1\u003e\n\u003cp\u003e在之前的章节中，我们都是用一个标准正态分布\\(N(0,1^2)\\)初始化所有的参数\\(w\\)和\\(b\\)，但是当神经元数量比较多时，会出现意想不到的问题。\u003c/p\u003e\n\u003cp\u003e假设一个神经网络的输入层有1000个神经元，且某个样本的1000维输入中，恰好有500维是0，另500维是1。我们目前考察隐藏层的第一个神经元，则该神经元为激活的输出为\\(z = \\sum_j w_j x_j+b\\)，因为输入中的500维是0，所以\\(z\\)相当于有501个来自\\(N(0,1^2)\\)的随机变量相加。因为\\(w_j\\)和\\(b\\)的初始化都是独立同分布的，所以\\(z\\)也是一个正态分布，均值为0，但方差变成了\\(\\sqrt{501} \\approx 22.4\\)，即\\(z\\sim N(0,\\sqrt{501}^2)\\)。我们知道对于正态分布，如果方差越小，则分布的形状是高廋型的；如果方差越大，则分布的形状是矮胖型的。所以\\(z\\)有很大的概率取值会远大于1或远小于-1。又因为激活函数是sigmoid，当\\(z\\)远大于1或远小于-1时，\\(\\sigma (z)\\)趋近于1或者0，且导数趋于0，变化缓慢，导致梯度消失。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz32.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-04-06-neural-networks-and-deep-learning-3-3-weight-initialization/ch3.8.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e请注意，这里的梯度消失和\u003ca href=\"https://bitjoy.net/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/\"\u003e之前介绍得梯度消失\u003c/a\u003e稍有不同，之前是说在误差反向传播过程中，损失函数对权重的导数中包含梯度消失项，所以可以通过更换损失函数来解决。但是这里的梯度消失并不是在误差反向传播过程中产生的，而是在正向传播产生的，跟损失函数没关系。\u003c/p\u003e\n\u003cp\u003e解决这个问题的方法很简单，根据上面的分析，如果输入\\(x_j\\)全为1，\\(w\\)和\\(b\\)都来自\\(N(0,1^2)\\)，则\\(z\\sim N(0, \\sqrt{n+1}^2)\\)，其中\\(n\\)为输入样本的维度。要减小\\(z\\)的方差，减小\\(w\\)和\\(b\\)的方差就可以了。因为\\(b\\)只有一个，对整体的影响不大，可以不修改\\(b\\)的分布，\\(b\\)依然来自\\(N(0,1^2)\\)。把\\(w_j\\)的分布修改为\\(N(0, (\\frac{1}{\\sqrt{n}})^2)\\)，此时\\(z\\sim N(0, \\sqrt{2}^2)\\)，\\(\\sqrt{2}=1.414\\)就非常接近1了，\\(z\\)的分布也变成了一个高廋型的，梯度消失问题也就不存在了。\u003c/p\u003e\n\u003cp\u003e如果是开头的例子，输入维度为1000，其中500为0，500为1，\\(w_j\\sim N(0, (\\frac{1}{\\sqrt{1000}})^2)\\)，\\(b\\sim N(0,1^2)\\)，则\\(z\\sim N(0, \\sqrt{\\frac{3}{2}}^2)\\)，\\(\\sqrt{3/2} = 1.22\\ldots\\)也是高廋型的，不会有梯度消失的问题。\u003c/p\u003e\n\u003cp\u003e由下图可知，在新的权重初始化策略下，网络很快就收敛了，比之前的方法快很多。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/weight_initialization_30.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"怎样选择超参数\"\u003e怎样选择超参数\u003c/h1\u003e\n\u003cp\u003e大原则：在网络优化的前期，尽量使网络结构、问题简单，以便快速得实验结果，不断尝试超参数取值，当找到正确的优化方向后，再慢慢把网络和问题变复杂，精细调整超参数。比如MNIST问题，开始可以减少训练数据，只取0和1的图片，做二分类；同时可以减少网络层数，验证集大小等，以便快速得到网络输出，判断网络性能变化。这样可以快速尝试新的超参数。\u003c/p\u003e\n\u003ch1 id=\"学习率\"\u003e学习率\\(\\eta\\)\u003c/h1\u003e\n\u003cp\u003e在误差反向传播中，学习率太大，虽然可以加速学习，但在后期可能导致网络震荡，无法收敛；学习率太小，导致学习速度太慢，训练时间过长。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/multiple_eta.png\"\u003e\u003c/p\u003e\n\u003cp\u003e确定学习率的方法是：首先随便选定一个值，比如0.01，然后不断增大10倍：0.1, 1, 10, 100…如果发现cost曲线在震荡，说明选大了，要降低，直到找到一个比较合适的值。这个过程只要找到合适的数量级就可以了，不一定要非常精确。比如发现0.1是比较合适的，那么可以再尝试0.2,0.3…，如果发现0.5不错，可以设学习率为0.5的一半0.25，这样可以使得在后续epoch中，不容易发生震荡。最好的方法是可变学习率，即前期学习率稍大（0.5），后期学习率稍小（0.1）之类的。\u003c/p\u003e\n\u003ch1 id=\"epoch\"\u003eepoch\u003c/h1\u003e\n\u003cp\u003eno-improvement-in-ten rule，就是说如果模型在最近的10个epoch中，验证集的accuracy都没有提高，则可以stop了。在早期实验中可以这么做，后续精细优化时可以改变ten，比如no-improvement-in-20/30等。\u003c/p\u003e\n\u003ch1 id=\"正则化参数\"\u003e正则化参数\\(\\lambda\\)\u003c/h1\u003e\n\u003cp\u003e首先不要正则（\\(\\lambda=0\\)），使用上面提到的方法确定学习率\\(\\eta\\)，在确定的学习率情况下，正则\\(\\lambda=1\\)开始进行优化，比如每次乘以10或者除以10，观察验证集上的accuracy指标，找到正则化所在的合适的数量级，然后再fine-tune。\u003c/p\u003e\n\u003ch1 id=\"mini-batch-size\"\u003eMini-batch size\u003c/h1\u003e\n\u003cp\u003e太小了，无法利用现有软件包的矩阵操作的优势，速度会很慢。极端情况下，如果mini-batch size=1，就是说每次只用一个sample做BP，则100次mini-batch=1会比一次mini-batch=100操作慢很多，因为很多软件包对矩阵操作有优化，而没有对for循环优化。太大了，则一次BP要很久，参数更新的次数也比较少。\u003c/p\u003e\n\u003ch1 id=\"其他技术\"\u003e其他技术\u003c/h1\u003e\n\u003ch2 id=\"随机梯度下降sgd的变种\"\u003e随机梯度下降SGD的变种\u003c/h2\u003e\n\u003ch3 id=\"海森矩阵法\"\u003e海森矩阵法\u003c/h3\u003e\n\u003cp\u003eSGD优化的目标就是最小化损失函数\\(C\\)，\\(C\\)是所有参数\\(w = w_1, w_2, \\ldots\\)的函数，即\\(C=C(w)\\)。希望能够通过改变\\(w\\)，不断最小化\\(C\\)，即找一个\\(\\Delta w\\)，使得\\(C(w+\\Delta w)\\)最小化。把\\(C(w+\\Delta w)\\)泰勒展开得到：\u003c/p\u003e\n$$\\begin{eqnarray}C(w+\\Delta w) \u0026 = \u0026 C(w) + \\sum_j \\frac{\\partial C}{\\partial w_j} \\Delta w_j\\nonumber \\\\ \u0026 \u0026 + \\frac{1}{2} \\sum_{jk} \\Delta w_j \\frac{\\partial^2 C}{\\partial w_j\\partial w_k} \\Delta w_k + \\ldots\\tag{1}\\end{eqnarray}$$\u003cp\u003e写成矩阵形式就是：\u003c/p\u003e","title":"Neural Networks and Deep Learning（三·三）权重初始化及其他"},{"content":"过拟合介绍 首先介绍一下神经网络中不同数据集的功能，包括训练集、验证集和测试集。\n训练集是用来训练网络参数的。当觉得在训练集上训练得差不多时，就可以在验证集上进行测试，如果验证集上的性能不好，则需要调整网络结构或者超参数，重新在训练集上训练。所以本质上验证集指导训练过程，也参与了训练和调参。为了防止网络对验证集过拟合，当网络在训练集和验证集上表现都不错时，就可以在测试集上进行测试了。测试集上的性能代表了模型的最终性能。\n当然如果发现网络在测试集上性能不好，可能还会反过来去优化网络，重新训练和验证，这么说测试集最终也变相参与了调优。如果一直这么推下去的话，就没完没了了，所以一般还是认为用验证集对模型进行优化，用测试集对模型性能进行测试。\n过拟合的含义就是网络在训练集上性能很好，但是在验证集（或者测试集）上的性能较差，这说明网络在训练集上训练过头了，对训练集产生了过拟合。为了便于叙述，本文没有验证集，直接使用测试集作为验证集对模型进行调优，所以主要考察网络在训练集和测试集上的性能表现。\n判断网络是否过拟合的方法就是观察网络在训练集和测试集上的accuracy和loss的变化曲线。对于accuracy，如果训练集的accuracy很高接近100%且收敛了，但测试集上的accuracy和训练集上的accuracy相差较大也收敛了（如下图收敛到82%左右），说明网络过拟合了。对于loss，如果训练集的loss一直在下降，但测试集的loss先下降后又上升，也说明网络过拟合了。这两种现象，虽然指标不同，但含义是一样的，即网络在训练集上的性能一直在提高甚至到完美水平，但在测试集上的性能提高到一定水平后不再变化甚至下降了。\n不过下面几张图反应的过拟合epoch时间可能不一样，比如对于测试集上的accuracy，可能在280左右过拟合，但是对于测试集上的loss，在15和280左右都可以认为是过拟合了，尤其是15，loss最低，之后loss反升，可以认为是一个合理的过拟合的点。具体哪个epoch之后过拟合，取决于问题本身关注哪个指标，比如MNIST分类问题，可能关注分类accuracy，所以可重点关注测试集上的accuracy那个图，认为是280左右过拟合，因为200~280的accuracy还一直有提升，虽然提升很有限。\n应对过拟合最好的方法就是增加训练数据，如果能把所有可能的数据都收集到，对所有数据产生过拟合，那相当于对所有数据都能预测得很好，那问题本质上已经解决了。\n但是，在实际应用场景中，不可能收集到所有数据，而且数据往往是严重不足的，此时，应对过拟合主要有三种方法：正则化、Dropout和数据增强，下面分别介绍这三个部分。\n正则化 正则化的思路就是修改损失函数，使损失函数考虑模型复杂度。考虑正则化的损失函数的通用公式如下：\n$$\\begin{eqnarray} C = C_0(w,x,y) + \\lambda\\Omega(w)\\tag{1}\\end{eqnarray}$$其中\\(C_0\\)为原始的没有正则化项的损失函数，比如MSE或者交叉熵损失等，\\(\\Omega(w)\\)表示正则化项，即用来惩罚模型复杂度的，\\(\\lambda\\)表示正则化参数，用来平衡\\(C_0\\)和\\(\\Omega(w)\\)的重要性。\n正则化又分为L2正则和L1正则，它们很类似，先详细介绍下L2正则。\n举个例子，L2正则化后的损失函数如下：\n$$\\begin{eqnarray} C = C_0 + \\frac{\\lambda}{2n}\\sum_w w^2,\\tag{2}\\end{eqnarray}$$前半部分就是普通的损失函数（比如MSE或者交叉熵损失），后半部分就是L2正则。L2正则是对网络中的所有权重\\(w\\)求平方和（\\(\\vec w\\)的L2范数，所以叫L2正则），然后除以\\(2n\\)，其中\\(n\\)是训练样本数，除以2应该是为了后面求导方便。\n(2)式的直观含义是，\\(\\min C\\)的过程中，我不但希望损失函数本身\\(C_0\\)足够小，还希望网络的权重\\(w\\)也比较小，最好不要出现很大的\\(w\\)。如果\\(\\lambda\\)越大，表示正则化越厉害，对大的\\(w\\)惩罚越严重。\n加入L2正则后的梯度也很容易计算，如下：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w} \u0026 = \u0026 \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\tag{3}\\\\ \\frac{\\partial C}{\\partial b} \u0026 = \u0026 \\frac{\\partial C_0}{\\partial b}.\\tag{4}\\end{eqnarray}$$对应的参数更新公式如下：\n$$\\begin{eqnarray}b \u0026 \\rightarrow \u0026 b -\\eta \\frac{\\partial C_0}{\\partial b}.\\tag{5}\\end{eqnarray}$$$$\\begin{eqnarray} w \u0026 \\rightarrow \u0026 w-\\eta \\frac{\\partial C_0}{\\partial w}-\\frac{\\eta \\lambda}{n} w \\tag{6}\\\\ \u0026 = \u0026 \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial C_0}{\\partial w}. \\tag{7}\\end{eqnarray}$$由(5)可知，偏移量\\(b\\)的梯度更新和没有正则化时是一样的，因为正则化并没有惩罚\\(b\\)，这个后面会解释为什么。由(7)可知，对\\(w\\)的梯度更新和没有正则化时很类似，只不过需要先对\\(w\\)进行缩放，缩放因子为\\(1-\\frac{\\eta\\lambda}{n}\\)，因为训练样本\\(n\\)往往很大，所以缩放因子在(0,1)，即先对\\(w\\)进行缩小，然后正常梯度下降，这种操作也被称为权值衰减。\\(\\lambda\\)最好根据\\(n\\)的大小进行调整，如果\\(n\\)非常大的话，\\(\\lambda\\)最好也大一些，否则权值衰减因子就会很小，正则化效果就不明显。\n如果是mini-batch进行更新的话，公式也类似，梯度对mini-batch中的\\(m\\)个样本进行平均。\n$$\\begin{eqnarray} w \\rightarrow \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m}\\sum_x \\frac{\\partial C_x}{\\partial w}, \\tag{8}\\end{eqnarray}$$$$\\begin{eqnarray}b \\rightarrow b – \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial b},\\tag{9}\\end{eqnarray}$$通过简单的L2正则，就能解决文章开头的过拟合问题，使得测试集上的accuracy在200~400个epoch时一直有提升。\n正则化不仅能解决过拟合问题，还能使结果更加稳定。我们都知道，神经网络的参数是随机初始化的，很有可能不同的随机初始值收敛到的网络权重不一样，导致最终模型性能有差异。加入正则化后，对所有权重都有正则化约束，使得梯度下降在探索不同\\(w\\)的方向时，都能得到一定程度的更新，而不会说某个方向比较好就一直在那个方向探索。\n那么正则化为什么能解决过拟合问题呢？从公式(1)可知，加入正则化之后，除了要优化网络对训练数据预测的准确度之外，还需要使得网络的权重\\(w\\)尽量小，权重越小则网络越简单，极端情况下如果权重等于0，则相当于少了一个参数，网络肯定更简单了。所以加入正则化之后，网络的复杂度降低了，网络的泛化能力就更强了，也就更不容易过拟合。\n以一个很简单的(x,y)的二维数据拟合为例，我们既可以用一个九次函数\\(y = a_0 x^9 + a_1 x^8 + \\ldots + a_9\\)完美拟合，也可以用一个一次函数\\(y = 2x\\)拟合。虽然九次函数能完美拟合，但是其最高次幂为9，一旦真实数据中有一些噪声，即x有一点噪声，则预测得到的y就会相差特别大，这其实就是非常严重的过拟合，即在训练数据上性能非常好，但在测试数据上性能可能较差，而且对噪声很不稳定。而一次函数虽然在训练集上不能完美拟合，但效果也不差，而且预想到它在应对有噪声的数据x时，也能预测得比较准确，所以一次函数的泛化效果更好，更不容易过拟合。\n对应到神经网络中，\\(w\\)的大小可以类比成上面例子中的最高次幂，如果\\(w\\)很大，则x的微小扰动都可能造成网络输出的很大变化，就类似于上面的9次函数，很可能学到了数据中的局部噪声的特征。而\\(w\\)较小的网络，可能就没学到这些局部噪声的特征，而只是学到了数据中全局的（或者大部分数据都有的）、高频的特征，这其实是好事，这种网络在新的数据上的泛化能力就会更好，不易被噪声干扰。\n其实哲学界还有一个“奥卡姆剃刀”原则，就是说如果一个数据集既能用简单模型解释，也能用复杂模型解释，那就尽量选择简单模型吧，简单就是美啊。\n对于神经网络来说，还有一个很神奇的事情，就拿本文构造的神经网络为例，如果隐藏层有100个神经元，则大概有80000个参数，但是我们的训练数据集中只有50000张谱图，训练数据小于模型参数个数，按理说会导致严重的过拟合，但实际效果并没有，网络在测试集上的accuracy也很高。所以这很神奇，好像和机器学习中的经验风险最小化定理冲突。有人猜测这可能和多层神经网络有“自正则化”的效果有关。\n最后，解释一下为什么正则化项中只有\\(w\\)没有\\(b\\)。因为每个神经元的操作是\\(\\sigma(wx+b)\\)，\\(w\\)和\\(x\\)是乘法，影响更大，\\(b\\)对神经元输出的影响并不大，没必要正则化，当然强行对\\(b\\)正则也可以，只是和没对\\(b\\)正则效果差不多。说多了都是玄学，经验之谈。\nL1正则和L2正则类似，只不过换成了对\\(w\\)的一范数，损失函数如下：\n$$\\begin{eqnarray} C = C_0 + \\frac{\\lambda}{n} \\sum_w |w|.\\tag{10}\\end{eqnarray}$$对\\(w\\)的权重更新公式如下，和(7)很类似，不细说了。\n$$\\begin{eqnarray} w \\rightarrow w’ =w-\\frac{\\eta \\lambda}{n} \\mbox{sgn}(w) – \\eta \\frac{\\partial C_0}{\\partial w},\\tag{11}\\end{eqnarray}$$L1正则和L2正则都有一个权重衰减因子，虽然因子不同，但目的都是为了让\\(w\\)衰减，尽量小。他们的差别是L2衰减的是\\(w\\)的一个比例（公式(7)，\\(w\\left(1 – \\frac{\\eta \\lambda}{n} \\right)\\)），而L1衰减的是一个固定值（公式(11)，\\(w-\\frac{\\eta \\lambda}{n} \\mbox{sgn}(w)\\)）。这就导致如果\\(w\\)本身很大的话，L1减的相对更少，而L2减的相对更多；如果\\(w\\)本身很小的的话，L1减的相对就更多，而L2减的相对更少。所以L1倾向于把小的\\(w\\)直接减到0，而对大的\\(w\\)不怎么减，而L2对所有\\(w\\)都一视同仁的减一定的比例。所以L1倾向于保留大的\\(w\\)，而直接把小的\\(w\\)正则为0，所以L1得到的非0权重可能更少，能得到稀疏模型，有利于特征选择。\n对L1和L2正则的区别，比较常见的解释是二维情况下的图解，网上很多，比如：https://blog.csdn.net/jinping_shi/article/details/52433975。\nDropout Dropout的做法和L1正则、L2正则很不一样，它不对损失函数进行修改，而是对网络结构进行修改。Dropout会在每个mini-batch训练时，随机删掉网络中一半的神经元（并不是真正的删除，而是暂时把对应的\\(w\\)设置为0）。它的效果相当于每次训练只用了一半的神经元，那么这次mini-batch和下次mini-batch相当于训练了不同的网络，最后预测时恢复所有的神经元。这样得到的预测结果相当于多个神经网络进行了average或者voting，使得预测结果更加鲁棒。比如有5个模型，其中3个模型预测对了，2个模型预测错了，做一下voting的话，就能得到正确的结果，而如果只是一个模型的话，如果刚好预测错了，那就错了。\nDropout的另一个解释时，因为网络在训练时会随机删掉一半的节点，那么节点间的依赖关系就减弱了，迫使神经元依赖较少的信息也要得到比较好的预测结果，所以网络会更加鲁棒。Dropout在深层神经网络中，特别有用，能有效防止过拟合。不过也正因为此，dropout会增加训练收敛的时间，这是可以理解的。\n数据增强 本博客开头就提到，解决过拟合最有效的方法就是增加训练数据集，但收集到的数据很少，有没有办法根据已收集的数据集，产生新的数据集呢，数据增强就是干这个的。\n对图片数据来说，可做的数据增强包括：旋转、移位、翻转等。比如MNIST数据集，对一张手写数字图片4旋转15度，这张图片依然表示数字4，虽然人眼很容易识别旋转15度之后依然是4，但对网络来说，这个变化非常大，可以认为是一个新的样本。不过旋转角度不能太大了，比如6旋转变成9了，就不能标注成6了。\n对于语音识别任务来说，数据增强可做的包括对语音添加背景噪声，快进，慢放等。总之，针对不同的任务，可以有不同的数据增强方法，基本原则就是增强之后的数据确实是现实世界中存在的数据，比如手写数字图片的轻微旋转，现实中就是有人写字写歪了一点；对语音进行快进，现实中就是有人说话快一点等。\n最后总结一下，防止过拟合的方法包括：L2正则、L1正则、Dropout和数据增强。\n","permalink":"http://localhost:1313/posts/2019-03-24-neural-networks-and-deep-learning-3-2-overfitting-and-regularization/","summary":"\u003ch1 id=\"过拟合介绍\"\u003e过拟合介绍\u003c/h1\u003e\n\u003cp\u003e首先介绍一下神经网络中不同数据集的功能，包括训练集、验证集和测试集。\u003c/p\u003e\n\u003cp\u003e训练集是用来训练网络参数的。当觉得在训练集上训练得差不多时，就可以在验证集上进行测试，如果验证集上的性能不好，则需要调整网络结构或者超参数，重新在训练集上训练。所以本质上验证集指导训练过程，也参与了训练和调参。为了防止网络对验证集过拟合，当网络在训练集和验证集上表现都不错时，就可以在测试集上进行测试了。测试集上的性能代表了模型的最终性能。\u003c/p\u003e\n\u003cp\u003e当然如果发现网络在测试集上性能不好，可能还会反过来去优化网络，重新训练和验证，这么说测试集最终也变相参与了调优。如果一直这么推下去的话，就没完没了了，所以一般还是认为用验证集对模型进行优化，用测试集对模型性能进行测试。\u003c/p\u003e\n\u003cp\u003e过拟合的含义就是网络在训练集上性能很好，但是在验证集（或者测试集）上的性能较差，这说明网络在训练集上训练过头了，对训练集产生了过拟合。为了便于叙述，本文没有验证集，直接使用测试集作为验证集对模型进行调优，所以主要考察网络在训练集和测试集上的性能表现。\u003c/p\u003e\n\u003cp\u003e判断网络是否过拟合的方法就是观察网络在训练集和测试集上的accuracy和loss的变化曲线。对于accuracy，如果训练集的accuracy很高接近100%且收敛了，但测试集上的accuracy和训练集上的accuracy相差较大也收敛了（如下图收敛到82%左右），说明网络过拟合了。对于loss，如果训练集的loss一直在下降，但测试集的loss先下降后又上升，也说明网络过拟合了。这两种现象，虽然指标不同，但含义是一样的，即网络在训练集上的性能一直在提高甚至到完美水平，但在测试集上的性能提高到一定水平后不再变化甚至下降了。\u003c/p\u003e\n\u003cp\u003e不过下面几张图反应的过拟合epoch时间可能不一样，比如对于测试集上的accuracy，可能在280左右过拟合，但是对于测试集上的loss，在15和280左右都可以认为是过拟合了，尤其是15，loss最低，之后loss反升，可以认为是一个合理的过拟合的点。具体哪个epoch之后过拟合，取决于问题本身关注哪个指标，比如MNIST分类问题，可能关注分类accuracy，所以可重点关注测试集上的accuracy那个图，认为是280左右过拟合，因为200~280的accuracy还一直有提升，虽然提升很有限。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/overfitting4.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/overfitting2.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/overfitting1.png\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/overfitting3.png\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e应对过拟合最好的方法就是增加训练数据，如果能把所有可能的数据都收集到，对所有数据产生过拟合，那相当于对所有数据都能预测得很好，那问题本质上已经解决了。\u003c/p\u003e\n\u003cp\u003e但是，在实际应用场景中，不可能收集到所有数据，而且数据往往是严重不足的，此时，应对过拟合主要有三种方法：正则化、Dropout和数据增强，下面分别介绍这三个部分。\u003c/p\u003e\n\u003ch1 id=\"正则化\"\u003e正则化\u003c/h1\u003e\n\u003cp\u003e正则化的思路就是修改损失函数，使损失函数考虑模型复杂度。考虑正则化的损失函数的通用公式如下：\u003c/p\u003e\n$$\\begin{eqnarray} C = C_0(w,x,y) + \\lambda\\Omega(w)\\tag{1}\\end{eqnarray}$$\u003cp\u003e其中\\(C_0\\)为原始的没有正则化项的损失函数，比如MSE或者交叉熵损失等，\\(\\Omega(w)\\)表示正则化项，即用来惩罚模型复杂度的，\\(\\lambda\\)表示正则化参数，用来平衡\\(C_0\\)和\\(\\Omega(w)\\)的重要性。\u003c/p\u003e\n\u003cp\u003e正则化又分为L2正则和L1正则，它们很类似，先详细介绍下L2正则。\u003c/p\u003e\n\u003cp\u003e举个例子，L2正则化后的损失函数如下：\u003c/p\u003e\n$$\\begin{eqnarray} C = C_0 + \\frac{\\lambda}{2n}\\sum_w w^2,\\tag{2}\\end{eqnarray}$$\u003cp\u003e前半部分就是普通的损失函数（比如MSE或者交叉熵损失），后半部分就是L2正则。L2正则是对网络中的所有权重\\(w\\)求平方和（\\(\\vec w\\)的L2范数，所以叫L2正则），然后除以\\(2n\\)，其中\\(n\\)是训练样本数，除以2应该是为了后面求导方便。\u003c/p\u003e\n\u003cp\u003e(2)式的直观含义是，\\(\\min C\\)的过程中，我不但希望损失函数本身\\(C_0\\)足够小，还希望网络的权重\\(w\\)也比较小，最好不要出现很大的\\(w\\)。如果\\(\\lambda\\)越大，表示正则化越厉害，对大的\\(w\\)惩罚越严重。\u003c/p\u003e\n\u003cp\u003e加入L2正则后的梯度也很容易计算，如下：\u003c/p\u003e\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w} \u0026 = \u0026 \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\tag{3}\\\\ \\frac{\\partial C}{\\partial b} \u0026 = \u0026 \\frac{\\partial C_0}{\\partial b}.\\tag{4}\\end{eqnarray}$$\u003cp\u003e对应的参数更新公式如下：\u003c/p\u003e\n$$\\begin{eqnarray}b \u0026 \\rightarrow \u0026 b -\\eta \\frac{\\partial C_0}{\\partial b}.\\tag{5}\\end{eqnarray}$$$$\\begin{eqnarray} w \u0026 \\rightarrow \u0026 w-\\eta \\frac{\\partial C_0}{\\partial w}-\\frac{\\eta \\lambda}{n} w \\tag{6}\\\\ \u0026 = \u0026 \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial C_0}{\\partial w}. \\tag{7}\\end{eqnarray}$$\u003cp\u003e由(5)可知，偏移量\\(b\\)的梯度更新和没有正则化时是一样的，因为正则化并没有惩罚\\(b\\)，这个后面会解释为什么。由(7)可知，对\\(w\\)的梯度更新和没有正则化时很类似，只不过需要先对\\(w\\)进行缩放，缩放因子为\\(1-\\frac{\\eta\\lambda}{n}\\)，因为训练样本\\(n\\)往往很大，所以缩放因子在(0,1)，即先对\\(w\\)进行缩小，然后正常梯度下降，这种操作也被称为权值衰减。\\(\\lambda\\)最好根据\\(n\\)的大小进行调整，如果\\(n\\)非常大的话，\\(\\lambda\\)最好也大一些，否则权值衰减因子就会很小，正则化效果就不明显。\u003c/p\u003e","title":"Neural Networks and Deep Learning（三·二）过拟合与正则化"},{"content":"原文的第三章内容较多，本博客将分三个部分进行介绍：梯度消失、过拟合与正则化、权重初始化及其他，首先介绍梯度消失问题。\n为简单起见，假设网络只包含一个输入和一个神经元，网络的损失是均方误差损失MSE，激活函数是Sigmoid函数。则该网络的参数只包含权重\\(w\\)和偏移量\\(b\\)。我们想训练这个网络，使得当输入为1时，输出0。\n假设我们随机初始化\\(w_0=0.6\\)，\\(b_0=0.9\\)，则网络的损失随着训练的epoch变化曲线如下，看起来挺好的，一开始损失下降很快，随着epoch增加，损失下降逐渐平缓，直至收敛。\n但是，如果随机初始化\\(w_0=2.0\\)，\\(b_0=2.0\\)，则网络的损失一开始下降得很缓慢，要训练到快200个epoch时，损失才快速下降。可以看到同样是300个epoch，由于初始化权重的差别，损失下降的趋势完全不一样，而且对于下面这种情况，到300个epoch时，损失还有下降的空间，所以期望的output不如上面的接近目标值0。\n为什么同样的网络，只是因为初始化权重的差异，损失的变化曲线却相差这么多呢，这和我们选择的损失函数与激活函数有关。\n回顾一下，我们在上一讲的末尾介绍到如果损失函数是MSE且激活函数是Sigmoid时，有\\(\\delta^L = (a^L-y) \\odot \\{\\sigma(z^L)(1-\\sigma(z^L))\\}\\)，又因为网络只有一个神经元，所以梯度如下：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w} \u0026 = \u0026 (a-y)\\sigma'(z) x = a \\sigma'(z),\\tag{1}\\\\\\frac{\\partial C}{\\partial b} \u0026 = \u0026 (a-y)\\sigma'(z) = a \\sigma'(z)\\tag{2}\\end{eqnarray}$$其中第二个等号是把\\(x=1\\)和\\(y=0\\)带入得到的。由此可见，误差对两个参数\\(w\\)和\\(b\\)的梯度都和激活函数的导数有关，因为激活函数是Sigmoid，当神经元的输出接近0或1时，梯度几乎为0，误差反向传播就会非常慢，导致上图出现损失下降非常慢的现象。这就是梯度消失的原因。\n为了解决这个问题，我们可以采取两种策略，一是替换损失函数，一是替换激活函数。\n第一种方法是将MSE的损失函数替换为交叉熵损失函数，激活函数依然是Sigmoid。我们考虑一个比本文开头更复杂的网络，仍然是一个输出神经元，但包含多个输入神经元。\n此时，交叉熵损失函数定义如下，其中的\\(n\\)表示训练样本数，\\(\\frac{1}{n}\\sum_x\\)表示对所有输入样本\\(x\\)的交叉熵损失求均值。\n$$\\begin{eqnarray}C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right]\\tag{3}\\end{eqnarray}$$我们首先考察为什么(3)可以是一个损失函数，损失函数需要满足如下两个条件：\n非负； 当网络输出和目标答案越接近，损失越小；反之损失越大。 简单代入几组不同的样本很容易验证交叉熵满足上述两个条件 ，所以交叉熵可以作为一个损失函数。\n下面我们再考察一下为什么交叉熵损失函数+Sigmoid激活函数可以解决梯度消失的问题。首先推导交叉熵损失\\(C\\)对权重\\(w_j\\)和\\(b\\)的梯度：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w_j} \u0026 = \u0026 -\\frac{1}{n} \\sum_x \\left(\\frac{y }{\\sigma(z)} -\\frac{(1-y)}{1-\\sigma(z)} \\right)\\frac{\\partial \\sigma}{\\partial w_j} \\tag{4}\\\\\u0026 = \u0026 -\\frac{1}{n} \\sum_x \\left(\\frac{y}{\\sigma(z)}-\\frac{(1-y)}{1-\\sigma(z)} \\right)\\sigma'(z) x_j\\tag{5}\\\\\u0026 = \u0026 \\frac{1}{n}\\sum_x \\frac{\\sigma'(z) x_j}{\\sigma(z) (1-\\sigma(z))}(\\sigma(z)-y).\\tag{6}\\end{eqnarray}$$上式分子Sigmoid的导数正好可以和分母抵消，得到：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w_j} = \\frac{1}{n} \\sum_x x_j(\\sigma(z)-y).\\tag{7}\\end{eqnarray}$$类似的，可得：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b} = \\frac{1}{n} \\sum_x (\\sigma(z)-y).\\tag{8}\\end{eqnarray}$$非常神奇的，我们发现交叉熵损失函数对参数的梯度中不存在Sigmoid的导数了！这样就不受Sigmoid两端梯度消失的影响了！而且从(7)和(8)还可以发现，当网络的输出\\(\\sigma(z)\\)和目标结果\\(y\\)差距越大，梯度越大；差距越小，梯度越小。这和人类的学习过程很相似，当犯错越大，教训越大，得到的经验也越多，提升也越明显。\n换成交叉熵损失函数+Sigmoid激活函数的组合后，我们再看看之前的两个例子：\n和预期的结果一样，无论哪一种情况，由于刚开始时网络的输出和预期答案差距较大，所以梯度也大，损失下降也很快，不再出现梯度消失的问题了。\n上面的讨论是基于输出层只有一个神经元的情况，如果输出层有\\(j\\)个神经元，交叉熵公式如下，结论是一样的。\n$$\\begin{eqnarray} C = -\\frac{1}{n} \\sum_x\\sum_j \\left[y_j \\ln a^L_j + (1-y_j) \\ln (1-a^L_j) \\right].\\tag{9}\\end{eqnarray}$$$$\\begin{eqnarray} \\delta^L = a^L-y.\\tag{10}\\end{eqnarray}$$$$\\begin{eqnarray} \\frac{\\partial C}{\\partial w^L_{jk}} \u0026 = \u0026 \\frac{1}{n} \\sum_x a^{L-1}_k (a^L_j-y_j).\\tag{11}\\end{eqnarray}$$如果输出层神经元的激活函数都是线性的，不再是Sigmoid函数，此时依然使用MSE损失函数也没有梯度消失的问题，公式如下：\n$$\\begin{eqnarray}a^L_j = z^L_j.\\tag{12}\\end{eqnarray}$$$$\\begin{eqnarray}\\delta^L = a^L-y.\\tag{13}\\end{eqnarray}$$$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w^L_{jk}} \u0026 = \u0026 \\frac{1}{n} \\sum_x a^{L-1}_k (a^L_j-y_j) \\tag{14}\\\\\\frac{\\partial C}{\\partial b^L_{j}} \u0026 = \u0026 \\frac{1}{n} \\sum_x (a^L_j-y_j).\\tag{15}\\end{eqnarray}$$ 那么交叉熵这个损失函数是怎么来的呢，是碰巧有人发现使用交叉熵能抵消Sigmoid的梯度消失问题吗？事实上，我们可以从MSE+Sigmoid的梯度消失问题中推导得到交叉熵损失函数。还是以开头只有一个神经元的网络为例，对于公式(1)和(2)，如果希望能抵消掉激活函数的导数\\(\\sigma'(z)\\)，则我们的目标为：\n$$\\begin{eqnarray} \\frac{\\partial C}{\\partial w_j} \u0026 = \u0026 x_j(a-y) \\tag{16}\\\\\\frac{\\partial C}{\\partial b } \u0026 = \u0026 (a-y).\\tag{17}\\end{eqnarray}$$又根据原始推导和Sigmoid导数有：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial a} \\sigma'(z)= \\frac{\\partial C}{\\partial a} a(1-a).\\tag{18}\\end{eqnarray}$$对比公式(17)有：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial a} = \\frac{a-y}{a(1-a)}.\\tag{19}\\end{eqnarray}$$据此可推导得到损失函数为：\n$$\\begin{eqnarray}C = -[y \\ln a + (1-y) \\ln (1-a)]+ {\\rm constant},\\tag{20}\\end{eqnarray}$$如果有多个输入样本，则损失函数为：\n$$\\begin{eqnarray}C = -\\frac{1}{n} \\sum_x [y \\ln a +(1-y) \\ln(1-a)] + {\\rm constant},\\tag{21}\\end{eqnarray}$$可以看到，上式本质上和公式(3)是一样的，这就是交叉熵损失函数！所以交叉熵并不神秘，不是拍脑袋凭空想出来的，而是可以根据目标求解出来的一个损失函数。\n上面介绍了第一种替换损失函数但保留Sigmoid激活函数来解决梯度消失的方法，第二种方法就是把损失函数和激活函数都换掉的方法，损失函数替换为log似然损失，激活函数替换为Softmax。\nSoftmax激活函数的公式如下：\n$$\\begin{eqnarray} a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}},\\tag{22}\\end{eqnarray}$$此时，某个神经元的输出\\(a_j^L\\)不再只跟它自己未激活的\\(z_j^L\\)有关，而是和所有的未激活值\\(z_k^L\\)有关。而且\\(a_j^L\\)有一个很好的特性，它用所有未激活值的和做了归一化，所以每个输出\\(a_j^L\\)可以看成输出为\\(j\\)的概率值，满足：\n$$\\begin{eqnarray}\\sum_j a^L_j \u0026 = \u0026 \\frac{\\sum_j e^{z^L_j}}{\\sum_k e^{z^L_k}} = 1.\\tag{23}\\end{eqnarray}$$log似然损失函数如下：\n$$\\begin{eqnarray}C \\equiv -\\ln a^L_y.\\tag{24}\\end{eqnarray}$$比如在手写数字识别问题中，如果输入是一个\\(7\\)的图片，那么对应这个样本的损失就是\\(-\\ln a^L_7\\)，如果输出的第7个神经元的概率\\(a_7^L\\)很高接近于1，则损失\\(-\\ln a^L_7\\)会很小；反之，如果\\(a_7^L\\)很低接近于0，则损失\\(-\\ln a^L_7\\)会很大。\n同样的，我们可以推导得到使用log似然损失+Softmax也能解决梯度消失的问题，对应的梯度如下，和公式(14)和(15)是一样的。\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b^L_j} \u0026 = \u0026 a^L_j-y_j \\tag{25}\\\\\\frac{\\partial C}{\\partial w^L_{jk}} \u0026 = \u0026 a^{L-1}_k (a^L_j-y_j) \\tag{26}\\end{eqnarray}$$事实上，Softmax相当于Sigmoid在多元情况下的推广，交叉熵损失(9)的每一项也就是log似然损失(24)。回顾逻辑回归那篇博客，有非常多的相似点。\n最后总结，对于一个神经网络，判断是否有梯度消失的问题，最根本的方法就是求解损失对参数的梯度，如果梯度中包含可能出现梯度消失的项，则存在梯度消失的问题，否则不存在。而梯度求解又和所选的损失函数和激活函数有关，目前比较好的组合是MSE+线性激活、交叉熵+Sigmoid、log似然+Softmax，这几种组合都不会有梯度消失的问题。\n","permalink":"http://localhost:1313/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/","summary":"\u003cp\u003e原文的第三章内容较多，本博客将分三个部分进行介绍：梯度消失、过拟合与正则化、权重初始化及其他，首先介绍梯度消失问题。\u003c/p\u003e\n\u003cp\u003e为简单起见，假设网络只包含一个输入和一个神经元，网络的损失是均方误差损失MSE，激活函数是Sigmoid函数。则该网络的参数只包含权重\\(w\\)和偏移量\\(b\\)。我们想训练这个网络，使得当输入为1时，输出0。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz28.png\"\u003e\u003c/p\u003e\n\u003cp\u003e假设我们随机初始化\\(w_0=0.6\\)，\\(b_0=0.9\\)，则网络的损失随着训练的epoch变化曲线如下，看起来挺好的，一开始损失下降很快，随着epoch增加，损失下降逐渐平缓，直至收敛。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/ch3.1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是，如果随机初始化\\(w_0=2.0\\)，\\(b_0=2.0\\)，则网络的损失一开始下降得很缓慢，要训练到快200个epoch时，损失才快速下降。可以看到同样是300个epoch，由于初始化权重的差别，损失下降的趋势完全不一样，而且对于下面这种情况，到300个epoch时，损失还有下降的空间，所以期望的output不如上面的接近目标值0。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2019-03-18-neural-networks-and-deep-learning-3-1-gradient-vanishing/ch3.2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e为什么同样的网络，只是因为初始化权重的差异，损失的变化曲线却相差这么多呢，这和我们选择的损失函数与激活函数有关。\u003c/p\u003e\n\u003cp\u003e回顾一下，我们在\u003ca href=\"https://bitjoy.net/posts/2018-12-14-neutral-networks-and-deep-learning-2-bp/\"\u003e上一讲\u003c/a\u003e的末尾介绍到如果损失函数是MSE且激活函数是Sigmoid时，有\\(\\delta^L = (a^L-y) \\odot \\{\\sigma(z^L)(1-\\sigma(z^L))\\}\\)，又因为网络只有一个神经元，所以梯度如下：\u003c/p\u003e\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w} \u0026 = \u0026 (a-y)\\sigma'(z) x = a \\sigma'(z),\\tag{1}\\\\\\frac{\\partial C}{\\partial b} \u0026 = \u0026 (a-y)\\sigma'(z) = a \\sigma'(z)\\tag{2}\\end{eqnarray}$$\u003cp\u003e其中第二个等号是把\\(x=1\\)和\\(y=0\\)带入得到的。由此可见，误差对两个参数\\(w\\)和\\(b\\)的梯度都和激活函数的导数有关，因为激活函数是Sigmoid，当神经元的输出接近0或1时，梯度几乎为0，误差反向传播就会非常慢，导致上图出现损失下降非常慢的现象。这就是梯度消失的原因。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\"\u003e\u003c/p\u003e\n\u003cp\u003e为了解决这个问题，我们可以采取两种策略，一是替换损失函数，一是替换激活函数。\u003c/p\u003e\n\u003cp\u003e第一种方法是将MSE的损失函数替换为交叉熵损失函数，激活函数依然是Sigmoid。我们考虑一个比本文开头更复杂的网络，仍然是一个输出神经元，但包含多个输入神经元。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz29.png\"\u003e\u003c/p\u003e\n\u003cp\u003e此时，交叉熵损失函数定义如下，其中的\\(n\\)表示训练样本数，\\(\\frac{1}{n}\\sum_x\\)表示对所有输入样本\\(x\\)的交叉熵损失求均值。\u003c/p\u003e\n$$\\begin{eqnarray}C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right]\\tag{3}\\end{eqnarray}$$\u003cp\u003e我们首先考察为什么(3)可以是一个损失函数，损失函数需要满足如下两个条件：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e非负；\u003c/li\u003e\n\u003cli\u003e当网络输出和目标答案越接近，损失越小；反之损失越大。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e简单代入几组不同的样本很容易验证交叉熵满足上述两个条件 ，所以交叉熵可以作为一个损失函数。\u003c/p\u003e\n\u003cp\u003e下面我们再考察一下为什么交叉熵损失函数+Sigmoid激活函数可以解决梯度消失的问题。首先推导交叉熵损失\\(C\\)对权重\\(w_j\\)和\\(b\\)的梯度：\u003c/p\u003e\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w_j} \u0026 = \u0026 -\\frac{1}{n} \\sum_x \\left(\\frac{y }{\\sigma(z)} -\\frac{(1-y)}{1-\\sigma(z)} \\right)\\frac{\\partial \\sigma}{\\partial w_j} \\tag{4}\\\\\u0026 = \u0026 -\\frac{1}{n} \\sum_x \\left(\\frac{y}{\\sigma(z)}-\\frac{(1-y)}{1-\\sigma(z)} \\right)\\sigma'(z) x_j\\tag{5}\\\\\u0026 = \u0026 \\frac{1}{n}\\sum_x \\frac{\\sigma'(z) x_j}{\\sigma(z) (1-\\sigma(z))}(\\sigma(z)-y).\\tag{6}\\end{eqnarray}$$\u003cp\u003e上式分子Sigmoid的导数正好可以和分母抵消，得到：\u003c/p\u003e","title":"Neural Networks and Deep Learning（三·一）梯度消失"},{"content":"这一讲介绍误差反向传播（backpropagation）网络，简称BP网络。\n以上一讲介绍的MNIST手写数字图片分类问题为研究对象，首先明确输入输出：输入就是一张28×28的手写数字图片，展开后可以表示成一个长度为784的向量；输出可以表示为一个长度为10的one-hot向量，比如输入是一张“3”的图片，则输出向量为(0,0,0,1,0,0,0,0,0,0,0)。\n然后构造一个如下的三层全连接网络。第一层为输入层，包含784个神经元，正好对应输入的一张28×28的图片。第二层为隐藏层，假设隐藏层有15个神经元。第三层为输出层，正好10个神经元，对应该图片的one-hot结果。\n全连接网络表示上一层的每个神经元都和下一层的每个神经元有连接，即每个神经元的输入来自上一层所有神经元的输出，每个神经元的输出连接到下一层的所有神经元。每条连边上都有一个权重w。\n每个神经元执行的操作非常简单，就是把跟它连接的每个输入乘以边上的权重，然后累加起来。\n比如上面的一个神经元，它的输出就是：\n$$\\begin{eqnarray}\\mbox{output} = \\left\\{ \\begin{array}{ll}0 \u0026 \\mbox{if} \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\1 \u0026 \\mbox{if} \\sum_j w_j x_j \u003e \\mbox{threshold}\\end{array}\\right.\\tag{1}\\end{eqnarray}$$其中的threshold就是该神经元激活的阈值，如果累加值超过threshold，则该神经元被激活，输出为1，否则为0。这就是最原始的感知机网络。感知机网络也可以写成如下的向量形式，用激活阈值b代替threshold，然后移到左边。神经网络中，每条边具有权重w，每个神经元具有激活阈值b。\n$$\\begin{eqnarray}\\mbox{output} = \\left\\{ \\begin{array}{ll} 0 \u0026 \\mbox{if } w\\cdot x + b \\leq 0 \\\\1 \u0026 \\mbox{if } w\\cdot x + b \u003e 0\\end{array}\\right.\\tag{2}\\end{eqnarray}$$ 但是感知机网络的这种激活方式不够灵活，它在threshold左右有一个突变，如果输入或者某个边上的权重稍微有一点变化，输出结果可能就千差万别了。于是后来人们提出了用sigmoid函数来当激活函数，它在0附近的斜率较大，在两边的斜率较小，能达到和阶梯函数类似的效果，而且函数光滑可导。sigmoid的函数形式如下，其中\\(z\\equiv w \\cdot x + b\\)为神经元激活之前的值。\n$$\\begin{eqnarray} \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}\\tag{3}\\end{eqnarray}$$sigmmoid函数还有一个优点就是它的导数很好计算，可以用它本身来表示：\n$$\\begin{eqnarray}\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\tag{4}\\end{eqnarray}$$BP网络的参数就是所有连线上的权重w和所有神经元中的激活阈值b，如果知道这些参数，给定一个输入x，则可以很容易的通过正向传播（feedforward）的方法计算到输出，即不断的执行\\(w \\cdot x + b\\)操作，然后用sigmoid激活，再把上一层的输出传递给下一层作为输入，直到最后一层。\n1 2 3 4 5 def feedforward(self, a): \u0026#34;\u0026#34;\u0026#34;Return the output of the network if ``a`` is input.\u0026#34;\u0026#34;\u0026#34; for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a 同时，网络的误差可以用均方误差（mean squared error, MSE）表示，即网络在最后一层的激活值（即网络的输出值）\\(a\\)和对应训练集输入\\(x\\)的正确答案\\(y(x)\\)的差的平方。有\\(n\\)个输入则误差取平均，\\(\\dfrac{1}{2}\\)是为了后续求导方便。\n$$\\begin{eqnarray} C(w,b) \\equiv\\frac{1}{2n} \\sum_x \\| y(x) – a\\|^2\\tag{5}\\end{eqnarray}$$训练BP网络的过程就是不断调整参数——权重weights和阈值biases——使得网络的误差\\(C\\)最小。使用的方法是梯度下降，即误差\\(C\\)对每个参数求偏导（梯度），然后参数向梯度的反方向更新，这样更新能保证误差\\(C\\)下降得最快，至于为什么下降最快，直观上很好理解，数学证明可以看原文，证明也很好理解，而且我认为很精彩。\n$$\\begin{eqnarray}w_k \u0026 \\rightarrow \u0026 w_k’ = w_k-\\eta \\frac{\\partial C}{\\partial w_k} \\tag{6}\\\\b_l \u0026 \\rightarrow \u0026 b_l’ = b_l-\\eta \\frac{\\partial C}{\\partial b_l}.\\tag{7}\\end{eqnarray}$$ 下面我们就来求解梯度下降中最重要的梯度，即\\(C\\)对每个参数\\(w\\)和\\(b\\)的偏导，先正式定义一些记号。\n创建网络时，可以对w和b进行随机初始化，代码如下。如果以本文开头的网络结构为例，用[748, 15, 10]输入到下面函数，随机初始化b和w。[748, 15, 10]分别表示第一、二、三层的神经元个数为748，15和10。\n1 2 3 4 5 6 def __init__(self, sizes): self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] 请注意第一层（输入层）神经元是没有阈值b的，所以biases列表只有两项，第一项为15×1的列向量，表示中间层的阈值；第二项为10×1的列向量，表示输出层的阈值。在网络中用\\(b^l_j\\)表示第\\(l\\)层的第\\(j\\)个神经元的阈值，如下\\(b_3^2\\)表示第2层的第3个神经元的阈值。\n权重由于关系到两层之间，所以本文的三层网络的weights列表也只有两项，第一项为15×748的矩阵，表示中间层和输入层的权重矩阵；第二项为10×15的矩阵，表示输出层和中间层的权重矩阵。在网络中用\\(w_{jk}^l\\)表示第\\(l-1\\)层的第\\(k\\)个神经元和第\\(l\\)层的第\\(j\\)个神经元的连接权重，这种表示方法和常规的顺序下标表示相反，但是在矩阵相乘时\\(z^l=w^la^{l-1}+b^l\\)（下方公式10），不用转置，更方便。\n类似的，我们再定义一个神经元激活之前的值为\\(z\\)，激活之后的值为\\(a=\\sigma(z)\\)，上下标的表示方法和\\(b\\)的表示方法是一样的。具体来说，\n$$\\begin{eqnarray} z^{l}_j = \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\tag{8}\\end{eqnarray}$$$$\\begin{eqnarray} a^{l}_j =\\sigma (z^{l}_j) = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right)\\tag{9}\\end{eqnarray}$$向量形式为：\n$$\\begin{eqnarray} z^l=w^la^{l-1}+b^l \\tag{10}\\end{eqnarray}$$$$\\begin{eqnarray} a^{l} = \\sigma (z^l) = \\sigma\\left( w^la^{l-1}+b^l \\right)\\tag{11}\\end{eqnarray}$$对于输入层神经元来说，输入就等于输出，没有激活一说，所以\\(a^1=x\\)。对于单个输入样本\\(x\\)来说，网络的误差如下，\\(L\\)表示网络的总层数，所以\\(a^L\\)表示网络最终的输出。\n$$\\begin{eqnarray}C = \\frac{1}{2} \\|y-a^L\\|^2 = \\frac{1}{2} \\sum_j (y_j-a^L_j)^2.\\tag{12}\\end{eqnarray}$$我们的目标就是求出\\(C\\)对每个参数的梯度。\n首先，引入一个很重要的中间量\\(\\delta^l_j\\)，表示误差\\(C\\)对第\\(l\\)层的第\\(j\\)个神经元的未激活值\\(z^l_j\\)的偏导：\n$$\\begin{eqnarray} \\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j}.\\tag{13}\\end{eqnarray}$$根据定义，对于输出层（假设共有\\(L\\)层），使用链式法则和公式(9)，有下面的等式。\n$$\\begin{eqnarray} \\delta^L_j = \\frac{\\partial C}{\\partial z^L_j} = \\frac{\\partial C}{\\partial a^L_j}\\frac{\\partial a^L_j}{\\partial z^L_j} =\\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j).\\tag{14}\\end{eqnarray}$$上式\\(\\frac{\\partial C}{\\partial a^L_j}\\)就是损失\\(C\\)对最后一层输出\\(a^L\\)的偏导，可以用\\(\\nabla_a C\\)表示。输出层的\\(\\delta^L_j\\)只和损失以及输出层激活前后的值有关。整体用向量表示为：\n$$\\begin{eqnarray} \\delta^L = \\nabla_a C \\odot \\sigma'(z^L).\\tag{BP1}\\end{eqnarray}$$其中的\\(\\odot\\)表示哈达玛乘积（Hadamard product），就是向量或矩阵的对应位相乘，比如\\((s \\odot t)_j = s_j t_j\\)，或：\n$$\\begin{eqnarray}\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right] \\odot \\left[\\begin{array}{c} 3 \\\\ 4\\end{array} \\right]= \\left[ \\begin{array}{c} 1 * 3 \\\\ 2 * 4 \\end{array} \\right]= \\left[ \\begin{array}{c} 3 \\\\ 8 \\end{array} \\right].\\tag{15}\\end{eqnarray}$$如果不是最后一层，则使用链式法则有：\n$$\\begin{eqnarray}\\delta^l_j \u0026 = \u0026 \\frac{\\partial C}{\\partial z^l_j} \\tag{16}\\\\\u0026 = \u0026 \\sum_k \\frac{\\partial C}{\\partial z^{l+1}_k} \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \\tag{17}\\\\ \u0026 = \u0026 \\sum_k \\frac{\\partial z^{l+1}_k}{\\partial z^l_j} \\delta^{l+1}_k,\\tag{18}\\end{eqnarray}$$又因为\n$$\\begin{eqnarray}z^{l+1}_k = \\sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \\sum_j w^{l+1}_{kj} \\sigma(z^l_j) +b^{l+1}_k.\\tag{19}\\end{eqnarray}$$所以\n$$\\begin{eqnarray}\\frac{\\partial z^{l+1}_k}{\\partial z^l_j} = w^{l+1}_{kj} \\sigma'(z^l_j).\\tag{20}\\end{eqnarray}$$将(20)代入(18)得到：\n$$\\begin{eqnarray}\\delta^l_j = \\sum_k w^{l+1}_{kj} \\delta^{l+1}_k \\sigma'(z^l_j).\\tag{21}\\end{eqnarray}$$写成向量形式就是：\n$$\\begin{eqnarray} \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\\tag{BP2}\\end{eqnarray}$$从(BP2)可知，中间层的\\(\\delta^l\\)可通过下一层的\\(\\delta^{l+1}\\)递推计算得到。这正体现了误差反向传播的思想，即误差从输出层反向一层一层传到输入层，所以正好可以通过(BP1)和(BP2)计算到所有层的\\(\\delta^l\\)。\n我们推导(BP1)和(BP2)是为了便于求解\\(C\\)对\\(w\\)和\\(b\\)的梯度，进而可以根据公式(6)和(7)更新参数。首先求解\\(C\\)对\\(b\\)的梯度：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial b^l_j} = \\delta^l_j \\tag{BP3}\\end{eqnarray}$$上式中间使用链式法则，中间左式正好是\\(\\delta^l_j\\)的定义(13)，中间右式根据(8)可知等于1，所以误差\\(C\\)对神经元阈值\\(b^l_j\\)的梯度正好等于\\(\\delta^l_j\\)。然后求解\\(C\\)对\\(w\\)的梯度：\n$$\\begin{eqnarray}\\frac{\\partial C}{\\partial w^l_{jk}} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\\tag{BP4}\\end{eqnarray}$$同样利用链式法则和(13)、(8)很容易就能得到上式。可以看到，借助中间量\\(\\delta^l_j\\)，\\(C\\)对\\(w\\)和\\(b\\)的梯度能很简洁清晰的表示出来，这就是为什么一开始要推导\\(\\delta^l_j\\)的原因。\n以上我们就求到了误差\\(C\\)对所有参数的梯度。总结一下就是：\n所以，对于单个输入样本\\(x\\)，网络的损失对所有参数的偏导可以用如下算法求解：\n在真实场景中，样本数往往很多，如果一个样本更新一次参数，则效率比较低，目前通常将一小批样本（batch）一起算一个累加的梯度，然后更新一次，这个过程称为一次迭代（iteration）。当所有样本都迭代过之后，称为一次epoch。下面是针对一次iteration的BP算法，其中的\\(\\eta\\)为学习率，通俗理解为往梯度下降的方向走多远，\\(\\eta\\)太大可能引起震荡无法收敛，\\(\\eta\\)太小训练又太慢，所以需要人工调参。\n在BP算法的4个公式中，最重要的是(BP1)，因为(BP1)计算了误差开始传播的初始值。(BP1)中的两个分量\\(\\nabla_a C\\)和\\(\\sigma'(z^L)\\)又和网络的设计有关。如果使用的损失是均方误差损失(12)，使用的激活函数是sigmoid函数(3)，则(BP1)可具体写成下面的等式，所以BP算法的4个公式中，所有变量都是已知的了。\n$$\\begin{eqnarray} \\delta^L = (a^L-y) \\odot \\{\\sigma(z^L)(1-\\sigma(z^L))\\}.\\tag{22}\\end{eqnarray}$$了解了BP的原理之后，代码就非常好理解了，具体可以看GitHub项目，删掉所有注释，真正的核心代码只有74行。使用[784, 30, 10]作为输入，即三层网络，每层神经元个数分别为784，30和10，调用如下代码（表示训练30个epochs，每个batch有10个样本，学习率为3.0），网络在测试集上的分类准确率能达到95%以上。\n1 net.SGD(training_data, 30, 10, 3.0, test_data=test_data) ","permalink":"http://localhost:1313/posts/2018-12-14-neutral-networks-and-deep-learning-2-bp/","summary":"\u003cp\u003e这一讲介绍误差反向传播（backpropagation）网络，简称BP网络。\u003c/p\u003e\n\u003cp\u003e以上一讲介绍的MNIST手写数字图片分类问题为研究对象，首先明确输入输出：输入就是一张28×28的手写数字图片，展开后可以表示成一个长度为784的向量；输出可以表示为一个长度为10的one-hot向量，比如输入是一张“3”的图片，则输出向量为(0,0,0,1,0,0,0,0,0,0,0)。\u003c/p\u003e\n\u003cp\u003e然后构造一个如下的三层全连接网络。第一层为输入层，包含784个神经元，正好对应输入的一张28×28的图片。第二层为隐藏层，假设隐藏层有15个神经元。第三层为输出层，正好10个神经元，对应该图片的one-hot结果。\u003c/p\u003e\n\u003cp\u003e全连接网络表示上一层的每个神经元都和下一层的每个神经元有连接，即每个神经元的输入来自上一层所有神经元的输出，每个神经元的输出连接到下一层的所有神经元。每条连边上都有一个权重w。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz12.png\"\u003e\u003c/p\u003e\n\u003cp\u003e每个神经元执行的操作非常简单，就是把跟它连接的每个输入乘以边上的权重，然后累加起来。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/tikz0.png\"\u003e\u003c/p\u003e\n\u003cp\u003e比如上面的一个神经元，它的输出就是：\u003c/p\u003e\n$$\\begin{eqnarray}\\mbox{output} = \\left\\{ \\begin{array}{ll}0 \u0026 \\mbox{if} \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\1 \u0026 \\mbox{if} \\sum_j w_j x_j \u003e \\mbox{threshold}\\end{array}\\right.\\tag{1}\\end{eqnarray}$$\u003cp\u003e其中的threshold就是该神经元激活的阈值，如果累加值超过threshold，则该神经元被激活，输出为1，否则为0。这就是最原始的感知机网络。感知机网络也可以写成如下的向量形式，用激活阈值b代替threshold，然后移到左边。神经网络中，每条边具有权重w，每个神经元具有激活阈值b。\u003c/p\u003e\n$$\\begin{eqnarray}\\mbox{output} = \\left\\{ \\begin{array}{ll} 0 \u0026 \\mbox{if } w\\cdot x + b \\leq 0 \\\\1 \u0026 \\mbox{if } w\\cdot x + b \u003e 0\\end{array}\\right.\\tag{2}\\end{eqnarray}$$\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/d/d9/Dirac_distribution_CDF.svg\"\u003e\n\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\"\u003e\u003c/p\u003e\n\u003cp\u003e但是感知机网络的这种激活方式不够灵活，它在threshold左右有一个突变，如果输入或者某个边上的权重稍微有一点变化，输出结果可能就千差万别了。于是后来人们提出了用sigmoid函数来当激活函数，它在0附近的斜率较大，在两边的斜率较小，能达到和阶梯函数类似的效果，而且函数光滑可导。sigmoid的函数形式如下，其中\\(z\\equiv w \\cdot x + b\\)为神经元激活之前的值。\u003c/p\u003e\n$$\\begin{eqnarray} \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}\\tag{3}\\end{eqnarray}$$\u003cp\u003esigmmoid函数还有一个优点就是它的导数很好计算，可以用它本身来表示：\u003c/p\u003e\n$$\\begin{eqnarray}\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\tag{4}\\end{eqnarray}$$\u003cp\u003eBP网络的参数就是所有连线上的权重w和所有神经元中的激活阈值b，如果知道这些参数，给定一个输入x，则可以很容易的通过正向传播（feedforward）的方法计算到输出，即不断的执行\\(w \\cdot x + b\\)操作，然后用sigmoid激活，再把上一层的输出传递给下一层作为输入，直到最后一层。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e5\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003efeedforward\u003c/span\u003e(self, a):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;Return the output of the network if ``a`` is input.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e b, w \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e zip(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebiases, self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eweights):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\ta \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sigmoid(np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edot(w, a)\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003eb)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e a\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e同时，网络的误差可以用均方误差（mean squared error, MSE）表示，即网络在最后一层的激活值（即网络的输出值）\\(a\\)和对应训练集输入\\(x\\)的正确答案\\(y(x)\\)的差的平方。有\\(n\\)个输入则误差取平均，\\(\\dfrac{1}{2}\\)是为了后续求导方便。\u003c/p\u003e","title":"Neural Networks and Deep Learning（二）BP网络"},{"content":"最近开始学习神经网络和深度学习，使用的是网上教程：http://neuralnetworksanddeeplearning.com/，这是学习心得第一讲，介绍经典的MNIST手写数字图片数据集。\nMNIST（Modified National Institute of Standards and Technology database）数据集改编自美国国家标准与技术研究所收集的更大的NIST数据集，该数据集来自250个不同人手写的数字图片，一半是人口普查局的工作人员，一半是高中生。该数据集包括60000张训练集图片和10000张测试集图片，训练集和测试集都提供了正确答案。每张图片都是28×28=784大小的灰度图片，也就是一个28×28的矩阵，里面每个值是一个像素点，值在[0,1]之间，0表示白色，1表示黑色，(0,1)之间表示不同的灰度。下面是该数据集中的一些手写数字图片，可以有一个感性的认识。\nMNIST数据集可以在Yann LeCun的网站上下载到：http://yann.lecun.com/exdb/mnist/，但是他提供的MNIST数据集格式比较复杂，需要自己写代码进行解析。目前很多深度学习框架都自带了MNIST数据集，比较流行的是转换为pkl格式的版本：http://deeplearning.net/data/mnist/mnist.pkl.gz，该版本把原始的60000张训练集进一步划分成了50000张小训练集和10000张验证集，下面以这个版本为例进行介绍。\npkl是python内置的一种格式，可以将python的各种数据结构序列化存储到磁盘中，需要时又可以读取并反序列化到内存中。mnist.pkl.gz做了两次操作，先pkl序列化，再gz压缩存储，所以要读取该文件，需要先解压再反序列化，在python3中，读取mnist.pkl.gz的方式如下：\n1 2 3 4 5 import pickle import gzip f = gzip.open(‘../data/mnist.pkl.gz’, ‘rb’) training_data, validation_data, test_data = pickle.load(f, encoding=’bytes’) f.close() 这样就得到了训练集、验证集和测试集。将数据集序列化到文件中的方法也很简单，需要注意的是pickle在序列化和反序列化时有不同的协议，可以用protocol参数进行设置。\n1 2 3 4 dataset=[training_data, validation_data, test_data] f=gzip.open(‘../data/mnist3.pkl.gz’,’wb’) pickle.dump(dataset,f,protocol=3) f.close() 我们从mnist.pkl.gz读取到的training_data, validation_data, test_data这三个数据的结构是一样的，每个都是一个二维的tuple。以training_data为例，training_data[0]是训练样本，是一个50000×784的矩阵，表示有50000个训练样本，每个训练样本是一个784的一维数组，784就是把一张28×28的图片展开reshape成的一维数组；training_data[1]是训练样本对应的类标号，大小为50000的一维数组，每个值为0~9中的某个数，表示对应样本的数字标号。\n对于第一次接触MNIST数据集的同学来说，以压缩的pkl格式存储的手写数字图片往往不利于他们感性直观的认识这个问题，下面介绍怎样将MNIST数据集打印成常见的png格式图片，也就是博客开头的真正的“图片”。\n就像上面介绍的一样，training_data, validation_data, test_data这三个数据的结构都是一个二维的tuple，下标0存储了n张图片数据，下标1存储了这n张图片对应的答案。现在我们把validation_data的第一张图片打印出来看看，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 def plot_digit(X, y): np.savetxt(\u0026#39;../fig/%d.csv\u0026#39;%y, X, delimiter=\u0026#39;,\u0026#39;) import matplotlib.pyplot as plt plt.imshow(X, cmap=\u0026#39;Greys\u0026#39;) # or \u0026#39;Greys_r\u0026#39; plt.savefig(\u0026#39;../fig/%d.png\u0026#39;%y) plt.show() training_data, validation_data, test_data = load_data() X=np.reshape(validation_data[0][0], (28, 28)) y=validation_data[1][0] plot_digit(X, y) 首先我们取出validation_data数据集的第一张图片，reshape成原始图片的28×28的矩阵形式，保存在X中，然后我们取出这张图片对应的正确答案，保存在y中，最后调用plot_digit函数打印这个数字。\n在plot_digit函数中，我们首先把矩阵X保存到一个csv表格中，如果我们打开这个表格，会发现传说中的图片真的就是一个28×28的矩阵，单元格的值在[0,1]之间，如果我们使用LibreOffice默认的条件格式进行着色的话，能明显看到一个数字3。\n另外，我们还可以matplotlib打印出这张手写数字图片，使用imshow并指定着色规则是灰度Greys即可，得到的就是本文开头看到的那种白底黑字了。\n好啦，有关MNIST数据集的介绍就到这里，完整代码可以查看我的Github：https://github.com/01joy/neural-networks-and-deep-learning/blob/master/src/mnist_loader.py，下一步开始学习使用BP网络进行图片分类。\n","permalink":"http://localhost:1313/posts/2018-11-25-neutral-networks-and-deep-learning-1-mnist-dataset/","summary":"\u003cp\u003e最近开始学习神经网络和深度学习，使用的是网上教程：\u003ca href=\"http://neuralnetworksanddeeplearning.com/\"\u003ehttp://neuralnetworksanddeeplearning.com/\u003c/a\u003e，这是学习心得第一讲，介绍经典的MNIST手写数字图片数据集。\u003c/p\u003e\n\u003cp\u003eMNIST（Modified National Institute of Standards and Technology database）数据集改编自美国国家标准与技术研究所收集的更大的NIST数据集，该数据集来自250个不同人手写的数字图片，一半是人口普查局的工作人员，一半是高中生。该数据集包括60000张训练集图片和10000张测试集图片，训练集和测试集都提供了正确答案。每张图片都是28×28=784大小的灰度图片，也就是一个28×28的矩阵，里面每个值是一个像素点，值在[0,1]之间，0表示白色，1表示黑色，(0,1)之间表示不同的灰度。下面是该数据集中的一些手写数字图片，可以有一个感性的认识。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/neuralnetworksanddeeplearning.com/images/digits_separate.png\"\u003e\u003c/p\u003e\n\u003cp\u003eMNIST数据集可以在Yann LeCun的网站上下载到：\u003ca href=\"http://yann.lecun.com/exdb/mnist/\"\u003ehttp://yann.lecun.com/exdb/mnist/\u003c/a\u003e，但是他提供的MNIST数据集格式比较复杂，需要自己写代码进行解析。目前很多深度学习框架都自带了MNIST数据集，比较流行的是转换为pkl格式的版本：\u003ca href=\"http://deeplearning.net/data/mnist/mnist.pkl.gz\"\u003ehttp://deeplearning.net/data/mnist/mnist.pkl.gz\u003c/a\u003e，该版本把原始的60000张训练集进一步划分成了50000张小训练集和10000张验证集，下面以这个版本为例进行介绍。\u003c/p\u003e\n\u003cp\u003epkl是python内置的一种格式，可以将python的各种数据结构序列化存储到磁盘中，需要时又可以读取并反序列化到内存中。mnist.pkl.gz做了两次操作，先pkl序列化，再gz压缩存储，所以要读取该文件，需要先解压再反序列化，在python3中，读取mnist.pkl.gz的方式如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e5\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pickle\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e gzip\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ef \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e gzip\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e../\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003emnist\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epkl\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egz\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e, \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003erb\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etraining_data, validation_data, test_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pickle\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eload(f, encoding\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003ebytes\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ef\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e这样就得到了训练集、验证集和测试集。将数据集序列化到文件中的方法也很简单，需要注意的是pickle在序列化和反序列化时有不同的协议，可以用protocol参数进行设置。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edataset\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e[training_data, validation_data, test_data]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ef\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003egzip\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e../\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003emnist3\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epkl\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egz\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e,\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003ewb\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epickle\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edump(dataset,f,protocol\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ef\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e我们从mnist.pkl.gz读取到的training_data, validation_data, test_data这三个数据的结构是一样的，每个都是一个二维的tuple。以training_data为例，training_data[0]是训练样本，是一个50000×784的矩阵，表示有50000个训练样本，每个训练样本是一个784的一维数组，784就是把一张28×28的图片展开reshape成的一维数组；training_data[1]是训练样本对应的类标号，大小为50000的一维数组，每个值为0~9中的某个数，表示对应样本的数字标号。\u003c/p\u003e","title":"Neural Networks and Deep Learning（一）MNIST数据集介绍"},{"content":"VSCode是微软开源的一个很强大的IDE，可以支持几乎所有编程语言，而且是跨平台的，Linux用户终于可以用上宇宙最强IDE了。我最近在使用VSCode编写调试Python项目，其调试功能很强大，和VS上调试C++的感觉是一样的，强烈推荐。\nVSCode还可以连接Github，进行版本控制。下面以我最近学习的深度学习项目为例，介绍下怎样在Ubuntu下使用VSCode连接Github。以我fork的repo为例：https://github.com/01joy/neural-networks-and-deep-learning。\n连接Github有两种方式，一种是HTTPS，另一种是SSH，在每个repo页面的右边，有一个Clone or download按钮，可以获取到这两种连接方式的地址。HTTPS方式和网址类似，以HTTPS开头；SSH方式以git@githu.com开头。使用HTTPS连接比较简单，但是每次push的时候需要输入用户名和密码，比较麻烦，如果想记住密码，需要把用户名和密码以明文的形式保存到一个文件中，个人感觉不方便且不安全。下面以SSH连接为例进行介绍。\n首先设置Github提交时的用户名和密码，一般设置成全局的：https://help.github.com/articles/setting-your-username-in-git/、https://help.github.com/articles/setting-your-commit-email-address-in-git/ 生成一对新的SSH公钥和私钥，并添加到ssh-agent中。注意生成的时候需要输入passphrase，这个passphrase不是Github的密码，自己随便取一个记住就好。https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/ 把SSH的公钥添加到Github账号中：https://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account/ 测试SSH连接是否成功：https://help.github.com/articles/testing-your-ssh-connection/ （可选）修改SSH密码，即第1步设置的passphrase：https://help.github.com/articles/working-with-ssh-key-passphrases/ 到这里，本级就能通过SSH连接Github了。 如果没有安装VSCode，可以直接通过Ubuntu的终端连接Github，步骤如下：\n在本地创建一个和远程repo名称一样的空文件夹 终端cd到该文件夹内 git init # 在该文件夹内初始化 git remote add origin git@github.com:01joy/notes-on-writing.git # 使用repo的SSH地址 git pull origin master # 把远程代码拉到本地 修改代码 git add . # 在根目录执行，添加所有修改 git commit -m ‘comments’ # commit第7步添加的修改 git push origin master # 把第8步发布到远程 如果安装了VSCode，其实和直接用终端是一样的，在菜单栏的Terminal下新建一个终端，在这个终端内执行上述代码，如果在第4步出现”Enter password to unlock the private key”时，输入创建SSH时第2步的密码即可，只需一次，下次就不用再输入密码了。点击File的Open Folder打开本地repo文件夹。点击VSCode左边栏的Explorer可以在编辑器下修改代码。切换到左边栏的Source Control可以进行Git相关操作，修改的文件右边会出现一个M，点击这个M会出现diff视图；Source Control左边的右上角有三个点，点击这个按钮会出现很多Git操作，包括commit、push等，其实相当于调用上述代码，效果是一样的。\nVSCode快捷键Ctrl+Shift+P会出现命令窗口，在里面输入commit、push等会出现相关操作的，能起到一定的加速效果，当然也可以自定义快捷键。\nHave Fun!\n","permalink":"http://localhost:1313/posts/2018-11-13-access-github-from-vscode-in-ubuntu/","summary":"\u003cp\u003eVSCode是微软开源的一个很强大的IDE，可以支持几乎所有编程语言，而且是跨平台的，Linux用户终于可以用上宇宙最强IDE了。我最近在使用VSCode编写调试Python项目，其调试功能很强大，和VS上调试C++的感觉是一样的，强烈推荐。\u003c/p\u003e\n\u003cp\u003eVSCode还可以连接Github，进行版本控制。下面以我最近学习的深度学习项目为例，介绍下怎样在Ubuntu下使用VSCode连接Github。以我fork的repo为例：\u003ca href=\"https://github.com/01joy/neural-networks-and-deep-learning\"\u003ehttps://github.com/01joy/neural-networks-and-deep-learning\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e连接Github有两种方式，一种是HTTPS，另一种是SSH，在每个repo页面的右边，有一个Clone or download按钮，可以获取到这两种连接方式的地址。HTTPS方式和网址类似，以HTTPS开头；SSH方式以git@githu.com开头。使用HTTPS连接比较简单，但是每次push的时候需要输入用户名和密码，比较麻烦，如果想记住密码，需要把用户名和密码以明文的形式保存到一个文件中，个人感觉不方便且不安全。下面以SSH连接为例进行介绍。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e首先设置Github提交时的用户名和密码，一般设置成全局的：\u003ca href=\"https://help.github.com/articles/setting-your-username-in-git/\"\u003ehttps://help.github.com/articles/setting-your-username-in-git/\u003c/a\u003e、\u003ca href=\"https://help.github.com/articles/setting-your-commit-email-address-in-git/\"\u003ehttps://help.github.com/articles/setting-your-commit-email-address-in-git/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e生成一对新的SSH公钥和私钥，并添加到ssh-agent中。注意生成的时候需要输入passphrase，这个passphrase不是Github的密码，自己随便取一个记住就好。\u003ca href=\"https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/\"\u003ehttps://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e把SSH的公钥添加到Github账号中：\u003ca href=\"https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/\"\u003ehttps://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e测试SSH连接是否成功：\u003ca href=\"https://help.github.com/articles/testing-your-ssh-connection/\"\u003ehttps://help.github.com/articles/testing-your-ssh-connection/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e（可选）修改SSH密码，即第1步设置的passphrase：\u003ca href=\"https://help.github.com/articles/working-with-ssh-key-passphrases/\"\u003ehttps://help.github.com/articles/working-with-ssh-key-passphrases/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e到这里，本级就能通过SSH连接Github了。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e如果没有安装VSCode，可以直接通过Ubuntu的终端连接Github，步骤如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e在本地创建一个和远程repo名称一样的空文件夹\u003c/li\u003e\n\u003cli\u003e终端cd到该文件夹内\u003c/li\u003e\n\u003cli\u003egit init # 在该文件夹内初始化\u003c/li\u003e\n\u003cli\u003egit remote add origin \u003ca href=\"mailto:git@github.com\"\u003egit@github.com\u003c/a\u003e:01joy/notes-on-writing.git # 使用repo的SSH地址\u003c/li\u003e\n\u003cli\u003egit pull origin master # 把远程代码拉到本地\u003c/li\u003e\n\u003cli\u003e修改代码\u003c/li\u003e\n\u003cli\u003egit add . # 在根目录执行，添加所有修改\u003c/li\u003e\n\u003cli\u003egit commit -m ‘comments’ # commit第7步添加的修改\u003c/li\u003e\n\u003cli\u003egit push origin master # 把第8步发布到远程\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e如果安装了VSCode，其实和直接用终端是一样的，在菜单栏的Terminal下新建一个终端，在这个终端内执行上述代码，如果在第4步出现”Enter password to unlock the private key”时，输入创建SSH时第2步的密码即可，只需一次，下次就不用再输入密码了。点击File的Open Folder打开本地repo文件夹。点击VSCode左边栏的Explorer可以在编辑器下修改代码。切换到左边栏的Source Control可以进行Git相关操作，修改的文件右边会出现一个M，点击这个M会出现diff视图；Source Control左边的右上角有三个点，点击这个按钮会出现很多Git操作，包括commit、push等，其实相当于调用上述代码，效果是一样的。\u003c/p\u003e\n\u003cp\u003eVSCode快捷键Ctrl+Shift+P会出现命令窗口，在里面输入commit、push等会出现相关操作的，能起到一定的加速效果，当然也可以自定义快捷键。\u003c/p\u003e\n\u003cp\u003eHave Fun!\u003c/p\u003e","title":"Ubuntu下使用VSCode连接Github"},{"content":"最近导师给我批注文章，说我的Word文档的批注行距极大，从入学到现在一直都是如此，对他造成了极大的困扰，希望我能解决这个问题。\n但是我自己用Word2016查看导师的批注，看不出行距极大的问题，显示完全是正常的。后来猜测导师用的是旧版的Word2010，于是在虚拟机中安装了Word2010，进行测试。\n经过长时间的Debug，终于发现问题所在。Word针对批注有一个默认的样式，为“批注文字（使用前隐藏）”，可以点击样式右下角的箭头，或者直接按快捷键Ctrl+Shift+Alt+S调出样式窗口。然后点击底部的管理样式就可以看到所有的样式了。有意思的是，批注文字样式默认是隐藏的，所以在下图的样式列表中是找不到这个样式的。\n找到批注文字样式，点击修改，在弹出的窗口中点击左下角的格式，选择段落，就可以看到批注的默认格式了。段落格式看不出什么异常，Word2016和Word2010的批注段落格式都是一样的，其中“如果定义了文档网格，则对齐到网格”都是默认选中的。\n有意思的是，Word2016和Word2010对网格的默认设置却不一样。在布局、页面设置中点击右下角的箭头，打开页面设置对话框。切换到文档网格选项卡。\nWord2016默认指定了行网格，而Word2010默认却是无网格。因为批注文字样式中选中了“如果定义了文档网格，则对齐到网格”这个选项，Word2016默认指定了行网格，所以批注文字会对齐到行网格，导致行间距太大，Word2010默认没有指定任何网格，所以其批注文字的行间距是正常。\nWord2016文档网格，默认指定了行网格\nWord2010文档网格，默认无网格\n解决办法就是，修改Word2016的设置，选中“无网格”，并设置为默认值，这样以后新建的Word文档默认都是无网格，批注的行间距也就正常了。\n","permalink":"http://localhost:1313/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/","summary":"\u003cp\u003e最近导师给我批注文章，说我的Word文档的批注行距极大，从入学到现在一直都是如此，对他造成了极大的困扰，希望我能解决这个问题。\u003c/p\u003e\n\u003cp\u003e但是我自己用Word2016查看导师的批注，看不出行距极大的问题，显示完全是正常的。后来猜测导师用的是旧版的Word2010，于是在虚拟机中安装了Word2010，进行测试。\u003c/p\u003e\n\u003cp\u003e经过长时间的Debug，终于发现问题所在。Word针对批注有一个默认的样式，为“\u003cstrong\u003e批注文字（使用前隐藏）\u003c/strong\u003e”，可以点击样式右下角的箭头，或者直接按快捷键Ctrl+Shift+Alt+S调出样式窗口。然后点击底部的管理样式就可以看到所有的样式了。有意思的是，批注文字样式默认是隐藏的，所以在下图的样式列表中是找不到这个样式的。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/word2016-open-style.webp\"\u003e\u003c/p\u003e\n\u003cp\u003e找到批注文字样式，点击修改，在弹出的窗口中点击左下角的格式，选择段落，就可以看到批注的默认格式了。段落格式看不出什么异常，Word2016和Word2010的批注段落格式都是一样的，其中“如果定义了文档网格，则对齐到网格”都是默认选中的。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/word2016-comment-style.webp\"\u003e\u003c/p\u003e\n\u003cp\u003e有意思的是，Word2016和Word2010对网格的默认设置却不一样。在布局、页面设置中点击右下角的箭头，打开页面设置对话框。切换到文档网格选项卡。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/word2016-open-page-setting.webp\"\u003e\u003c/p\u003e\n\u003cp\u003eWord2016默认指定了行网格，而Word2010默认却是无网格。因为批注文字样式中选中了“\u003cstrong\u003e如果定义了文档网格，则对齐到网格\u003c/strong\u003e”这个选项，Word2016默认指定了行网格，所以批注文字会对齐到行网格，导致行间距太大，Word2010默认没有指定任何网格，所以其批注文字的行间距是正常。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/word2016-doc-grid.webp\"\u003e\nWord2016文档网格，默认指定了行网格\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2018-09-01-the-line-space-problem-of-annotation-in-word-2016/word2010-doc-grid.webp\"\u003e\nWord2010文档网格，默认无网格\u003c/p\u003e\n\u003cp\u003e解决办法就是，修改Word2016的设置，选中“无网格”，并设置为默认值，这样以后新建的Word文档默认都是无网格，批注的行间距也就正常了。\u003c/p\u003e","title":"Word2016批注行距太大的问题"},{"content":"2017年是目前为止最折腾的一年。\n科研工作 今年的科研任务包括两个方面，一方面是pLink2软件的完善和发布，另一方面是完成pLink2文章初稿。软件方面，上半年忙着修改完善算法，下半年重写界面代码，修改各种bug并反复测试。最终在12月31日晚通过邮件正式发布。文章方面，元旦软件发布之后，一月份抽两周时间完成了初稿，算是自己的第一篇全英文初稿，正文加附录接近一万词。不过因为有一个实验结果不太好，还需要接着完善算法，文字也很稚嫩，需要反复修改。希望能在2018年上半年投出去。\n个人提高 上半年忙着找工作，一直在刷题看书攒面经，经过自己的不懈努力，收获了微软、百度、头条、Face++等心仪的Offer。在确定自己找工作没问题之后，被李沐的博客“忽悠”，下半年华丽丽的转博了，赶在了2015级最后一次转博前夕。\n因为找工作，看了7本专业书；因为有Kindle，以及忙里偷闲，竟也看了9本非专业书。元旦完成软件发布任务之后，奖励自己去电影院连看了一整天的电影，这种休假方式也是蛮奇葩的，今年累计去电影院看的电影数已经达到16了。上半年和欣欣看了一场“OFO轻睐演唱会”，第一次参加演唱会，现场的感觉和看视频不一样，气氛很热烈，大家都很兴奋，会情不自禁跟着一起唱。国庆第一次一个人远行，去了郑州、登封和杭州，加上12月份参加厦门质谱会议，今年去的第三个城市已经达到了4个。\n运动方面。在两个师兄的帮助以及室友的陪练下，真的学会了蛙泳，今年8月份还拿到了深水证，为此贺老师每个月奖励我100块钱，简单粗暴又有效的奖励机制:-)。临近年底的时候，心血来潮，准备提高乒乓球技术，混入了所里的乒乓球圈子，拜师王老师门下，经过训练以及看视频学习，竟偶尔能赢浩哥了，今年再接再厉。\n今年约好了和哥一起回家过年，给家里买了一台55寸的乐视超级电视，给父母的红包也涨了不少。总体来说，家里在一年一年变好。\n对照年初定的目标，\n发表pLink 2文章。只完成了初稿。 至少完成毕业工作的80%。转博了。 刷完LeetCode所有简单题和中等题，找工作之前最好刷完两遍。完成。 找到一个满意的工作。完成，具体请看https://bitjoy.net/posts/2018-02-04-2018-campus-recruiting/。 读10本书。完成16：《编程珠玑》、《C++ Primer》、《程序员面试笔试宝典》、《STL源码剖析》、《剑指Offer》、《深度探索C++对象模型》、《编程之美》、《人间失格》、《枪炮、病菌与钢铁》、《杀死一只知更鸟》、《别闹了，费曼先生》、《月亮与六便士》、《突破极限》、《解忧杂货店》、《北京折叠》、《以色列，一个国家的诞生123》。 去电影院看10场电影。完成16：《爱乐之城》、《摔跤吧！爸爸》、《银河护卫队2》、《战狼2》、《敦刻尔克》、《羞羞的铁拳》、《看不见的客人》、《东方快车谋杀案》、《寻梦环游记》、《帕丁顿熊2》、《芳华》、《解忧杂货店》、《前任3：再见前任》、《无问西东》、《南极之恋》、《太空救援》。 看一场话剧（音乐会、歌剧等都可以）。完成。2017年7月2日，北京工人体育场，OFO轻睐演唱会。 学会游泳。完成，学会蛙泳和踩水，年中拿到深水证。 去第三个城市。完成，国庆去了郑州、登封、杭州，12月份第一次坐飞机去了厦门。 总体来说，2017年的目标都完成了，而且好几项是超额完成。2018年目标如下：\n发表pLink2文章，科研的重中之重。 开展新课题，或SUMO或深度学习。 完成博士课程的学习。 读10本书。 去电影院看10场电影。 去北京公园年票范围中的19家公园。 看一场话剧（音乐会、歌剧等都可以）。 学会自由泳。 乒乓球稳赢。 去第三个城市。 机动目标，高温假带父母来北京玩。 突然发现每年的目标都差不多，读万卷书和行万里路是每年都有的保留项目。\n找工作，分手，转博，软件发布，文章写作构成了我2017年的365天，喜忧参半，在跌跌撞撞中前行。2018年要保持一如既往的冲劲，打赢转博之后的第一仗！\n","permalink":"http://localhost:1313/posts/2018-02-18-summary-of-2017/","summary":"\u003cp\u003e2017年是目前为止最折腾的一年。\u003c/p\u003e\n\u003ch1 id=\"科研工作\"\u003e科研工作\u003c/h1\u003e\n\u003cp\u003e今年的科研任务包括两个方面，一方面是pLink2软件的完善和发布，另一方面是完成pLink2文章初稿。软件方面，上半年忙着修改完善算法，下半年重写界面代码，修改各种bug并反复测试。最终在12月31日晚通过邮件正式发布。文章方面，元旦软件发布之后，一月份抽两周时间完成了初稿，算是自己的第一篇全英文初稿，正文加附录接近一万词。不过因为有一个实验结果不太好，还需要接着完善算法，文字也很稚嫩，需要反复修改。希望能在2018年上半年投出去。\u003c/p\u003e\n\u003ch1 id=\"个人提高\"\u003e个人提高\u003c/h1\u003e\n\u003cp\u003e上半年忙着找工作，一直在刷题看书攒面经，经过自己的不懈努力，收获了微软、百度、头条、Face++等心仪的Offer。在确定自己找工作没问题之后，被李沐的博客“忽悠”，下半年华丽丽的转博了，赶在了2015级最后一次转博前夕。\u003c/p\u003e\n\u003cp\u003e因为找工作，看了7本专业书；因为有Kindle，以及忙里偷闲，竟也看了9本非专业书。元旦完成软件发布任务之后，奖励自己去电影院连看了一整天的电影，这种休假方式也是蛮奇葩的，今年累计去电影院看的电影数已经达到16了。上半年和欣欣看了一场“OFO轻睐演唱会”，第一次参加演唱会，现场的感觉和看视频不一样，气氛很热烈，大家都很兴奋，会情不自禁跟着一起唱。国庆第一次一个人远行，去了郑州、登封和杭州，加上12月份参加厦门质谱会议，今年去的第三个城市已经达到了4个。\u003c/p\u003e\n\u003cp\u003e运动方面。在两个师兄的帮助以及室友的陪练下，真的学会了蛙泳，今年8月份还拿到了深水证，为此贺老师每个月奖励我100块钱，简单粗暴又有效的奖励机制:-)。临近年底的时候，心血来潮，准备提高乒乓球技术，混入了所里的乒乓球圈子，拜师王老师门下，经过训练以及看视频学习，竟偶尔能赢浩哥了，今年再接再厉。\u003c/p\u003e\n\u003cp\u003e今年约好了和哥一起回家过年，给家里买了一台55寸的乐视超级电视，给父母的红包也涨了不少。总体来说，家里在一年一年变好。\u003c/p\u003e\n\u003cp\u003e对照年初定的目标，\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e发表pLink 2文章。只完成了初稿。\u003c/li\u003e\n\u003cli\u003e至少完成毕业工作的80%。转博了。\u003c/li\u003e\n\u003cli\u003e刷完LeetCode所有简单题和中等题，找工作之前最好刷完两遍。完成。\u003c/li\u003e\n\u003cli\u003e找到一个满意的工作。完成，具体请看\u003ca href=\"https://bitjoy.net/posts/2018-02-04-2018-campus-recruiting/\"\u003ehttps://bitjoy.net/posts/2018-02-04-2018-campus-recruiting/\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e读10本书。完成16：《编程珠玑》、《C++ Primer》、《程序员面试笔试宝典》、《STL源码剖析》、《剑指Offer》、《深度探索C++对象模型》、《编程之美》、《人间失格》、《枪炮、病菌与钢铁》、《杀死一只知更鸟》、《别闹了，费曼先生》、《月亮与六便士》、《突破极限》、《解忧杂货店》、《北京折叠》、《以色列，一个国家的诞生123》。\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影。完成16：《爱乐之城》、《摔跤吧！爸爸》、《银河护卫队2》、《战狼2》、《敦刻尔克》、《羞羞的铁拳》、《看不见的客人》、《东方快车谋杀案》、《寻梦环游记》、《帕丁顿熊2》、《芳华》、《解忧杂货店》、《前任3：再见前任》、《无问西东》、《南极之恋》、《太空救援》。\u003c/li\u003e\n\u003cli\u003e看一场话剧（音乐会、歌剧等都可以）。完成。2017年7月2日，北京工人体育场，OFO轻睐演唱会。\u003c/li\u003e\n\u003cli\u003e学会游泳。完成，学会蛙泳和踩水，年中拿到深水证。\u003c/li\u003e\n\u003cli\u003e去第三个城市。完成，国庆去了郑州、登封、杭州，12月份第一次坐飞机去了厦门。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e总体来说，2017年的目标都完成了，而且好几项是超额完成。2018年目标如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e发表pLink2文章，科研的重中之重。\u003c/li\u003e\n\u003cli\u003e开展新课题，或SUMO或深度学习。\u003c/li\u003e\n\u003cli\u003e完成博士课程的学习。\u003c/li\u003e\n\u003cli\u003e读10本书。\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影。\u003c/li\u003e\n\u003cli\u003e去北京公园年票范围中的19家公园。\u003c/li\u003e\n\u003cli\u003e看一场话剧（音乐会、歌剧等都可以）。\u003c/li\u003e\n\u003cli\u003e学会自由泳。\u003c/li\u003e\n\u003cli\u003e乒乓球稳赢。\u003c/li\u003e\n\u003cli\u003e去第三个城市。\u003c/li\u003e\n\u003cli\u003e机动目标，高温假带父母来北京玩。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e突然发现每年的目标都差不多，读万卷书和行万里路是每年都有的保留项目。\u003c/p\u003e\n\u003cp\u003e找工作，分手，转博，软件发布，文章写作构成了我2017年的365天，喜忧参半，在跌跌撞撞中前行。2018年要保持一如既往的冲劲，打赢转博之后的第一仗！\u003c/p\u003e","title":"2017年终总结"},{"content":"证明你能做一件事的最好方法就是做成这件事！\n从保研开始，我根本就没打算读博士，心里想的是，好好学习，认真刷题，顺利毕业，高薪就业。\n2016年底，也就是进实验室半年之后，贺老师开始“怂恿”我读博：“贫寒人家子弟，有个高学历，在这个拼爹的时代，更容易出人头地。年轻时多读点儿难读的书，也会更好地在未来人工智能时代生存。”我不为所动。\n2017年春节在家，疯狂刷题看书，为开学后的实习面试以及半年之后的校招面试准备着。\n2017年2月份，开年工作计划会，老师说只要我愿意读博，博士的三个课题都帮我规划好了，要知道我们实验室没有哪个博士生是在三年级之前就确定方向的，大家都是摸着石头过河。\n我还是不为所动，疯狂刷题看书，攒实习面经。\n到了8月，老师最后来信希望我能认真考虑一下读博的事情：“pFind+NIBS是难得的良性成长环境，换一个新环境未见得能成长像现在这么快”。甚至把我的博士女朋友都搬出来了。我跟欣欣聊了聊，思想开始有点动摇了。但是那段时间忙于找工作，没空想太多。\n9月10日教师节，我和欣欣分手，与工作无关，与硕士博士无关。\n后来有一天，当我在对比百度凤巢和微软的Offer时，偶然看到凤巢前辈李沐博士写的一篇博客：《博士这五年》。李沐是上海交大ACM班的，毕业之后去了百度凤巢，但是后来毅然辞职去CMU攻读博士学位。博士五年期间，他不但发表了多篇很牛的paper，而且亲手写了一个类似TensorFlow的深度学习平台MXNet，MXNet现已加入Apache家族，并被Amazon选为官方深度学习框架。他博士答辩的评委有来自Google, Amazon, Apple的AI负责人，阵容非常强大。最后，李沐光荣毕业，加入Amazon。\n这篇博客的结尾在谈到如何选择工作和读博时有一段话，令我印象深刻：“不过我觉得还是会选择读博。赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次……更重要的是理想和情怀。人一生要工作五十年，为什么不花五年来追求下理想和情怀呢？”。\n看完这篇博客之后，那一整天，我都没心思上班。校招至今，拿了一堆Offer，不能说轻而易举，但是我现在知道拿Offer就跟考试一样，只要准备好，不会差到哪里去。甚至可以说，校招面试比回答贺老师的问题要简单多了。未来可以工作的时间还很长，何不花几年时间来挑战自己呢。当你在面对两难选择时，选择更难的那个，日后你会感谢当初自己的选择。\n人拼了命的工作，到底是为了什么，除了更早的成为工厂里的螺丝钉，成为房奴、孩奴，你还能得到什么。是的，提前三年工作，你也许能赚到100万，能积累更丰富的工作经验和更广阔的人脉。但是，这又怎样，这些东西该来的肯定会来，博士毕业之后，我同样能得到，只是比大多数人晚了一点。\n与其早早毕业，成为一个nobody，还不如再潜心修炼几年，成为somebody。博士这几年，我能得到更加系统全面的训练，pFind+NIBS的良性科研环境，也不可多得。曾经在知乎上看到有个人回答为什么选择读博：“在一个很安全的环境里，父母健在，自己不用操心赚钱养家；有老板给你提供指导、资金；你可以安心研究自己感兴趣的问题；发表的文章也将署上自己的名字，流传后世；习得的技能也将转化为自己的能力；获得的博士学位也将是自己的荣誉…”，这么好的事情，为什么不去做呢？\n那一天之后，我内心几乎就决定要转博了。\n然而，话虽如此，想到要放弃到手的大Offer，继续在实验室里待至少三年；想到免不了要经历大多数博士师兄师姐们经历过的痛苦日子；想到彼时同学们都已经年入x万，有房有车，说不定我日后的面试官就是现在的同学；想到我最亲密的女朋友对我的不信任；想到自己的苦衷无处倾诉…\n那段日子过得很艰难，也许是我到目前为止最低谷的时期。左手是各大互联网公司的Offer，立即可以实现我很多的愿望；右手是若干年未知的博士磨砺。虽然内心知道往右走是正确的，但还是下不了这个狠心。脑海中的两个小人，吵个不停。\n期间和很多在读的、已毕业的博士师兄师姐们聊过，也和很多公司的面试官聊过，当然也和老师父母聊过。得到的回答无外乎三种：读、不读，根据自己的情况决定。这些谈话更像是换了种方式的倾述，我已经记不清具体的内容了，只知道，我做选择的决心越来越坚定了。\n之后恰逢十一长假，给自己放了一个长长的假。规划去了郑州、登封、杭州。了却夙愿，重新开始。\n10月9日，长假结束。我给老师发了一封邮件：“贺老师，您好。非常感谢您的信任和等待，我决定读博了！这将是我人生二十多年来所作的第一个重大决定，我接受挑战！”\n人这一生，说长也长，说短也短，去做你认为对的事情吧，去追求你想要的生活。愿我们都能绽放美丽，不负芳华！\n","permalink":"http://localhost:1313/posts/2018-02-14-why-phd/","summary":"\u003cp\u003e证明你能做一件事的最好方法就是做成这件事！\u003c/p\u003e\n\u003cp\u003e从保研开始，我根本就没打算读博士，心里想的是，好好学习，认真刷题，顺利毕业，高薪就业。\u003c/p\u003e\n\u003cp\u003e2016年底，也就是进实验室半年之后，贺老师开始“怂恿”我读博：“贫寒人家子弟，有个高学历，在这个拼爹的时代，更容易出人头地。年轻时多读点儿难读的书，也会更好地在未来人工智能时代生存。”我不为所动。\u003c/p\u003e\n\u003cp\u003e2017年春节在家，\u003ca href=\"http://code.bitjoy.net/\"\u003e疯狂刷题看书\u003c/a\u003e，为开学后的实习面试以及半年之后的校招面试准备着。\u003c/p\u003e\n\u003cp\u003e2017年2月份，开年工作计划会，老师说只要我愿意读博，博士的三个课题都帮我规划好了，要知道我们实验室没有哪个博士生是在三年级之前就确定方向的，大家都是摸着石头过河。\u003c/p\u003e\n\u003cp\u003e我还是不为所动，疯狂刷题看书，攒实习面经。\u003c/p\u003e\n\u003cp\u003e到了8月，老师最后来信希望我能认真考虑一下读博的事情：“pFind+NIBS是难得的良性成长环境，换一个新环境未见得能成长像现在这么快”。甚至把我的博士女朋友都搬出来了。我跟欣欣聊了聊，思想开始有点动摇了。但是那段时间忙于找工作，没空想太多。\u003c/p\u003e\n\u003cp\u003e9月10日教师节，我和欣欣分手，与工作无关，与硕士博士无关。\u003c/p\u003e\n\u003cp\u003e后来有一天，当我在对比百度凤巢和微软的Offer时，偶然看到凤巢前辈李沐博士写的一篇博客：\u003ca href=\"https://zhuanlan.zhihu.com/p/25099638\"\u003e《博士这五年》\u003c/a\u003e。李沐是上海交大ACM班的，毕业之后去了百度凤巢，但是后来毅然辞职去CMU攻读博士学位。博士五年期间，他不但发表了多篇很牛的paper，而且亲手写了一个类似TensorFlow的深度学习平台MXNet，MXNet现已加入Apache家族，并被Amazon选为官方深度学习框架。他博士答辩的评委有来自Google, Amazon, Apple的AI负责人，阵容非常强大。最后，李沐光荣毕业，加入Amazon。\u003c/p\u003e\n\u003cp\u003e这篇博客的结尾在谈到如何选择工作和读博时有一段话，令我印象深刻：“不过我觉得还是会选择读博。赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次……更重要的是理想和情怀。人一生要工作五十年，为什么不花五年来追求下理想和情怀呢？”。\u003c/p\u003e\n\u003cp\u003e看完这篇博客之后，那一整天，我都没心思上班。校招至今，拿了一堆Offer，不能说轻而易举，但是我现在知道拿Offer就跟考试一样，只要准备好，不会差到哪里去。甚至可以说，校招面试比回答贺老师的问题要简单多了。未来可以工作的时间还很长，何不花几年时间来挑战自己呢。当你在面对两难选择时，选择更难的那个，日后你会感谢当初自己的选择。\u003c/p\u003e\n\u003cp\u003e人拼了命的工作，到底是为了什么，除了更早的成为工厂里的螺丝钉，成为房奴、孩奴，你还能得到什么。是的，提前三年工作，你也许能赚到100万，能积累更丰富的工作经验和更广阔的人脉。但是，这又怎样，这些东西该来的肯定会来，博士毕业之后，我同样能得到，只是比大多数人晚了一点。\u003c/p\u003e\n\u003cp\u003e与其早早毕业，成为一个nobody，还不如再潜心修炼几年，成为somebody。博士这几年，我能得到更加系统全面的训练，pFind+NIBS的良性科研环境，也不可多得。曾经在知乎上看到有个人回答为什么选择读博：“在一个很安全的环境里，父母健在，自己不用操心赚钱养家；有老板给你提供指导、资金；你可以安心研究自己感兴趣的问题；发表的文章也将署上自己的名字，流传后世；习得的技能也将转化为自己的能力；获得的博士学位也将是自己的荣誉…”，这么好的事情，为什么不去做呢？\u003c/p\u003e\n\u003cp\u003e那一天之后，我内心几乎就决定要转博了。\u003c/p\u003e\n\u003cp\u003e然而，话虽如此，想到要放弃到手的大Offer，继续在实验室里待至少三年；想到免不了要经历大多数博士师兄师姐们经历过的痛苦日子；想到彼时同学们都已经年入x万，有房有车，说不定我日后的面试官就是现在的同学；想到我最亲密的女朋友对我的不信任；想到自己的苦衷无处倾诉…\u003c/p\u003e\n\u003cp\u003e那段日子过得很艰难，也许是我到目前为止最低谷的时期。左手是各大互联网公司的Offer，立即可以实现我很多的愿望；右手是若干年未知的博士磨砺。虽然内心知道往右走是正确的，但还是下不了这个狠心。脑海中的两个小人，吵个不停。\u003c/p\u003e\n\u003cp\u003e期间和很多在读的、已毕业的博士师兄师姐们聊过，也和很多公司的面试官聊过，当然也和老师父母聊过。得到的回答无外乎三种：读、不读，根据自己的情况决定。这些谈话更像是换了种方式的倾述，我已经记不清具体的内容了，只知道，我做选择的决心越来越坚定了。\u003c/p\u003e\n\u003cp\u003e之后恰逢十一长假，给自己放了一个长长的假。规划去了郑州、登封、杭州。了却夙愿，重新开始。\u003c/p\u003e\n\u003cp\u003e10月9日，长假结束。我给老师发了一封邮件：“贺老师，您好。非常感谢您的信任和等待，我决定读博了！这将是我人生二十多年来所作的第一个重大决定，我接受挑战！”\u003c/p\u003e\n\u003cp\u003e人这一生，说长也长，说短也短，去做你认为对的事情吧，去追求你想要的生活。愿我们都能绽放美丽，不负芳华！\u003c/p\u003e","title":"一念成博"},{"content":"作为一名曾经的2018届硕士毕业生，为找工作忙活了大半年，最终收获了微软、百度、头条、Face++等十多个Offer。校招季对我来说，在9月份就差不多结束了。本来很早就酝酿了这篇博客，但是由于之后一系列事情，耽搁至今，趁着提交完年终技术报告，回家之前，把这段经历记录一下。\n首先介绍一些计算机专业校招的基本情况。由于移动互联网、人工智能等浪潮的兴起，计算机专业的毕业生就业前景可谓一片大好，尤其是对于名校毕业基础扎实的同学，应届生薪资倒挂老员工的事情几乎每年都在上演。所以首先祝贺所有CSer，这是属于你们的时代，各行各业都有属于你的舞台，尽情去施展才华吧。\n本专业的毕业生就业去向主要有这么几类：国内互联网公司、国外互联网公司（外企）、国企。其中国内互联网公司又分大厂和新兴创业公司，大厂如BAT、网易、360、京东、华为等，创业公司主要集中在人工智能这块，如商汤科技、Face++、头条、滴滴等。外企大概也分为两类，一类是来自日本的企业，比如Indeed、WAP；另一类是来自美国的企业，比如Microsoft、Google、Hulu、FreeWheel、Amazon等。国企是指传统的国有企业里面的IT部门，比如各大银行、证监会等。这几类公司的校招时间刚好都错开了，一般来说，日企来华校招是最早的，大概每年5月份就来了；接着是国内互联网公司的内推季，大概在7~8月份；进入9月份之后，就是国内互联网公司的正式校招了；美国的企业大概会在9~10月份启动校招，有可能一直持续到11月份；国企就比较晚了，听说最晚能持续到第二年3、4月份的。这种安排，对我们来说，既是好事，也是坏事，好处就是对于纠结的同学，可以每种类型的公司都试一试，多拿几个offer，最后根据自己的情况决定去哪里；坏处就是持续时间真的很长，面到最后，身心俱疲，需要做好心理准备。\n我经历过的面试主要是国内互联网和部分外企的研发岗，下面也将主要介绍这两类企业，按时间先后顺序。\nIndeed（FAILED） Indeed是全球最大的招聘信息搜索引擎公司，总部位于美国德州的首府奥斯汀，2012年被日本的Recruit收购，然后成立了Indeed Tokyo办公室。本文提到的Indeed都是指Indeed Tokyo，即拿到offer的话，要求去东京工作，不过可以轮转去奥斯汀总部。\nIndeed是最早开始校招的，当国内公司还在实习招聘的时候，它就跑来进行校招了。我参加了2017年4月17日在北大举办的校园宣讲会，介绍了Indeed的基本情况和招聘流程，以及抽奖机械硬盘等。Indeed的办公室很有科技范，其工位设置尤为吸引人，是六边形的环形设计，每个人既可以专注于自己的工作，又便于和组内同事讨论。宣讲的人包括HR和从该校毕业的学长，这个HR是中国人，后面有一轮HR面也是他，大家可以多多留意。\nIndeed最大的吸引力是，700万~800万日元的年薪，折合人民币大概四五十万吧，这样诱人的薪资，让每个路过其宣传海报的同学都驻足观看。当然其面试难度也不小，首先有一轮在线笔试，这个在线笔试有三次机会，只要有一次全部AC，就算通过。在线笔试题一共4道，难度比LeetCode稍大，但是一定提醒大家，他们家的题都有数据范围，而且范围很小，前3题用暴力解法几乎都可以过，所以一定要先试试暴力求解，不行再想DP。\n通过在线笔试之后，会有一个大约30分钟的HR面，就是上面提到的来华宣讲的中国人。这个面试严格来说是Case interview，通过Skype进行，主要考察逻辑逻辑思维能力和英文口语能力。由于是中国人，所以刚开始会用中文介绍下题意，然后让你思考一下，最后用英文给出解答。我当时的题目是，如何把微信支付的流水从xxx提高到yyy。由于提前非常认真的看了http://www.caseinterview.com/的视频教学，学到很多，这次HR面顺利通过。\n通过HR面之后，还有一轮Skype技术面，是从Indeed Tokyo那边打过来的，需要解算法题，通常是一题+好几个follow up。不过很多是往年的原题，在一亩三分地上都有，大家可以仔细在上面看看。我当时被问到的题是之前准备过的，但是没答好，比较突兀的给出了最优解，面试官可能觉得我是背答案了吧。。。\n如果这轮Skype技术面也通过的话，就可以免费飞到东京参加on-site面了，听说on-site面是3轮面试，一整个上午或一整个下午，几乎也是原题，可以在一亩三分地上找到。\n说来也奇怪，Indeed每年的面试题都差不多，但通过面试的人总是寥寥无几，这才是高级的面试官，考察的是应聘者的解题思路，而不是答案。\nIndeed Tokyo很不错，如果能拿到Offer，说明你很优秀，离人生巅峰也不远了。\nWorks Applications（FAILED） Works Applications简称WAP，是一家日本的ERP软件开发公司，ERP全称是Enterprise Resource Planning，简单理解就是面向企业用户的各种管理系统。WAP是正宗的日本企业，其风格和Indeed Tokyo截然不同，上班要求穿正装，估计各种行为规范也不少，但是钱也不少，折合人民币估计也有四十多万吧。WAP虽然总部在东京，但它在上海有办公室，国内校招生基本上都在上海办公。\nWAP的招聘流程和Indeed很像，首先会有一个宣讲会，建议大家都参加，类似于报名考试。宣讲会之后会收到一个在线笔试的链接，要求3天之内做完2道编程题，题目比较简单。在线笔试通过之后，有一轮在线技术面试，使用的是牛客网平台，要求视频面时不能离开面试页面查资料。视频面也比较简单，大概Leetcode的easy~medium题。\n对于WAP，前期的在线面试只是开胃小菜，好戏还在后头。通过两轮在线面试之后，会邀请去某个酒店现场面试。现场面试有三轮，全程英文，一般是先来段英文自我介绍，然后开始做题。比较搞笑的是，见到一面面试官时，被问到感觉如何，我说good，然后面试官说别人都是很nervous，我居然说good，感觉要被自己坑了，还好出的题都会做。前两面都不难，大概LeetCode中等题，第三面感觉是一个boss，已经不考LeetCode算法题了，考类似智力题的东西，比如有人被考到囚犯和帽子颜色的问题，我被问到的是怎样实现求两数的平均值，常规的(a+b)/2有可能导致a+b溢出，我想了很多方法，面试官都不满意，后来发现《程序员面试笔试宝典》上有。求平均值的问题可以先转换为求和，用位运算是a+b=((a\u0026amp;b)\u0026laquo;1)+(a^b)，a+b就是按位加，对应二进制也是按位加，要进位的情况就是对应位都为1，所以先用a\u0026amp;b找出需要进位的位，然后左移1位表示进位；还有些位可能只有一个1或者没有1，这部分加和的结果可以用异或表示，即a^b，所以a+b=((a\u0026amp;b)\u0026laquo;1)+(a^b)。那么，求平均值就是(a+b)/2=(a\u0026amp;b)+((a^b)\u0026raquo;1)。要是早点看了《程序员面试笔试宝典》，我估计也能拿到WAP的Offer了。\n三轮技术面之后，会有一个HR面，听说如果前面的技术面过关的话，HR面会遇到日本boss，直接发放Offer；否则是一个中国人，寒暄几句之后，被告知技术面没有通过，但是可以参加暑期为期一周的实习活动，实习通过的话，也可以获得Offer。每年的实习主题都差不多，比如做一个酒店管理系统、电影院管理系统之类的，由于我觉得时间代价太高了，没有参加暑期实习。\n虽然WAP的工资很好，但是要想拿Offer，比Indeed简单，LeetCode中等题足够，好好准备一下现场第三面。另外，即使拿到Offer，也要考虑一下工作内容是否符合自己的兴趣，毕竟ERP和当前火热的AI相比还是太古老了，而且穿正装上班估计也只此一家了。\n深信服（OFFER） 深信服公司是面向企业的安全与云计算解决方案供应商，可以理解为企业版360。听说创始人是从华为跳出来的，公司整体风格和华为很像，从宣讲会上还听说这家薪资不错，尤其是博士，宣称比BAT华为都高。\n深信服的提前批招聘也很早，7月初就来所里宣讲了。首先有两轮电话技术面试，面试官都会提前短信约时间，给人感觉不错。电话面试的内容比较广，网络、操作系统、C++、算法等都会问到。面试官手里应该有一个问题清单，挨个问下去，不会的跳过，节奏比较快。所以面试深信服之前，要好好复习计算机基础，尤其是网络相关的，因为其主营业务和网络密切相关。\n能通过前两轮电话面试的，基础都很扎实，接下来会邀请去参加他们的星云计划暑期夏令营。原本夏令营是要去深圳总部的，但是北京的很多同学都没时间，于是临时把夏令营分成了南北两波，北京的同学被安排在九华山庄度假村。在这里会听好多深信服的介绍和讲座，其中有一个清华的博士，在校期间发过很牛的Paper，自称是那一届的全国博士Top5，谈了很多选择去深信服的理由，核心思想是博士在深信服有很大的自主权，可以试错，主导一些项目，而且薪资估计真的很高。最后会有一个Boss面，主要是问项目经历，Boss是连夜赶来北京的，面试的时候哈欠连天，也没问什么实质性的问题。去的人应该都过了。其实这个夏令营主要是去体验生活的:)\n最后的Offer，中规中矩，薪资并没有想象的高，也不是自己喜欢做的事情，拒。\n华为（OFFER） 华为就不用介绍了，早年凭借电信网络产品赚得盆满钵满，近几年的智能手机业务更是冲出国门走向世界，真的是我国民族企业的榜样。华为严格来说是一个制造商企业，不算互联网企业，而且其招聘比较看重学历，给人感觉有点像国企。但是毕竟其产品都是计算机相关设备，对计算机专业人才的需求还是很旺盛的。华为的另一大特点是有钱，并且舍得给员工砸钱，我上一届的硕士师兄去了华为，工资碾压BAT，成功倒挂一大批老员工。仔细看看近几年各大重点高校的毕业生去向，去华为的占了很大比例，如果你想快速积累财富，又能吃苦，去华为能很好的满足你的要求。\n因为师兄去了华为，3月份收到内部通知说可以提前批内推了，于是把简历给了师兄进行内推。7月初的时候要求做一个性格测试，华为特色，其他公司都没有这一环节，据说是在筛选符合华为价值观的同学。7月22日参加华为提前批优招，真的是优招，去的大部分是清北中科院的，猜测还要求本科是985高校。 优招面试很简单，因为是业务面试，主要问问项目，面试官是那种成功人士风格的Boss。二面就不问技术了，会问周围同学老师是怎样评价自己的，科研压力大吗，想去哪工作之类的，类似的问题也是在衡量应聘者和华为公司的match程度。我应该是非常match的，面试结束的时候，Boss还跟我握手了！\n优招面试结束后没几天，会有一个在线笔试，编程题，三道题，最好全AC，我是前两题AC，第三题过了80%。至此，华为所有的笔试面试都结束了。但是直到9月初，才被再次邀请去华为北研参加Offer沟通会，这个会和大一刚入学参加各大社团的招新差不多，华为的各大部门开始抢人，我去了2012实验室中央软件院。\n四维图新（OFFER） 华为虽然是最早面完的，但是Offer迟迟没有下来，国内其他互联网公司又还没开始面试，心急之下，看到四维图新在招聘C++研发工程师，做地图搜索的，和自己有点关系。网上查了一下，发现还是腾讯地图的数据供应商，而且还是母校武大测绘学院有很紧密的合作，应该是个靠谱的公司。\n跑去面试，可能是公司比较小，面试流程还很原始，直接在接待室问了我几个问题，有些题目有一定难度，连红黑树都被问到了。然后被直接拉去工位，打开VS，开始编程，所幸全部AC。等了一会，直接HR面，拿到普通OFFER。我说想申请SP，HR说下周再来一轮Boss面吧。于是下周又跑去Boss面，Boss果然是Boss，气场就不一样，问题也很灵活，都是他们地图搜索开发过程中的实际问题，比如给定中国地图和一个GPS坐标，怎样快速定位这个坐标。类似的题目很有意思，虽然有一个题目回答得不是很好，但总体上聊得还比较开心。Boss面完之后，又一轮HR面，被告知拿到SP，而且如果能来实习，实习表现好，且能申请到户口指标，则有可能有户口。\n这个Offer是我校招季拿到的最早的Offer，薪资还不错，也算是稳住了阵脚。但是公司规模和名气都不算大，暂时拿来保底吧。\n百度（OFFER） 百度公司和我的专业是最匹配的了，国内做搜索技术最强的，非百度莫属。百度很人性化的一点是，公司不同部门的招聘分开进行，互不冲突，所以可以同时向不同部门投递简历。我就一口气投递了网页搜索部、商务搜索部和基础架构部。很幸运，同时拿到了这三个部门的提前批Offer。\n百度各部门的面试流程都很像，前两轮技术面，第三轮是Boss面或者HR面，越往后面试官的级别越高，第三面的面试官很可能就是你未来的Leader。第一轮面试比较基础，问一些网络、操作系统、C++的基础知识，然后写两道算法题。第二面先写两道算法题，然后问项目，项目问得很细，我的几个搜索引擎的项目，不但问了项目的实现细节，还问了很多follow up，比如，在实战场景中，千亿级别的数据量，怎样建索引使得查询更高效，如何实现怎个搜索过程等。因为面的是搜索部门，他们对相关的技术非常了解，不要抱任何侥幸心理，不会就说不会，切莫班门弄虎。第三面Boss面比较宏观，问问职业规划，如果面试官对你比较感兴趣，会主动介绍本部门的工作，凤巢的三面面试官甚至直接加了我的微信，受宠若惊啊。\n提前批面试完毕之后，9月初会有一个在线笔试，这个笔试也会刷人，所以不要掉以轻心，一定要认真准备。我当时是因为宿舍网络问题，被坑死了，那个在线笔试的系统也很变态，是个国外的系统，动不动就掉线，还只能登陆3次，超过自动退出。于是，很悲剧的3题只AC了2题。之后的几天，一直寝食难安，担心会栽在最后的笔试上。\n所幸，没过多久，收到了电话通知，笔试通过，需要确定部门，让我从三个部门中选一个。我当时那个纠结啊，网页搜索部、商务搜索部和基础架构部都是百度非常核心的部门，基架的低层技术很强，网搜是典型的文本检索，商搜是广告检索，网搜的三面面试官对我很好，时不时在微信上联系我；我和商搜的三面面试官也聊得很开心，商搜是百度最赚钱的部门，各种大牛非常之多。几番权衡之后，选择了商搜（凤巢），同时也拿到了SP。\nMicrosoft（OFFER） 微软是我面的唯一一个美国外企，面试流程数它最多了，前后经历了：1轮在线笔试+2轮skype面试+3轮on-site面试。\n首先，要拿到微软的skype面试机会就很难，需要通过Hihocoder的在线笔试。Hihocoder的题型和难度都相比于LeetCode复杂得多，我有一次很幸运的做到了前100名好像，拿到了skype面试机会。两轮skype面试难度也不小，比如search range，不但要求bug free，还要求你写测试用例；还比如对快排进行优化；手写堆排序；概率题等。微软的在线编程和skype面试和国内互联网不太一样，建议大家看看一亩三分地上的面经。\n过了两轮skype面之后，会被邀请去参加他们的探星夏令营，大概是在8月中旬，地点就在丹棱街的微软大厦。探星夏令营第一天是参观，我因为实验室忙就没去，第二天是三轮面试。我因为研究的方向是搜索引擎，所以被安排到bing组面试了。微软的现场面试难度也不小，不是像LeetCode那样直接叫你写个DP、排序什么的，而是给出一个实际问题，需要将其抽象成一个计算机问题，然后才是代码实现。前两面顺利通过。此时已经是下午4点多了，HR说三面安排不过来，让回去等。这一等直接从8月中旬等到9月初，期间还以为是二面挂了，“让回去等”是委婉的拒绝 ，看来微软还是说话算话的。三面是Boss面，和国内互联网比较像，面项目，问了很多细节，然后根据项目衍生出一个字符串压缩的题目，让写压缩和解压缩的代码。虽然写完了，但是没保证bug free，和面试官聊了聊可能的bug以及解决方案。\n过了大概一周，面试结果出来了，没有直接说给Offer，但是说面试反馈非常Positive，让加一个微信群。国庆节之前，收到微软HR电话，让我们稍安勿躁，国庆后会给正式Offer。后来直到10月31日，才收到HR的电话，正式通知Offer详情。接起电话，HR就说准备好纸笔，因为Offer内容比较多，然后就说了Package里面的各种福利，各种美金。总的来说，Package加起来在硕士里面应该是Top级别的，外企各种Balance，不加班，做的是自己喜欢的方向，而且还有可能拿户口，甚至人肉翻墙，可以说这个Offer是非常诱人的。\n京东（OFFER） 京东和百度类似，也是部门自己招聘，所以可以面多个部门。我面了AI和大数据部门以及商业推荐部门。印象比较深的是，原本面了一个做分布式的组，一面发现我更适合做搜索和架构，然后就被推荐到一个做京东智能音箱的组，这个组的三面面试官是从雅虎北研过来的，听口音感觉是广东人。因为我是做搜索，智能音箱里面也需要搜索，两个人聊得很不错，面试官当场就说帮我争取SP。\n面完技术面之后，过了大概一周，还要进行HR面。面试通知邮件也没说是哪个部门的。其中有个部门的HR面居然是群面，太奇葩了，也是我经历过的唯一一个群面。一屋子3个面试官，6个学生，就菜鸟网络和京东物流的对比展开讨论。首先自我介绍，有清华北大的，也有中科院各所的，还有北邮的。每次讨论我都是倒数几个发言的，对于这种压力测试，真是不适用。不过还好，HR后来跟我说我的表现不错。\nHR跟我谈薪资的时候，我客套说差不多就行，后来这两个部门都拿到了Offer，薪资还真的就是差不多，白菜价。因为已经有其他选择，也没有再争取SP。听别人说争取一下能有28左右？感觉京东的定价真是因人而异啊。后来有一天还收到三面面试官的电话，问我去向定哪了，真觉得有点愧对他。\n360（OFFER） 本来不打算面360，但是该公司在8月8号组织了一场中科院专场招聘会，在所有OFFER都还没有最终确定的情况下，去360逛一逛也没坏处。360的办公楼在酒仙桥，和MTK在一起，周围在施工，几乎没有吃饭的地方，给人的第一印象不是很好。10点钟到现场之后，已经人山人海了，和菜市场没什么区别，中间等待的时间都超过了面试时间。\n面试分为三轮，前两轮是技术面，第三轮是HR面。一面问了一些基础知识，写了一两个算法题。二面遇到了负责360地图开发的程序员，因为地图中也涉及POI搜索，聊得很欢。HR面被问到知道360的哪些产品，虽然我现在一个360的产品都不用了，但是知道的还是不少。\n面完之后，觉得Offer稳了，然后开心的回所里。第二天收到邮件通知，面试通过，还需参加一个在线笔试，类似于行测。做完之后，查看状态，被告知所有面试笔试都通过了，个人信息已经在Offer池中，但是没有正式Offer。Offer池是什么鬼，也就是没人要被扔到池子里等人捞呗。问了下其他人，大部分也是被扔到池子里了，只听说有一个人收到书面Offer。从此对360无感，无论是你们组织面试，还是我们参加面试，费了一天劲，硬是不发OFFER，坑爹。后来在10月16日，收到一封360的邮件，正式书面Offer，难道是被人相中捞起来了，真是无语。拒。\n阿里巴巴（FAILED） 阿里内推只能选一个部门，内推失败之后也只有一次校招机会，所以大家选部门一定要慎重，根据自己的实力和兴趣进行选择。当时群里给出了蚂蚁金服的内推消息之后，我第一时间就选择内推蚂蚁金服了。结果面了两轮之后查状态已经挂了，也没感觉面得差。可能是因为内推蚂蚁金服的人太多了，实力要求也很高，而且自己做搜索引擎的，和蚂蚁金服不太match。\n因为内推挂了之后，无法再面其他部门了。只能参加校招流程，校招在线笔试之后一直就没消息，状态也没更新，难度笔试挂了？\n总之，阿里的校招比较严格，选部门和岗位的时候一定要慎重。好的部门大家都想进，研发岗竞争也相对更激烈，算法岗还好一些，期间还有一个阿里面试官问我愿不愿意转算法，还加了我微信，说转算法希望挺大的，无奈自己没准备算法，婉拒。从校招体验来说，阿里是最差的，可能是仗着店大欺人，每次电话面试从来不提前预约时间，晚上十点钟还打电话过来面试。我室友拿到了淘宝的口头Offer，等了将近一个月也没发正式Offer，由于是公司座机，也联系不上。阿里的HC可能也比较少，我周围很少有听说拿到阿里Offer的。如果有同学拿到了，去杭州享受生活还是很不错的。\n腾讯（FAILED） 腾讯和阿里类似，一次内推机会，一次校招机会，而且只能选一个部门。开始两轮电话面试，和面试官解释了半天我的搜索引擎和索引，感觉面试官没理解，这也是电话面试的弊端。后来换了一个部门面试，说我的基础很扎实，过两天北京有一个专场面试，可以去参加现场面。于是就跑去一个酒店参加现场面了，遇到一个很奇葩的面试官，认为我不会网络编程就是基础差，直接把我轰走了，由于我手头已经有很多心仪的Offer，而且还另有打算，就淡定的和面试官辩论了起来，最后他居然失态了，想想他要是我的Leader，真是可怕。\n总的来说，BAT里面，阿里面试是最难的，其次是百度，然后是腾讯。阿里和腾讯不在北京，电话面试效果大打折扣，而且蚂蚁金服对系统要求很高，腾讯偏爱网络，不是这个方向的还真拿不定。我本科一同学，读了本校网络方向的研究生，拿到了腾讯的Offer，听说薪资不低，在深圳，也很不错。\n谷歌（FAILED） 嗯，作为一个CSer，明知道肯定进不了谷歌，但试是一定要试的。外企的招聘一视同仁，像微软谷歌之类的好像都没有内推。谷歌校招首先要参加它的在线笔试，即Kickstart。好像是三道题，每道题有小、大两个数据集，如果完整通过一道题的小、大数据集，即可进入到电话面试环节。无奈Kickstart对于没参加过ACM的同学来说，难度不小，很多DP题，我两次都只做出来两题的小数据集，过不了大数据集，没有拿到面试机会。想冲击谷歌的同学，需要多加练习编程题，而且最好是LeetCode Hard或者Hihocoder，尤其是DP题。\n谷歌虽然很难进，但是每年计算所都有几个人能拿到谷歌的OFFER，这种人真的是大牛。\n今日头条（OFFER） 头条的内推分两种，一种是白金码，需要笔试，但是比校招提前；另一种是铂金码，不用笔试，直接面试。头条的每个员工只有一个铂金码，比较珍贵。我当时就向上一届的师兄要了一个铂金码。无奈第一次内推面试的时间和深信服的夏令营时间冲突，就延后到9月初参加内推面试。\n很巧的是，内推一面的面试官居然是内推我的师兄，不过我们两个都觉得这样不太好，就临时换了其他面试官。头条面试包括3轮技术面+一轮HR面。技术面比其他公司稍难，LeetCode中等偏难的题+一些实际应用的场景题，还会问一些网络、操作系统的知识。\n面试完大概一周，收到邮件通知，面试通过。10月中旬又收到正式的书面Offer，不得不说，头条的Package真的好大，月薪加各种补贴，稳稳的Top3了。公司就在中航广场，离青年公寓非常之近。\n搜狗（OFFER） 有个本科同学在隔壁的搜狐上班，暑假请他帮忙内推了搜狐和搜狗，结果等了一个多月，搜狐都没给我面试通知，搜狗倒是在9月初叫我去五道口的搜狐网络大厦面试了。可能是因为搜狗要上市了，面试难度不小，和头条一个水平，面试官看起来也比较严肃。我面的是复合搜索部，除了基础知识，算法题外，还会问到和搜索引擎实际业务有关的问题。\n面完两面之后，被告知三面面试官不在，让我回去等三面通知。还以为这是婉言拒绝呢。等了一周，果然收到电话，再去进行第三面。面试自我感觉良好，可是迟迟没有收到Offer，直到11月3号的下午1:50，正当我骑着车去所里上班的时候，收到搜狗HR电话，正式通知Offer。那时候我早已另有打算了，不过了解一下搜狗的行情也是不错的。HR说头条的薪资已经是他们的SP了，问开出什么条件能够挽回我，SSP需要case by case的和Leader谈之类的。我谢过她之后，婉言拒绝了。\nFace++（OFFER） Face++是人脸识别的创业公司，支付宝上的人脸识别技术用的就是这家的。原本不打算投这家以人工智能为主的公司，担心研发岗在里面不受重视。某天有个Face++的师兄在群里发了一个内推信息，抱着试一试的心态，还是投了一份简历。9月11日收到面试邀请，当时因为别的事情心情很低落，就跟他们说不打算再面试了。过了两周，9月25日，又收到面试邀请，感觉如果不去面试的话，实在对不起内推我的师兄以及这个HR，加之公司就在隔壁，索性就答应去面试了。\n面试的过程很意外，三位面试官都很年轻，而且表现出了非常高的专业素质，其中一面面试官针对我的海量浮点数排序算法，详细的问了浮点数在内存中的表示方法，以及规格化浮点数和非规格化浮点数的区别，最后还给我留了一个家庭作业。我在无数次的面试中都会讲这个项目，只有这一个面试官问到了核心，顿时让我很佩服。二面面试官就两个有序数组求中位数的问题，要求我不用传统方法，只从中位数的数学含义进行求解，我在面试官一步步的引导下，也想出了正确解答，当时还觉得好开心。面试形式很自由，一会坐着聊，一会站着在白板上写代码，整个过程我和面试官有非常多的互动，感觉就像是两个同事在互相探讨一个问题。这次面试是我校招季的最后一次面试，也是我认为面试质量最高的一次，我非常享受这样的面试过程。\n面试结束之前，面试官详细的跟我介绍了他们的工作，研发在AI公司里也占有很重要的角色，他们内部都有自己的深度学习平台，Face++内部就自己搭建了一套类似TensorFlow的平台，完成海量数据的深度学习模型训练和优化。研发工程师在里面也要学习机器学习深度学习等算法，只有这样才能写出更加高效的系统代码。所以这类公司很欢迎研发能力强，又懂算法的同学。\n面试结束没几天，收到电话通知OFFER。\n除了上面列到的公司，我其实还面了很多家公司，每家公司的面试都大同小异，我就不再赘述了。\n汇总一下结果吧： 拿到Offer的公司：微软、百度、京东、今日头条、搜狗、Face++、华为、美团、360、四维图新、深信服、蘑菇街、PingCap、好未来 参加校招被拒的公司：谷歌、亚马逊、阿里巴巴、腾讯、拼多多 Offer年薪箱线图： 最后我想说的是，IT行业虽然就业前景很好，但是要想打赢校招这场职场第一仗，必须要下足功夫。越早准备越好，一般来说，理想的情况是：研一参加各种比赛或者实习；研二上认真刷题看书；研二下试一试实习面试，攒经验；暑假内推国内互联网公司，9月前拿下一波OFFER；9~11月面外企，拿下另一波OFFER；12月挑挑选选，签三方。\n针对研发岗，一些有用的资料：\n信息来源：\n校内就业群，很多师兄师姐会发内推消息，一定要抓住内推，可以省去很多麻烦 各大高校BBS就业板块，比如水木清华、北大、北邮人等 刷题网站：\nLeetcode，把所有Easy和Medium题刷两遍，坚持参加他们每周的比赛 Hihocoder，也有每周一题，难度较大，坚持参加他们的定期比赛 书籍：\n《程序员面试笔试宝典》：推荐，内容很多很详细，分门别类了，不过有一些小错误，是这本https://item.jd.com/11612615.html 《剑指OFFER》：和上一本书以及LeetCode很多类似的内容 《大话设计模式》：了解一些常用的设计模式，而且要会写比如工厂模式、单例模式 《STL源码剖析》：通俗易懂的源码讲解书 《深度探索C++对象模型》：对C++面向对象有很深入的探讨，读过之后对C++的低层有更多的了解 《编程珠玑》：很薄的一本编程技巧书，真的是字字珠玑，里面对堆排序的介绍很受用 《数学之美》：浅入了解一些机器学习、自然语言处理的知识 《编程之美》：难度较大，如果面微软，请坚持看完 《深入理解计算机系统》：很厚一本书，我买了没时间看，有富余时间的可以看 最最后，9月份经历了一些”我从哪里来，我要到哪里去“的事情，甚至开始拷问人生了。拒掉了所有的Offer，决定继续攻读博士学位，这也就是为什么标题中加了一个”伪“字。硕士3年或者博士6年，相对于人生来说，真的很短，不要花这么宝贵的时间仅仅为了找一份好的工作，去追求一下理想和情怀吧。不要担心，三年之后，我还可以再写一篇这样的面经。\n祝大家都能找到心仪的工作！\n","permalink":"http://localhost:1313/posts/2018-02-04-2018-campus-recruiting/","summary":"\u003cp\u003e作为一名曾经的2018届硕士毕业生，为找工作忙活了大半年，最终收获了微软、百度、头条、Face++等十多个Offer。校招季对我来说，在9月份就差不多结束了。本来很早就酝酿了这篇博客，但是由于之后一系列事情，耽搁至今，趁着提交完年终技术报告，回家之前，把这段经历记录一下。\u003c/p\u003e\n\u003cp\u003e首先介绍一些计算机专业校招的基本情况。由于移动互联网、人工智能等浪潮的兴起，计算机专业的毕业生就业前景可谓一片大好，尤其是对于名校毕业基础扎实的同学，应届生薪资倒挂老员工的事情几乎每年都在上演。所以首先祝贺所有CSer，这是属于你们的时代，各行各业都有属于你的舞台，尽情去施展才华吧。\u003c/p\u003e\n\u003cp\u003e本专业的毕业生就业去向主要有这么几类：国内互联网公司、国外互联网公司（外企）、国企。其中国内互联网公司又分大厂和新兴创业公司，大厂如BAT、网易、360、京东、华为等，创业公司主要集中在人工智能这块，如商汤科技、Face++、头条、滴滴等。外企大概也分为两类，一类是来自日本的企业，比如Indeed、WAP；另一类是来自美国的企业，比如Microsoft、Google、Hulu、FreeWheel、Amazon等。国企是指传统的国有企业里面的IT部门，比如各大银行、证监会等。这几类公司的校招时间刚好都错开了，一般来说，日企来华校招是最早的，大概每年5月份就来了；接着是国内互联网公司的内推季，大概在7~8月份；进入9月份之后，就是国内互联网公司的正式校招了；美国的企业大概会在9~10月份启动校招，有可能一直持续到11月份；国企就比较晚了，听说最晚能持续到第二年3、4月份的。这种安排，对我们来说，既是好事，也是坏事，好处就是对于纠结的同学，可以每种类型的公司都试一试，多拿几个offer，最后根据自己的情况决定去哪里；坏处就是持续时间真的很长，面到最后，身心俱疲，需要做好心理准备。\u003c/p\u003e\n\u003cp\u003e我经历过的面试主要是国内互联网和部分外企的研发岗，下面也将主要介绍这两类企业，按时间先后顺序。\u003c/p\u003e\n\u003ch1 id=\"indeedfailed\"\u003eIndeed（FAILED）\u003c/h1\u003e\n\u003cp\u003eIndeed是全球最大的招聘信息搜索引擎公司，总部位于美国德州的首府奥斯汀，2012年被日本的Recruit收购，然后成立了Indeed Tokyo办公室。本文提到的Indeed都是指Indeed Tokyo，即拿到offer的话，要求去东京工作，不过可以轮转去奥斯汀总部。\u003c/p\u003e\n\u003cp\u003eIndeed是最早开始校招的，当国内公司还在实习招聘的时候，它就跑来进行校招了。我参加了2017年4月17日在北大举办的校园宣讲会，介绍了Indeed的基本情况和招聘流程，以及抽奖机械硬盘等。Indeed的办公室很有科技范，其工位设置尤为吸引人，是六边形的环形设计，每个人既可以专注于自己的工作，又便于和组内同事讨论。宣讲的人包括HR和从该校毕业的学长，这个HR是中国人，后面有一轮HR面也是他，大家可以多多留意。\u003c/p\u003e\n\u003cp\u003eIndeed最大的吸引力是，700万~800万日元的年薪，折合人民币大概四五十万吧，这样诱人的薪资，让每个路过其宣传海报的同学都驻足观看。当然其面试难度也不小，首先有一轮在线笔试，这个在线笔试有三次机会，只要有一次全部AC，就算通过。在线笔试题一共4道，难度比LeetCode稍大，但是一定提醒大家，他们家的题都有数据范围，而且范围很小，前3题用暴力解法几乎都可以过，所以一定要先试试暴力求解，不行再想DP。\u003c/p\u003e\n\u003cp\u003e通过在线笔试之后，会有一个大约30分钟的HR面，就是上面提到的来华宣讲的中国人。这个面试严格来说是Case interview，通过Skype进行，主要考察逻辑逻辑思维能力和英文口语能力。由于是中国人，所以刚开始会用中文介绍下题意，然后让你思考一下，最后用英文给出解答。我当时的题目是，如何把微信支付的流水从xxx提高到yyy。由于提前非常认真的看了\u003ca href=\"http://www.caseinterview.com/\"\u003ehttp://www.caseinterview.com/\u003c/a\u003e的视频教学，学到很多，这次HR面顺利通过。\u003c/p\u003e\n\u003cp\u003e通过HR面之后，还有一轮Skype技术面，是从Indeed Tokyo那边打过来的，需要解算法题，通常是一题+好几个follow up。不过很多是往年的原题，在一亩三分地上都有，大家可以仔细在上面看看。我当时被问到的题是之前准备过的，但是没答好，比较突兀的给出了最优解，面试官可能觉得我是背答案了吧。。。\u003c/p\u003e\n\u003cp\u003e如果这轮Skype技术面也通过的话，就可以免费飞到东京参加on-site面了，听说on-site面是3轮面试，一整个上午或一整个下午，几乎也是原题，可以在一亩三分地上找到。\u003c/p\u003e\n\u003cp\u003e说来也奇怪，Indeed每年的面试题都差不多，但通过面试的人总是寥寥无几，这才是高级的面试官，考察的是应聘者的解题思路，而不是答案。\u003c/p\u003e\n\u003cp\u003eIndeed Tokyo很不错，如果能拿到Offer，说明你很优秀，离人生巅峰也不远了。\u003c/p\u003e\n\u003ch1 id=\"works-applicationsfailed\"\u003eWorks Applications（FAILED）\u003c/h1\u003e\n\u003cp\u003eWorks Applications简称WAP，是一家日本的ERP软件开发公司，ERP全称是Enterprise Resource Planning，简单理解就是面向企业用户的各种管理系统。WAP是正宗的日本企业，其风格和Indeed Tokyo截然不同，上班要求穿正装，估计各种行为规范也不少，但是钱也不少，折合人民币估计也有四十多万吧。WAP虽然总部在东京，但它在上海有办公室，国内校招生基本上都在上海办公。\u003c/p\u003e\n\u003cp\u003eWAP的招聘流程和Indeed很像，首先会有一个宣讲会，建议大家都参加，类似于报名考试。宣讲会之后会收到一个在线笔试的链接，要求3天之内做完2道编程题，题目比较简单。在线笔试通过之后，有一轮在线技术面试，使用的是牛客网平台，要求视频面时不能离开面试页面查资料。视频面也比较简单，大概Leetcode的easy~medium题。\u003c/p\u003e\n\u003cp\u003e对于WAP，前期的在线面试只是开胃小菜，好戏还在后头。通过两轮在线面试之后，会邀请去某个酒店现场面试。现场面试有三轮，全程英文，一般是先来段英文自我介绍，然后开始做题。比较搞笑的是，见到一面面试官时，被问到感觉如何，我说good，然后面试官说别人都是很nervous，我居然说good，感觉要被自己坑了，还好出的题都会做。前两面都不难，大概LeetCode中等题，第三面感觉是一个boss，已经不考LeetCode算法题了，考类似智力题的东西，比如有人被考到囚犯和帽子颜色的问题，我被问到的是怎样实现求两数的平均值，常规的(a+b)/2有可能导致a+b溢出，我想了很多方法，面试官都不满意，后来发现《程序员面试笔试宝典》上有。求平均值的问题可以先转换为求和，用位运算是a+b=((a\u0026amp;b)\u0026laquo;1)+(a^b)，a+b就是按位加，对应二进制也是按位加，要进位的情况就是对应位都为1，所以先用a\u0026amp;b找出需要进位的位，然后左移1位表示进位；还有些位可能只有一个1或者没有1，这部分加和的结果可以用异或表示，即a^b，所以a+b=((a\u0026amp;b)\u0026laquo;1)+(a^b)。那么，求平均值就是(a+b)/2=(a\u0026amp;b)+((a^b)\u0026raquo;1)。要是早点看了《程序员面试笔试宝典》，我估计也能拿到WAP的Offer了。\u003c/p\u003e\n\u003cp\u003e三轮技术面之后，会有一个HR面，听说如果前面的技术面过关的话，HR面会遇到日本boss，直接发放Offer；否则是一个中国人，寒暄几句之后，被告知技术面没有通过，但是可以参加暑期为期一周的实习活动，实习通过的话，也可以获得Offer。每年的实习主题都差不多，比如做一个酒店管理系统、电影院管理系统之类的，由于我觉得时间代价太高了，没有参加暑期实习。\u003c/p\u003e\n\u003cp\u003e虽然WAP的工资很好，但是要想拿Offer，比Indeed简单，LeetCode中等题足够，好好准备一下现场第三面。另外，即使拿到Offer，也要考虑一下工作内容是否符合自己的兴趣，毕竟ERP和当前火热的AI相比还是太古老了，而且穿正装上班估计也只此一家了。\u003c/p\u003e\n\u003ch1 id=\"深信服offer\"\u003e深信服（OFFER）\u003c/h1\u003e\n\u003cp\u003e深信服公司是面向企业的安全与云计算解决方案供应商，可以理解为企业版360。听说创始人是从华为跳出来的，公司整体风格和华为很像，从宣讲会上还听说这家薪资不错，尤其是博士，宣称比BAT华为都高。\u003c/p\u003e\n\u003cp\u003e深信服的提前批招聘也很早，7月初就来所里宣讲了。首先有两轮电话技术面试，面试官都会提前短信约时间，给人感觉不错。电话面试的内容比较广，网络、操作系统、C++、算法等都会问到。面试官手里应该有一个问题清单，挨个问下去，不会的跳过，节奏比较快。所以面试深信服之前，要好好复习计算机基础，尤其是网络相关的，因为其主营业务和网络密切相关。\u003c/p\u003e\n\u003cp\u003e能通过前两轮电话面试的，基础都很扎实，接下来会邀请去参加他们的星云计划暑期夏令营。原本夏令营是要去深圳总部的，但是北京的很多同学都没时间，于是临时把夏令营分成了南北两波，北京的同学被安排在九华山庄度假村。在这里会听好多深信服的介绍和讲座，其中有一个清华的博士，在校期间发过很牛的Paper，自称是那一届的全国博士Top5，谈了很多选择去深信服的理由，核心思想是博士在深信服有很大的自主权，可以试错，主导一些项目，而且薪资估计真的很高。最后会有一个Boss面，主要是问项目经历，Boss是连夜赶来北京的，面试的时候哈欠连天，也没问什么实质性的问题。去的人应该都过了。其实这个夏令营主要是去体验生活的:)\u003c/p\u003e\n\u003cp\u003e最后的Offer，中规中矩，薪资并没有想象的高，也不是自己喜欢做的事情，拒。\u003c/p\u003e\n\u003ch1 id=\"华为offer\"\u003e华为（OFFER）\u003c/h1\u003e\n\u003cp\u003e华为就不用介绍了，早年凭借电信网络产品赚得盆满钵满，近几年的智能手机业务更是冲出国门走向世界，真的是我国民族企业的榜样。华为严格来说是一个制造商企业，不算互联网企业，而且其招聘比较看重学历，给人感觉有点像国企。但是毕竟其产品都是计算机相关设备，对计算机专业人才的需求还是很旺盛的。华为的另一大特点是有钱，并且舍得给员工砸钱，我上一届的硕士师兄去了华为，工资碾压BAT，成功倒挂一大批老员工。仔细看看近几年各大重点高校的毕业生去向，去华为的占了很大比例，如果你想快速积累财富，又能吃苦，去华为能很好的满足你的要求。\u003c/p\u003e\n\u003cp\u003e因为师兄去了华为，3月份收到内部通知说可以提前批内推了，于是把简历给了师兄进行内推。7月初的时候要求做一个性格测试，华为特色，其他公司都没有这一环节，据说是在筛选符合华为价值观的同学。7月22日参加华为提前批优招，真的是优招，去的大部分是清北中科院的，猜测还要求本科是985高校。 优招面试很简单，因为是业务面试，主要问问项目，面试官是那种成功人士风格的Boss。二面就不问技术了，会问周围同学老师是怎样评价自己的，科研压力大吗，想去哪工作之类的，类似的问题也是在衡量应聘者和华为公司的match程度。我应该是非常match的，面试结束的时候，Boss还跟我握手了！\u003c/p\u003e\n\u003cp\u003e优招面试结束后没几天，会有一个在线笔试，编程题，三道题，最好全AC，我是前两题AC，第三题过了80%。至此，华为所有的笔试面试都结束了。但是直到9月初，才被再次邀请去华为北研参加Offer沟通会，这个会和大一刚入学参加各大社团的招新差不多，华为的各大部门开始抢人，我去了2012实验室中央软件院。\u003c/p\u003e\n\u003ch1 id=\"四维图新offer\"\u003e四维图新（OFFER）\u003c/h1\u003e\n\u003cp\u003e华为虽然是最早面完的，但是Offer迟迟没有下来，国内其他互联网公司又还没开始面试，心急之下，看到四维图新在招聘C++研发工程师，做地图搜索的，和自己有点关系。网上查了一下，发现还是腾讯地图的数据供应商，而且还是母校武大测绘学院有很紧密的合作，应该是个靠谱的公司。\u003c/p\u003e\n\u003cp\u003e跑去面试，可能是公司比较小，面试流程还很原始，直接在接待室问了我几个问题，有些题目有一定难度，连红黑树都被问到了。然后被直接拉去工位，打开VS，开始编程，所幸全部AC。等了一会，直接HR面，拿到普通OFFER。我说想申请SP，HR说下周再来一轮Boss面吧。于是下周又跑去Boss面，Boss果然是Boss，气场就不一样，问题也很灵活，都是他们地图搜索开发过程中的实际问题，比如给定中国地图和一个GPS坐标，怎样快速定位这个坐标。类似的题目很有意思，虽然有一个题目回答得不是很好，但总体上聊得还比较开心。Boss面完之后，又一轮HR面，被告知拿到SP，而且如果能来实习，实习表现好，且能申请到户口指标，则有可能有户口。\u003c/p\u003e\n\u003cp\u003e这个Offer是我校招季拿到的最早的Offer，薪资还不错，也算是稳住了阵脚。但是公司规模和名气都不算大，暂时拿来保底吧。\u003c/p\u003e\n\u003ch1 id=\"百度offer\"\u003e百度（OFFER）\u003c/h1\u003e\n\u003cp\u003e百度公司和我的专业是最匹配的了，国内做搜索技术最强的，非百度莫属。百度很人性化的一点是，公司不同部门的招聘分开进行，互不冲突，所以可以同时向不同部门投递简历。我就一口气投递了网页搜索部、商务搜索部和基础架构部。很幸运，同时拿到了这三个部门的提前批Offer。\u003c/p\u003e\n\u003cp\u003e百度各部门的面试流程都很像，前两轮技术面，第三轮是Boss面或者HR面，越往后面试官的级别越高，第三面的面试官很可能就是你未来的Leader。第一轮面试比较基础，问一些网络、操作系统、C++的基础知识，然后写两道算法题。第二面先写两道算法题，然后问项目，项目问得很细，我的几个搜索引擎的项目，不但问了项目的实现细节，还问了很多follow up，比如，在实战场景中，千亿级别的数据量，怎样建索引使得查询更高效，如何实现怎个搜索过程等。因为面的是搜索部门，他们对相关的技术非常了解，不要抱任何侥幸心理，不会就说不会，切莫班门弄虎。第三面Boss面比较宏观，问问职业规划，如果面试官对你比较感兴趣，会主动介绍本部门的工作，凤巢的三面面试官甚至直接加了我的微信，受宠若惊啊。\u003c/p\u003e\n\u003cp\u003e提前批面试完毕之后，9月初会有一个在线笔试，这个笔试也会刷人，所以不要掉以轻心，一定要认真准备。我当时是因为宿舍网络问题，被坑死了，那个在线笔试的系统也很变态，是个国外的系统，动不动就掉线，还只能登陆3次，超过自动退出。于是，很悲剧的3题只AC了2题。之后的几天，一直寝食难安，担心会栽在最后的笔试上。\u003c/p\u003e\n\u003cp\u003e所幸，没过多久，收到了电话通知，笔试通过，需要确定部门，让我从三个部门中选一个。我当时那个纠结啊，网页搜索部、商务搜索部和基础架构部都是百度非常核心的部门，基架的低层技术很强，网搜是典型的文本检索，商搜是广告检索，网搜的三面面试官对我很好，时不时在微信上联系我；我和商搜的三面面试官也聊得很开心，商搜是百度最赚钱的部门，各种大牛非常之多。几番权衡之后，选择了商搜（凤巢），同时也拿到了SP。\u003c/p\u003e\n\u003ch1 id=\"microsoftoffer\"\u003eMicrosoft（OFFER）\u003c/h1\u003e\n\u003cp\u003e微软是我面的唯一一个美国外企，面试流程数它最多了，前后经历了：1轮在线笔试+2轮skype面试+3轮on-site面试。\u003c/p\u003e\n\u003cp\u003e首先，要拿到微软的skype面试机会就很难，需要通过Hihocoder的在线笔试。Hihocoder的题型和难度都相比于LeetCode复杂得多，我有一次很幸运的做到了前100名好像，拿到了skype面试机会。两轮skype面试难度也不小，比如search range，不但要求bug free，还要求你写测试用例；还比如对快排进行优化；手写堆排序；概率题等。微软的在线编程和skype面试和国内互联网不太一样，建议大家看看一亩三分地上的面经。\u003c/p\u003e\n\u003cp\u003e过了两轮skype面之后，会被邀请去参加他们的探星夏令营，大概是在8月中旬，地点就在丹棱街的微软大厦。探星夏令营第一天是参观，我因为实验室忙就没去，第二天是三轮面试。我因为研究的方向是搜索引擎，所以被安排到bing组面试了。微软的现场面试难度也不小，不是像LeetCode那样直接叫你写个DP、排序什么的，而是给出一个实际问题，需要将其抽象成一个计算机问题，然后才是代码实现。前两面顺利通过。此时已经是下午4点多了，HR说三面安排不过来，让回去等。这一等直接从8月中旬等到9月初，期间还以为是二面挂了，“让回去等”是委婉的拒绝 ，看来微软还是说话算话的。三面是Boss面，和国内互联网比较像，面项目，问了很多细节，然后根据项目衍生出一个字符串压缩的题目，让写压缩和解压缩的代码。虽然写完了，但是没保证bug free，和面试官聊了聊可能的bug以及解决方案。\u003c/p\u003e\n\u003cp\u003e过了大概一周，面试结果出来了，没有直接说给Offer，但是说面试反馈非常Positive，让加一个微信群。国庆节之前，收到微软HR电话，让我们稍安勿躁，国庆后会给正式Offer。后来直到10月31日，才收到HR的电话，正式通知Offer详情。接起电话，HR就说准备好纸笔，因为Offer内容比较多，然后就说了Package里面的各种福利，各种美金。总的来说，Package加起来在硕士里面应该是Top级别的，外企各种Balance，不加班，做的是自己喜欢的方向，而且还有可能拿户口，甚至人肉翻墙，可以说这个Offer是非常诱人的。\u003c/p\u003e\n\u003ch1 id=\"京东offer\"\u003e京东（OFFER）\u003c/h1\u003e\n\u003cp\u003e京东和百度类似，也是部门自己招聘，所以可以面多个部门。我面了AI和大数据部门以及商业推荐部门。印象比较深的是，原本面了一个做分布式的组，一面发现我更适合做搜索和架构，然后就被推荐到一个做京东智能音箱的组，这个组的三面面试官是从雅虎北研过来的，听口音感觉是广东人。因为我是做搜索，智能音箱里面也需要搜索，两个人聊得很不错，面试官当场就说帮我争取SP。\u003c/p\u003e\n\u003cp\u003e面完技术面之后，过了大概一周，还要进行HR面。面试通知邮件也没说是哪个部门的。其中有个部门的HR面居然是群面，太奇葩了，也是我经历过的唯一一个群面。一屋子3个面试官，6个学生，就菜鸟网络和京东物流的对比展开讨论。首先自我介绍，有清华北大的，也有中科院各所的，还有北邮的。每次讨论我都是倒数几个发言的，对于这种压力测试，真是不适用。不过还好，HR后来跟我说我的表现不错。\u003c/p\u003e\n\u003cp\u003eHR跟我谈薪资的时候，我客套说差不多就行，后来这两个部门都拿到了Offer，薪资还真的就是差不多，白菜价。因为已经有其他选择，也没有再争取SP。听别人说争取一下能有28左右？感觉京东的定价真是因人而异啊。后来有一天还收到三面面试官的电话，问我去向定哪了，真觉得有点愧对他。\u003c/p\u003e\n\u003ch1 id=\"360offer\"\u003e360（OFFER）\u003c/h1\u003e\n\u003cp\u003e本来不打算面360，但是该公司在8月8号组织了一场中科院专场招聘会，在所有OFFER都还没有最终确定的情况下，去360逛一逛也没坏处。360的办公楼在酒仙桥，和MTK在一起，周围在施工，几乎没有吃饭的地方，给人的第一印象不是很好。10点钟到现场之后，已经人山人海了，和菜市场没什么区别，中间等待的时间都超过了面试时间。\u003c/p\u003e\n\u003cp\u003e面试分为三轮，前两轮是技术面，第三轮是HR面。一面问了一些基础知识，写了一两个算法题。二面遇到了负责360地图开发的程序员，因为地图中也涉及POI搜索，聊得很欢。HR面被问到知道360的哪些产品，虽然我现在一个360的产品都不用了，但是知道的还是不少。\u003c/p\u003e\n\u003cp\u003e面完之后，觉得Offer稳了，然后开心的回所里。第二天收到邮件通知，面试通过，还需参加一个在线笔试，类似于行测。做完之后，查看状态，被告知所有面试笔试都通过了，个人信息已经在Offer池中，但是没有正式Offer。Offer池是什么鬼，也就是没人要被扔到池子里等人捞呗。问了下其他人，大部分也是被扔到池子里了，只听说有一个人收到书面Offer。从此对360无感，无论是你们组织面试，还是我们参加面试，费了一天劲，硬是不发OFFER，坑爹。后来在10月16日，收到一封360的邮件，正式书面Offer，难道是被人相中捞起来了，真是无语。拒。\u003c/p\u003e\n\u003ch1 id=\"阿里巴巴failed\"\u003e阿里巴巴（FAILED）\u003c/h1\u003e\n\u003cp\u003e阿里内推只能选一个部门，内推失败之后也只有一次校招机会，所以大家选部门一定要慎重，根据自己的实力和兴趣进行选择。当时群里给出了蚂蚁金服的内推消息之后，我第一时间就选择内推蚂蚁金服了。结果面了两轮之后查状态已经挂了，也没感觉面得差。可能是因为内推蚂蚁金服的人太多了，实力要求也很高，而且自己做搜索引擎的，和蚂蚁金服不太match。\u003c/p\u003e\n\u003cp\u003e因为内推挂了之后，无法再面其他部门了。只能参加校招流程，校招在线笔试之后一直就没消息，状态也没更新，难度笔试挂了？\u003c/p\u003e","title":"伪·2018届校招面经"},{"content":"2017年12月7日~13日，打着参加“第三届全国质谱分析学术报告会”的旗号，实验室一行11人开启了为期一周的厦门之旅。有关学术交流的总结报告，已经在实验室内部分享了，这篇博客还是来聊聊吃喝玩乐的事吧:)\n第一次乘坐飞机 本次出行分为飞机组和火车组，大部队说飞机不安全要坐火车，我因为想体验一下乘坐飞机的感受，于是选择了飞机组。第一次坐飞机，体验有三点：1）快。从北京到厦门，横跨整个中国，只花了3个小时，真的好快，除去吃午餐等时间，连一部电影都没看完！2）噪。起飞和降落的时候噪声特别大，平飞的时候噪声也不小，而且快要到目的地时，会有短暂的耳胀，孙老师说第一次坐飞机的人好像都会出现耳胀的情况。还有就是有时候飞机比较颠簸，放在桌子上的水都快晃出来了。3）美。我因为第一次坐飞机，特地选了一个靠窗的位置，想体验一把俯瞰神州大地的感觉。从窗户往下看时，地形地貌和谷歌卫星地图一摸一样，窗外的云就像一朵朵棉花糖，很漂亮，很像各种神话剧中的天庭。\n飞机上不让开手机，这是在北京首都国际机场起飞前抓拍的照片，以后坐飞机记得带相机\n南国风光 下飞机之后，立即体验到了厦门这座城市的“热情”，20℃左右的温度，卸下厚重的羽绒服，看着路边的红花绿叶正艳，来到沙滩上，吹吹海风，一身清爽。南方的城市由于经常下雨，街道看起来很干净，柏油马路显露着其原本的黝黑色，路边的叶子绿的发亮，不像在北京被蒙上厚厚的一层灰。路上车辆和红绿灯不多，也很少听到鸣笛，听说厦门全岛禁止鸣笛，斑马线处虽然没有红绿灯，但是有摄像头，强制车辆遇到行人时必须礼让。这种规定在北京是不可能实行的吧。总体而言，厦门彰显了南方城市应有的魅力，和杭州类似，但又更加清新靓丽，是一个休闲生活的好地方。\n厦门大学——也许是中国最美丽的大学 本次参会，我们特地提前一天到达，留出时间来参观游览，地点之一就是厦门大学。厦大很美，这是来自一个在武大待了4年的人的由衷赞美。得益于其依山傍水的地理位置，校园内有锦绣的芙蓉湖、美丽的情人谷水库等景点，气氛十分静谧，这点相比于武大小小的鉴湖就好很多。由于地处热带，校园内随处可见高大挺拔的棕榈树，配上古朴的清水墙、多彩的琉璃顶，给人一种别样的美感。如果你要问厦大和武大哪个更美，我只能说她们都美！武大是幽静娴雅的大家闺秀，厦大则像开放热情的新时代女子。武大更加符合中国传统的园林审美，厦大更具风情和现代气息。两校都有山有水，都无愧于中国最美丽的大学的称号！\n看见有人在厦大的情人谷水库泛舟，真是“亦可赛艇”呀:)\n厦大还有一个特别的地方——芙蓉隧道，长达一公里，隧道内壁两侧，画满了风格各异，色彩缤纷的涂鸦，号称是中国最文艺的隧道，中国最长的涂鸦隧道。致青春，怀念那种开放、包容的大学氛围。\n环游鼓浪屿 会议结束的那天下午，我们前往国家5A级旅游景区鼓浪屿风景名胜区游玩。鼓浪屿，面积不到2平方千米，人口约2万，有“海上花园”、“万国建筑博览会”、“钢琴之岛”之美称。除环岛电动车外不允许机动车辆上岛，因此气氛幽静。2005年《中国国家地理》杂志将鼓浪屿评为“中国最美的城区”第一名。2017年7月在波兰克拉科夫举行的第41届世界遗产大会上被正式列入《世界遗产名录》。鼓浪屿、厦门岛和大陆的关系，就像月球、地球和太阳的关系。\n我们首先从厦门国际邮轮中心乘坐邮轮前往鼓浪屿的三丘田码头，邮轮北边能看到远处横跨鹭江海沧大桥，非常的简洁漂亮，据说是亚洲第一、世界第二（仅次于丹麦）的三跨连续全漂浮钢箱梁悬索桥，代表着20世纪中国建桥水平最高成就。由于是国际邮轮中心，所以也能看到不少外国邮轮，我当时就看到了欧洲和韩国的邮轮经过。\n20分钟之后，就到达了鼓浪屿的三丘田码头。我们沿着鼓浪屿的海岸线，优哉游哉的走着，嬉笑打闹，听着浪涛声，赏着落日余晖，好不惬意。\n我们原本是打算步行环岛游览一周的，但是走到卢戆章雕塑的地方，好多人都走不动了，打算穿过岛屿，提前返回。我个人其实挺想继续前进到鼓声洞的，鼓浪屿名称的由来就是因为在鼓声洞能听到阵阵浪涛拍击岩洞而发出轰隆巨响。\n横穿鼓浪屿的路不是很好走，蜿蜒曲折、上下颠簸，走到龙头路小吃街的时候，大家都饿得不行。小吃街排队的食客特别多，为了早点吃饱，大家分头行动，每两三个人排一家店，所以最终还吃了不少美食，比如沈家闽南肠粉、小马哥起司马铃薯、汤满贯等。有意思的是，在沈家闽南肠粉和小马哥起司马铃薯的中间，有一家土耳其冰淇淋店，两边的食客都排起了长龙，唯独这家店门前冷落鞍马稀。大家吃着美食就开始聊起来了，虽说厦门冬天不冷，但也没有热到对冰淇淋有那么大的渴望。更重要的是，走到此处的游客肯定饿了，首先想到的是填饱肚子，冰淇淋属于饭后甜点，应该算“奢侈品”了，如果这个店换成“土耳其烤肉”，估计能火，哈哈。\n岛上其实还有很多景点，比如风琴博物馆、菽庄花园、日光岩、海底世界等，但是我们都没进去，可能是因为沿途的风景太美了，这些收费的室内景点还不足以吸引我们吧。鼓浪屿的风景太美，可玩、可看的景点太多，半天的时间绝对不够用。下次再去，一定要细细品味。\nhttp://j.map.baidu.com/M2cPN\n厦门植物园 回北京之前，我们去了火车站附近的厦门植物园游玩。厦门植物园也称为万石植物园，背靠五老峰南普陀，集植物景观、自然景观、人文景观于一体，景色相当不错。我们沿路经过了百花厅、奇趣植物区、新碑林、摩崖石刻、长寿峡、半山观景台、多肉植物区、雨林世界、药用植物区，看到了很多奇花异草，整个行程在多肉植物区看到高大的仙人掌时达到高潮，非常值得游览的一个景点。\n厦门植物园石碑 类似食人花的猪笼草，开启之后像一个笼子 奇丑无比的白花异木棉 万石丛中 半山观景台，背景是厦门世茂双子塔 疯狂生长的仙人掌和仙人球 类似毛毛虫的仙人掌。。。 放了一个大招，生出许多仙人掌:) 胡吃海喝 在厦门的这一周，大家都说不是在吃就是在吃的路上，不论是酒店的自助餐还是在外面吃饭，都吃得很不错，我基本上每餐都是十分饱。在酒店的伙食，相对来说，早餐是最好的，午餐和晚餐比较一般，可能是因为早餐酒店提供，午餐和晚餐是会议主办方提供吧，毕竟会议注册费很便宜，伙食也不会太好。不过每餐都有海鲜、厦门特色沙茶面、各种肉、水果、甜点、饮料，相比于北京的食堂是好太多了。\n丰盛的早餐\n到厦门的第一天晚上，我和师弟去中山路小吃街逛了逛，结果并没有任何惊喜，感觉全国的小吃街都差不多：烧烤、臭豆腐、各种煎饼包子、粥、饼。中山路的新华书店倒是值得一逛，他们家第三层的书不少，国内外文学、小说、工具书一应俱全，一改我对新华书店只有教科书的旧思想。\n在外面吃的话，每餐必点被称为花蛤的“虫子”，还有另一种称为蛏（cheng）子的“虫子”，和花蛤很像，但是是长方形，且有两条美腿。知乎上有一个回答仔细的辨析了花蛤、蛏子、蚬子、蚶子、蛤蜊、海瓜子、贝壳的区别，很有意思。不过话说这些海底动物长得都很奇怪，有些还有很多软体触角，看着让人恶心。席间，有个师兄讲了个笑话，问为啥海底动物长得都这么丑，答案是因为海底没光，反正都看不见，大家就随便长长了:)。\n说到海产品，必不可少的是鱼了，厦门的清蒸金昌鱼非常好吃，味道鲜美，没有小刺，和上次在杭州吃到的西湖醋鱼简直是一个天上，一个地下，强烈推荐。\n离开厦门的最后一餐，我们去当地的特色饭店小眼镜大排档吃饭，虽然叫大排档，但其实是一个正经的餐厅。点了不少大菜，比如鲍*，大龙*，真是开眼了。。。\n咳咳，以上就是本次厦门之行的次要内容，主要内容当然是开会听各种院士大牛的报告啦，不过作为一个码农，跑去参加理化生同学的质谱会议，第一天就被一个大姐姐笑话“你们学计算机的为什么也跑来参加质谱会议呀？”，我表示我也很无奈。第一次外出参会，虽然我们是配角，也听不懂那些生化的报告，但还是精心制作了属于自己的第一份墙报，也算是入了研究生的门了。\n最后，贴出我在鼓浪屿买的“课业成功”的冰箱贴，希望自己读博顺利吧~\n","permalink":"http://localhost:1313/posts/2017-12-16-a-trip-to-xiamen/","summary":"\u003cp\u003e2017年12月7日~13日，打着参加“第三届全国质谱分析学术报告会”的旗号，实验室一行11人开启了为期一周的厦门之旅。有关学术交流的总结报告，已经在实验室内部分享了，这篇博客还是来聊聊吃喝玩乐的事吧:)\u003c/p\u003e\n\u003ch1 id=\"第一次乘坐飞机\"\u003e第一次乘坐飞机\u003c/h1\u003e\n\u003cp\u003e本次出行分为飞机组和火车组，大部队说飞机不安全要坐火车，我因为想体验一下乘坐飞机的感受，于是选择了飞机组。第一次坐飞机，体验有三点：1）快。从北京到厦门，横跨整个中国，只花了3个小时，真的好快，除去吃午餐等时间，连一部电影都没看完！2）噪。起飞和降落的时候噪声特别大，平飞的时候噪声也不小，而且快要到目的地时，会有短暂的耳胀，孙老师说第一次坐飞机的人好像都会出现耳胀的情况。还有就是有时候飞机比较颠簸，放在桌子上的水都快晃出来了。3）美。我因为第一次坐飞机，特地选了一个靠窗的位置，想体验一把俯瞰神州大地的感觉。从窗户往下看时，地形地貌和谷歌卫星地图一摸一样，窗外的云就像一朵朵棉花糖，很漂亮，很像各种神话剧中的天庭。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/airport.webp\"\u003e\n飞机上不让开手机，这是在北京首都国际机场起飞前抓拍的照片，以后坐飞机记得带相机\u003c/p\u003e\n\u003ch1 id=\"南国风光\"\u003e南国风光\u003c/h1\u003e\n\u003cp\u003e下飞机之后，立即体验到了厦门这座城市的“热情”，20℃左右的温度，卸下厚重的羽绒服，看着路边的红花绿叶正艳，来到沙滩上，吹吹海风，一身清爽。南方的城市由于经常下雨，街道看起来很干净，柏油马路显露着其原本的黝黑色，路边的叶子绿的发亮，不像在北京被蒙上厚厚的一层灰。路上车辆和红绿灯不多，也很少听到鸣笛，听说厦门全岛禁止鸣笛，斑马线处虽然没有红绿灯，但是有摄像头，强制车辆遇到行人时必须礼让。这种规定在北京是不可能实行的吧。总体而言，厦门彰显了南方城市应有的魅力，和杭州类似，但又更加清新靓丽，是一个休闲生活的好地方。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xiamen1.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xiamen2.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xiamen3.webp\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"厦门大学也许是中国最美丽的大学\"\u003e厦门大学——也许是中国最美丽的大学\u003c/h1\u003e\n\u003cp\u003e本次参会，我们特地提前一天到达，留出时间来参观游览，地点之一就是厦门大学。厦大很美，这是来自一个在武大待了4年的人的由衷赞美。得益于其依山傍水的地理位置，校园内有锦绣的芙蓉湖、美丽的情人谷水库等景点，气氛十分静谧，这点相比于武大小小的鉴湖就好很多。由于地处热带，校园内随处可见高大挺拔的棕榈树，配上古朴的清水墙、多彩的琉璃顶，给人一种别样的美感。如果你要问厦大和武大哪个更美，我只能说她们都美！武大是幽静娴雅的大家闺秀，厦大则像开放热情的新时代女子。武大更加符合中国传统的园林审美，厦大更具风情和现代气息。两校都有山有水，都无愧于中国最美丽的大学的称号！\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu1.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu2.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu3.webp\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu4.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu5.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu6.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmu7.webp\"\u003e\n看见有人在厦大的情人谷水库泛舟，真是“亦可赛艇”呀:)\u003c/p\u003e\n\u003cp\u003e厦大还有一个特别的地方——芙蓉隧道，长达一公里，隧道内壁两侧，画满了风格各异，色彩缤纷的涂鸦，号称是中国最文艺的隧道，中国最长的涂鸦隧道。致青春，怀念那种开放、包容的大学氛围。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle1.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle2.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle3.webp\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle4.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle5.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle6.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle7.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle8.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/doodle9.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"环游鼓浪屿\"\u003e环游鼓浪屿\u003c/h1\u003e\n\u003cp\u003e会议结束的那天下午，我们前往国家5A级旅游景区鼓浪屿风景名胜区游玩。鼓浪屿，面积不到2平方千米，人口约2万，有“海上花园”、“万国建筑博览会”、“钢琴之岛”之美称。除环岛电动车外不允许机动车辆上岛，因此气氛幽静。2005年《中国国家地理》杂志将鼓浪屿评为“中国最美的城区”第一名。2017年7月在波兰克拉科夫举行的第41届世界遗产大会上被正式列入《世界遗产名录》。鼓浪屿、厦门岛和大陆的关系，就像月球、地球和太阳的关系。\u003c/p\u003e\n\u003cp\u003e我们首先从厦门国际邮轮中心乘坐邮轮前往鼓浪屿的三丘田码头，邮轮北边能看到远处横跨鹭江海沧大桥，非常的简洁漂亮，据说是亚洲第一、世界第二（仅次于丹麦）的三跨连续全漂浮钢箱梁悬索桥，代表着20世纪中国建桥水平最高成就。由于是国际邮轮中心，所以也能看到不少外国邮轮，我当时就看到了欧洲和韩国的邮轮经过。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu1.webp\"\u003e\u003c/p\u003e\n\u003cp\u003e20分钟之后，就到达了鼓浪屿的三丘田码头。我们沿着鼓浪屿的海岸线，优哉游哉的走着，嬉笑打闹，听着浪涛声，赏着落日余晖，好不惬意。\u003c/p\u003e\n\u003cp\u003e我们原本是打算步行环岛游览一周的，但是走到卢戆章雕塑的地方，好多人都走不动了，打算穿过岛屿，提前返回。我个人其实挺想继续前进到鼓声洞的，鼓浪屿名称的由来就是因为在鼓声洞能听到阵阵浪涛拍击岩洞而发出轰隆巨响。\u003c/p\u003e\n\u003cp\u003e横穿鼓浪屿的路不是很好走，蜿蜒曲折、上下颠簸，走到龙头路小吃街的时候，大家都饿得不行。小吃街排队的食客特别多，为了早点吃饱，大家分头行动，每两三个人排一家店，所以最终还吃了不少美食，比如沈家闽南肠粉、小马哥起司马铃薯、汤满贯等。有意思的是，在沈家闽南肠粉和小马哥起司马铃薯的中间，有一家土耳其冰淇淋店，两边的食客都排起了长龙，唯独这家店门前冷落鞍马稀。大家吃着美食就开始聊起来了，虽说厦门冬天不冷，但也没有热到对冰淇淋有那么大的渴望。更重要的是，走到此处的游客肯定饿了，首先想到的是填饱肚子，冰淇淋属于饭后甜点，应该算“奢侈品”了，如果这个店换成“土耳其烤肉”，估计能火，哈哈。\u003c/p\u003e\n\u003cp\u003e岛上其实还有很多景点，比如风琴博物馆、菽庄花园、日光岩、海底世界等，但是我们都没进去，可能是因为沿途的风景太美了，这些收费的室内景点还不足以吸引我们吧。鼓浪屿的风景太美，可玩、可看的景点太多，半天的时间绝对不够用。下次再去，一定要细细品味。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu2.webp\"\u003e\n\u003ca href=\"http://j.map.baidu.com/M2cPN\"\u003ehttp://j.map.baidu.com/M2cPN\u003c/a\u003e\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu3.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu4.webp\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu5.webp\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu6.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/gulangyu7.webp\"\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"厦门植物园\"\u003e厦门植物园\u003c/h1\u003e\n\u003cp\u003e回北京之前，我们去了火车站附近的厦门植物园游玩。厦门植物园也称为万石植物园，背靠五老峰南普陀，集植物景观、自然景观、人文景观于一体，景色相当不错。我们沿路经过了百花厅、奇趣植物区、新碑林、摩崖石刻、长寿峡、半山观景台、多肉植物区、雨林世界、药用植物区，看到了很多奇花异草，整个行程在多肉植物区看到高大的仙人掌时达到高潮，非常值得游览的一个景点。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e厦门植物园石碑\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e类似食人花的猪笼草，开启之后像一个笼子\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg1.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg2.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e奇丑无比的白花异木棉\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e万石丛中\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg3.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg4.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e半山观景台，背景是厦门世茂双子塔\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e疯狂生长的仙人掌和仙人球\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg5.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg6.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e类似毛毛虫的仙人掌。。。\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e放了一个大招，生出许多仙人掌:)\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg7.webp\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/xmbg8.webp\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"胡吃海喝\"\u003e胡吃海喝\u003c/h1\u003e\n\u003cp\u003e在厦门的这一周，大家都说不是在吃就是在吃的路上，不论是酒店的自助餐还是在外面吃饭，都吃得很不错，我基本上每餐都是十分饱。在酒店的伙食，相对来说，早餐是最好的，午餐和晚餐比较一般，可能是因为早餐酒店提供，午餐和晚餐是会议主办方提供吧，毕竟会议注册费很便宜，伙食也不会太好。不过每餐都有海鲜、厦门特色沙茶面、各种肉、水果、甜点、饮料，相比于北京的食堂是好太多了。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-16-a-trip-to-xiamen/breakfast.webp\"\u003e\n丰盛的早餐\u003c/p\u003e\n\u003cp\u003e到厦门的第一天晚上，我和师弟去中山路小吃街逛了逛，结果并没有任何惊喜，感觉全国的小吃街都差不多：烧烤、臭豆腐、各种煎饼包子、粥、饼。中山路的新华书店倒是值得一逛，他们家第三层的书不少，国内外文学、小说、工具书一应俱全，一改我对新华书店只有教科书的旧思想。\u003c/p\u003e\n\u003cp\u003e在外面吃的话，每餐必点被称为花蛤的“虫子”，还有另一种称为蛏（cheng）子的“虫子”，和花蛤很像，但是是长方形，且有两条美腿。知乎上有一个回答仔细的辨析了\u003ca href=\"https://www.zhihu.com/question/25165185/answer/30233547\"\u003e花蛤、蛏子、蚬子、蚶子、蛤蜊、海瓜子、贝壳的区别\u003c/a\u003e，很有意思。不过话说这些海底动物长得都很奇怪，有些还有很多软体触角，看着让人恶心。席间，有个师兄讲了个笑话，问为啥海底动物长得都这么丑，答案是因为海底没光，反正都看不见，大家就随便长长了:)。\u003c/p\u003e\n\u003cp\u003e说到海产品，必不可少的是鱼了，厦门的清蒸金昌鱼非常好吃，味道鲜美，没有小刺，和上次在杭州吃到的西湖醋鱼简直是一个天上，一个地下，强烈推荐。\u003c/p\u003e","title":"厦门之行"},{"content":"上一篇博客主要介绍了逻辑回归的理论知识，这篇博客咱们用Python机器学习包sklearn中的LogisticRegression做一个分类的实例。\n数据还是学生样本，只有两个特征，分别是两门课的分数score1和score2，类标号y表示这个学生是否能被录取。先上分类效果图：\n完整的Python代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Wed Nov 08 17:49:41 2017 @author: zhenlin \u0026#34;\u0026#34;\u0026#34; import numpy as np import pandas as pd from sklearn.cross_validation import train_test_split from sklearn.linear_model import LogisticRegression import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve from sklearn.metrics import classification_report # 1. 构造数据 sample_number = 200 # 第一个高斯分布参数 mean1 = [0, 4] # 两个维度上的均值 cov1 = [[5, 3], [3, 10]] # 两个维度的协方差矩阵，必须满足对称半正定 # 第二个高斯分布参数 mean2 = [7, 5] cov2 = [[7, 2], [2, 15]] # 从两个二元高斯分布中随机采样数据点 class1_x1, class1_x2 = np.random.multivariate_normal(mean1, cov1, sample_number).T # .T表示转置 class2_x1, class2_x2 = np.random.multivariate_normal(mean2, cov2, sample_number).T # 两个高斯分布对应两个类标号 data = [[class1_x1[i],class1_x2[i],0] for i in range(sample_number)]+[[class2_x1[i],class2_x2[i],1] for i in range(sample_number)] # 填充到pandas中 data = pd.DataFrame(data,columns=[\u0026#39;score1\u0026#39;,\u0026#39;score2\u0026#39;,\u0026#39;result\u0026#39;]) score_data = data[[\u0026#39;score1\u0026#39;,\u0026#39;score2\u0026#39;]] result_data = data[\u0026#39;result\u0026#39;] # 2. 训练模型 average_precision = 0 # 平均准确度 iters = 10 # 交叉验证次数 for i in xrange(iters): # 数据划分，80%用于训练，20%用于预测 x_train, x_test, y_train, y_test = train_test_split(score_data, result_data, test_size = 0.2) # 构造默认逻辑回归模型 model = LogisticRegression() # 训练 model.fit(x_train, y_train) # 预测 predict_y = model.predict(x_test) # 计算测试集上的准确度 average_precision += np.mean(predict_y == y_test) average_precision /= iters # 3. 绘制分类面 - 法1 x1_min, x1_max = score_data[\u0026#39;score1\u0026#39;].min() - .5, score_data[\u0026#39;score1\u0026#39;].max() + .5 def generate_face(prob): y = -np.log(1.0 / prob - 1.0) n = 500 x1 = np.linspace(x1_min, x1_max, n) # w1x1+w2x2+b=y x2 = (-model.coef_[0][0] / float(model.coef_[0][1])) * x1 + (y - model.intercept_) / float(model.coef_[0][1]) return x1, x2 pos_data = data[data[\u0026#39;result\u0026#39;] == 1] neg_data = data[data[\u0026#39;result\u0026#39;] == 0] plt.scatter(x = pos_data[\u0026#39;score1\u0026#39;], y = pos_data[\u0026#39;score2\u0026#39;], color = \u0026#39;black\u0026#39;, marker = \u0026#39;o\u0026#39;) plt.scatter(x = neg_data[\u0026#39;score1\u0026#39;], y = neg_data[\u0026#39;score2\u0026#39;], color = \u0026#39;red\u0026#39;, marker = \u0026#39;*\u0026#39;) face_04_x1, face_04_x2 = generate_face(0.4) face_05_x1, face_05_x2 = generate_face(0.5) face_06_x1, face_06_x2 = generate_face(0.6) plt.plot(face_04_x1, face_04_x2) plt.plot(face_05_x1, face_05_x2) plt.plot(face_06_x1, face_06_x2) plt.xlim(score_data[\u0026#39;score1\u0026#39;].min(), score_data[\u0026#39;score1\u0026#39;].max()) plt.ylim(score_data[\u0026#39;score2\u0026#39;].min(), score_data[\u0026#39;score2\u0026#39;].max()) plt.xlabel(\u0026#39;score1\u0026#39;) plt.ylabel(\u0026#39;score2\u0026#39;) plt.legend([\u0026#39;prob_threshold = 0.4\u0026#39;, \u0026#39;prob_threshold = 0.5\u0026#39;, \u0026#39;prob_threshold = 0.6\u0026#39;], loc=\u0026#39;center left\u0026#39;, bbox_to_anchor=(1, 0.865)) plt.show() # 4. 绘制分类面 - 法2 pos_data = data[data[\u0026#39;result\u0026#39;] == 1] neg_data = data[data[\u0026#39;result\u0026#39;] == 0] h = 0.02 s1_min, s1_max = score_data[\u0026#39;score1\u0026#39;].min() - .5, score_data[\u0026#39;score1\u0026#39;].max() + .5 s2_min, s2_max = score_data[\u0026#39;score2\u0026#39;].min() - .5, score_data[\u0026#39;score2\u0026#39;].max() + .5 # 生成s1在[s1_min, s1_max]，且s2在[s2_min, s2_max]的网格数据点 # meshgrid含义参见：http://blog.sciencenet.cn/blog-791749-675394.html s1, s2 = np.meshgrid(np.arange(s1_min, s1_max, h), np.arange(s2_min, s2_max, h)) # 把两个坐标的值按列拼在一起构成二维数据点 Z = model.predict(np.c_[s1.ravel(), s2.ravel()]) # 绘制边界和散点 Z = Z.reshape(s1.shape) # 坐标点是(s1[i], s2[i])，对应颜色是Z[i]，颜色主题使用plt.cm.Paired plt.pcolormesh(s1, s2, Z, cmap = plt.cm.Paired) plt.scatter(x = pos_data[\u0026#39;score1\u0026#39;], y = pos_data[\u0026#39;score2\u0026#39;], color = \u0026#39;black\u0026#39;, marker = \u0026#39;o\u0026#39;) plt.scatter(x = neg_data[\u0026#39;score1\u0026#39;], y = neg_data[\u0026#39;score2\u0026#39;], color = \u0026#39;red\u0026#39;, marker = \u0026#39;*\u0026#39;) plt.xlim(s1.min(), s1.max()) plt.ylim(s2.min(), s2.max()) plt.xlabel(\u0026#39;score1\u0026#39;) plt.ylabel(\u0026#39;score2\u0026#39;) plt.show() # 5. 评估模型 # 对于测试数据，模型输出1的概率 answer = model.predict_proba(x_test)[:,1] # 计算不同概率阈值下的P和R precision, recall, thresholds = precision_recall_curve(y_test, answer) # prob \u0026gt; 0.5的报告为1 report = answer \u0026gt; 0.5 print(classification_report(y_test, report, target_names = [\u0026#39;neg\u0026#39;, \u0026#39;pos\u0026#39;])) print(\u0026#39;average precision: %f\u0026#39;%average_precision) # 6. 绘制PRC曲线 # step阶跃图，在点(recall[i],precision[i])进行跳变 plt.step(recall, precision, color=\u0026#39;b\u0026#39;, alpha=0.2, where=\u0026#39;post\u0026#39;) # 对PRC下方填充颜色 plt.fill_between(recall, precision, step=\u0026#39;post\u0026#39;, alpha=0.2, color=\u0026#39;b\u0026#39;) plt.xlabel(\u0026#39;Recall\u0026#39;) plt.ylabel(\u0026#39;Precision\u0026#39;) plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1.0]) plt.title(\u0026#39;2-class Precision-Recall curve\u0026#39;) plt.show() 下面将逐模块介绍代码细节，大神可以略过。\n一、构造数据 数据的来源可以有很多种方式，sklearn包中自带7个Toy datasets，比如我们耳熟能详的鸢尾花卉数据集iris和手写数字数据集digits。为了学一学numpy，我们尝试自行构造数据集，这样也方便读者重复。我们可以从两个不同参数的高斯分布中采样相同数量的数据点，由于数据包含两个维度(score1, score2)，所以必须是二维高斯分布。二维高斯分布概率密度函数为：\n参数包括：\n\\(\\mu\\)中的\\(\\mu_X\\)和\\(\\mu_Y\\)分别表示两个维度上的均值，\\(\\Sigma\\)是这两个维度上的协方差矩阵，需要满足对称半正定。\nnumpy中的random.multivariate_normal可以从多元正态分布中进行采样，传入的参数包括均值mean, 协方差矩阵cov和采样数据点个数。比如本文中第二类数据点的均值为[7,5]，表示在score1维上的均值为7，在score2维上的均值为5，从图上黑点的横纵坐标分布也可以看出来；协方差矩阵为\\(\\begin{bmatrix}7 \u0026 2\\\\2 \u0026 15\\end{bmatrix}\\)，表示score1和score2上的方差分别为7和15，然后标准差的积再乘以相关系数等于2。用这些参数调用multivariate_normal就能得到对应二维平面\\(x_1Ox_2\\)上的数据点了。\n最后，给两类数据点贴上不同的类标号，填充到Pandas中就大功告成了。Pandas中每一行表示一个样本，共有三列，分别表示score1、score2和result，其中result就是类标号。\n二、训练模型 由于是直接调用sklearn中的逻辑回归函数，所以这一步非常简单。为了评估模型的准确度，我们做了一个交叉验证，即随机把数据分成80%用于训练，20%用于预测，重复10次，求预测准确度的平均值。这可以用模型选择中的train_test_split快速完成。\n调用sklearn.linear_model.LogisticRegression()就能得到一个逻辑回归模型，该函数有很多参数，但是作为入门，所有参数都使用默认值。训练直接调用model.fit(x,y)，预测直接调用model.predict(x)。需要注意的是，model中带了很多成员变量，比如训练得到的分类面参数coef_和intercept_等，后面会用到。\n三、绘制分类面（法1） 上一篇博客提到，虽然\\(f(\\mathbf{x})\\)被Sigmoid函数映射到了概率空间，但逻辑回归的分类面还是\\(f(\\mathbf{x})=\\mathbf{w^T x}+b=0\\)。model通过fit函数训练之后，就能得到分类面的法向量\\(\\mathbf{w}\\)（coef_）和截距\\(b\\)（intercept_），也就是说对于分类面\\(w_1x_1+w_2x_2+b=0\\)，我们已经知道了\\(w_1\\)、\\(w_2\\)和\\(b\\)，所以就可以在平面\\(x_1Ox_2\\)上画出这条直线了。具体的方法就是使用np.linspace先在\\(x_1\\)上选取足够多的点，代入\\(w_1x_1+w_2x_2+b=0\\)得到对应的\\(x_2\\)。该分类面如下图的prob_threshold=0.5。\n事实上，逻辑回归的分类面可以不止一个，我们上面得到的分类面是\\(f(\\mathbf{x})=0\\)，代入到\\(g(\\mathbf{x})\\)就是\\(g(\\mathbf{x})=0.5\\)，也就是说当逻辑回归计算到的概率\u0026gt;=0.5分类为1，\u0026lt;0.5分类为0。但是我们也可以提高这个阈值，比如要求概率\u0026gt;=0.6分类为1，\u0026lt;0.6分类为0，这时，相当于我们对于分类为1的阈值提高了，要求更严格了，所以分类面应该向右边黑点方向移动。\n求解prob_threshold=0.6的分类面也不难，令\\(g(\\mathbf{x})=0.6\\)，得到\\(f(\\mathbf{x})=-lg(\\frac{1}{0.6}-1)\\)，剩下的过程和\\(f(\\mathbf{x})=0\\)是一样的。由此得到的分类面如上图的prob_threshold=0.6那条线，确实在prob_threshold=0.5的右边。类似的，可以画出prob_threshold=0.4的分类面。\n四、绘制分类面（法2） 还有一种简单粗暴的方法可以快速绘制出分类面。分类面的本质是在该分类面左右两侧的类标签不一样，如果我们把平面上所有点都预测一遍，对预测值为1的标上一种颜色，对预测值为0的标上另一种颜色，那么这两种颜色的交界处自然就是分类面了。\n首先，我们使用np.meshgrid生成网格数据点，关于np.meshgrid的用法，这篇博客的介绍很好理解。然后对所有网格数据点调用model.predict进行预测。最后使用plt.pcolormesh的plt.cm.Paired颜色主题进行着色，即类标签为1的一种颜色，类标签为0的另一种颜色。最后把原始训练数据点画上去，就得到博客开篇的那张分类面图：\n五、评估模型 model.predict是直接预测出类标号1或者0，而model.predict_proba是给出类标号分别为1和0的概率，用户可以自行根据prob_threshold进行分类。\n1 precision, recall, thresholds = precision_recall_curve(y_test, answer) precision_recall_curve的参数是正确答案y_test和model.predict_proba预测出来的概率，返回值分别表示不同threshold阈值下的precision和recall。sklearn官方的例子如下：\n1 2 3 y_true = np.array([0, 0, 1, 1]) y_scores = np.array([0.1, 0.4, 0.35, 0.8]) precision, recall, thresholds = precision_recall_curve(y_true, y_scores) 得到的结果如下：\n1 2 3 4 5 6 precision array([ 0.66..., 0.5 , 1. , 1. ]) recall array([ 1. , 0.5, 0.5, 0. ]) thresholds array([ 0.35, 0.4 , 0.8 ]) 比如当threshold=0.4时，表示\u0026gt;=0.4分类为1，\u0026lt;0.4分类为0，则预测结果为[0,1,0,1]。正确率为预测为1的结果中对了几个1/2=0.5，召回率为召回了多少个正确答案为1的结果1/2=0.5。其他阈值的计算类似。\n最后调用classification_report会算出不同类别的precision、recall和F1，以及对应的支持数据个数。\n1 2 3 4 5 6 7 8 precision recall f1-score support neg 0.89 1.00 0.94 42 pos 1.00 0.87 0.93 38 avg / total 0.94 0.94 0.94 80 average precision: 0.925000 六、绘制PRC曲线 PRC曲线就是precision recall curve，由于上一步已经调用precision_recall_curve得到了不同阈值下的precision和recall，这一步直接拿来用就好了。为了防止画出来的曲线抖动形成毛刺，我们使用plt.step阶跃函数来绘制，起到平滑的作用。最后使用plt.fill_between填充曲线下方的面积。得到下图：\n至此，整个示例讲解完毕。\n如果要使用逻辑回归处理多分类问题，只需要构造好多标签的训练数据就好了，剩下的就交给模型自己处理。LogisticRegression的multi_class参数可以设置使用何种策略求解多分类问题，one-vs-rest (OvR)即构建k个二元分类器，multinomial即使用Softmax回归，默认使用OvR。\n参考：\nhttp://blog.csdn.net/u011721501/article/details/49661585 ","permalink":"http://localhost:1313/posts/2017-12-05-logistic-regression-in-python/","summary":"\u003cp\u003e上一篇博客主要介绍了逻辑回归的理论知识，这篇博客咱们用Python机器学习包sklearn中的LogisticRegression做一个分类的实例。\u003c/p\u003e\n\u003cp\u003e数据还是学生样本，只有两个特征，分别是两门课的分数score1和score2，类标号y表示这个学生是否能被录取。先上分类效果图：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-12-05-logistic-regression-in-python/lr_face_3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e完整的Python代码如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 46\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 47\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 48\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 49\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 50\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 51\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 52\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 53\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 54\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 55\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 56\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 57\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 58\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 59\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 60\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 61\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 62\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 63\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 64\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 65\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 66\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 67\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 68\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 69\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 70\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 71\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 72\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 73\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 74\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 75\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 76\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 77\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 78\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 79\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 80\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 81\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 82\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 83\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 84\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 85\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 86\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 87\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 88\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 89\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 90\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 91\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 92\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 93\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 94\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 95\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 96\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 97\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 98\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 99\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e100\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e101\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e102\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e103\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e104\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e105\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e106\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e107\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e108\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e109\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e110\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e111\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e112\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e113\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e114\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e115\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e116\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e117\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e118\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e119\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e120\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e121\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e122\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e123\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e124\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e125\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e126\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e127\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e128\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e129\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e130\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e131\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e132\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e133\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e134\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e135\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e136\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e137\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e138\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e139\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e140\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e141\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e142\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e143\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e144\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e145\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -*- coding: utf-8 -*-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eCreated on Wed Nov 08 17:49:41 2017\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e@author: zhenlin\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np  \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pandas \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e pd\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.cross_validation \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e train_test_split\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.linear_model \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e LogisticRegression\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e plt\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.metrics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e precision_recall_curve\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.metrics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e classification_report\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 1. 构造数据\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esample_number \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e200\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 第一个高斯分布参数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emean1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e] \u003cspan style=\"color:#75715e\"\u003e# 两个维度上的均值\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecov1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [[\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e], [\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e]] \u003cspan style=\"color:#75715e\"\u003e# 两个维度的协方差矩阵，必须满足对称半正定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 第二个高斯分布参数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emean2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecov2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [[\u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e], [\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e]]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 从两个二元高斯分布中随机采样数据点\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eclass1_x1, class1_x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emultivariate_normal(mean1, cov1, sample_number)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eT \u003cspan style=\"color:#75715e\"\u003e# .T表示转置\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eclass2_x1, class2_x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emultivariate_normal(mean2, cov2, sample_number)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eT\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 两个高斯分布对应两个类标号\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edata \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [[class1_x1[i],class1_x2[i],\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(sample_number)]\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e[[class2_x1[i],class2_x2[i],\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e] \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(sample_number)]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 填充到pandas中\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edata \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataFrame(data,columns\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003escore_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e]]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresult_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 2. 训练模型\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eaverage_precision \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# 平均准确度\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eiters \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# 交叉验证次数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e xrange(iters):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 数据划分，80%用于训练，20%用于预测\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    x_train, x_test, y_train, y_test \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e train_test_split(score_data, result_data, test_size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 构造默认逻辑回归模型\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e LogisticRegression()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 训练\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efit(x_train, y_train)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 预测\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    predict_y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epredict(x_test)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 计算测试集上的准确度\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    average_precision \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emean(predict_y \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e y_test)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eaverage_precision \u003cspan style=\"color:#f92672\"\u003e/=\u003c/span\u003e iters\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 3. 绘制分类面 - 法1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ex1_min, x1_max \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin() \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e, score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egenerate_face\u003c/span\u003e(prob):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003enp\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elog(\u003cspan style=\"color:#ae81ff\"\u003e1.0\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e prob \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1.0\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    n \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e500\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    x1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elinspace(x1_min, x1_max, n)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# w1x1+w2x2+b=y\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003emodel\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecoef_[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e][\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e float(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecoef_[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e][\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])) \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e x1 \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e (y \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eintercept_) \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e float(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecoef_[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e][\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e x1, x2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epos_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eneg_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escatter(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pos_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e], y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pos_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e], color \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;black\u0026#39;\u003c/span\u003e, marker \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;o\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escatter(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e neg_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e], y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e neg_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e], color \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;red\u0026#39;\u003c/span\u003e, marker \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;*\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eface_04_x1, face_04_x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e generate_face(\u003cspan style=\"color:#ae81ff\"\u003e0.4\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eface_05_x1, face_05_x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e generate_face(\u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eface_06_x1, face_06_x2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e generate_face(\u003cspan style=\"color:#ae81ff\"\u003e0.6\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eplot(face_04_x1, face_04_x2)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eplot(face_05_x1, face_05_x2)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eplot(face_06_x1, face_06_x2)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlim(score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin(), score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylim(score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin(), score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elegend([\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;prob_threshold = 0.4\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;prob_threshold = 0.5\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;prob_threshold = 0.6\u0026#39;\u003c/span\u003e], loc\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;center left\u0026#39;\u003c/span\u003e, bbox_to_anchor\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0.865\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 4. 绘制分类面 - 法2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epos_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eneg_data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e data[data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;result\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eh \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.02\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003es1_min, s1_max \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin() \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e, score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003es2_min, s2_max \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin() \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e, score_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax() \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 生成s1在[s1_min, s1_max]，且s2在[s2_min, s2_max]的网格数据点\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# meshgrid含义参见：http://blog.sciencenet.cn/blog-791749-675394.html\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003es1, s2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emeshgrid(np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003earange(s1_min, s1_max, h), np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003earange(s2_min, s2_max, h))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 把两个坐标的值按列拼在一起构成二维数据点\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eZ \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epredict(np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ec_[s1\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eravel(), s2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eravel()])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 绘制边界和散点\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eZ \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Z\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ereshape(s1\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 坐标点是(s1[i], s2[i])，对应颜色是Z[i]，颜色主题使用plt.cm.Paired\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epcolormesh(s1, s2, Z, cmap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e plt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ePaired)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escatter(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pos_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e], y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pos_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e], color \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;black\u0026#39;\u003c/span\u003e, marker \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;o\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003escatter(x \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e neg_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e], y \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e neg_data[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e], color \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;red\u0026#39;\u003c/span\u003e, marker \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;*\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlim(s1\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin(), s1\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylim(s2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emin(), s2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score1\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;score2\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 5. 评估模型\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 对于测试数据，模型输出1的概率\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eanswer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epredict_proba(x_test)[:,\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 计算不同概率阈值下的P和R\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprecision, recall, thresholds \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e precision_recall_curve(y_test, answer)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# prob \u0026gt; 0.5的报告为1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ereport \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e answer \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(classification_report(y_test, report, target_names \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;neg\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;pos\u0026#39;\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;average precision: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%f\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003eaverage_precision)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 6. 绘制PRC曲线\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# step阶跃图，在点(recall[i],precision[i])进行跳变\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estep(recall, precision, color\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;b\u0026#39;\u003c/span\u003e, alpha\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e, where\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;post\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 对PRC下方填充颜色\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efill_between(recall, precision, step\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;post\u0026#39;\u003c/span\u003e, alpha\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.2\u003c/span\u003e, color\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;b\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Recall\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylabel(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Precision\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eylim([\u003cspan style=\"color:#ae81ff\"\u003e0.0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e1.05\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003exlim([\u003cspan style=\"color:#ae81ff\"\u003e0.0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e1.0\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etitle(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;2-class Precision-Recall curve\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eplt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e下面将逐模块介绍代码细节，大神可以略过。\u003c/p\u003e","title":"逻辑回归之Python应用实例"},{"content":"最近实验室在组织学习NG的机器学习视频，我也跟进了一下。讲到逻辑回归那一课，一直想不明白，逻辑回归到底是怎样分类的？逻辑回归的分类面在哪里？逻辑回归有像SVM的max margin那样清晰的推导过程吗？为什么需要Sigmoid函数？今天就让我们来一窥逻辑回归的始末。\n假设有一堆学生样本，为了简单起见，只有两个特征，分别是两门课的分数score1和score2，类标号y表示这个学生是否能被录取。可视化如下图，黑点表示y=1即被录取，红点表示y=0即未被录取，可以看到黑点处在score1和score2都比较高的区域。我们的任务就是给定这些训练样本，需要确定一个分类面来划分这两类数据。\n分类面的实质就是\\(y=\\mathbf{w^T x}+b\\)，其中\\(\\mathbf{w}\\)和\\(\\mathbf{x}\\)都是向量，对应到本例中，展开为\\(y=w_1x_1+w_2x_2+b\\)。所以，寻找分类面的过程就是寻找倾斜度\\(\\mathbf{w}\\)和截距\\(b\\)。\n因为分类的结果是离散的，只有0或者1，可以用感知机来分类。即\n但是怎样找这里的\\(\\mathbf{w}\\)和\\(b\\)使得分类结果最好呢，我们需要定义一个优化的目标函数或者说损失函数。这里只能定义为分类错误的个数，只能一点点挪动超平面来找分类错误最少的超平面了，即只能用暴力枚举的方法来找\\(\\mathbf{w}\\)和\\(b\\)。\n另一种方法是令我们的分类面算出来的值和真实标号越接近越好，即最小化误差\\((\\mathbf{w^T}x^{(i)}+b-y^{(i)})^2\\)，然后通过梯度下降求解\\(\\mathbf{w}\\)和\\(b\\)。\n但是这会有一个问题，对于上图数据，可以求到一个比较好的分类面，如下左图。但是如果数据中出现了如下右图右边的一个离群点或者噪声，为了最小化这个点的误差，即\\((\\mathbf{w^T}x^{(i)}+b-1)^2\\)，导致分类面往右偏了，这一偏直接导致很多y=1的样本分类错误。所以这种最小化误差的方法对离群点很敏感。\n假设分类超平面还是\\(f(\\mathbf{x})=\\mathbf{w^T x}+b\\)，我们希望的分类效果是这样的：\\(f(\\mathbf{x})=0\\)是分类面；\\(f(\\mathbf{x})\u003e0\\)分类为1，且不管\\(f(\\mathbf{x})\\)多大，都分为1；\\(f(\\mathbf{x})\u003c0\\)分类为0，且不管\\(f(\\mathbf{x})\\)多小，都分为0。\n因为类标号是离散的{0,1}，所以想到把\\(f(\\mathbf{x})\\)映射到[0,1]之间，即\\(g(f(\\mathbf{x}))\\)。为了满足上述条件，\\(g(f(\\mathbf{x}))\\)需要满足：\n\\(g(0)=0.5\\)，即在分类面上无法判断类标号是0还是1 当\\(f(\\mathbf{x})\u003e0\\)时，\\(g(f(\\mathbf{x}))\u003e0.5\\) 当\\(f(\\mathbf{x})\\rightarrow+\\infty\\)，\\(g(f(\\mathbf{x}))\\rightarrow 1\\)，且\\(g'(f(\\mathbf{x}))\\rightarrow 0\\)，即对于上右图右边的离群点，分类为1，且导数趋近于0，表示对其不敏感 当\\(f(\\mathbf{x})\u003c0\\)时，\\(g(f(\\mathbf{x}))\u003c0.5\\) 当\\(f(\\mathbf{x})\\rightarrow-\\infty\\)，\\(g(f(\\mathbf{x}))\\rightarrow 0\\)，且\\(g'(f(\\mathbf{x}))\\rightarrow 0\\)，和第3点类似 满足上述性质的函数之一就是Sigmoid函数，其定义域为\\([-\\infty,+\\infty]\\)，值域为[0,1]，正好把原始的函数结果\\(f(\\mathbf{x})\\)映射到了[0,1]，而概率的取值范围正好是[0,1]，所以Sigmoid函数正好可以作为分类为1的概率。\n所以逻辑回归最终的形式就是：\n$$g(\\mathbf{x})=\\frac{1}{1+e^{-(\\mathbf{w^T x}+b)}}$$分类面依然还是\\(f(\\mathbf{x})=\\mathbf{w^T x}+b=0\\)，因为\\(f(\\mathbf{x})=0\\)时，\\(g(\\mathbf{x})=0.5\\)，正好满足上述条件1。\nSigmoid函数的另一个优点是，它把原始函数值映射到了概率空间，这样就可以用极大似然的方法求解参数\\(\\mathbf{w}\\)和\\(b\\)。下面用参数\\(\\mathbf{\\theta}\\)代表参数\\(\\mathbf{w}\\)和\\(b\\)，用\\(h_{\\mathbf{\\theta}}(\\mathbf{x})\\)代表\\(g(f(\\mathbf{x}))\\)。则有：\n$$P(y=1|\\mathbf{x};\\mathbf{\\theta})=h_{\\mathbf{\\theta}}(\\mathbf{x})$$$$P(y=0|\\mathbf{x};\\mathbf{\\theta})=1-h_{\\mathbf{\\theta}}(\\mathbf{x})$$合并成一个式子就是：\n$$P(y|\\mathbf{x};\\mathbf{\\theta})=h_{\\mathbf{\\theta}}(\\mathbf{x})^y(1-h_{\\mathbf{\\theta}}(\\mathbf{x}))^{1-y}$$由于所有样本独立同分布（I.I.D.），似然函数就是\n$$L(\\mathbf{\\theta})=P(\\mathbf{y}|X;\\mathbf{\\theta})=\\prod\\limits_{i}P(y^{(i)}|\\mathbf{x}^{(i)};\\mathbf{\\theta})=\\prod\\limits_{i}h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})^{y^{(i)}}(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))^{1-y^{(i)}}$$最大化似然的含义就是，在给定样本\\(X\\)的情况下，我们想找一个参数\\(\\mathbf{\\theta}\\)，使得观测到类标号\\(\\mathbf{y}\\)的概率最大。\n最大化似然等价于最大化log似然，log展开之后就是：\n$$l(\\mathbf{\\theta})=logL(\\mathbf{\\theta})=\\sum\\limits_{i}y^{(i)}logh_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})+(1-y^{(i)})log(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))$$而很巧的是，最大化log似然，其实等效于最小化log对数损失。对于单个样本，损失为：\n$$cost(h_{\\theta}(\\mathbf{x}),y) = \\begin{cases}-log(h_{\\theta}(\\mathbf{x})) \u0026 \\text {if y=1} \\\\ -log(1-h_{\\theta}(\\mathbf{x})) \u0026 \\text{if y=0} \\end{cases}$$即如果正确类标号是1，但算出来的\\(h_{\\theta}(\\mathbf{x})\\)很接近0的话，则损失\\(-log(h_{\\theta}(\\mathbf{x}))\\)就会很大。类标号为0的情况类似。把这两种情况合成一个式子就是：\n$$cost(h_{\\theta}(\\mathbf{x}),y) = -ylog(h_{\\theta}(\\mathbf{x})) – (1-y)log(1-h_{\\theta}(\\mathbf{x}))$$所有样本的损失之和就是：\n$$J(\\mathbf{\\theta})=cost(h_{\\theta}(X),\\mathbf{y}) = \\sum\\limits_{i}-y^{(i)}logh_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-(1-y^{(i)})log(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))$$所以最大化对数似然\\(\\max l(\\mathbf{\\theta})\\)和最小化对数损失\\(\\min J(\\mathbf{\\theta})\\)是等效的！最优化求解的方法用梯度下降和牛顿法都可以。\n和Sigmoid很像的函数还有很多，比如y=arctan(x)也可以，不过Sigmoid有一个很好的特点，即它的导数可以由自身算出来，\\(f'(x)=f(x)(1-f(x))\\)。\n传统的逻辑回归只能处理二分类问题，那么怎样将其扩展到解决多分类问题呢？有两种方法，一种方法是建立k个二元分类器，比如类标号有A,B,C，则建立3个二元分类器，分别是1）A和非A；2）B和非B；3）C和非C。训练每个2元分类器时，都对所有的数据重新标号为0或1，这样共需要训练k个二元分类器。\n还有一种方法是直接将二元逻辑回归推广到多元回归，即Softmax回归，有关Softmax回归的内容，请参考此博客，非常详细。简单来说，二元逻辑回归的假设函数是：多元Softmax回归的假设函数在形式上有所不同：其中是模型的参数。请注意这一项对概率分布进行归一化，使得所有概率之和为1。\nSoftmax回归的损失函数如下，其实就是logistic回归损失函数的推广：\n二元逻辑回归是多元Softmax回归在k=2时的特例，令k=2并利用Softmax回归参数冗余的特点，可以得到一般形式的二元逻辑回归假设函数，具体可以看原博客。\n面对一个多元分类问题，是选择Softmax回归还是k个二元分类器呢，这取决于你的类别之间是否互斥，如果互斥，可以用Softmax回归，否则，请用k个二元分类器。\n这就是逻辑回归的理论知识，下一篇博客将介绍逻辑回归在Python中的应用。\n参考：\nhttp://www.cnblogs.com/sparkwen/p/3441197.html https://tech.meituan.com/intro_to_logistic_regression.html http://blog.csdn.net/bitcarmanlee/article/details/51165444 http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92 ","permalink":"http://localhost:1313/posts/2017-11-26-introduction-to-logistic-regression/","summary":"\u003cp\u003e最近实验室在组织学习\u003ca href=\"http://open.163.com/special/opencourse/machinelearning.html\"\u003eNG的机器学习视频\u003c/a\u003e，我也跟进了一下。讲到逻辑回归那一课，一直想不明白，逻辑回归到底是怎样分类的？逻辑回归的分类面在哪里？逻辑回归有像SVM的max margin那样清晰的推导过程吗？为什么需要Sigmoid函数？今天就让我们来一窥逻辑回归的始末。\u003c/p\u003e\n\u003cp\u003e假设有一堆学生样本，为了简单起见，只有两个特征，分别是两门课的分数score1和score2，类标号y表示这个学生是否能被录取。可视化如下图，黑点表示y=1即被录取，红点表示y=0即未被录取，可以看到黑点处在score1和score2都比较高的区域。我们的任务就是给定这些训练样本，需要确定一个分类面来划分这两类数据。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-11-26-introduction-to-logistic-regression/lr_samples.png\"\u003e\u003c/p\u003e\n\u003cp\u003e分类面的实质就是\\(y=\\mathbf{w^T x}+b\\)，其中\\(\\mathbf{w}\\)和\\(\\mathbf{x}\\)都是向量，对应到本例中，展开为\\(y=w_1x_1+w_2x_2+b\\)。所以，寻找分类面的过程就是寻找倾斜度\\(\\mathbf{w}\\)和截距\\(b\\)。\u003c/p\u003e\n\u003cp\u003e因为分类的结果是离散的，只有0或者1，可以用感知机来分类。即\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-11-26-introduction-to-logistic-regression/perceptron.svg\"\u003e\u003c/p\u003e\n\u003cp\u003e但是怎样找这里的\\(\\mathbf{w}\\)和\\(b\\)使得分类结果最好呢，我们需要定义一个优化的目标函数或者说损失函数。这里只能定义为分类错误的个数，只能一点点挪动超平面来找分类错误最少的超平面了，即只能用暴力枚举的方法来找\\(\\mathbf{w}\\)和\\(b\\)。\u003c/p\u003e\n\u003cp\u003e另一种方法是令我们的分类面算出来的值和真实标号越接近越好，即最小化误差\\((\\mathbf{w^T}x^{(i)}+b-y^{(i)})^2\\)，然后通过梯度下降求解\\(\\mathbf{w}\\)和\\(b\\)。\u003c/p\u003e\n\u003cp\u003e但是这会有一个问题，对于上图数据，可以求到一个比较好的分类面，如下左图。但是如果数据中出现了如下右图右边的一个离群点或者噪声，为了最小化这个点的误差，即\\((\\mathbf{w^T}x^{(i)}+b-1)^2\\)，导致分类面往右偏了，这一偏直接导致很多y=1的样本分类错误。所以这种最小化误差的方法对离群点很敏感。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-11-26-introduction-to-logistic-regression/lr_face_1.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-11-26-introduction-to-logistic-regression/lr_face_2.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e假设分类超平面还是\\(f(\\mathbf{x})=\\mathbf{w^T x}+b\\)，我们希望的分类效果是这样的：\\(f(\\mathbf{x})=0\\)是分类面；\\(f(\\mathbf{x})\u003e0\\)分类为1，且不管\\(f(\\mathbf{x})\\)多大，都分为1；\\(f(\\mathbf{x})\u003c0\\)分类为0，且不管\\(f(\\mathbf{x})\\)多小，都分为0。\u003c/p\u003e\n\u003cp\u003e因为类标号是离散的{0,1}，所以想到把\\(f(\\mathbf{x})\\)映射到[0,1]之间，即\\(g(f(\\mathbf{x}))\\)。为了满足上述条件，\\(g(f(\\mathbf{x}))\\)需要满足：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\\(g(0)=0.5\\)，即在分类面上无法判断类标号是0还是1\u003c/li\u003e\n\u003cli\u003e当\\(f(\\mathbf{x})\u003e0\\)时，\\(g(f(\\mathbf{x}))\u003e0.5\\)\u003c/li\u003e\n\u003cli\u003e当\\(f(\\mathbf{x})\\rightarrow+\\infty\\)，\\(g(f(\\mathbf{x}))\\rightarrow 1\\)，且\\(g'(f(\\mathbf{x}))\\rightarrow 0\\)，即对于上右图右边的离群点，分类为1，且导数趋近于0，表示对其不敏感\u003c/li\u003e\n\u003cli\u003e当\\(f(\\mathbf{x})\u003c0\\)时，\\(g(f(\\mathbf{x}))\u003c0.5\\)\u003c/li\u003e\n\u003cli\u003e当\\(f(\\mathbf{x})\\rightarrow-\\infty\\)，\\(g(f(\\mathbf{x}))\\rightarrow 0\\)，且\\(g'(f(\\mathbf{x}))\\rightarrow 0\\)，和第3点类似\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e满足上述性质的函数之一就是Sigmoid函数，其定义域为\\([-\\infty,+\\infty]\\)，值域为[0,1]，正好把原始的函数结果\\(f(\\mathbf{x})\\)映射到了[0,1]，而概率的取值范围正好是[0,1]，所以Sigmoid函数正好可以作为分类为1的概率。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\"\u003e\u003c/p\u003e\n\u003cp\u003e所以逻辑回归最终的形式就是：\u003c/p\u003e\n$$g(\\mathbf{x})=\\frac{1}{1+e^{-(\\mathbf{w^T x}+b)}}$$\u003cp\u003e分类面依然还是\\(f(\\mathbf{x})=\\mathbf{w^T x}+b=0\\)，因为\\(f(\\mathbf{x})=0\\)时，\\(g(\\mathbf{x})=0.5\\)，正好满足上述条件1。\u003c/p\u003e\n\u003cp\u003eSigmoid函数的另一个优点是，它把原始函数值映射到了概率空间，这样就可以用极大似然的方法求解参数\\(\\mathbf{w}\\)和\\(b\\)。下面用参数\\(\\mathbf{\\theta}\\)代表参数\\(\\mathbf{w}\\)和\\(b\\)，用\\(h_{\\mathbf{\\theta}}(\\mathbf{x})\\)代表\\(g(f(\\mathbf{x}))\\)。则有：\u003c/p\u003e\n$$P(y=1|\\mathbf{x};\\mathbf{\\theta})=h_{\\mathbf{\\theta}}(\\mathbf{x})$$$$P(y=0|\\mathbf{x};\\mathbf{\\theta})=1-h_{\\mathbf{\\theta}}(\\mathbf{x})$$\u003cp\u003e合并成一个式子就是：\u003c/p\u003e\n$$P(y|\\mathbf{x};\\mathbf{\\theta})=h_{\\mathbf{\\theta}}(\\mathbf{x})^y(1-h_{\\mathbf{\\theta}}(\\mathbf{x}))^{1-y}$$\u003cp\u003e由于所有样本独立同分布（I.I.D.），似然函数就是\u003c/p\u003e\n$$L(\\mathbf{\\theta})=P(\\mathbf{y}|X;\\mathbf{\\theta})=\\prod\\limits_{i}P(y^{(i)}|\\mathbf{x}^{(i)};\\mathbf{\\theta})=\\prod\\limits_{i}h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})^{y^{(i)}}(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))^{1-y^{(i)}}$$\u003cp\u003e最大化似然的含义就是，在给定样本\\(X\\)的情况下，我们想找一个参数\\(\\mathbf{\\theta}\\)，使得观测到类标号\\(\\mathbf{y}\\)的概率最大。\u003c/p\u003e\n\u003cp\u003e最大化似然等价于最大化log似然，log展开之后就是：\u003c/p\u003e\n$$l(\\mathbf{\\theta})=logL(\\mathbf{\\theta})=\\sum\\limits_{i}y^{(i)}logh_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})+(1-y^{(i)})log(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))$$\u003cp\u003e而很巧的是，最大化log似然，其实等效于最小化log对数损失。对于单个样本，损失为：\u003c/p\u003e\n$$cost(h_{\\theta}(\\mathbf{x}),y) = \\begin{cases}-log(h_{\\theta}(\\mathbf{x})) \u0026 \\text {if y=1} \\\\ -log(1-h_{\\theta}(\\mathbf{x})) \u0026 \\text{if y=0} \\end{cases}$$\u003cp\u003e即如果正确类标号是1，但算出来的\\(h_{\\theta}(\\mathbf{x})\\)很接近0的话，则损失\\(-log(h_{\\theta}(\\mathbf{x}))\\)就会很大。类标号为0的情况类似。把这两种情况合成一个式子就是：\u003c/p\u003e\n$$cost(h_{\\theta}(\\mathbf{x}),y) = -ylog(h_{\\theta}(\\mathbf{x})) – (1-y)log(1-h_{\\theta}(\\mathbf{x}))$$\u003cp\u003e所有样本的损失之和就是：\u003c/p\u003e\n$$J(\\mathbf{\\theta})=cost(h_{\\theta}(X),\\mathbf{y}) = \\sum\\limits_{i}-y^{(i)}logh_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-(1-y^{(i)})log(1-h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}))$$\u003cp\u003e所以最大化对数似然\\(\\max l(\\mathbf{\\theta})\\)和最小化对数损失\\(\\min J(\\mathbf{\\theta})\\)是等效的！最优化求解的方法用梯度下降和牛顿法都可以。\u003c/p\u003e\n\u003cp\u003e和Sigmoid很像的函数还有很多，比如y=arctan(x)也可以，不过Sigmoid有一个很好的特点，即它的导数可以由自身算出来，\\(f'(x)=f(x)(1-f(x))\\)。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e传统的逻辑回归只能处理二分类问题，那么怎样将其扩展到解决多分类问题呢？有两种方法，一种方法是建立k个二元分类器，比如类标号有A,B,C，则建立3个二元分类器，分别是1）A和非A；2）B和非B；3）C和非C。训练每个2元分类器时，都对所有的数据重新标号为0或1，这样共需要训练k个二元分类器。\u003c/p\u003e\n\u003cp\u003e还有一种方法是直接将二元逻辑回归推广到多元回归，即Softmax回归，\u003ca href=\"http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92\"\u003e有关Softmax回归的内容，请参考此博客，非常详细\u003c/a\u003e。简单来说，二元逻辑回归的假设函数是：多元Softmax回归的假设函数在形式上有所不同：其中是模型的参数。请注意这一项对概率分布进行归一化，使得所有概率之和为1。\u003c/p\u003e\n\u003cp\u003eSoftmax回归的损失函数如下，其实就是logistic回归损失函数的推广：\u003c/p\u003e\n\u003cp\u003e二元逻辑回归是多元Softmax回归在k=2时的特例，令k=2并利用Softmax回归参数冗余的特点，可以得到一般形式的二元逻辑回归假设函数，具体可以看原博客。\u003c/p\u003e\n\u003cp\u003e面对一个多元分类问题，是选择Softmax回归还是k个二元分类器呢，这取决于你的类别之间是否互斥，如果互斥，可以用Softmax回归，否则，请用k个二元分类器。\u003c/p\u003e\n\u003cp\u003e这就是逻辑回归的理论知识，下一篇博客将介绍逻辑回归在Python中的应用。\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://www.cnblogs.com/sparkwen/p/3441197.html\"\u003ehttp://www.cnblogs.com/sparkwen/p/3441197.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://tech.meituan.com/intro_to_logistic_regression.html\"\u003ehttps://tech.meituan.com/intro_to_logistic_regression.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://blog.csdn.net/bitcarmanlee/article/details/51165444\"\u003ehttp://blog.csdn.net/bitcarmanlee/article/details/51165444\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92\"\u003ehttp://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"初探逻辑回归"},{"content":"今年国庆假期连着中秋，加起来有8天时间，考虑到上半年忙着找工作，太累了，打算利用这个假期好好放松一下。\n一、行程规划 9月份的某个周末，规划好行程，制定了人生第一份独自出游的计划。整个过程只花了一天的时间，包括景点规划、住宿、车票购买。其实9月初就在脑海中规划这个事情，当时想的还是2个人同行，但是中间遇到了一些事情，最终只能一个人出游。一个人的话，不用考虑太多，为了节省路上的时间，全程购买高铁票，唯独从杭州回北京的高铁票没有了，幸运的是利用分流抢票软件抢到了一张硬座。住宿方面，一个人住青旅是再好不过了，既便宜、又可以遇到有意思的驴友。国内正规青旅可以在YHAChina上预定，我就是在这个网站上预定了杭州荷方青年旅社。没有在YHA上注册的青旅，可以通过booking.com预定，booking的优势是预定不收费，入住当天前取消也不收费，很不错，我就是在这个网站上预定了郑州的畅旅太空舱宾馆，不过到店之后老板建议我取消网站上的预定，进行线下支付，可以便宜几块钱。。。\n二、郑州半日游 3号到达郑州之后开始下雨，booking上的青旅地址有误，浪费了不少时间才找到宾馆并办理了入住。这个太空舱宾馆是在某个高层小区的6层，由居住房改造而成的，开始我还担心是黑店，要求查证件，毕竟第一次住青旅，还是有点担心。后来陆续来了一些驴友，有俄罗斯帅哥、韩国萌妹，又考察了一下房间和网上的照片是一样的，也就不太担心了。太空舱宾馆在医学院地铁站旁边，楼下有一个永辉超市，地理位置还是很不错的。\n办完入住之后已经4点多了，行程单上的郑州大学、河南博物馆肯定是没时间去了。于是去了农科路的一家巴奴火锅店吃火锅，一个人吃火锅心里真不是滋味。\n回青旅的路上，需要经过二七广场站，虽然有点晚了，但想着别错过郑州为数不多的景点，就跑出来看了看。二七广场周边都是一些商场，没啥意思，二七纪念塔也已经闭馆了。看着联体双塔造型的二七纪念塔，觉得好奇怪，为什么要建成联体的呢，网上查阅资料才得知，此塔为纪念二七大罢工（也称为京汉铁路工人大罢工）中牺牲的汪胜友、司文德两位烈士，我猜大多数郑州人也不知道吧。\n三、少林寺一日游 4号早上6:30起床，7点出门去郑州长途汽车中心站，准备坐大巴前往登封市。无奈天公不作美，还在下雨，汽车中心站在郑州火车站旁边，地铁只能到郑州火车站，前前后后还要步行。所以虽然青旅离中心站不远，但到中心站的时候已经8:15了。\n中心站大厅有很多自动售票机，而且可以在线支付，所以快速购买了8:40前往登封的汽车票，票价28元。进站的时候，安检居然不让带水进去，太可恶了，只能把水扔掉。上车之后，磨蹭到9点才发车，到达登封汽车站的时间已经是中午11点了。这个时候，有很多黑车拉客去少林寺景区，需要四五十块钱，在我犹豫的时候，直达少林寺景区的8路公交来了，只要5块钱，所以这里提醒大家，从登封汽车站去少林寺景区，等8路公交是最实惠的。\n前往少林寺景区的路上拥堵不堪。将近12点才到达景区门口。买了少林寺景区通票100元（学生票半价，后悔本科的时候没多出去玩玩）。\n进入景区之后，跟着人群，先后经过了演武厅、少林寺常住院、塔林。演武厅要2点才上演武术表演，所以略过。少林寺常住院里面就是各种殿，比如大雄宝殿、藏经阁。基本上每个殿里面都会有一个佛像，供游客参拜。说实话，这些殿堂，没有解说，看完也就看完了，并没有留下深刻的印象。\n3点钟的时候，做了个明智的选择，乘少林索道去看自然风光了（注意嵩杨索道是去看二祖庵的，还是人文景观）。下了索道，瞬间被眼前的景色惊艳住，由于下雨，山上烟雾缭绕，如临仙境；山涧瀑布飞流直下，哗哗作响；三皇栈道奇峰怪石，好不惊险。本来还想去感受下垂悬雾中的吊桥，由于时间关系只得作罢。\n回来的路上，雨过天晴，一扫连日来的阴霾，真是豁然开朗，心情极其舒畅！\n返程时没必要先坐8路公交到登封汽车站了，在景区出口右侧有从少林寺直达郑州火车站的大巴，票价30，很方便。\n四、杭州半日游 5号早上6点起床，赶8点去杭州的高铁。下午1点多到达杭州，根据导航前往荷方青年旅社。走出定安路地铁口，扑鼻而来的桂花香，甜到心里了，看着周围的白墙黛瓦、花格窗棂，我知道我来到江南了！\n入住的杭州荷方青年旅社在清河坊步行街的尽头，所以导航的时候，顺带逛了逛清河坊小吃街。沿街的酒幡和各种幌子瞬间有种回到南宋的感觉。旅舍由二十世纪五六十年代的传统江南风格的四合院民居改建而成，傍山而居。白墙黛瓦，花格窗棂，青石板，柚子树，清爽舒适。\n办理好入住之后，根据计划前往浙大紫金港校区。浙大今年120周年校庆，校门口站了两个志愿者，看起来一脸青涩，我问他们浙大的外语学院和计算机学院在哪里，他们说他们是大一新生，不知道这两个学院在哪…真不知道志愿者是干啥的。\n浙大紫金港校区里面有个人工湖，叫启真湖，还挺大的，有点类似于北大未名湖，湖里还养殖了黑天鹅，比武大的鉴湖有意思。除此之外，其他的建筑并没有太多的特点，也就是一个正常的学校了。走在校园里，也是满园的桂花飘香，此时好想回到武大。\n吃过晚饭之后，听说南山路的夜景不错，于是在南山路上溜达了一圈，权当饭后消食。\n五、西湖一日游 西湖很大，为了不错过每一个景点，我在网上查了很多攻略，其中知乎上一个用户的骑行线路深得我心，我是完完全全的根据他给的线路图一个景点一个景点遍历的。虽然清河坊离柳浪闻莺最近，但我还是硬生生先骑车到断桥残雪作为起点，固执可见一斑:-)\n另外，吸取了游少林寺看不懂人文景观的教训，这次我提前下载了一个口袋导游APP，该APP可以根据定位自动播放所在景点的介绍，很不错。\n漫步在西湖边，给我最大的感受就是舒适、悠闲。依然是熟悉的桂花香，随处可见的荷叶，清风徐来，杨柳依依。若是走累了，坐在湖边的石头上，观鱼逗鱼也别有一番乐趣。\n这次环游西湖很有意思的一件事是自己和自己玩定向越野。当我来到第一个景点断桥残雪时，我发现有一个御碑亭和对应的标志碑，上面都写着“断桥残雪”，只不过御碑亭上是康熙、乾隆等皇帝题写的，标志碑是中华人民共和国国务院立的。所以西湖十景的每个景点肯定都有对应的御碑亭和标志碑，我当时就决定，每到一个景点，请路人帮我拍一张和标志碑的合影，一来观景有了目标，二来强迫自己开口说话，和路人搭讪。\n由于我是严格按照上面的路线图游完的，没有经过双峰插云和三潭印月，所以只有8张合影。\n看着这些照片，我只想说路人和路人的拍照水平差别好大:-) 其实重点不是照片，而是寻找标志碑和找路人拍照的过程。比如雷锋夕照的标志碑并不在雷峰塔脚下，而是要走出雷峰塔，去到旁边的小山丘上才能找到。而柳浪闻莺的标志碑就更难找了，这个公园很大，标志碑的位置很隐蔽，我用百度地图导航来回走了3遍都没找到，最后问了景点的一个售货员才知道，这个标志碑在钱王祠前面，路的内测，需要绕一个很长的弯才能找到。最后当我找到柳浪闻莺的标志碑的时候，天已经黑了，由于位置比较偏僻，几乎没有游客，我等了十分钟，终于等来一个游客，帮我拍了一张合影，我猜80%的游客都未曾找到过这个标志碑。\n关于拍照。以前自己不敢也不想麻烦路人帮忙拍照，现在算是勇敢的做出尝试，其实路人是很愿意帮助别人的，当然我找的都是年轻人。有意思的是，在雷峰塔脚下，我看到一个和我一样独自游览雷峰塔的年轻人，想自拍又苦于无人帮助，于是我主动提出帮忙，正好也借机让他帮我拍照。互拍完之后我们就散了，没想到登上雷峰塔塔顶之后，我们又相遇了，一阵欣喜之后，又决定互拍。真是猿粪呀。\n环游西湖结束之后，准备找一个特色餐厅吃饭。阿溜说她之前在西湖边的外婆家吃过，挺不错的。外婆家的slogan是“我家就在西湖边”，那就去尝尝地道的外婆菜咯。取完号之后发现前面还有40+桌在等，火爆程度可见一斑。幸好带了kindle，就坐在湖边的长凳上看起了《月亮与六便士》。等到大概8点的时候，终于叫上号了。点了西湖有名的西湖醋鱼和东坡肉，还点了一杯稻花米乳。这大概是我点菜最失败的一次了，西湖醋鱼是草鱼，刺太多，而且有点腥；东坡肉一整块也没完全切开，拿下去重新切了之后，一块块七零八落，观感极差。至于稻花米乳，就是真真正正的稻米磨成的汁，无味。虽然菜不好吃，但没尝试过怎么知道呢，所以秉承“勇于尝试”的精神，我并没有感到太沮丧。\n回到清河坊特色街已经晚上10点了，为了挑选纪念品，挨个店进去考察。到最后发现能体现杭州西湖特色的也就是印有西湖十景的冰箱贴了，以后决定每去一个地方都买当地的冰箱贴，争取博士毕业前把工位上的墙贴满。\n六、西溪一日游 制定计划的时候，考虑到6号骑行环游西湖肯定会很累，所以决定7号去西溪，乘坐游船转一圈，正好歇歇脚。\n西溪湿地很大，分为东、中、西三个部分，中部是生活街区，免费开放的，东西部要收费。门票全价80，船票全价60，电瓶车5元。\n我在西溪天堂买好所有的票之后，乘坐电瓶车到天目山路周家村出入口，沿着游船线路，先后游玩了周家村→渔村烟雨→深潭口。\n周家村在办火柿节，没什么好玩的。渔村烟雨的花样比较多，主要有三个展厅，第一个展厅演示了西溪当地人养蚕织布的情景；第二个展厅西溪人家展示了西溪当地人家的家具、饮食等风格，诸如灶台、风车、打谷机等都和我老家的很像；第三个展厅表演西溪当地人婚嫁的情景，因为此处陆路不通，迎娶新娘只能乘船，挺有意思的。深潭口有一片养殖珍珠的水域，电影《非诚勿扰》曾在这里取景，除此之外，好像没有什么特别的地方。\n西溪游船如下左图所示，十几分钟就能从一个景点到达下一个景点，每经过一个景点，就会在船票上戳一个孔表示你不能再在这个点上船了。下次有机会要试一下右图的摇橹船，哈哈。\n西溪湿地公园给我的感觉和西湖是完全不一样的，西湖是美景+人文情怀，是阳春白雪似的美，声名远扬，游人如织；而西溪湿地更多的是江南水乡的体现，是下里巴人的美，更加贴近普通老百姓的生活，游人可能只有西湖的20%？\n转完一圈之后，在高庄出口骑小黄车回到西溪天堂，乘坐公交赶往杭州火车站。至此，结束了整个假期的旅行。\n七、总结 此次旅行，是我第一次独自规划、独自出行，前后游玩了郑州、杭州两个城市，规划的景点基本都去了，预期的目标也基本达到了，完成度80%。自己还算比较满意。\n总的来说，郑州和杭州这两座城市的差别还是很明显的。河南很好的把不同中心划分在了不同的城市，郑州作为行政中心和教育中心，而洛阳、开封、登封等城市则作为旅游文化中心，所以郑州其实没什么好玩的。而杭州是一个非常鲜明的旅游城市，西湖、西溪、西泠，三西旅游胜地，市民和游客的素质也比较高。\n另外有一点让我印象深刻的是，无论是郑州还是杭州，地铁和公交的购票系统都非常先进，全部可以互联网购票并支付。郑州的地铁可以用微信购票，然后到取票机上换取纸质票，这极大的方便了那些忘带或不愿带零钱的朋友。另外，郑州的安检是我遇到过的最严格的安检，无论是汽车站、火车站还是地铁站，进站安检时恨不得把你全身上下都摸一遍。说到购票的便利程度，杭州更胜一筹，杭州全城只要涉及交易的地方，都可以用支付宝，雷峰塔景区甚至有两个支付宝付款专用窗口，另外连公交车上都专门配备了支付宝扫码机，用户只需要在支付宝→城市生活上添加一张公交卡，就可以在线购票刷卡了，超级方便。我严重怀疑，这肯定是因为马云爸爸在杭州:-)至于交通的拥堵程度，虽然是旅游旺季，但是郑州和杭州的地铁、公交拥堵程度远低于平日的北京，这点作为帝都的北京还是扳回一局。\n旅途中稍微有一点让我遗憾的是，青旅并没有我预想的那么有趣。我原本以为青旅里住着一群来自天南海北的驴友，一到晚上，大家就聚在一起侃大山，分享各自的故事。事实证明，住青旅的人很大一部分是穷学生或者上班族，他们从白天到晚上都在旅游，结束了一天的旅行之后，回到青旅洗个澡睡觉，第二天6点钟爬起来赶火车，所以大家其实是没有时间促膝长谈的。\n最后，费用方面，花费2500-3000左右，算是穷游了，主要费用花在了交通（40%）、食宿（40%）、门票（20%），这个费用对我来说可以接受。\n好啦，今年的“第三座城”旅行计划超额完成任务！\n","permalink":"http://localhost:1313/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/","summary":"\u003cp\u003e今年国庆假期连着中秋，加起来有8天时间，考虑到上半年忙着找工作，太累了，打算利用这个假期好好放松一下。\u003c/p\u003e\n\u003ch1 id=\"一行程规划\"\u003e一、行程规划\u003c/h1\u003e\n\u003cp\u003e9月份的某个周末，规划好行程，制定了人生第一份独自出游的计划。整个过程只花了一天的时间，包括景点规划、住宿、车票购买。其实9月初就在脑海中规划这个事情，当时想的还是2个人同行，但是中间遇到了一些事情，最终只能一个人出游。一个人的话，不用考虑太多，为了节省路上的时间，全程购买高铁票，唯独从杭州回北京的高铁票没有了，幸运的是利用\u003ca href=\"http://www.12306bypass.com/\"\u003e分流抢票软件\u003c/a\u003e抢到了一张硬座。住宿方面，一个人住青旅是再好不过了，既便宜、又可以遇到有意思的驴友。国内正规青旅可以在\u003ca href=\"http://www.yhachina.com/index.php?hostID=1\"\u003eYHAChina\u003c/a\u003e上预定，我就是在这个网站上预定了杭州荷方青年旅社。没有在YHA上注册的青旅，可以通过\u003ca href=\"https://www.booking.com/s/34_6/11148825\"\u003ebooking.com\u003c/a\u003e预定，booking的优势是预定不收费，入住当天前取消也不收费，很不错，我就是在这个网站上预定了郑州的畅旅太空舱宾馆，不过到店之后老板建议我取消网站上的预定，进行线下支付，可以便宜几块钱。。。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/plan_2017_10.png\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/trip_2017_10.png\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"二郑州半日游\"\u003e二、郑州半日游\u003c/h1\u003e\n\u003cp\u003e3号到达郑州之后开始下雨，booking上的青旅地址有误，浪费了不少时间才找到宾馆并办理了入住。这个太空舱宾馆是在某个高层小区的6层，由居住房改造而成的，开始我还担心是黑店，要求查证件，毕竟第一次住青旅，还是有点担心。后来陆续来了一些驴友，有俄罗斯帅哥、韩国萌妹，又考察了一下房间和网上的照片是一样的，也就不太担心了。太空舱宾馆在医学院地铁站旁边，楼下有一个永辉超市，地理位置还是很不错的。\u003c/p\u003e\n\u003cp\u003e办完入住之后已经4点多了，行程单上的郑州大学、河南博物馆肯定是没时间去了。于是去了农科路的一家巴奴火锅店吃火锅，一个人吃火锅心里真不是滋味。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/zhengzhou_2.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/zhengzhou_4.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e回青旅的路上，需要经过二七广场站，虽然有点晚了，但想着别错过郑州为数不多的景点，就跑出来看了看。二七广场周边都是一些商场，没啥意思，二七纪念塔也已经闭馆了。看着联体双塔造型的二七纪念塔，觉得好奇怪，为什么要建成联体的呢，网上查阅资料才得知，\u003ca href=\"http://blog.sina.com.cn/s/blog_63558ae80101aaxd.html\"\u003e此塔为纪念二七大罢工（也称为京汉铁路工人大罢工）中牺牲的汪胜友、司文德两位烈士\u003c/a\u003e，我猜大多数郑州人也不知道吧。\u003c/p\u003e\n\u003ch1 id=\"三少林寺一日游\"\u003e三、少林寺一日游\u003c/h1\u003e\n\u003cp\u003e4号早上6:30起床，7点出门去郑州长途汽车中心站，准备坐大巴前往登封市。无奈天公不作美，还在下雨，汽车中心站在郑州火车站旁边，地铁只能到郑州火车站，前前后后还要步行。所以虽然青旅离中心站不远，但到中心站的时候已经8:15了。\u003c/p\u003e\n\u003cp\u003e中心站大厅有很多自动售票机，而且可以在线支付，所以快速购买了8:40前往登封的汽车票，票价28元。进站的时候，安检居然不让带水进去，太可恶了，只能把水扔掉。上车之后，磨蹭到9点才发车，到达登封汽车站的时间已经是中午11点了。这个时候，有很多黑车拉客去少林寺景区，需要四五十块钱，在我犹豫的时候，直达少林寺景区的8路公交来了，只要5块钱，所以这里提醒大家，从登封汽车站去少林寺景区，等8路公交是最实惠的。\u003c/p\u003e\n\u003cp\u003e前往少林寺景区的路上拥堵不堪。将近12点才到达景区门口。买了少林寺景区通票100元（学生票半价，后悔本科的时候没多出去玩玩）。\u003c/p\u003e\n\u003cp\u003e进入景区之后，跟着人群，先后经过了演武厅、少林寺常住院、塔林。演武厅要2点才上演武术表演，所以略过。少林寺常住院里面就是各种殿，比如大雄宝殿、藏经阁。基本上每个殿里面都会有一个佛像，供游客参拜。说实话，这些殿堂，没有解说，看完也就看完了，并没有留下深刻的印象。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_1.png\"\u003e\n\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e3点钟的时候，做了个明智的选择，乘少林索道去看自然风光了（注意嵩杨索道是去看二祖庵的，还是人文景观）。下了索道，瞬间被眼前的景色惊艳住，由于下雨，山上烟雾缭绕，如临仙境；山涧瀑布飞流直下，哗哗作响；三皇栈道奇峰怪石，好不惊险。本来还想去感受下垂悬雾中的吊桥，由于时间关系只得作罢。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_3.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_4.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_5.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_6.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e回来的路上，雨过天晴，一扫连日来的阴霾，真是豁然开朗，心情极其舒畅！\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_7.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/shaolinsi_8.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e返程时没必要先坐8路公交到登封汽车站了，在景区出口右侧有从少林寺直达郑州火车站的大巴，票价30，很方便。\u003c/p\u003e\n\u003ch1 id=\"四杭州半日游\"\u003e四、杭州半日游\u003c/h1\u003e\n\u003cp\u003e5号早上6点起床，赶8点去杭州的高铁。下午1点多到达杭州，根据导航前往荷方青年旅社。走出定安路地铁口，扑鼻而来的桂花香，甜到心里了，看着周围的白墙黛瓦、花格窗棂，我知道我来到江南了！\u003c/p\u003e\n\u003cp\u003e入住的杭州荷方青年旅社在清河坊步行街的尽头，所以导航的时候，顺带逛了逛清河坊小吃街。沿街的酒幡和各种幌子瞬间有种回到南宋的感觉。旅舍由二十世纪五六十年代的传统江南风格的四合院民居改建而成，傍山而居。白墙黛瓦，花格窗棂，青石板，柚子树，清爽舒适。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/hangzhou_1.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/hangzhou_2.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e办理好入住之后，根据计划前往浙大紫金港校区。浙大今年120周年校庆，校门口站了两个志愿者，看起来一脸青涩，我问他们浙大的外语学院和计算机学院在哪里，他们说他们是大一新生，不知道这两个学院在哪…真不知道志愿者是干啥的。\u003c/p\u003e\n\u003cp\u003e浙大紫金港校区里面有个人工湖，叫启真湖，还挺大的，有点类似于北大未名湖，湖里还养殖了黑天鹅，比武大的鉴湖有意思。除此之外，其他的建筑并没有太多的特点，也就是一个正常的学校了。走在校园里，也是满园的桂花飘香，此时好想回到武大。\u003c/p\u003e\n\u003cp\u003e吃过晚饭之后，听说南山路的夜景不错，于是在南山路上溜达了一圈，权当饭后消食。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/hangzhou_3.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/hangzhou_4.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"五西湖一日游\"\u003e五、西湖一日游\u003c/h1\u003e\n\u003cp\u003e西湖很大，为了不错过每一个景点，我在网上查了很多攻略，其中\u003ca href=\"https://www.zhihu.com/question/19834881/answer/211720257\"\u003e知乎上一个用户的骑行线路\u003c/a\u003e深得我心，我是完完全全的根据他给的线路图一个景点一个景点遍历的。虽然清河坊离柳浪闻莺最近，但我还是硬生生先骑车到断桥残雪作为起点，固执可见一斑:-)\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e另外，吸取了游少林寺看不懂人文景观的教训，这次我提前下载了一个口袋导游APP，该APP可以根据定位自动播放所在景点的介绍，很不错。\u003c/p\u003e\n\u003cp\u003e漫步在西湖边，给我最大的感受就是舒适、悠闲。依然是熟悉的桂花香，随处可见的荷叶，清风徐来，杨柳依依。若是走累了，坐在湖边的石头上，观鱼逗鱼也别有一番乐趣。\u003c/p\u003e\n\u003cp\u003e这次环游西湖很有意思的一件事是自己和自己玩定向越野。当我来到第一个景点断桥残雪时，我发现有一个御碑亭和对应的标志碑，上面都写着“断桥残雪”，只不过御碑亭上是康熙、乾隆等皇帝题写的，标志碑是中华人民共和国国务院立的。所以西湖十景的每个景点肯定都有对应的御碑亭和标志碑，我当时就决定，每到一个景点，请路人帮我拍一张和标志碑的合影，一来观景有了目标，二来强迫自己开口说话，和路人搭讪。\u003c/p\u003e\n\u003cp\u003e由于我是严格按照上面的路线图游完的，没有经过双峰插云和三潭印月，所以只有8张合影。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_2.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_3.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_5.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_6.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_8.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_9.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_7.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xihu_4.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e看着这些照片，我只想说路人和路人的拍照水平差别好大:-) 其实重点不是照片，而是寻找标志碑和找路人拍照的过程。比如雷锋夕照的标志碑并不在雷峰塔脚下，而是要走出雷峰塔，去到旁边的小山丘上才能找到。而柳浪闻莺的标志碑就更难找了，这个公园很大，标志碑的位置很隐蔽，我用百度地图导航来回走了3遍都没找到，最后问了景点的一个售货员才知道，这个标志碑在钱王祠前面，路的内测，需要绕一个很长的弯才能找到。最后当我找到柳浪闻莺的标志碑的时候，天已经黑了，由于位置比较偏僻，几乎没有游客，我等了十分钟，终于等来一个游客，帮我拍了一张合影，我猜80%的游客都未曾找到过这个标志碑。\u003c/p\u003e\n\u003cp\u003e关于拍照。以前自己不敢也不想麻烦路人帮忙拍照，现在算是勇敢的做出尝试，其实路人是很愿意帮助别人的，当然我找的都是年轻人。有意思的是，在雷峰塔脚下，我看到一个和我一样独自游览雷峰塔的年轻人，想自拍又苦于无人帮助，于是我主动提出帮忙，正好也借机让他帮我拍照。互拍完之后我们就散了，没想到登上雷峰塔塔顶之后，我们又相遇了，一阵欣喜之后，又决定互拍。真是猿粪呀。\u003c/p\u003e\n\u003cp\u003e环游西湖结束之后，准备找一个特色餐厅吃饭。阿溜说她之前在西湖边的外婆家吃过，挺不错的。外婆家的slogan是“我家就在西湖边”，那就去尝尝地道的外婆菜咯。取完号之后发现前面还有40+桌在等，火爆程度可见一斑。幸好带了kindle，就坐在湖边的长凳上看起了《月亮与六便士》。等到大概8点的时候，终于叫上号了。点了西湖有名的西湖醋鱼和东坡肉，还点了一杯稻花米乳。这大概是我点菜最失败的一次了，西湖醋鱼是草鱼，刺太多，而且有点腥；东坡肉一整块也没完全切开，拿下去重新切了之后，一块块七零八落，观感极差。至于稻花米乳，就是真真正正的稻米磨成的汁，无味。虽然菜不好吃，但没尝试过怎么知道呢，所以秉承“勇于尝试”的精神，我并没有感到太沮丧。\u003c/p\u003e\n\u003cp\u003e回到清河坊特色街已经晚上10点了，为了挑选纪念品，挨个店进去考察。到最后发现能体现杭州西湖特色的也就是印有西湖十景的冰箱贴了，以后决定每去一个地方都买当地的冰箱贴，争取博士毕业前把工位上的墙贴满。\u003c/p\u003e\n\u003ch1 id=\"六西溪一日游\"\u003e六、西溪一日游\u003c/h1\u003e\n\u003cp\u003e制定计划的时候，考虑到6号骑行环游西湖肯定会很累，所以决定7号去西溪，乘坐游船转一圈，正好歇歇脚。\u003c/p\u003e\n\u003cp\u003e西溪湿地很大，分为东、中、西三个部分，中部是生活街区，免费开放的，东西部要收费。门票全价80，船票全价60，电瓶车5元。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e我在西溪天堂买好所有的票之后，乘坐电瓶车到天目山路周家村出入口，沿着游船线路，先后游玩了周家村→渔村烟雨→深潭口。\u003c/p\u003e\n\u003cp\u003e周家村在办火柿节，没什么好玩的。渔村烟雨的花样比较多，主要有三个展厅，第一个展厅演示了西溪当地人养蚕织布的情景；第二个展厅西溪人家展示了西溪当地人家的家具、饮食等风格，诸如灶台、风车、打谷机等都和我老家的很像；第三个展厅表演西溪当地人婚嫁的情景，因为此处陆路不通，迎娶新娘只能乘船，挺有意思的。深潭口有一片养殖珍珠的水域，电影《非诚勿扰》曾在这里取景，除此之外，好像没有什么特别的地方。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_2.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_3.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_4.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_6.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_5.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_7.jpg\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e西溪游船如下左图所示，十几分钟就能从一个景点到达下一个景点，每经过一个景点，就会在船票上戳一个孔表示你不能再在这个点上船了。下次有机会要试一下右图的摇橹船，哈哈。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_8.jpg\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-10-08-2017-solo-travel-in-zhengzhou-and-hangzhou/xixi_9.jpg\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e西溪湿地公园给我的感觉和西湖是完全不一样的，西湖是美景+人文情怀，是阳春白雪似的美，声名远扬，游人如织；而西溪湿地更多的是江南水乡的体现，是下里巴人的美，更加贴近普通老百姓的生活，游人可能只有西湖的20%？\u003c/p\u003e","title":"2017年国庆旅行——郑州、杭州"},{"content":"上一篇博客简单介绍了Linux平台下不同性能分析工具的特点，最后是Google出品的gperftools胜出。今天我们将举一个用gperftools对链接库进行性能分析的例子。\n假设我们创建了一个TestGperftools项目，其中包括一个动态链接库模块complex和主程序main，build文件夹用于存放动态链接库和可执行程序，文件结构如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 czl@ubuntu:~/czl/TestGperftools$ tree . ├── build ├── complex │ ├── complex.cpp │ ├── complex.h │ └── Makefile ├── main │ ├── main.cpp │ └── Makefile └── Makefile 3 directories, 6 files 其中complex借用了这篇博客的第三个公式计算圆周率π，主要用来模拟一个很耗时的操作。该模块会生成一个libcomplex.so动态链接库供外部调用。其complex.cpp函数如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #include \u0026#34;complex.h\u0026#34; Complex::Complex() {} void Complex::recordPI(double x) { pis.push_back(x); if (pis.size() \u0026gt; 10000) pis.clear(); } double Complex::calculatePI(int n) { double x = 2, z = 2; int a = 1, b = 3; while (n--) { z = z * a / b; x += z; a++; b += 2; recordPI(x); } return x; } Complex::~Complex() {} calculatePI为计算π的函数，传入参数n表示循环计算的次数，也就是第三个公式中累加的次数，当然次数越多，精度越高，时间越长。recordPI是刻意加上去的一个函数，作为calculatePI的被调函数，同时包含STL的一些操作。该模块的Makefile如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SRC = complex.cpp OBJS = $(SRC:.cpp=.o) BUILD_DIR = ../build TARGET = libcomplex.so all: $(TARGET) clean: -rm -f $(OBJS) $(TARGET) $(OBJS): $(SRC) $(CXX) -fPIC -c $(SRC) $(TARGET): $(OBJS) $(CXX) -shared -o $(TARGET) $(OBJS) cp $(TARGET) $(BUILD_DIR) 注意生成动态链接库的Makefile格式，编译时需要-fPIC生成位置无关目标文件，链接时需要-shared选项。由Makefile文件可知，该模块会生成libcomplex.so动态链接库。\n下面我们来看一下main.cpp文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #include\u0026#34;../complex/complex.h\u0026#34; #include\u0026lt;iostream\u0026gt; #include\u0026lt;iomanip\u0026gt; using namespace std; vector \u0026lt; long long \u0026gt; vll; void recordSum(long long s) { vll.push_back(s); if (vll.size() \u0026gt; 10000) vll.clear(); } long long calculateSum(long long n) { long long s = 0; for (int i = 1; i \u0026lt;= n; ++i) { s += i; recordSum(s); } return s; } int main() { long long n = 1000000000; Complex c; double p = c.calculatePI(n); long long s = calculateSum(n); cout \u0026lt;\u0026lt; \u0026#34;pi=\u0026#34; \u0026lt;\u0026lt; fixed \u0026lt;\u0026lt; setprecision(13) \u0026lt;\u0026lt; p \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;1+2+…+\u0026#34; \u0026lt;\u0026lt; n \u0026lt;\u0026lt; \u0026#34;=\u0026#34; \u0026lt;\u0026lt; s \u0026lt;\u0026lt; endl; return 0; } main函数除了调用Complex类计算π之外，还会调用自身的calculateSum计算1+2+…+n的和，并且也会调用一个刻意的recordSum函数。该模块的Makefile如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SRC = main.cpp OBJS = $(SRC:.cpp=.o) BUILD_DIR = ../build LIBS = -lcomplex TARGET = main all: $(TARGET) clean: -rm -f $(OBJS) $(TARGET) $(OBJS): $(SRC) $(CXX) -c $(SRC) $(TARGET): $(OBJS) $(CXX) -L$(BUILD_DIR) -o $(TARGET) $(OBJS) $(LIBS) cp $(TARGET) $(BUILD_DIR) 可以看到该模块链接了-lcomplex动态链接库，注意第16行链接的顺序不能乱，-L要在-o前面，$(OBJS)要在$(LIBS)的前面。\n编译的方法为：直接在TestGperftools目录中执行make，主make会分别进入complex和main文件夹执行各自的子make，最后动态链接库libcomplex.so和可执行程序main都复制到了build文件夹中。具体代码请看根目录中的Makefile文件，这里不再列出。\n接下来我们要安装gperftools性能分析工具，在Ubuntu下安装非常简单，直接执行以下命令，该命令安装了两个软件，google-perftools和graphviz，前者就是gperftools，后者是用于可视化性能分析结果的。\n1 sudo apt-get install google-perftools graphviz 下面我们开始对main进行性能分析，我们预期libcomplex.so中的calculatePI会是主要的性能瓶颈，看看gperftools给出的结果是否符合预期。\n进入到build文件夹，执行以下命令： 1 $ export LD_LIBRARY_PATH=. 该命令表示查找动态链接库的首选位置为当前路径，如果没有设置，在执行main程序时，会找不到libcomplex.so而报错：./main: error while loading shared libraries: libcomplex.so: cannot open shared object file: No such file or directory\n执行以下命令： 1 $ LD_PRELOAD=/usr/lib/libprofiler.so.0 CPUPROFILE=./main.prof ./main LD_PRELOAD用于预加载gperftools的动态链接库，有些发行版可能是/usr/lib/libprofiler.so，我的是后面有.0，也有可能是其他数字；CPUPPROFILE用于指定生成的性能分析结果文件路径，我设置的是在当前文件夹下生成main.prof；最后接上需要分析的可执行程序./main，如果你的可执行程序由命令行参数，也可以一并跟在./main后面。\n程序执行结束之后，输出如下： 1 2 3 pi=3.1415926535898 1+2+…+1000000000=500000000500000000 PROFILE: interrupts/evictions/bytes = 4680/1585/112344 前两行是程序输出，最后一行是gperftools输出。\n最后进行性能分析，执行以下命令： 1 $ google-pprof –web ./main ./main.prof –web表示性能分析结果以网页的形式展示，后面依次跟可执行程序和上一步生成的*.prof文件。程序自动打开浏览器，性能分析结果如下图所示。\n详情页中，左上角给出了汇总结果，总共采样了4680次，扔掉了采样次数小于23和4次的节点和边。gperftools 性能分析图中，每个节点代表一个函数，节点内包含三个信息，从上往下依次为：函数名、该函数自身被采样到的次数（及占总采样数的比例）、该函数和被调函数被采样到的次数（及占总采样数的比例），节点的大小正比于该函数自身被采样到的次数；每条边代表调用者到被调用者被采样到的次数。简而言之，图中节点越大，该函数自身耗时占比越大，越可能是性能瓶颈。\n由图可知，Complex::calculatePI确实是性能瓶颈，占用了23.0%的时间，而calculateSum只占用了6.3%的时间。另外，我们刻意增加的recordPI和recordSum也占用了不少的时间，因为需要不断的push_back以及重新分配内存，所以占用的时间也很多。\n总的来说，这样的分析结果是符合预期的，我们能够根据这样一个热点分布图，找到性能瓶颈，然后有针对的进行性能优化。比如针对上面的例子，我们可以优化calculatePI，如果对π的精度要求不是很高，可以减少while循环的次数；对于vector的push_back，因为我们是大于10000时会clear一下，为了避免多次的clear和reallocate，可以预先设置vector的大小为10000，然后重复对0~9999下标进行填充，这样就只需一次分配内存了。 本博客完整的TestGperftools项目可以查看我的Github：TestGperftools。\ngperftools套装中还包括a memory leak detgector和a heap profiler，用于检查程序中是否有内存泄漏的风险以及对内存堆栈的分析，具体介绍可以查看其Github上的WIKI。\n一些有用的链接：\n3步学会使用gperftools的超简易教程：https://wiki.geany.org/howtos/profiling/gperftools gperftools官方文档：https://gperftools.github.io/gperftools/cpuprofile.html ","permalink":"http://localhost:1313/posts/2017-02-07-gperftools-tutorial/","summary":"\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2017-02-07-introduction-to-performance-analysis-tools-in-linux/\"\u003e上一篇博客简单介绍了Linux平台下不同性能分析工具的特点\u003c/a\u003e，最后是Google出品的gperftools胜出。今天我们将举一个用gperftools对链接库进行性能分析的例子。\u003c/p\u003e\n\u003cp\u003e假设我们创建了一个TestGperftools项目，其中包括一个动态链接库模块complex和主程序main，build文件夹用于存放动态链接库和可执行程序，文件结构如下所示。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eczl@ubuntu:~/czl/TestGperftools$ tree\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e.\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e├── build\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e├── complex\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e│   ├── complex.cpp\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e│   ├── complex.h\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e│   └── Makefile\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e├── main\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e│   ├── main.cpp\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e│   └── Makefile\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e└── Makefile\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e directories, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e files\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e其中complex借用了\u003ca href=\"http://www.cppfans.com/articles/basecalc/c_pi_10000.asp\"\u003e这篇博客\u003c/a\u003e的第三个公式计算圆周率π，主要用来模拟一个很耗时的操作。该模块会生成一个libcomplex.so动态链接库供外部调用。其complex.cpp函数如下：\u003c/p\u003e","title":"Gperftools性能分析工具使用教程"},{"content":"这半年一直在研究pLink 2的加速策略，了解了相关的性能分析工具，现记录如下。\n对软件进行加速的技术路线一般为：首先对代码进行性能分析（ performance analysis，也称为 profiling），然后针对性能瓶颈提出优化方案，最后在大数据集上评测加速效果。所以进行性能优化的前提就是准确地测量代码中各个模块的时间消耗。听起来很简单，不就是测量每行代码的运行时间吗，直接用time_t t=clock();不就好了，但是事情并没有那么简单。如果只进行粗粒度的性能分析，比如测量几个大的模块的运行时间，clock()还比较准确，但是如果测量的是运算量比较小的函数调用，而且有大量的小函数调用，clock()就不太准确了。\n比如下面的一段代码，我最开始的性能分析方法是在fun1()~fun3()前后添加time_t t=clock()，然后作差求和的。但是3个fun()加起来的时间居然不等于整个while循环的时间，有将近50%的时间不翼而飞了！\n1 2 3 4 5 6 7 8 9 10 11 while (true) { if (fun1()) { for (int i = 0; i \u0026lt; k; ++k) { if (flag1) { fun2(); } } } else { fun3(); } } 一种可能是while循环以及内部的for循环本身占用了较多的时间，但是好像不太可能呀。还有一种可能是clock()测量有误，time_t只能精确到秒，clock_t只能精确到毫秒，如果一次fun*()的时间太短，导致一次测量几乎为0，那么多次的while和for循环调用累加的时间也几乎为0，导致实际测量到的fun*()时间远小于真实时间。所以自己用代码进行性能分析可能会有较大的误差，最好借助已有的性能分析工具。\n性能分析和操作系统有较大的关系。因为C++11以前的多线程在不同操作系统中有不同的实现，比如在Windows中使用的是Win32 threads，需要包含windows.h头文件，在Linux中使用的是POSIX Threads，需要包含pthread.h头文件，所以选择性能分析工具首先需要看代码使用的多线程是哪个版本。如果使用的是Win32 threads，则需要在Windows平台选择热点分析工具；如果使用的是POSIX Threads，则需要在Linux平台选择热点分析工具；当然，如果代码中没有多线程或者采用的多线程是C++11标准统一的多线程，原则上可以忽略操作系统的限制。\nWindows平台上，之前用过微软的Visual Studio工具进行Profiling，效果很不错，网上的介绍也很多，这里就不详细介绍了。\n通过网络搜索我发现了三款Linux平台下主流的热点分析工具，分别是GNU gprof、Valgrind和Google perftools，三款工具的主要特点如下表：\n工具 使用命令 是否需要重新编译 Profiling速度 是否支持多线程热点分析 是否支持链接库热点分析 GNU gprof ./test; gprof ./test ./gmon.out 是 慢 否 否 Valgrind Valgrind –tool=callgrind ./test 否 非常慢 是 是 Google perftools LD_PRELOAD=/usr/lib/libprofiler.so CPUPROFILE=./test.prof ./test 否 快 是 是 GNU gprof是GNU G++自带的热点分析工具，使用方法是：1. 使用-pg选项重新编译代码；2. 执行程序./test，生成热点分析结果gmont.out；3.使用gprof查看结果gprof ./test ./gmon.out。因为gprof要求用-pg重新编译代码，需要在Debug模式下进行Profiling，所以速度较慢。另外gprof不支持多线程的热点分析。这个工具另一个大问题是，不支持链接库的热点分析。很多大型项目为了模块化管理会生成很多动态链接库供其他程序调用，如果要分析每个模块的热点，这个工具就不适用了。\nValgrind是一系列工具的套装，包括内存分析、热点分析等。它的使用非常简单，安装好之后，直接调用Vallgrind中的callgrind工具即可，命令为Valgrind –tool=callgrind ./test。使用该工具Profiling无需重新编译代码，也支持多线程和链接库的热点分析，但是由于Profiling原理的特殊性，其Profiling速度非常之慢，比直接运行程序慢了将近50倍，所以并不适合稍大型程序的热点分析。本人试用之后发现其结果展示做得也不是很好。\nGoogle perftools原是Google内部的性能分析工具，后来在Github上开源了，地址是https://github.com/gperftools/gperftools。gperftools 的工作原理为通过定期采样当前正在执行的指令进行性能分析，如果某个函数被采样到的次数越多，则该函数在执行时占用的时间比例越大，很可能就是性能瓶颈。 gperftools 可以在被测软件处于 Release 模式下进行性能分析，所以能最大程度的模拟软件的实际使用情况。这个工具使用起来也非常简单，只需Preload其.so文件并指定生成的Profiling文件路径即可，命令为LD_PRELOAD=/usr/lib/libprofiler.so CPUPROFILE=./test.prof ./test。程序结束之后，使用命令google-pprof –web ./test ./test.prof即可查看热点分析结果。使用该工具Profiling无需重新编译代码，也支持多线程和链接库的热点分析，同时由于其是通过定期采样正在执行的指令进行热点分析，所以Profiling速度非常快，和正常release下执行程序的速度几乎相当。\n通过试用，发现gperftools的易用性、可视化效果等都是最好的，所以推荐大家使用gperftools。我最看重gperftools的特点是支持链接库和多线程的热点分析。下一篇博客将简单举一个用gperftools对链接库进行性能分析的例子。\n参考：\nLinux平台性能分析工具“综述”：http://gernotklingler.com/blog/gprof-valgrind-gperftools-evaluation-tools-application-level-cpu-profiling-linux/\n","permalink":"http://localhost:1313/posts/2017-02-07-introduction-to-performance-analysis-tools-in-linux/","summary":"\u003cp\u003e这半年一直在研究pLink 2的加速策略，了解了相关的性能分析工具，现记录如下。\u003c/p\u003e\n\u003cp\u003e对软件进行加速的技术路线一般为：首先对代码进行性能分析（ performance analysis，也称为 profiling），然后针对性能瓶颈提出优化方案，最后在大数据集上评测加速效果。所以进行性能优化的前提就是准确地测量代码中各个模块的时间消耗。听起来很简单，不就是测量每行代码的运行时间吗，直接用time_t t=clock();不就好了，但是事情并没有那么简单。如果只进行粗粒度的性能分析，比如测量几个大的模块的运行时间，clock()还比较准确，但是如果测量的是运算量比较小的函数调用，而且有大量的小函数调用，clock()就不太准确了。\u003c/p\u003e\n\u003cp\u003e比如下面的一段代码，我最开始的性能分析方法是在fun1()~fun3()前后添加time_t t=clock()，然后作差求和的。但是3个fun()加起来的时间居然不等于整个while循环的时间，有将近50%的时间不翼而飞了！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e (true) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (fun1()) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e (\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e; i \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e k; \u003cspan style=\"color:#f92672\"\u003e++\u003c/span\u003ek) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e (flag1) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        fun2();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  } \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    fun3();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e一种可能是while循环以及内部的for循环本身占用了较多的时间，但是好像不太可能呀。还有一种可能是clock()测量有误，time_t只能精确到秒，clock_t只能精确到毫秒，如果一次fun*()的时间太短，导致一次测量几乎为0，那么多次的while和for循环调用累加的时间也几乎为0，导致实际测量到的fun*()时间远小于真实时间。所以自己用代码进行性能分析可能会有较大的误差，最好借助已有的性能分析工具。\u003c/p\u003e","title":"Linux性能分析工具简介"},{"content":"2017农历新年的钟声都已经敲响了，我这2016的年终总结才开始动笔。\n2016年，经历了很多，也成长了很多，遇到了很多曾经以为只会出现在电视剧中的场景，令人开始怀疑这个世界。前几天在朋友圈看到一个同学发的状态，觉得很适合作为这篇年终总结的开端。（同学你要是觉得被侵权了，告诉我，我立马删掉:-)）\n每个家庭的故事都会是一部长篇史诗。曾经总以为很多情节只会出现在电视剧中，现实的生活很是平淡无味，没有任何波澜，偶尔甚至还会抱怨一下自己不是故事的女主角，其不知现实的生活相比于电视剧，往往是有过之而无不及。 今天哥哥来电话了，从“天津”，一个美丽的谎言，一直在继续，还会坚持很久… 奶奶很开心… ——by angel\n关于学习和科研。上半年在雁栖湖完成了研一的下半个学期，完完全全的结束了自己的学生时代。下半年开始进入实验室，直面惨淡的科研。原本以为会由组里的大师兄超哥指导我，没想到中秋前3天，直接接到H Boss的指令，要在中秋前完成一项我从来没做过的评测。不知道怎样设计实验，不知道怎样计算评价指标，甚至连需要评测的软件都不熟。不过好在加班加点完成了。\n凌乱的工位\n从9月到10月中旬，一直在各种数据集上做各种对比评测，基本上一周做完一个评测数据集，完成一份报告，直接提交给H Boss，下一周在做另一个评测的同时，要根据导师的反馈建议修改完上一个评测报告。这一个多月的时间，共完成了5份报告共计14个版本，真的是要吐了。\n后来发现pLink要比对手慢，于是就尝试各种加速策略。开始从外围查找原因，尝试了各种策略，虽然多多少少能加速，但是效果都不完美，有可能对精度有影响。直到12月份，借助谷歌的开源性能分析工具gperftools才找到了软件的性能瓶颈，开始优化并取得了好几倍的加速效果。12月份对软件本身的性能优化大概是我这半年做的唯一一个和计算机本身有点关系的工作吧，可能也是为数不多的我比较享受的一件事。\n整个这半年的工作，都是H Boss御驾亲征，亲自指导，当然这样有利有弊。好处是能直接和H Boss对话，机会难得呀，H那种严于律己、追求完美的品质，H的为人处世、口才都是非常值得我学习的。坦白说，虽然我只是无名小卒一个，但从小到大，真正让我从心里佩服的人没几个，H Boss绝对算是其中之一。当然不便之处也非常明显，首先会感觉特别累和压抑，除了每周一次的面谈，每天的邮件，还经常在晚上11点多收到老师的工作微信。所以这半年确实不轻松，工作日晚上基本都在加班，而且加班到晚上11点也是常事，周末也至少工作一天。几乎没有时间运动，身体素质应该下降了不少。与人的沟通也非常少，好几个师兄师姐都问我为啥看我整天都在工位上坐着，好像从来没有动过。夸张点说就是每天晚上下班要走的时候，才发现自己这一天还没有说话。\n当然半年高强度的工作，也有不少的收获，基本摸清了pLink代码的来龙去脉，也加速了两三倍，自己的表现也稍微得到了老师和同学的肯定。不过我自己还是不太满意的，加速并没有达到理想的效果。\n关于亲情。随着我们两兄弟的大学毕业，家里的情况也在稍稍好转，但是只能算是曲折前进吧。以前小的时候，都是爸妈两个人闹，现在哥哥出场了，真是可笑。谈了快两年的女朋友，女方父母又是要查户口本，又是催着要付定金，说什么不给定金就要拉回老家相亲。这TM比电视剧还荒唐，真把自己当商品了，是不是给的钱多就跟谁呀，混蛋。哥哥也不是个省油的灯，分手之后没过多久说什么被公司派去新加坡学习了，去了之后，连个固定的联系方式都没有，三天两头失联，都老大不小的人了，还让父母担心。工作了两年，一分钱都没攒到，连大学的助学贷款都要我这个还在上学的弟弟替他还。女朋友分手也就算了，没赚到钱也不要紧，关键是你不能让家人这样担心你呀，你定期给家人打个电话，说说你到底在哪里干什么，既然到了新加坡，发几张国外的照片回来分享一下，不可以吗？已经两年过年没回家了，而且两年除夕居然连个电话都没有，这不是不孝是什么，混蛋。\n今年妈妈也终于愿意外出挣钱了，虽然不多，但是起码在和爸爸一起努力。家里装修好了一层房子，但是也就是把墙什么的弄好了，家具还没制备。本来想着过年回家给家里买个小米电视，但是爸妈死活说不要买，现在买了也就过年看几天，不划算。后来只好作罢。放假给老爸买的红米手机，终于在除夕这一天拿到手了。给爸妈包的红包，也在按计划逐年的递增着。\n自己有时候也埋怨家里，为什么家境这么的不堪，为什么父母没有达到我理想的高度，为什么哥哥这么不争气，为什么没有人关心我。但是埋怨有用吗，肯定是没用的，还是要看各自的造化。\n关于爱情。我还是太幼稚，看看我的家境吧，有哪个女生愿意摊上我家这些破事呢，我就不应该奢望有什么爱情。不过今年上半年，爱情确实来过，抛开所有的一切，纯粹的校园爱情。可是10月份的一件事，彻底打醒了我，爱情没有那么简单，需要考虑的问题太多了。关于那段时间的记忆，写过很多文字，也流过很多泪。回顾整个下半年，欣欣和我的状态都不太好，除了那件事的原因，和工作变化也有很大的关系。我们都从雁栖湖到中关村，需要经历一个由学生到科研工作者的角色转变，面对科研的未知，都显得有些手足无措。科研的不顺，生活的压力以及家里的一些烦心事，一股脑的涌向了我们，矛盾也时有发生。经历过不少的磕磕绊绊，总算顺利度过了2016年，没有了热恋时期的疯狂，生活终要归于平淡。正所谓陪伴是最长情的告白，爱情的意义是否就在于两个人一起经历，一起成长呢。让我们共同守护。\n关于友情。是的，我还欠很多人一顿饭。很多同学，如果长时间不见面，恐怕真的要忘掉了。\n关于个人提高。上半年忙于课程学习，下半年忙于科研，花在个人提高上的时间真的是太少了。不过感谢有欣欣一直做我的榜样，我现在的小目标就是希望比欣欣看更多的书、刷更多的题。\n另外下半年回到市区之后，也去了一些之前没去过的地方，比如：清华艺术博物馆、繁星戏剧村、中国美术馆、三联书店、香山等地。其中前两个地方都是和欣欣一起去的，感觉超棒~第一次看达芬奇的特展，开始了解这样一个天才，后来还看过他的传记；第一次去剧院看话剧，感觉和看电影完全不一样，小剧场的效果也是棒棒的。2016年12月31日也是一个值得纪念的日子：\n2017跨年活动~\n今年借着CNCP2016会议的机会，去了一次大连，见识了一下海滨城市的风貌，后来还跑去渤海学游泳，海水很脏，而且咸得发苦。希望今后每年都去一个除了上班地点和家里之外的第三个城市。\n大连滨海国家地质公园\n大连滨海国家地质公园\n最后看看年初计划的完成情况：\n完成国科大下学期的课程任务：完成 接手pLink软件：完成 刷完LeetCode所有题目：进度147/461，没有完成 读10本书：目前读了《数学之美》、《大话设计模式》、《我不知道该说什么，关于死亡还是爱情》、《男人来自火星、女人来自金星，卷I》、《只有医生知道，卷I》、《文学的种子》、《讲理》、《暗时间》、《达·芬奇传：放飞的心灵》、《人间失格》，刚好10本，圆满完成任务:-) 去电影院看10场电影：目前看了《美人鱼》、《北京遇上西雅图之不二情书》、《忍者神龟2：破影而出》、《七月与安生》、《湄公河行动》、《比利·林恩的中场战事》、《你的名字》、《血战钢锯岭》，只有8场，其中7场是和欣欣一起看的~ 改正坐姿：有一段时间刻意改正了，但是这东西貌似改不过来？ 除了LeetCode完成度太差之外，其他计划完成度还是蛮高的。下面定一下2017年的年度计划：\n发表pLink 2文章 至少完成毕业工作的80% 刷完LeetCode所有简单题和中等题，找工作之前最好刷完两遍 找到一个满意的工作 读10本书 去电影院看10场电影 看一场话剧（音乐会、歌剧等都可以） 学会游泳 去第三个城市 简短总结一下我的2016：完成了由上课到科研的转变；开始有能力感恩家人；遇到了欣欣，由一个人变成了两个人；第一次去剧场看话剧。展望2017，找工作和准备毕业迫在眉睫，注定又是繁忙的一年！\n最后用汪老师的年终总结PPT封面的一句话来结束吧：\n","permalink":"http://localhost:1313/posts/2017-01-28-2016-summary/","summary":"\u003cp\u003e2017农历新年的钟声都已经敲响了，我这2016的年终总结才开始动笔。\u003c/p\u003e\n\u003cp\u003e2016年，经历了很多，也成长了很多，遇到了很多曾经以为只会出现在电视剧中的场景，令人开始怀疑这个世界。前几天在朋友圈看到一个同学发的状态，觉得很适合作为这篇年终总结的开端。（同学你要是觉得被侵权了，告诉我，我立马删掉:-)）\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e每个家庭的故事都会是一部长篇史诗。曾经总以为很多情节只会出现在电视剧中，现实的生活很是平淡无味，没有任何波澜，偶尔甚至还会抱怨一下自己不是故事的女主角，其不知现实的生活相比于电视剧，往往是有过之而无不及。\n今天哥哥来电话了，从“天津”，一个美丽的谎言，一直在继续，还会坚持很久…\n奶奶很开心…\n——by angel\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003e关于学习和科研\u003c/strong\u003e。\u003ca href=\"https://bitjoy.net/posts/2016-08-20-2016-mid-year-summary/\"\u003e上半年在雁栖湖完成了研一的下半个学期，完完全全的结束了自己的学生时代。\u003c/a\u003e下半年开始进入实验室，直面惨淡的科研。原本以为会由组里的大师兄超哥指导我，没想到中秋前3天，直接接到H Boss的指令，要在中秋前完成一项我从来没做过的评测。不知道怎样设计实验，不知道怎样计算评价指标，甚至连需要评测的软件都不熟。不过好在加班加点完成了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"my-office-cubicle\" loading=\"lazy\" src=\"/posts/2017-01-28-2016-summary/my-office-cubicle.jpg\"\u003e\n凌乱的工位\u003c/p\u003e\n\u003cp\u003e从9月到10月中旬，一直在各种数据集上做各种对比评测，基本上一周做完一个评测数据集，完成一份报告，直接提交给H Boss，下一周在做另一个评测的同时，要根据导师的反馈建议修改完上一个评测报告。这一个多月的时间，共完成了5份报告共计14个版本，真的是要吐了。\u003c/p\u003e\n\u003cp\u003e后来发现pLink要比对手慢，于是就尝试各种加速策略。开始从外围查找原因，尝试了各种策略，虽然多多少少能加速，但是效果都不完美，有可能对精度有影响。直到12月份，借助谷歌的开源性能分析工具gperftools才找到了软件的性能瓶颈，开始优化并取得了好几倍的加速效果。12月份对软件本身的性能优化大概是我这半年做的唯一一个和计算机本身有点关系的工作吧，可能也是为数不多的我比较享受的一件事。\u003c/p\u003e\n\u003cp\u003e整个这半年的工作，都是H Boss御驾亲征，亲自指导，当然这样有利有弊。好处是能直接和H Boss对话，机会难得呀，H那种严于律己、追求完美的品质，H的为人处世、口才都是非常值得我学习的。坦白说，虽然我只是无名小卒一个，但从小到大，真正让我从心里佩服的人没几个，H Boss绝对算是其中之一。当然不便之处也非常明显，首先会感觉特别累和压抑，除了每周一次的面谈，每天的邮件，还经常在晚上11点多收到老师的工作微信。所以这半年确实不轻松，工作日晚上基本都在加班，而且加班到晚上11点也是常事，周末也至少工作一天。几乎没有时间运动，身体素质应该下降了不少。与人的沟通也非常少，好几个师兄师姐都问我为啥看我整天都在工位上坐着，好像从来没有动过。夸张点说就是每天晚上下班要走的时候，才发现自己这一天还没有说话。\u003c/p\u003e\n\u003cp\u003e当然半年高强度的工作，也有不少的收获，基本摸清了pLink代码的来龙去脉，也加速了两三倍，自己的表现也稍微得到了老师和同学的肯定。不过我自己还是不太满意的，加速并没有达到理想的效果。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于亲情\u003c/strong\u003e。随着我们两兄弟的大学毕业，家里的情况也在稍稍好转，但是只能算是曲折前进吧。以前小的时候，都是爸妈两个人闹，现在哥哥出场了，真是可笑。谈了快两年的女朋友，女方父母又是要查户口本，又是催着要付定金，说什么不给定金就要拉回老家相亲。这TM比电视剧还荒唐，真把自己当商品了，是不是给的钱多就跟谁呀，混蛋。哥哥也不是个省油的灯，分手之后没过多久说什么被公司派去新加坡学习了，去了之后，连个固定的联系方式都没有，三天两头失联，都老大不小的人了，还让父母担心。工作了两年，一分钱都没攒到，连大学的助学贷款都要我这个还在上学的弟弟替他还。女朋友分手也就算了，没赚到钱也不要紧，关键是你不能让家人这样担心你呀，你定期给家人打个电话，说说你到底在哪里干什么，既然到了新加坡，发几张国外的照片回来分享一下，不可以吗？已经两年过年没回家了，而且两年除夕居然连个电话都没有，这不是不孝是什么，混蛋。\u003c/p\u003e\n\u003cp\u003e今年妈妈也终于愿意外出挣钱了，虽然不多，但是起码在和爸爸一起努力。家里装修好了一层房子，但是也就是把墙什么的弄好了，家具还没制备。本来想着过年回家给家里买个小米电视，但是爸妈死活说不要买，现在买了也就过年看几天，不划算。后来只好作罢。放假给老爸买的红米手机，终于在除夕这一天拿到手了。给爸妈包的红包，也在按计划逐年的递增着。\u003c/p\u003e\n\u003cp\u003e自己有时候也埋怨家里，为什么家境这么的不堪，为什么父母没有达到我理想的高度，为什么哥哥这么不争气，为什么没有人关心我。但是埋怨有用吗，肯定是没用的，还是要看各自的造化。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于爱情\u003c/strong\u003e。我还是太幼稚，看看我的家境吧，有哪个女生愿意摊上我家这些破事呢，我就不应该奢望有什么爱情。不过今年上半年，爱情确实来过，抛开所有的一切，纯粹的校园爱情。可是10月份的一件事，彻底打醒了我，爱情没有那么简单，需要考虑的问题太多了。关于那段时间的记忆，写过很多文字，也流过很多泪。回顾整个下半年，欣欣和我的状态都不太好，除了那件事的原因，和工作变化也有很大的关系。我们都从雁栖湖到中关村，需要经历一个由学生到科研工作者的角色转变，面对科研的未知，都显得有些手足无措。科研的不顺，生活的压力以及家里的一些烦心事，一股脑的涌向了我们，矛盾也时有发生。经历过不少的磕磕绊绊，总算顺利度过了2016年，没有了热恋时期的疯狂，生活终要归于平淡。正所谓陪伴是最长情的告白，爱情的意义是否就在于两个人一起经历，一起成长呢。让我们共同守护。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于友情\u003c/strong\u003e。是的，我还欠很多人一顿饭。很多同学，如果长时间不见面，恐怕真的要忘掉了。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e关于个人提高\u003c/strong\u003e。上半年忙于课程学习，下半年忙于科研，花在个人提高上的时间真的是太少了。不过感谢有欣欣一直做我的榜样，我现在的小目标就是希望比欣欣看更多的书、刷更多的题。\u003c/p\u003e\n\u003cp\u003e另外下半年回到市区之后，也去了一些之前没去过的地方，比如：清华艺术博物馆、繁星戏剧村、中国美术馆、三联书店、香山等地。其中前两个地方都是和欣欣一起去的，感觉超棒~第一次看达芬奇的特展，开始了解这样一个天才，后来还看过他的传记；第一次去剧院看话剧，感觉和看电影完全不一样，小剧场的效果也是棒棒的。2016年12月31日也是一个值得纪念的日子：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"from-2016-to-2017\" loading=\"lazy\" src=\"/posts/2017-01-28-2016-summary/from-2016-to-2017.jpg\"\u003e\n2017跨年活动~\u003c/p\u003e\n\u003cp\u003e今年借着CNCP2016会议的机会，去了一次大连，见识了一下海滨城市的风貌，后来还跑去渤海学游泳，海水很脏，而且咸得发苦。希望今后每年都去一个除了上班地点和家里之外的第三个城市。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-01-28-2016-summary/dalian1-201608.jpg\"\u003e\n大连滨海国家地质公园\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/posts/2017-01-28-2016-summary/dalian2-201608.jpg\"\u003e\n大连滨海国家地质公园\u003c/p\u003e\n\u003cp\u003e最后看看\u003ca href=\"https://bitjoy.net/posts/2016-01-03-2016-happy-new-year/\"\u003e年初计划\u003c/a\u003e的完成情况：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cdel\u003e完成国科大下学期的课程任务：完成\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e\u003cdel\u003e接手pLink软件：完成\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e刷完LeetCode所有题目：进度147/461，没有完成\u003c/li\u003e\n\u003cli\u003e\u003cdel\u003e读10本书：目前读了《数学之美》、《大话设计模式》、《我不知道该说什么，关于死亡还是爱情》、《男人来自火星、女人来自金星，卷I》、《只有医生知道，卷I》、《文学的种子》、《讲理》、《暗时间》、《达·芬奇传：放飞的心灵》、《人间失格》，刚好10本，圆满完成任务:-)\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影：目前看了《美人鱼》、《北京遇上西雅图之不二情书》、《忍者神龟2：破影而出》、《七月与安生》、《湄公河行动》、《比利·林恩的中场战事》、《你的名字》、《血战钢锯岭》，只有8场，其中7场是和欣欣一起看的~\u003c/li\u003e\n\u003cli\u003e改正坐姿：有一段时间刻意改正了，但是这东西貌似改不过来？\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e除了LeetCode完成度太差之外，其他计划完成度还是蛮高的。下面定一下2017年的年度计划：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e发表pLink 2文章\u003c/li\u003e\n\u003cli\u003e至少完成毕业工作的80%\u003c/li\u003e\n\u003cli\u003e刷完LeetCode所有简单题和中等题，找工作之前最好刷完两遍\u003c/li\u003e\n\u003cli\u003e找到一个满意的工作\u003c/li\u003e\n\u003cli\u003e读10本书\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影\u003c/li\u003e\n\u003cli\u003e看一场话剧（音乐会、歌剧等都可以）\u003c/li\u003e\n\u003cli\u003e学会游泳\u003c/li\u003e\n\u003cli\u003e去第三个城市\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e简短总结一下我的2016：完成了由上课到科研的转变；开始有能力感恩家人；遇到了欣欣，由一个人变成了两个人；第一次去剧场看话剧。展望2017，找工作和准备毕业迫在眉睫，注定又是繁忙的一年！\u003c/p\u003e\n\u003cp\u003e最后用汪老师的年终总结PPT封面的一句话来结束吧：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"from-2016-to-2017-2\" loading=\"lazy\" src=\"/posts/2017-01-28-2016-summary/from-2016-to-2017-2.png\"\u003e\u003c/p\u003e","title":"2016年终总结"},{"content":"今天中午和超哥在食堂吃饭的时候聊起了电动汽车加电站的问题，很有意思。\n超哥现在开的是一辆电动汽车，他说目前也挺满意的，只要在北京市内开，几乎没问题，充电桩到处都有，马力也足，开起来没有任何噪声。唯一的问题是需要每天充电，去到稍微远一点的京郊可能会电量不足。\n然后就讨论到目前电动汽车的瓶颈，主要还是在电池上，一个是续航时间短，另一个是充电时间慢。\n然后我就想现在的加油车为什么没有上面的两个问题呢，第一个问题，如果加的油少的话，是不是也会出现续航时间短的问题呢，所以把电动汽车的电池做大一点，密度高一点是不是就可以了呢，虽然技术上可能会有难度，但是我觉得并没有第二个问题严重，所以我觉得电池续航短不是太大的问题。。。\n对于第二个问题，在加油站给汽车加油只需要几分钟的时间，但是电动车在充电桩充电可能需要几十分钟甚至一个多小时，所以充电时间慢确实是一个很严重的问题。我当时就说一辆越野车如果要跑沙漠的话，会在车上预存好几桶油备用，类似的，电动车能不能在车上备上几个电池，没电了就换呢，就像手机备用电池一样。然后超哥进一步说不如干脆在现有加油站的基础上，建一个加电站，每辆电动车进站之后，卸下车上的电池，换上提前充满电的电池，整个过程和加油完全一样，还比加油干净。\n我突然觉得，哇塞，这个idea不错呀，统一所有电动车的电池，电池没电之后，到站换电池，一个电池就像一个小型的集装箱（或者docker）一样，被卸下来，然后插上满电电池，so easy~但是为啥没公司这么做呢。此时旁边坐着的一位老师也加入了对话，他说电池寿命有长有短，如果自己刚买的新车，没电了拿去换了一个旧电池，肯定不爽；再说了，要统一全国的电池标准，几乎是不可能的，在全国建这样的加电站，没有哪个公司能承担得起这样的成本，最终羊毛出在羊身上，电动车的价格肯定会上涨的….\n这位老师说的都对，但是我依然觉得这个idea是可行的，关键是要看有关部门有没有这个魄力来做这件事。比如国家或行业层面可以强制统一电池的标准，XX汽车协会规定今后的汽车电池必须做成0.5m*0.5m*0.5m的方块，正负极距离5cm，便于拆卸等；同时要求电动汽车的电池安放位置必须在汽车的后右侧等一些列规定。即使国家层面没人愿意做这件事，哪家有魄力的电动汽车公司，是不是可以尝试一下呢，比如特斯拉，统一旗下所有电车的电池规格，并在全球建造加电站，统一更换特斯拉的电池。如果特斯拉这样做了，我相信买特斯拉的车主是愿意承担一部分费用的，毕竟这样的加电站最终还是方便了自己。至于说新车换到旧电池，其实大可不必担心，说不定你的旧车会换到别人的新电池呢，而且特斯拉可以建立一个标准，只有电池能量转换效率大于80%的电池才能进入加电站循环，这样保证了每个人换到的电池续航有保障，至于新旧，我才不管呢，反正下一次加电又要换了。\n我个人还是挺喜欢电动汽车的，节能、环保、静音，还看起来酷酷的:-)好希望这个idea能在未来实现呀~\n知乎：电动汽车为什么不统一电池，充电站更换相同档次满电电池？\n","permalink":"http://localhost:1313/posts/2017-01-03-electric-vehicle-charging-station/","summary":"\u003cp\u003e今天中午和超哥在食堂吃饭的时候聊起了电动汽车加电站的问题，很有意思。\u003c/p\u003e\n\u003cp\u003e超哥现在开的是一辆电动汽车，他说目前也挺满意的，只要在北京市内开，几乎没问题，充电桩到处都有，马力也足，开起来没有任何噪声。唯一的问题是需要每天充电，去到稍微远一点的京郊可能会电量不足。\u003c/p\u003e\n\u003cp\u003e然后就讨论到目前电动汽车的瓶颈，主要还是在电池上，一个是续航时间短，另一个是充电时间慢。\u003c/p\u003e\n\u003cp\u003e然后我就想现在的加油车为什么没有上面的两个问题呢，第一个问题，如果加的油少的话，是不是也会出现续航时间短的问题呢，所以把电动汽车的电池做大一点，密度高一点是不是就可以了呢，虽然技术上可能会有难度，但是我觉得并没有第二个问题严重，所以我觉得电池续航短不是太大的问题。。。\u003c/p\u003e\n\u003cp\u003e对于第二个问题，在加油站给汽车加油只需要几分钟的时间，但是电动车在充电桩充电可能需要几十分钟甚至一个多小时，所以充电时间慢确实是一个很严重的问题。我当时就说一辆越野车如果要跑沙漠的话，会在车上预存好几桶油备用，类似的，电动车能不能在车上备上几个电池，没电了就换呢，就像手机备用电池一样。然后超哥进一步说不如干脆在现有加油站的基础上，建一个加电站，每辆电动车进站之后，卸下车上的电池，换上提前充满电的电池，整个过程和加油完全一样，还比加油干净。\u003c/p\u003e\n\u003cp\u003e我突然觉得，哇塞，这个idea不错呀，统一所有电动车的电池，电池没电之后，到站换电池，一个电池就像一个小型的集装箱（或者docker）一样，被卸下来，然后插上满电电池，so easy~但是为啥没公司这么做呢。此时旁边坐着的一位老师也加入了对话，他说电池寿命有长有短，如果自己刚买的新车，没电了拿去换了一个旧电池，肯定不爽；再说了，要统一全国的电池标准，几乎是不可能的，在全国建这样的加电站，没有哪个公司能承担得起这样的成本，最终羊毛出在羊身上，电动车的价格肯定会上涨的….\u003c/p\u003e\n\u003cp\u003e这位老师说的都对，但是我依然觉得这个idea是可行的，关键是要看有关部门有没有这个魄力来做这件事。比如国家或行业层面可以强制统一电池的标准，XX汽车协会规定今后的汽车电池必须做成0.5m*0.5m*0.5m的方块，正负极距离5cm，便于拆卸等；同时要求电动汽车的电池安放位置必须在汽车的后右侧等一些列规定。即使国家层面没人愿意做这件事，哪家有魄力的电动汽车公司，是不是可以尝试一下呢，比如特斯拉，统一旗下所有电车的电池规格，并在全球建造加电站，统一更换特斯拉的电池。如果特斯拉这样做了，我相信买特斯拉的车主是愿意承担一部分费用的，毕竟这样的加电站最终还是方便了自己。至于说新车换到旧电池，其实大可不必担心，说不定你的旧车会换到别人的新电池呢，而且特斯拉可以建立一个标准，只有电池能量转换效率大于80%的电池才能进入加电站循环，这样保证了每个人换到的电池续航有保障，至于新旧，我才不管呢，反正下一次加电又要换了。\u003c/p\u003e\n\u003cp\u003e我个人还是挺喜欢电动汽车的，节能、环保、静音，还看起来酷酷的:-)好希望这个idea能在未来实现呀~\u003c/p\u003e\n\u003cp\u003e知乎：\u003ca href=\"https://www.zhihu.com/question/28793345\"\u003e电动汽车为什么不统一电池，充电站更换相同档次满电电池？\u003c/a\u003e\u003c/p\u003e","title":"电动汽车加电站"},{"content":"最怕空气突然安静 最怕朋友突然的关心 最怕回忆突然翻滚绞痛着不平息 最怕突然听到你的消息 想念如果会有声音 不愿那是悲伤的哭泣 事到如今终於让自已属於我自已 只剩眼泪还骗不过自己 突然好想你你会在哪里 过的快乐或委屈 突然好想你突然锋利的回忆 突然模糊的眼睛 我们像一首最美丽的歌曲 变成两部悲伤的电影 为什麽你带我走过最难忘的旅行 然後留下最痛的纪念品 我们那麽甜那麽美那麽相信 那麽疯那麽热烈的曾经 为何我们还是要奔向 各自的幸福和遗憾中老去 突然好想你你会在哪里 过的快乐或委屈 突然好想你突然锋利的回忆 突然模糊的眼睛 最怕空气突然安静 最怕朋友突然的关心 最怕回忆突然翻滚绞痛着不平息 最怕突然听到你的消息 最怕此生已经决定自己过 没有你却又突然听到你的消息 就好像是突然之间，整个世界都失去了你的声音，以前每天都会收到你的喜怒哀乐、衣食住行、午安晚安，突然之间，空气都安静了，没有了你的消息。翻遍你的空间、朋友圈、微博、博客，都没有消息，不知道你在干什么，好伤心。\n想你，想知道你在哪里，想知道你在干什么，想要发消息给你，又害怕不能收到你的回信，然后郁闷一整天。想要引起你的注意，绞尽脑汁故意发一些不着边际的微博，等了一整天，没有收到你的点赞或评论。\n约你出来吃饭，你一句简单得不能再简单的“可以”，冷冰冰。见到你，裹着厚厚的棉衣，戴着帽子和手套，没有一丝的眼神交流，两个人就这样默默的吃着不知道什么味道的饭菜。\n想要打破这宁静的空气，之前想到无数要和你说的人和事，现在却一句话也说不出。两个最熟悉的人，突然之间，像多年未见的朋友，因完全不同的人生轨迹而没有任何共同语言，成了最熟悉的陌生人。\n你说最近科研压力很大，周围的同学又是发论文，又是发专利，你却一无所有。挑战赛答辩和开题答辩在即，手足无措。\n你说未来太渺茫，没钱买房买车，就算月入两万，要在北京买房也得攒20年。即使买了房，在北京还要买车还要摇号，看病也很贵，还要担心孩子上学各种问题。\n我向来是个不会安慰别人的人，但是我尽力想要告诉你，你已经很优秀了，从小到大的尖子生，多才多艺，研一在雁栖湖那么多高手，你照样轻松拿下第一。你的编程能力也远超你们实验室的人，只不过你目前处于一种有力没处使的状态，你的导师让你干一些杂活，如果你的导师也让专心指导你的挑战赛，你现在肯定也写论文了。\n我尽力安慰你，希望你对对未来乐观一点。面包会有的，不要太在意这些东西，生活快乐最重要，不要太在意别人的看法，自己纵向比较有进步就行了。突然觉得自己词穷，完全不会安慰一个人。\n世界上比你悲惨的人多得太多了，为什么不想想自己有的，至少你有一个健康的身体。\n是的，我猜到了，你不理我和我们上周的体检结果有关。我有病，是的，我有病，而且是不可能治好的病，是随时都可能发病的病。我知道这对于你来说很为难，你有矛盾，我理解，只是我希望你能把你的所有顾虑都告诉我，至少让我和你一起分担，如果你说因为我的病，因为我那卑微的背景，想要离开我，我无话可说，这也无可厚非，我接受。\n刚开始交往时，我没有告诉你，是我的错。上周的检查结果至少说明你是健康的，我很庆幸，没有伤害到你。如果因为我而让你不开心，让你纠结，请一定让我知道，我会默默走开，我是一个讲理的人。希望你永远健康快乐。\n你说这东西还有窗口期，我认，再过几个月，我们再去检查一次，我默默祈祷你是健康的。我知道你和我一样，对一件微小的事也要纠结半天，我不想让你那样难过。\n我原本以为我是一个耐得住安静和寂寞的人，我一度还觉得你像一个叽叽喳喳的小鸟不停的在我耳边发出噪声。直到有一天，我发现你的声音不见了，我开始慌了，不安、烦躁、忧郁充斥我的大脑，想要马上见到你问个一清二楚。也许这就是日久生情，这就是感情吧。\n熟悉了你随性而又纠结的脾气；习惯了每次和你一起吃饭时纠结菜里到底有没有混猪肉；习惯了每次吃饭时要双份碗筷和三份汤的感觉；渐渐喜欢上了和你一起漫无目的的逛街试衣服的感觉；习惯了买两本一样的书，然后每次你看得都比我快，逼迫我不得不也快快的看；喜欢和你一起看电影出去玩的感觉。\n也不知从什么时候开始，微信里的情侣表情都积了厚厚的一层灰；好久好久都没有掰你了，你的手指应该纤细如初了吧。带上耳机，听了一整晚的音乐，已经很久很久没有听歌了。\n以前常常对电视剧里的爱情嗤之以鼻，完全不能理解他们为什么要为爱情哭得死去活来，现在，我大概理解了。\n你说每个正常人遇到我这种情况，都会矛盾，会需要思考、抉择和权衡，是的，爱情完全不是小说里的义无反顾，说到底是各种利益的权衡。\n10月真是一个让人忧伤的月份，亲人的离世，自己也前前后后进了三次医院，揭开了深藏心底N年的伤疤，曾经一直陪伴在身边的人也想要离开。科研上的压力就更不说了，老板一直不停的催促，每天活得像个陀螺，没有方向的转，没有一丝的停顿。\n这病态的社会。\n亲爱的，我愿意等你，我尊重你的决定，如果你离开，我会祝福你，如果你留下，我希望你不是在怜悯我。\n","permalink":"http://localhost:1313/posts/2016-10-28-waiting-for-you/","summary":"\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e最怕空气突然安静\n最怕朋友突然的关心\n最怕回忆突然翻滚绞痛着不平息\n最怕突然听到你的消息\n\n想念如果会有声音\n不愿那是悲伤的哭泣\n事到如今终於让自已属於我自已\n只剩眼泪还骗不过自己\n\n突然好想你你会在哪里\n过的快乐或委屈\n突然好想你突然锋利的回忆\n突然模糊的眼睛\n\n我们像一首最美丽的歌曲\n变成两部悲伤的电影\n为什麽你带我走过最难忘的旅行\n然後留下最痛的纪念品\n\n我们那麽甜那麽美那麽相信\n那麽疯那麽热烈的曾经\n为何我们还是要奔向\n各自的幸福和遗憾中老去\n\n突然好想你你会在哪里\n过的快乐或委屈\n突然好想你突然锋利的回忆\n突然模糊的眼睛\n\n最怕空气突然安静\n最怕朋友突然的关心\n最怕回忆突然翻滚绞痛着不平息\n最怕突然听到你的消息\n最怕此生已经决定自己过\n没有你却又突然听到你的消息\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e就好像是突然之间，整个世界都失去了你的声音，以前每天都会收到你的喜怒哀乐、衣食住行、午安晚安，突然之间，空气都安静了，没有了你的消息。翻遍你的空间、朋友圈、微博、博客，都没有消息，不知道你在干什么，好伤心。\u003c/p\u003e\n\u003cp\u003e想你，想知道你在哪里，想知道你在干什么，想要发消息给你，又害怕不能收到你的回信，然后郁闷一整天。想要引起你的注意，绞尽脑汁故意发一些不着边际的微博，等了一整天，没有收到你的点赞或评论。\u003c/p\u003e\n\u003cp\u003e约你出来吃饭，你一句简单得不能再简单的“可以”，冷冰冰。见到你，裹着厚厚的棉衣，戴着帽子和手套，没有一丝的眼神交流，两个人就这样默默的吃着不知道什么味道的饭菜。\u003c/p\u003e\n\u003cp\u003e想要打破这宁静的空气，之前想到无数要和你说的人和事，现在却一句话也说不出。两个最熟悉的人，突然之间，像多年未见的朋友，因完全不同的人生轨迹而没有任何共同语言，成了最熟悉的陌生人。\u003c/p\u003e\n\u003cp\u003e你说最近科研压力很大，周围的同学又是发论文，又是发专利，你却一无所有。挑战赛答辩和开题答辩在即，手足无措。\u003c/p\u003e\n\u003cp\u003e你说未来太渺茫，没钱买房买车，就算月入两万，要在北京买房也得攒20年。即使买了房，在北京还要买车还要摇号，看病也很贵，还要担心孩子上学各种问题。\u003c/p\u003e\n\u003cp\u003e我向来是个不会安慰别人的人，但是我尽力想要告诉你，你已经很优秀了，从小到大的尖子生，多才多艺，研一在雁栖湖那么多高手，你照样轻松拿下第一。你的编程能力也远超你们实验室的人，只不过你目前处于一种有力没处使的状态，你的导师让你干一些杂活，如果你的导师也让专心指导你的挑战赛，你现在肯定也写论文了。\u003c/p\u003e\n\u003cp\u003e我尽力安慰你，希望你对对未来乐观一点。面包会有的，不要太在意这些东西，生活快乐最重要，不要太在意别人的看法，自己纵向比较有进步就行了。突然觉得自己词穷，完全不会安慰一个人。\u003c/p\u003e\n\u003cp\u003e世界上比你悲惨的人多得太多了，为什么不想想自己有的，至少你有一个健康的身体。\u003c/p\u003e\n\u003cp\u003e是的，我猜到了，你不理我和我们上周的体检结果有关。我有病，是的，我有病，而且是不可能治好的病，是随时都可能发病的病。我知道这对于你来说很为难，你有矛盾，我理解，只是我希望你能把你的所有顾虑都告诉我，至少让我和你一起分担，如果你说因为我的病，因为我那卑微的背景，想要离开我，我无话可说，这也无可厚非，我接受。\u003c/p\u003e\n\u003cp\u003e刚开始交往时，我没有告诉你，是我的错。上周的检查结果至少说明你是健康的，我很庆幸，没有伤害到你。如果因为我而让你不开心，让你纠结，请一定让我知道，我会默默走开，我是一个讲理的人。希望你永远健康快乐。\u003c/p\u003e\n\u003cp\u003e你说这东西还有窗口期，我认，再过几个月，我们再去检查一次，我默默祈祷你是健康的。我知道你和我一样，对一件微小的事也要纠结半天，我不想让你那样难过。\u003c/p\u003e\n\u003cp\u003e我原本以为我是一个耐得住安静和寂寞的人，我一度还觉得你像一个叽叽喳喳的小鸟不停的在我耳边发出噪声。直到有一天，我发现你的声音不见了，我开始慌了，不安、烦躁、忧郁充斥我的大脑，想要马上见到你问个一清二楚。也许这就是\u003ca href=\"http://zhihu.com/question/26049681/answer/32014770\"\u003e日久生情\u003c/a\u003e，这就是感情吧。\u003c/p\u003e\n\u003cp\u003e熟悉了你随性而又纠结的脾气；习惯了每次和你一起吃饭时纠结菜里到底有没有混猪肉；习惯了每次吃饭时要双份碗筷和三份汤的感觉；渐渐喜欢上了和你一起漫无目的的逛街试衣服的感觉；习惯了买两本一样的书，然后每次你看得都比我快，逼迫我不得不也快快的看；喜欢和你一起看电影出去玩的感觉。\u003c/p\u003e\n\u003cp\u003e也不知从什么时候开始，微信里的情侣表情都积了厚厚的一层灰；好久好久都没有掰你了，你的手指应该纤细如初了吧。带上耳机，听了一整晚的音乐，已经很久很久没有听歌了。\u003c/p\u003e\n\u003cp\u003e以前常常对电视剧里的爱情嗤之以鼻，完全不能理解他们为什么要为爱情哭得死去活来，现在，我大概理解了。\u003c/p\u003e\n\u003cp\u003e你说每个正常人遇到我这种情况，都会矛盾，会需要思考、抉择和权衡，是的，爱情完全不是小说里的义无反顾，说到底是各种利益的权衡。\u003c/p\u003e\n\u003cp\u003e10月真是一个让人忧伤的月份，亲人的离世，自己也前前后后进了三次医院，揭开了深藏心底N年的伤疤，曾经一直陪伴在身边的人也想要离开。科研上的压力就更不说了，老板一直不停的催促，每天活得像个陀螺，没有方向的转，没有一丝的停顿。\u003c/p\u003e\n\u003cp\u003e这病态的社会。\u003c/p\u003e\n\u003cp\u003e亲爱的，我愿意等你，我尊重你的决定，如果你离开，我会祝福你，如果你留下，我希望你不是在怜悯我。\u003c/p\u003e","title":"最怕空气突然安静"},{"content":"今天阅读《C++ Primer, 5e》的第二章，介绍C++的基本内置类型，觉得有一些平时工作容易出错的知识点，现摘录如下：\n1 unsigned char c = -1; // 假设char占8比特，c的值为255 当我们赋给无符号类型一个超出它表示范围的值时，结果是初始值对无符号类型表示数值总数取模后的余数。例如，8比特大小的unsigned char可以表示0至255区间内的值，如果我们赋了一个区间以外的值，则实际的结果是该值对256取模后所得的余数。因此，把-1赋给8比特大小的unsigned char所得的结果是255。\n1 signed char c2 = 256; // 假设char占8比特，c2的值是未定义的 当我们赋给带符号类型一个超出它表示范围的值时，结果是未定义的（undefined）。此时，程序可能继续工作、可能崩溃，也可能生成垃圾数据。\n1 2 3 4 unsigned u = 10; int i = -42; std::cout \u0026lt;\u0026lt; i + i \u0026lt;\u0026lt; std::endl; // 输出-84 std::cout \u0026lt;\u0026lt; u + i \u0026lt;\u0026lt; std::endl; // 如果int占32位，输出4294967264 在第一个输出表达式里，两个（负）整数相加并得到了期望的结果。在第二个输出表达式里，相加前首先把整数-42转换成无符号数。把负数转换成无符号数类似于直接给无符号数赋一个负数，结果等于这个负数加上无符号数的模。unsigned (int)的取值范围是0~\\(2^{32}-1\\)，所以总数有\\(2^{32}\\)个数，-42%\\(2^{32}\\)=-42+\\(2^{32}\\)，u+i=10+(-42+\\(2^{32}\\))=4294967264。\n1 2 3 unsigned u1 = 42, u2 = 10; std::cout \u0026lt;\u0026lt; u1 – u2 \u0026lt;\u0026lt; std::endl; // 正确：输出32 std::cout \u0026lt;\u0026lt; u2 – u1 \u0026lt;\u0026lt; std::endl; // 正确：不过，结果是取模后的值 当从无符号数中减去一个值时，不管这个值是不是无符号数，我们都必须确保结果不能是一个负值。\n无符号数不会小于0这一事实同样关系到循环的写法。例如我们常用的循环如下：\n1 2 for (int i = 10; i \u0026gt;= 0; --i) std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; 可能你会觉得反正也不打算输出负数，可以用无符号数来重写这个循环。然而，这个不经意的改变却意味着死循环；\n1 2 3 // 错误：变量u永远也不会小于0，循环条件一直成立 for (unsigned u = 10; u \u0026gt;= 0; --u) std::cout \u0026lt;\u0026lt; u \u0026lt;\u0026lt; std::endl; 来看看当u等于0时发生了什么，这次迭代输出0，然后继续执行for语句里的表达式。表达式\u0026ndash;u从u当中减去1，得到的结果-1并不满足无符号数的要求，此时像所有表示范围之外的其他数字一样，-1被自动地转换成一个合法的无符号数。假设int类型占32位，则当u等于0时，–u的结果将会是-1%\\(2^{32}\\)=4294967295。\n一种解决的办法是，用while语句来代替for语句，因为前者让我们能够在输出变量之前（而非之后）先减1：\n1 2 3 4 5 unsigned u = 11; // 确定要输出的最大数，从比它大1的数开始 while (u \u0026gt; 0){ --u; std::cout \u0026lt;\u0026lt; u \u0026lt;\u0026lt; std::endl; } ","permalink":"http://localhost:1313/posts/2016-09-24-memo-about-cpp-built-in-types/","summary":"\u003cp\u003e今天阅读《C++ Primer, 5e》的第二章，介绍C++的基本内置类型，觉得有一些平时工作容易出错的知识点，现摘录如下：\u003c/p\u003e\n\u003chr\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eunsigned\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e c \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e; \u003cspan style=\"color:#75715e\"\u003e// 假设char占8比特，c的值为255\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e当我们赋给无符号类型一个超出它表示范围的值时，结果是初始值对无符号类型表示数值总数取模后的余数。例如，8比特大小的unsigned char可以表示0至255区间内的值，如果我们赋了一个区间以外的值，则实际的结果是该值对256取模后所得的余数。因此，把-1赋给8比特大小的unsigned char所得的结果是255。\u003c/p\u003e\n\u003chr\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003esigned\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e c2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e256\u003c/span\u003e; \u003cspan style=\"color:#75715e\"\u003e// 假设char占8比特，c2的值是未定义的\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e当我们赋给带符号类型一个超出它表示范围的值时，结果是未定义的（undefined）。此时，程序可能继续工作、可能崩溃，也可能生成垃圾数据。\u003c/p\u003e\n\u003chr\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eunsigned\u003c/span\u003e u \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e42\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003ecout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eendl; \u003cspan style=\"color:#75715e\"\u003e// 输出-84\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003ecout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e u \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eendl; \u003cspan style=\"color:#75715e\"\u003e// 如果int占32位，输出4294967264\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e在第一个输出表达式里，两个（负）整数相加并得到了期望的结果。在第二个输出表达式里，相加前首先把整数-42转换成无符号数。把负数转换成无符号数类似于直接给无符号数赋一个负数，结果等于这个负数加上无符号数的模。unsigned (int)的取值范围是0~\\(2^{32}-1\\)，所以总数有\\(2^{32}\\)个数，-42%\\(2^{32}\\)=-42+\\(2^{32}\\)，u+i=10+(-42+\\(2^{32}\\))=4294967264。\u003c/p\u003e\n\u003chr\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eunsigned\u003c/span\u003e u1 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e42\u003c/span\u003e, u2 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003ecout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e u1 \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e u2 \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eendl; \u003cspan style=\"color:#75715e\"\u003e// 正确：输出32\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003ecout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e u2 \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e u1 \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eendl; \u003cspan style=\"color:#75715e\"\u003e// 正确：不过，结果是取模后的值\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e当从无符号数中减去一个值时，不管这个值是不是无符号数，我们都必须确保结果不能是一个负值。\u003c/p\u003e","title":"C++基本数据类型备忘"},{"content":"随机矩阵是这样一类方阵，其元素为非负实数，且行和或列和为1。如果行和为1，则称为行随机矩阵；如果列和为1，则称为列随机矩阵；如果行和和列和都为1，则称为双随机矩阵。\n前面我们介绍的谷歌矩阵和HMM中的转移矩阵都属于随机矩阵，所以随机矩阵也称为概率矩阵、转移矩阵、或马尔可夫矩阵。\n随机矩阵有一个性质，就是其所有特征值的绝对值小于等于1，且其最大特征值为1。下面通过两种方法证明这个结论。\n首先，随机矩阵A肯定有特征值1，即\n$$\\begin{equation}A\\vec 1=1\\times\\vec 1\\end{equation}$$其中的单位向量\\(\\vec 1=(\\frac{1}{n},…,\\frac{1}{n})^T\\)，因为A的行和为1，所以上述等式成立。即1是A的特征值。\n反证法 假设存在大于1的特征值\\(\\lambda\\)，则有\\(A\\vec x=\\lambda\\vec x\\)。令\\(x_k\\)是\\(\\vec x\\)中最大的元素。又因为A的元素非负，且行和为1，所以\\(\\lambda\\vec x\\)中的每个元素都是\\(\\vec x\\)中元素的凸组合，所以\\(\\lambda\\vec x\\)中的每个元素都小于等于\\(x_k\\)。\n$$\\begin{equation}a_{i1}x_1+a_{i2}x_2+…+a_{in}x_n=\\lambda x_i\\leq x_k\\end{equation}$$但是如果\\(\\lambda\u003e1\\)，则\\(\\lambda x_k\u003ex_k\\)，和(2)式矛盾，所以\\(\\lambda\\leq 1\\)。又因为(1)式，所以A的最大特征值为1。\n常规证法 设对称随机矩阵A的特征值\\(\\lambda\\)对应的特征向量为\\(x\\)（为了简便，以下省略向量符号），则有\\(Ax=\\lambda x\\)，即\\(x^TAx=\\lambda x^Tx\\)，欲证明\\(|\\lambda|\\leq 1\\)，只需证明\n$$\\begin{equation}\\lambda=\\frac{\u003c x, Ax \u003e}{\u003c x, x \u003e}\\leq 1\\end{equation}$$根据定义有：\n$$\\begin{equation}\u003c x, Ax \u003e=\\sum_{i=1}^na_{ii}x_i^2+2\\sum_{i \u003c j, i\\sim j}a_{ij}x_ix_j\\end{equation}$$对于\\(i \u003c j, i\\sim j\\)，有：\n$$\\begin{equation}a_{ij}(x_i-x_j)^2=a_{ij}x_i^2-2a_{ij}x_ix_j+a_{ij}x_j^2\\end{equation}$$两边求和并移项得到：\n$$ \\begin{equation} \\begin{array} \\displaystyle{2\\sum_{i \u003c j}}a_{ij}x_ix_j \u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003c j}a_{ij}x_j^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003c j}a_{ji}x_j^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003e j}a_{ij}x_i^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_i(\\sum_{j\\neq i}a_{ij}x_i^2)-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_i(x_i^2(1-a_{ii}))-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2} \\end{array} \\end{equation} $$第2、3个等号都是因为A是对称矩阵，所以可以把\\(a_{ij}\\)替换为\\(a_{ji}\\)，然后互换\\(i,j\\)下标。最后一个等号是因为A的行和为1。\n将(6)代入(4)式得到：\n$$ \\begin{equation} \\begin{array} \\displaystyle{\u003c x, Ax \u003e} \u0026 = \u0026 \\displaystyle{\\sum_ia_{ii}x_i^2+\\sum_i(x_i^2(1-a_{ii}))-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_ix_i^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2} \\end{array} \\end{equation} $$所以：\n$$ \\begin{equation} \\lambda=\\frac{\u003c x, Ax \u003e}{\u003c x, x\u003e} = \\frac{\\sum_ix_i^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}{\\sum_ix_i^2} \\leq 1 \\end{equation} $$又因为(1)式，所以A的最大特征值为1。\n随机矩阵的第二大特征值\\(\\lambda(A)\\)也很有用，\\(1-\\lambda(A)\\)被称为矩阵A的谱间隔（spectral gap），它衡量的是最大特征值和第二大特征值之间的差值。\\(\\lambda(A)\\)在马尔可夫随机游走领域有重要作用。\n$$\\begin{equation}||A^lp-1||_2\\leq \\lambda^l(A)\\end{equation}$$上式是扩张图（Expander）领域很重要的一个引理，A为扩张图的邻接矩阵，\\(p\\)为在所有节点上的初始概率分布，\\(\\lambda(A)\\)为矩阵A的第二大的特征值。因为\\(\\lambda(A)\u003c1\\)，所以\\(\\lambda^l(A)\\)会快速的降到0。也就是说，在初始概率\\(p\\)上，随机游走\\(l\\)步，很快就能达到均匀分布\\(1\\)。\n参考：《Computational Complexity: A Modern Approach》书上7.A.RANDOM WALKS AND EIGENVALUES介绍了这个引理，该书地址：http://theory.cs.princeton.edu/complexity/，相关内容在第153页。\n","permalink":"http://localhost:1313/posts/2016-08-23-the-eigenvalue-of-stochastic-matrix/","summary":"\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Stochastic_matrix\"\u003e随机矩阵\u003c/a\u003e是这样一类方阵，其元素为非负实数，且行和或列和为1。如果行和为1，则称为行随机矩阵；如果列和为1，则称为列随机矩阵；如果行和和列和都为1，则称为双随机矩阵。\u003c/p\u003e\n\u003cp\u003e前面我们介绍的\u003ca href=\"https://bitjoy.net/posts/2016-08-04-googles-pagerank-and-beyond/\"\u003e谷歌矩阵\u003c/a\u003e和\u003ca href=\"https://bitjoy.net/posts/2016-08-20-introduction-to-hmm-1/\"\u003eHMM中的转移矩阵\u003c/a\u003e都属于随机矩阵，所以随机矩阵也称为概率矩阵、转移矩阵、或马尔可夫矩阵。\u003c/p\u003e\n\u003cp\u003e随机矩阵有一个性质，就是其所有特征值的绝对值小于等于1，且其最大特征值为1。下面通过两种方法证明这个结论。\u003c/p\u003e\n\u003cp\u003e首先，随机矩阵A肯定有特征值1，即\u003c/p\u003e\n$$\\begin{equation}A\\vec 1=1\\times\\vec 1\\end{equation}$$\u003cp\u003e其中的单位向量\\(\\vec 1=(\\frac{1}{n},…,\\frac{1}{n})^T\\)，因为A的行和为1，所以上述等式成立。即1是A的特征值。\u003c/p\u003e\n\u003ch1 id=\"反证法\"\u003e\u003ca href=\"https://mikespivey.wordpress.com/2013/01/17/eigenvalue-stochasti/\"\u003e反证法\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e假设存在大于1的特征值\\(\\lambda\\)，则有\\(A\\vec x=\\lambda\\vec x\\)。令\\(x_k\\)是\\(\\vec x\\)中最大的元素。又因为A的元素非负，且行和为1，所以\\(\\lambda\\vec x\\)中的每个元素都是\\(\\vec x\\)中元素的凸组合，所以\\(\\lambda\\vec x\\)中的每个元素都小于等于\\(x_k\\)。\u003c/p\u003e\n$$\\begin{equation}a_{i1}x_1+a_{i2}x_2+…+a_{in}x_n=\\lambda x_i\\leq x_k\\end{equation}$$\u003cp\u003e但是如果\\(\\lambda\u003e1\\)，则\\(\\lambda x_k\u003ex_k\\)，和(2)式矛盾，所以\\(\\lambda\\leq 1\\)。又因为(1)式，所以A的最大特征值为1。\u003c/p\u003e\n\u003ch1 id=\"常规证法\"\u003e常规证法\u003c/h1\u003e\n\u003cp\u003e设\u003cstrong\u003e对称随机矩阵A\u003c/strong\u003e的特征值\\(\\lambda\\)对应的特征向量为\\(x\\)（为了简便，以下省略向量符号），则有\\(Ax=\\lambda x\\)，即\\(x^TAx=\\lambda x^Tx\\)，欲证明\\(|\\lambda|\\leq 1\\)，只需证明\u003c/p\u003e\n$$\\begin{equation}\\lambda=\\frac{\u003c x, Ax \u003e}{\u003c x, x \u003e}\\leq 1\\end{equation}$$\u003cp\u003e根据定义有：\u003c/p\u003e\n$$\\begin{equation}\u003c x, Ax \u003e=\\sum_{i=1}^na_{ii}x_i^2+2\\sum_{i \u003c j, i\\sim j}a_{ij}x_ix_j\\end{equation}$$\u003cp\u003e对于\\(i \u003c j, i\\sim j\\)，有：\u003c/p\u003e\n$$\\begin{equation}a_{ij}(x_i-x_j)^2=a_{ij}x_i^2-2a_{ij}x_ix_j+a_{ij}x_j^2\\end{equation}$$\u003cp\u003e两边求和并移项得到：\u003c/p\u003e\n$$\n\\begin{equation}\n\\begin{array}\n\\displaystyle{2\\sum_{i \u003c j}}a_{ij}x_ix_j \u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003c j}a_{ij}x_j^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\\n\u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003c j}a_{ji}x_j^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\ \u0026 = \u0026 \\displaystyle{\\sum_{i \u003c j}a_{ij}x_i^2+\\sum_{i \u003e j}a_{ij}x_i^2-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\\n\u0026 = \u0026 \\displaystyle{\\sum_i(\\sum_{j\\neq i}a_{ij}x_i^2)-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\\\\\n\u0026 = \u0026 \\displaystyle{\\sum_i(x_i^2(1-a_{ii}))-\\sum_{i \u003c j}a_{ij}(x_i-x_j)^2}\n\\end{array}\n\\end{equation}\n$$\u003cp\u003e第2、3个等号都是因为A是对称矩阵，所以可以把\\(a_{ij}\\)替换为\\(a_{ji}\\)，然后互换\\(i,j\\)下标。最后一个等号是因为A的行和为1。\u003c/p\u003e","title":"随机矩阵及其特征值"},{"content":"马尔可夫聚类算法（The Markov Cluster Algorithm, MCL）是一种快速可扩展的基于图的聚类算法。它的基本思想为：在一个稀疏图G中，如果某个区域A是稠密的（是一个聚类），则在A中随机游走k步，还在A内的概率很大，也就是说，A内的k步路径（k-length path）很多。所以我们可以在图中随机游走k步，如果某个区域连通的概率很大，则该区域是一个聚类。随机游走的下一步只和当前所处节点有关，也就是说这是一个马尔可夫的随机游走过程。\n我们用一个例子来演示马尔可夫聚类算法的过程。\n上图是一个很小的网络，我们用肉眼大概能看出有三个聚类，分别是左边的{1,6,7,10}，中间的{2,3,5}和右边的{4,8,9,11,12}。我们用MCL看看结果如何。\n为了随机游走，我们常用邻接矩阵来表示图，如果i,j有边，则N[i][j]=1，否则N[i][j]=0。又随机游走可能有自回路，所以加上单位矩阵I，得到矩阵N+I。\nMCL有两个关键的步骤，分别是Expansion和Inflation。\nExpansion就是不断对矩阵进行幂次运算，相当于随机游走。假设随机游走了2步，则得到如下图的关联矩阵\\((N+I)^2\\)，第1行第10列为4，说明1到10的2-length path有4条：1→6→10，1→7→10，1→1→10，1→10→10。随机游走k步之后，\\((N+I)^k[i][j]\\)越大，说明\\(i\\)和\\(j\\)之间的连通性越强。\n$$\\begin{equation}Expand(M)=M^k\\end{equation}$$\nInflation是为了增强更强的连接，减弱更弱的连接，只有这样才能得到边界比较明确的聚类。MCL的做法是对元素做幂次运算，然后按列归一化，公式为：\n$$\\begin{equation}(\\Gamma_rM)_{pq}=\\frac{(M_{pq})^r}{\\sum_{i=1}^k(M_{iq})^r}\\end{equation}$$参数经验值是\\(k=r=2\\)。不断做Expansion和Inflation操作，直到算法收敛，得到若干个聚类。中间过程请点此查看，下图为最终结果。\n从图中可以看出，和1有边的只剩下6,7,10了，所以得到聚类{1,6,7,10}，同理能得到聚类{2,3,5}和{4,8,9,11,12} ，和我们肉眼得到的结果是一致的。\nMCL算法的原理很简单，得到的聚类效果也不错。下面总结一下MCL的算法过程：\n给定无向图G，Expansion和Inflation的参数\\(k\\)和\\(r\\) 生成G的邻接矩阵\\(N\\) 添加自回路，得到矩阵\\(N+I\\) 循环对\\(N+I\\)做Expansion和Inflation操作，即计算公式(1)和(2)，直到收敛 根据最终得到的矩阵，进行划分聚类 此算法是我在上《生物信息学中的算法设计》课上是学到的，当时觉得这个算法真是神奇，如此简单，但又如此有效，实在高明。查阅文献得知，此为Stijn van Dongen的博士论文，本博客的图片均来自其博士论文，想深入了解图聚类算法，请下载他的论文。\n","permalink":"http://localhost:1313/posts/2016-08-22-the-markov-cluster-algorithm/","summary":"\u003cp\u003e马尔可夫聚类算法（The Markov Cluster Algorithm, MCL）是一种快速可扩展的基于图的聚类算法。它的基本思想为：在一个稀疏图G中，如果某个区域A是稠密的（是一个聚类），则在A中随机游走k步，还在A内的概率很大，也就是说，A内的k步路径（k-length path）很多。所以我们可以在图中随机游走k步，如果某个区域连通的概率很大，则该区域是一个聚类。随机游走的下一步只和当前所处节点有关，也就是说这是一个马尔可夫的随机游走过程。\u003c/p\u003e\n\u003cp\u003e我们用一个例子来演示马尔可夫聚类算法的过程。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"mcl-1\" loading=\"lazy\" src=\"/posts/2016-08-22-the-markov-cluster-algorithm/mcl-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上图是一个很小的网络，我们用肉眼大概能看出有三个聚类，分别是左边的{1,6,7,10}，中间的{2,3,5}和右边的{4,8,9,11,12}。我们用MCL看看结果如何。\u003c/p\u003e\n\u003cp\u003e为了随机游走，我们常用邻接矩阵来表示图，如果i,j有边，则N[i][j]=1，否则N[i][j]=0。又随机游走可能有自回路，所以加上单位矩阵I，得到矩阵N+I。\u003c/p\u003e\n\u003cp\u003eMCL有两个关键的步骤，分别是Expansion和Inflation。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExpansion\u003c/strong\u003e就是不断对矩阵进行幂次运算，相当于随机游走。假设随机游走了2步，则得到如下图的关联矩阵\\((N+I)^2\\)，第1行第10列为4，说明1到10的2-length path有4条：1→6→10，1→7→10，1→1→10，1→10→10。随机游走k步之后，\\((N+I)^k[i][j]\\)越大，说明\\(i\\)和\\(j\\)之间的连通性越强。\u003c/p\u003e\n$$\\begin{equation}Expand(M)=M^k\\end{equation}$$\u003cp\u003e\u003cimg alt=\"mcl-2\" loading=\"lazy\" src=\"/posts/2016-08-22-the-markov-cluster-algorithm/mcl-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInflation\u003c/strong\u003e是为了增强更强的连接，减弱更弱的连接，只有这样才能得到边界比较明确的聚类。MCL的做法是对元素做幂次运算，然后按列归一化，公式为：\u003c/p\u003e\n$$\\begin{equation}(\\Gamma_rM)_{pq}=\\frac{(M_{pq})^r}{\\sum_{i=1}^k(M_{iq})^r}\\end{equation}$$\u003cp\u003e参数经验值是\\(k=r=2\\)。不断做Expansion和Inflation操作，直到算法收敛，得到若干个聚类。\u003ca href=\"/posts/2016-08-22-the-markov-cluster-algorithm/mcl-3.pdf\"\u003e中间过程请点此查看\u003c/a\u003e，下图为最终结果。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"mcl-4\" loading=\"lazy\" src=\"/posts/2016-08-22-the-markov-cluster-algorithm/mcl-4.png\"\u003e\u003c/p\u003e\n\u003cp\u003e从图中可以看出，和1有边的只剩下6,7,10了，所以得到聚类{1,6,7,10}，同理能得到聚类{2,3,5}和{4,8,9,11,12} ，和我们肉眼得到的结果是一致的。\u003c/p\u003e\n\u003cp\u003eMCL算法的原理很简单，得到的聚类效果也不错。下面总结一下MCL的算法过程：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e给定无向图G，Expansion和Inflation的参数\\(k\\)和\\(r\\)\u003c/li\u003e\n\u003cli\u003e生成G的邻接矩阵\\(N\\)\u003c/li\u003e\n\u003cli\u003e添加自回路，得到矩阵\\(N+I\\)\u003c/li\u003e\n\u003cli\u003e循环对\\(N+I\\)做Expansion和Inflation操作，即计算公式(1)和(2)，直到收敛\u003c/li\u003e\n\u003cli\u003e根据最终得到的矩阵，进行划分聚类\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e此算法是我在上《生物信息学中的算法设计》课上是学到的，当时觉得这个算法真是神奇，如此简单，但又如此有效，实在高明。查阅文献得知，此为\u003ca href=\"http://www.micans.org/mcl/\"\u003eStijn van Dongen的博士论文\u003c/a\u003e，本博客的图片均来自其博士论文，想深入了解图聚类算法，请\u003ca href=\"http://micans.org/mcl/lit/svdthesis.pdf.gz\"\u003e下载他的论文\u003c/a\u003e。\u003c/p\u003e","title":"马尔可夫聚类算法"},{"content":"上一回介绍了HMM的解码问题，今天我们介绍HMM的学习问题和识别问题，先来看学习问题。\n正如上一回结束时所说，HMM的学习问题是：仅已知观测序列\\(\\vec y\\)，要估计出模型参数组\\(\\vec\\lambda=(\\mu,A,B)\\)，其中\\(\\mu\\)为初始概率分布向量，\\(A\\)为转移概率矩阵，\\(B\\)为发射概率矩阵。\n算法设计 求解HMM的参数学习问题，就是求解如下的最优化问题：\n$$\\begin{equation} P(\\vec Y = \\vec y|\\hat \\lambda)=\\max\\limits_{\\vec \\lambda} P(\\vec Y = \\vec y|\\vec \\lambda)\\end{equation}$$也就是找一个参数\\(\\vec \\lambda\\)，使得模型在该参数下最有可能产生当前的观测\\(\\vec y\\)。如果使用极大似然法求解，对于似然函数\\(P(\\vec Y=\\vec y|\\vec \\lambda)=\\sum\\limits_{i_1,…,i_T}\\mu_{i_1}b_{i_1y_1}a_{i_1i_2}…a_{i_{T-1}i_T}b_{i_Ty_T}\\)而言，这个最大值问题的计算量过大，在实际中是不可能被采用的。为此，人们构造了一个递推算法，使其能相当合理地给出模型参数\\(\\vec \\lambda\\)的粗略估计。其核心思想是：并不要求备选\\(\\vec\\lambda\\)使得\\(P(\\vec Y=\\vec y|\\vec \\lambda)\\)达到最大或局部极大，而只要求使\\(P(\\vec Y=\\vec y|\\vec \\lambda)\\)相当大，从而使计算变为实际可能。\nEM算法 为此，我们定义一个描述模型“趋势”的量\\(Q(\\vec\\lambda^*|\\vec\\lambda)\\)代替似然函数\\(P(\\vec Y=\\vec y|\\vec\\lambda)\\)，其定义为：\n$$\\begin{equation} Q(\\vec\\lambda^*|\\vec\\lambda)=\\sum\\limits_{\\vec x}P(\\vec x,\\vec y|\\vec\\lambda)\\ln P(\\vec x,\\vec y|\\vec\\lambda^*)\\end{equation}$$利用在\\(0 \u003c x \u003c 1\\)时，不等式\\(\\ln x\\leq x-1\\)成立，可以证明：\n$$\\begin{equation} Q(\\vec\\lambda^*|\\vec\\lambda)-Q(\\vec\\lambda|\\vec\\lambda)\\leq P(\\vec Y=\\vec y|\\vec\\lambda^*)-P(\\vec Y=\\vec y|\\vec\\lambda)\\end{equation}$$由此可见，对于固定的\\(\\vec\\lambda\\)，只要\\(Q(\\vec\\lambda^*|\\vec\\lambda)\u003eQ(\\vec\\lambda|\\vec\\lambda)\\)，就有\\(P(\\vec Y=\\vec y|\\vec\\lambda^*)\u003eP(\\vec Y=\\vec y|\\vec\\lambda)\\)。于是想把模型\\(\\vec\\lambda_m\\)修改为更好的模型\\(\\vec\\lambda_{m+1}\\)，只需找\\(\\vec\\lambda_{m+1}\\)使得：\n$$\\begin{equation}Q(\\vec\\lambda_{m+1}|\\vec\\lambda_m)=\\sup_{\\vec\\lambda}Q(\\vec\\lambda|\\vec\\lambda_m)\\end{equation}$$即只要把\\(Q(\\vec\\lambda|\\vec\\lambda_m)\\)关于\\(\\vec\\lambda\\)的最大值处取成\\(\\vec\\lambda_{m+1}\\)，就有\\(P(\\vec Y=\\vec y|\\vec\\lambda_{m+1})\u003eP(\\vec Y=\\vec y|\\vec\\lambda_m)\\)。\n这样得到的模型序列\\(\\{\\vec\\lambda_m\\}\\)能保证\\(P(\\vec Y=\\vec y|\\vec\\lambda_m)\\)关于\\(m\\)是严格递增的，虽然在这里还不能在理论上证明\\(P(\\vec Y=\\vec y|\\vec\\lambda_m)\\)收敛到\\(\\max_{\\vec\\lambda}P(\\vec Y=\\vec y|\\vec\\lambda)\\)，但是当\\(m\\)充分大时，\\(\\vec\\lambda_m\\)也还能提供在实际中较为满意的粗略近似。\n综上论述，我们把如上得到的近似模型列\\(\\vec\\lambda_m\\)的方法归结为两个步骤：\nE步骤（求期望）：计算$$\\begin{equation}Q(\\vec\\lambda^*|\\vec\\lambda)=\\sum\\limits_{\\vec x}P(\\vec x,\\vec y|\\vec\\lambda)\\ln P(\\vec x,\\vec y|\\vec\\lambda^*)\\end{equation}$$ M步骤（求最大）：求\\(\\vec\\lambda_{m+1}\\)使$$\\begin{equation}Q(\\vec\\lambda_{m+1}|\\vec\\lambda_m)=\\sup_{\\vec\\lambda}Q(\\vec\\lambda|\\vec\\lambda_m)\\end{equation}$$ 这两个步骤合起来构成的算法，称为期望最大化（Expectation-maximization, EM）算法。EM算法是针对在测量数据不完全时，求参数的一种近似于最大似然估计的统计方法。\nBaum-Welch算法 隐Markov模型中的M-步骤的解可以有显式表示，这就是一组把模型参数修改为新的模型参数的递推公式，这组公式正好是在隐Markov模型中普遍应用的著名的Baum-Welch公式。\n$$\\begin{equation}\\hat\\mu_i^{m+1}=\\frac{P(\\vec Y=\\vec y,X_1=i|\\vec\\lambda_m)}{P(\\vec Y=\\vec y|\\vec\\lambda_m)}=\\gamma_1(i)\\end{equation}$$$$\\begin{equation}\\hat a_{ij}^{m+1}=\\frac{\\sum\\limits_{t=1}^{T-1}P(X_t=i,X_{t+1}=j|\\vec Y=\\vec y,\\vec\\lambda_m)}{\\sum\\limits_{t=1}^{T-1}P(X_t=i|\\vec Y=\\vec y,\\vec\\lambda_m)}\\triangleq\\frac{\\sum\\limits_{t=1}^{T-1}\\xi_t(i,j)}{\\sum\\limits_{t=1}^{T-1}\\gamma_t(i)}\\end{equation}$$$$\\begin{equation}\\hat b_{il}^{m+1}=\\frac{\\sum\\limits_{t=1}^TP(\\vec Y=\\vec y,X_t=i|\\vec\\lambda_m)I_{\\{l\\}}(y_t)}{\\sum\\limits_{t=1}^TP(\\vec Y=\\vec y,X_t=i|\\vec\\lambda_m)}\\triangleq\\frac{\\sum\\limits_{t=1,y_t=l}^T\\gamma_t(i)}{\\sum\\limits_{t=1}^T\\gamma_t(i)}\\end{equation}$$Baum-Welch算法用到了如下几个公式：\n向前算法，\\(\\alpha_t(i)=P(Y_1=y_1,…,Y_t=y_t,X_t=i|\\lambda)\\)，满足前\\(t\\)个状态，推进到满足前\\(t+1\\)个状态（\\(t\\rightarrow t+1\\)）：\\(\\begin{equation}\\alpha_1(i)=\\mu_ib_{iy_1}\\quad \\alpha_{t+1}(i)=\\sum\\limits_j\\alpha_t(j)a_{ji}b_{iy_{t+1}}\\end{equation}\\) 向后算法，\\(\\beta_t(i)=P(Y_{t+1}=y_{t+1},…,Y_T=y_T|X_t=i,\\lambda)\\)，满足后\\(t-1\\)个状态，推进到满足后\\(t\\)个状态（\\(t+1\\rightarrow t\\)）：\\(\\begin{equation}\\beta_T(i)=1\\quad \\beta_t(i)=\\sum\\limits_j\\beta_{t+1}(j)a_{ij}b_{jy_{t+1}}\\end{equation}\\) 向前向后算法，满足所有观测状态，且\\(t\\)时刻的隐状态为\\(i\\)：\\(\\begin{equation}\\gamma_t(i)=P(X_t=i|\\vec Y=\\vec y,\\vec\\lambda)=\\frac{P(\\vec Y=\\vec y,X_t=i|\\vec\\lambda)}{\\sum\\limits_iP(\\vec Y=\\vec y,X_t=i|\\vec\\lambda)}=\\frac{\\alpha_t(i)\\beta_t(i)}{\\sum\\limits_i\\alpha_t(i)\\beta_t(i)}\\end{equation}\\) 以及记号\\(\\begin{equation}\\xi_t(i,j)\\triangleq P(X_t=i,X_{t+1}=j|\\vec Y=\\vec y,\\vec\\lambda)=\\frac{\\alpha_t(i)a_{ij}b_{jy_{t+1}}\\beta_{t+1}(j)}{\\sum\\limits_i\\alpha_t(i)\\beta_t(i)}\\end{equation}\\) 算法流程 最后，我们可以将Baum-Welch公式应用于EM算法中的M步骤，来逐步改进模型参数\\(\\vec\\lambda\\)。为了使训练结果更加可信，通常应该有多条观测序列。假设输入为所有\\(k\\)次观测序列集合\\(S\\)和收敛阈值\\(\\epsilon\\)，输出为训练得到的模型参数\\(\\hat{\\vec\\lambda}\\)，则基于Baum-Welch公式的EM算法求解HMM学习问题的伪代码如下：\n现在要求解另一个韦小宝的骰子的问题：韦小宝有两个有偏的骰子A,B，A,B掷出相同点数的概率不同，每次韦小宝随机拿一个骰子并投掷，记录下正面朝上的点数，重复100次，得到一条长度为100的点数序列，如此重复100次，得到100条类似的序列。现只给定这100条点数序列，要求解出韦小宝每次投掷的是哪个骰子，并分析这两个骰子有什么区别。\n这就是一个典型的HMM的参数学习问题，利用上述伪代码可以很快的求解出模型参数\\(\\vec\\lambda\\)，A,B的发射概率就是它们的不同点。\nHMM的识别问题是：对于一个特定的观测链\\(\\vec y\\)，已知它可能是由已经学习好的若干模型之一所得的观测，要决定此观测究竟是得自其中哪一个模型，这称为识别问题。\n判决步骤：\n根据参数求出在每一个模型中，出现给定样本的概率\\(P(\\vec Y=\\vec y|\\lambda_k)\\)，归一化就得到给定样本来自每个模型的概率\\(P(\\lambda_k|\\vec Y=\\vec y)\\)。 利用贝叶斯原理，就可以得到最好模型的猜测。 本博客开头提到，要求解\\(P(\\vec Y=\\vec y|\\lambda)\\)需要指数时间（\\(O(N^T)\\)）：\n$$\\begin{equation}P(\\vec Y=\\vec y|\\vec \\lambda)=\\sum\\limits_{i_1,…,i_T}\\mu_{i_1}b_{i_1y_1}a_{i_1i_2}…a_{i_{T-1}i_T}b_{i_Ty_T}\\end{equation}$$所以可以利用向前算法（式(10)）或者向后算法（式(11)），对应的结果分别为：\n$$\\begin{equation}P(\\vec Y=\\vec y|\\lambda)=\\sum_{i=1}^N\\alpha_T(i)\\end{equation}$$$$\\begin{equation}P(\\vec Y=\\vec y|\\lambda)=\\sum_{i=1}^N\\beta_1(i)\\mu_ib_{iy_1}\\end{equation}$$然后利用贝叶斯公式得到\\(P(\\lambda_k|\\vec Y=\\vec y)\\)，使结果最大的\\(k\\)即为所求模型。\n","permalink":"http://localhost:1313/posts/2016-08-21-introduction-to-hmm-2/","summary":"\u003cp\u003e上一回介绍了HMM的解码问题，今天我们介绍HMM的学习问题和识别问题，先来看学习问题。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e正如上一回结束时所说，\u003cstrong\u003eHMM的学习问题\u003c/strong\u003e是：仅已知观测序列\\(\\vec y\\)，要估计出模型参数组\\(\\vec\\lambda=(\\mu,A,B)\\)，其中\\(\\mu\\)为初始概率分布向量，\\(A\\)为转移概率矩阵，\\(B\\)为发射概率矩阵。\u003c/p\u003e\n\u003ch1 id=\"算法设计\"\u003e算法设计\u003c/h1\u003e\n\u003cp\u003e求解HMM的参数学习问题，就是求解如下的最优化问题：\u003c/p\u003e\n$$\\begin{equation} P(\\vec Y = \\vec y|\\hat \\lambda)=\\max\\limits_{\\vec \\lambda} P(\\vec Y = \\vec y|\\vec \\lambda)\\end{equation}$$\u003cp\u003e也就是找一个参数\\(\\vec \\lambda\\)，使得模型在该参数下最有可能产生当前的观测\\(\\vec y\\)。如果使用极大似然法求解，对于似然函数\\(P(\\vec Y=\\vec y|\\vec \\lambda)=\\sum\\limits_{i_1,…,i_T}\\mu_{i_1}b_{i_1y_1}a_{i_1i_2}…a_{i_{T-1}i_T}b_{i_Ty_T}\\)而言，这个最大值问题的计算量过大，在实际中是不可能被采用的。为此，人们构造了一个递推算法，使其能相当合理地给出模型参数\\(\\vec \\lambda\\)的粗略估计。其核心思想是：并不要求备选\\(\\vec\\lambda\\)使得\\(P(\\vec Y=\\vec y|\\vec \\lambda)\\)达到最大或局部极大，而只要求使\\(P(\\vec Y=\\vec y|\\vec \\lambda)\\)相当大，从而使计算变为实际可能。\u003c/p\u003e\n\u003ch1 id=\"em算法\"\u003eEM算法\u003c/h1\u003e\n\u003cp\u003e为此，我们定义一个描述模型“趋势”的量\\(Q(\\vec\\lambda^*|\\vec\\lambda)\\)代替似然函数\\(P(\\vec Y=\\vec y|\\vec\\lambda)\\)，其定义为：\u003c/p\u003e\n$$\\begin{equation} Q(\\vec\\lambda^*|\\vec\\lambda)=\\sum\\limits_{\\vec x}P(\\vec x,\\vec y|\\vec\\lambda)\\ln P(\\vec x,\\vec y|\\vec\\lambda^*)\\end{equation}$$\u003cp\u003e利用在\\(0 \u003c x \u003c 1\\)时，不等式\\(\\ln x\\leq x-1\\)成立，可以证明：\u003c/p\u003e\n$$\\begin{equation} Q(\\vec\\lambda^*|\\vec\\lambda)-Q(\\vec\\lambda|\\vec\\lambda)\\leq P(\\vec Y=\\vec y|\\vec\\lambda^*)-P(\\vec Y=\\vec y|\\vec\\lambda)\\end{equation}$$\u003cp\u003e由此可见，对于固定的\\(\\vec\\lambda\\)，只要\\(Q(\\vec\\lambda^*|\\vec\\lambda)\u003eQ(\\vec\\lambda|\\vec\\lambda)\\)，就有\\(P(\\vec Y=\\vec y|\\vec\\lambda^*)\u003eP(\\vec Y=\\vec y|\\vec\\lambda)\\)。于是想把模型\\(\\vec\\lambda_m\\)修改为更好的模型\\(\\vec\\lambda_{m+1}\\)，只需找\\(\\vec\\lambda_{m+1}\\)使得：\u003c/p\u003e\n$$\\begin{equation}Q(\\vec\\lambda_{m+1}|\\vec\\lambda_m)=\\sup_{\\vec\\lambda}Q(\\vec\\lambda|\\vec\\lambda_m)\\end{equation}$$\u003cp\u003e即只要把\\(Q(\\vec\\lambda|\\vec\\lambda_m)\\)关于\\(\\vec\\lambda\\)的最大值处取成\\(\\vec\\lambda_{m+1}\\)，就有\\(P(\\vec Y=\\vec y|\\vec\\lambda_{m+1})\u003eP(\\vec Y=\\vec y|\\vec\\lambda_m)\\)。\u003c/p\u003e\n\u003cp\u003e这样得到的模型序列\\(\\{\\vec\\lambda_m\\}\\)能保证\\(P(\\vec Y=\\vec y|\\vec\\lambda_m)\\)关于\\(m\\)是严格递增的，虽然在这里还不能在理论上证明\\(P(\\vec Y=\\vec y|\\vec\\lambda_m)\\)收敛到\\(\\max_{\\vec\\lambda}P(\\vec Y=\\vec y|\\vec\\lambda)\\)，但是当\\(m\\)充分大时，\\(\\vec\\lambda_m\\)也还能提供在实际中较为满意的粗略近似。\u003c/p\u003e","title":"隐马尔可夫模型及其应用（2）学习问题\u0026识别问题"},{"content":"隐马尔可夫模型（Hidden Markov Model, HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。\n先举一个简单的例子以直观地理解HMM的实质——韦小宝的骰子。\n假设韦小宝有两个骰子，一个正常的骰子A，A以1/6的概率均等的出现每个点；一个不正常的骰子B，B出现5,6点数的概率为0.3，出现其他点数的概率为0.1。显然投掷B更容易出现大的点数。每次试验第一次投掷时，韦小宝会以0.4的概率出千（即投掷B）。但是在一次试验中，韦小宝不太可能一直出千，所以骰子会在A、B之间转换，比如这次投了B，下次可能会以0.1的概率投A。A、B之间的转移概率如下图。\n某一次试验，我们观察到韦小宝掷出的骰子序列为\\(O=(1,3,4,5,5,6,6,3,2,6)\\)，请问韦小宝什么时候出千了。这个问题就可以通过HMM求解。\nHMM有2个状态：\n观测状态。我们观察到的骰子序列称为观测状态\\(\\mathbf{Y}=\\{y_1,y_2,…,y_T\\}\\) 隐状态。隐含在每个观测状态里面的是隐状态\\(\\mathbf{X}=\\{x_1,x_2,…,x_T\\}\\) T是时间，也可以认为是观测的次数。HMM有3个参数：\n初始分布\\(\\mathbf{\\mu}=(\\mu_i)\\)，\\(\\mu_i=Pr(x_1=i)\\)，即第一次观测时，每个隐状态出现的概率 转移概率矩阵\\(A=(a_{ij})\\)，\\(a_{ij}=Pr(x_{t+1}=j|x_t=i)\\)，即t时刻的隐状态为i，t+1时刻转移到隐状态j的概率 发射概率矩阵\\(B=(b_{il})\\)，\\(b_{il}=Pr(y_t=l|x_t=i)\\)，即t时候隐状态为i的情况下，观测到状态为l的概率 参数\\(\\mathbf{\\lambda=\\{\\mu,A,B\\}}\\)称为HMM的模型参数。具体到上面的例子，我们有初始分布和转移概率为：\n发射概率为：\n观测状态为\\(\\mathbf{Y}=(1,3,4,5,5,6,6,3,2,6)\\)，问题就是求解出隐状态\\(\\mathbf{X}\\)，此问题被称为HMM的解码问题，可以由著名的维特比算法（Viterbi algorithm）解决。\n解码问题是要求出使得观测状态\\(Y\\)出现概率最大的隐状态\\(X\\)，假设有N个隐状态（本例为2），共有T个时刻（本例为10），则每个时刻有N个取值可能，则共有\\(N^T\\)条可能的隐状态链（本例为\\(2^{10}\\)）。我们需要求出每一条隐状态链下T个发射概率的乘积，然后取最大值，这是指数时间复杂度的（\\(O(N^T)\\)）。\n但是Viterbi算法是一个动态规划算法，只需多项式时间即可解决该问题。该算法的原理很好理解，假设我们求得到\\(s_{i2}\\)的最大概率路径为下图中的红线\\(s_{11}\\rightarrow s_{22}\\rightarrow … s_{i2}\\)，则在求经过\\(s_{i2}\\)到\\(s_{(i+1)1}\\)的最大概率路径时，不需要再测试\\(s_{13}\\rightarrow s_{21}\\rightarrow s_{i2}\\rightarrow s_{(i+1)1}\\)这条路径（下图蓝线），因为显然已经知道红线概率大于蓝线概率了。图中还有很多类似蓝线的路径都可以不用计算了，大大提高了求解速度。\n因为计算第\\(i+1\\)时刻的累积概率只和第\\(i\\)时刻的概率有关，每次至多计算\\(N*N\\)个概率乘积（可以从\\(i\\)时刻的\\(N\\)个状态到达\\(i+1\\)时刻的某个状态，\\(i+1\\)时刻共有\\(N\\)个状态），最多计算T次（共T个时刻），所以时间复杂度降到了\\(O(N^2T)\\)。\n下面我们形式化的描述Viterbi算法。\n假设\\(\\delta_t(i)\\)为\\(t\\)时刻取到隐状态\\(i\\)，且1~t的观测状态都符合观测值\\(Y\\)的各个路径的最大概率，即\n$$ \\begin{equation}\\delta_t(i)=\\underset{i_1,…,i_{t-1}}{\\max}Pr(X_t=i,X_{t-1}=i_{t-1},…,X_1=i_1,Y_t=y_t,…,Y_1=y_1|\\mathbf{\\lambda})\\end{equation} $$联系上图，可认为\\(\\delta_t(i)\\)为红线。则递推公式为：\n$$ \\begin{equation}\\delta_{t+1}(i)=b_{iy_{t+1}}\\underset{j}{\\max}(\\delta_t(j)a_{ji})\\end{equation} $$由\\(j\\)到\\(i\\)的转移概率，再乘上\\(i\\)发射\\(y_{t+1}\\)的概率。\n在初始时刻\\(t=1\\)，有：\n$$ \\begin{equation}\\delta_1(i)=\\mu_ib_{iy_1}\\end{equation} $$最后的全局最大概率为\\(\\underset{j}{\\max}\\delta_T(j)\\)。为了得到完整路径，我们保留每一隐状态取得最大概率时的上一隐状态，即：\n$$ \\begin{equation}\\psi_{t+1}(i)=j^*\\end{equation} $$其中\\(j^*\\)要满足\n$$ \\begin{equation}\\delta_{t+1}(i)=b_{iy_{t+1}}\\delta_t(j^*)a_{j^*i}\\end{equation} $$最后使用如下回溯法得到所有最佳隐状态：\n$$\\begin{equation}X_T=i^*\\in\\{i:\\delta_T(i)=\\underset{j}{\\max}\\delta_T(j)\\}\\end{equation}$$$$\\begin{equation}X_t=\\psi_{t+1}(X_{t+1})\\end{equation}$$下面我们利用Viterbi算法来求解韦小宝的骰子这个例子。\n\\(t=1\\)时，\\(y_1=1\\)，有\\(\\delta_1(A)=0.6*1/6=0.1\\)，\\(\\delta_1(B)=0.4*0.1=0.04\\)。\n\\(t=2\\)时，\\(y_2=3\\)，有：\n隐状态为A：a）A-\u0026gt;A有\\(\\delta_2(A)=(1/6)*0.1*0.8=1.33*10^{-2}\\)；b）B-\u0026gt;A有\\(\\delta_2(A)=(1/6)*0.04*0.1=6.6*10^{-4}\\)。所以A-\u0026gt;A，\\(\\psi_2(A)=A\\)。 隐状态为B：a）A-\u0026gt;B有\\(\\delta_2(B)=0.1*0.1*0.2=2*10^{-3}\\)；b）B-\u0026gt;B有\\(\\delta_2(B)=0.1*0.04*0.9=3.6*10^{-3}\\)。所以B-\u0026gt;B，\\(\\psi_2(B)=B\\)。 如此计算下去，可以得到如下表： \\(t=10\\)时最大概率为\\(\\delta_{10}(B)\\)，经过回溯得到最佳隐状态为：\n所以HMM很神奇吧，可以抓住韦小宝从第5次开始就一直在出千，而且出千之后，掷出的点数大部分为5和6。\nViterbi算法还可用于解决语音识别或者拼音输入法。我们知道中文的一个拼音可以对应多个汉字，连续的一段拼音就能组成成千上万种可能的句子，哪一个句子才是最佳候选呢？我们可以把每个拼音当成观测状态，同音的汉字当成可能的隐状态。通过背景语料库统计得到每个汉字出现在词首的概率、汉字之间的转移概率和汉字与拼音之间的发射概率，这样我们就能得到模型参数，然后利用Viterbi算法求解出一个最佳的隐状态序列，这样就能完成一个简易的拼音输入法。\nHMM在实际中主要有3个方面的应用，分别是：\n从一段观测序列\\(\\mathbf{Y}\\)及已知模型\\(\\mathbf{\\lambda=(\\mu,A,B)}\\)出发，估计出隐状态\\(\\mathbf{X}\\)的最佳值，称为解码问题，这是状态估计问题。这篇博客讨论的就是这个问题。 从一段观测序列\\(\\mathbf{Y}\\)出发，估计模型参数组\\(\\mathbf{\\lambda=(\\mu,A,B)}\\)，称为学习问题，就是参数估计问题。 对于一个特定的观测链\\(\\mathbf{Y}\\)，已知它可能是由已经学习好的若干模型之一所得的观测，要决定此观测究竟是得自其中哪一个模型，这称为识别问题，就是分类问题。 关于HMM的学习问题和识别问题，请听下回分解。\n","permalink":"http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/","summary":"\u003cp\u003e\u003ca href=\"https://zh.wikipedia.org/wiki/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B\"\u003e隐马尔可夫模型\u003c/a\u003e（Hidden Markov Model, HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。\u003c/p\u003e\n\u003cp\u003e先举一个简单的例子以直观地理解HMM的实质——韦小宝的骰子。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-2\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e假设韦小宝有两个骰子，一个正常的骰子A，A以1/6的概率均等的出现每个点；一个不正常的骰子B，B出现5,6点数的概率为0.3，出现其他点数的概率为0.1。显然投掷B更容易出现大的点数。每次试验\u003cstrong\u003e第一次投掷时\u003c/strong\u003e，韦小宝会以0.4的概率出千（即投掷B）。但是在一次试验中，韦小宝不太可能一直出千，所以骰子会在A、B之间转换，比如这次投了B，下次可能会以0.1的概率投A。A、B之间的转移概率如下图。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-1\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e某一次试验，我们观察到韦小宝掷出的骰子序列为\\(O=(1,3,4,5,5,6,6,3,2,6)\\)，请问韦小宝什么时候出千了。这个问题就可以通过HMM求解。\u003c/p\u003e\n\u003cp\u003eHMM有2个状态：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e观测状态。我们观察到的骰子序列称为观测状态\\(\\mathbf{Y}=\\{y_1,y_2,…,y_T\\}\\)\u003c/li\u003e\n\u003cli\u003e隐状态。隐含在每个观测状态里面的是隐状态\\(\\mathbf{X}=\\{x_1,x_2,…,x_T\\}\\)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eT是时间，也可以认为是观测的次数。HMM有3个参数：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e初始分布\\(\\mathbf{\\mu}=(\\mu_i)\\)，\\(\\mu_i=Pr(x_1=i)\\)，即第一次观测时，每个隐状态出现的概率\u003c/li\u003e\n\u003cli\u003e转移概率矩阵\\(A=(a_{ij})\\)，\\(a_{ij}=Pr(x_{t+1}=j|x_t=i)\\)，即t时刻的隐状态为i，t+1时刻转移到隐状态j的概率\u003c/li\u003e\n\u003cli\u003e发射概率矩阵\\(B=(b_{il})\\)，\\(b_{il}=Pr(y_t=l|x_t=i)\\)，即t时候隐状态为i的情况下，观测到状态为l的概率\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e参数\\(\\mathbf{\\lambda=\\{\\mu,A,B\\}}\\)称为HMM的模型参数。具体到上面的例子，我们有初始分布和转移概率为：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-3\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e发射概率为：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-4\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-4.png\"\u003e\u003c/p\u003e\n\u003cp\u003e观测状态为\\(\\mathbf{Y}=(1,3,4,5,5,6,6,3,2,6)\\)，问题就是求解出隐状态\\(\\mathbf{X}\\)，此问题被称为HMM的解码问题，可以由著名的\u003ca href=\"https://zh.wikipedia.org/wiki/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95\"\u003e维特比算法（Viterbi algorithm）\u003c/a\u003e解决。\u003c/p\u003e\n\u003cp\u003e解码问题是要求出使得观测状态\\(Y\\)出现概率最大的隐状态\\(X\\)，假设有N个隐状态（本例为2），共有T个时刻（本例为10），则每个时刻有N个取值可能，则共有\\(N^T\\)条可能的隐状态链（本例为\\(2^{10}\\)）。我们需要求出每一条隐状态链下T个发射概率的乘积，然后取最大值，这是指数时间复杂度的（\\(O(N^T)\\)）。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-5\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-5.png\"\u003e\u003c/p\u003e\n\u003cp\u003e但是Viterbi算法是一个动态规划算法，只需多项式时间即可解决该问题。该算法的原理很好理解，假设我们求得到\\(s_{i2}\\)的最大概率路径为下图中的红线\\(s_{11}\\rightarrow s_{22}\\rightarrow … s_{i2}\\)，则在求经过\\(s_{i2}\\)到\\(s_{(i+1)1}\\)的最大概率路径时，不需要再测试\\(s_{13}\\rightarrow s_{21}\\rightarrow s_{i2}\\rightarrow s_{(i+1)1}\\)这条路径（下图蓝线），因为显然已经知道红线概率大于蓝线概率了。图中还有很多类似蓝线的路径都可以不用计算了，大大提高了求解速度。\u003c/p\u003e\n\u003cp\u003e因为计算第\\(i+1\\)时刻的累积概率只和第\\(i\\)时刻的概率有关，每次至多计算\\(N*N\\)个概率乘积（可以从\\(i\\)时刻的\\(N\\)个状态到达\\(i+1\\)时刻的某个状态，\\(i+1\\)时刻共有\\(N\\)个状态），最多计算T次（共T个时刻），所以时间复杂度降到了\\(O(N^2T)\\)。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-6\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-6.png\"\u003e\u003c/p\u003e\n\u003cp\u003e下面我们形式化的描述Viterbi算法。\u003c/p\u003e\n\u003cp\u003e假设\\(\\delta_t(i)\\)为\\(t\\)时刻取到隐状态\\(i\\)，且1~t的观测状态都符合观测值\\(Y\\)的各个路径的最大概率，即\u003c/p\u003e\n$$\n\\begin{equation}\\delta_t(i)=\\underset{i_1,…,i_{t-1}}{\\max}Pr(X_t=i,X_{t-1}=i_{t-1},…,X_1=i_1,Y_t=y_t,…,Y_1=y_1|\\mathbf{\\lambda})\\end{equation}\n$$\u003cp\u003e联系上图，可认为\\(\\delta_t(i)\\)为红线。则递推公式为：\u003c/p\u003e\n$$\n\\begin{equation}\\delta_{t+1}(i)=b_{iy_{t+1}}\\underset{j}{\\max}(\\delta_t(j)a_{ji})\\end{equation}\n$$\u003cp\u003e由\\(j\\)到\\(i\\)的转移概率，再乘上\\(i\\)发射\\(y_{t+1}\\)的概率。\u003c/p\u003e\n\u003cp\u003e在初始时刻\\(t=1\\)，有：\u003c/p\u003e\n$$\n\\begin{equation}\\delta_1(i)=\\mu_ib_{iy_1}\\end{equation}\n$$\u003cp\u003e最后的全局最大概率为\\(\\underset{j}{\\max}\\delta_T(j)\\)。为了得到完整路径，我们保留每一隐状态取得最大概率时的上一隐状态，即：\u003c/p\u003e\n$$\n\\begin{equation}\\psi_{t+1}(i)=j^*\\end{equation}\n$$\u003cp\u003e其中\\(j^*\\)要满足\u003c/p\u003e\n$$\n\\begin{equation}\\delta_{t+1}(i)=b_{iy_{t+1}}\\delta_t(j^*)a_{j^*i}\\end{equation}\n$$\u003cp\u003e最后使用如下回溯法得到所有最佳隐状态：\u003c/p\u003e\n$$\\begin{equation}X_T=i^*\\in\\{i:\\delta_T(i)=\\underset{j}{\\max}\\delta_T(j)\\}\\end{equation}$$$$\\begin{equation}X_t=\\psi_{t+1}(X_{t+1})\\end{equation}$$\u003cp\u003e下面我们利用Viterbi算法来求解韦小宝的骰子这个例子。\u003c/p\u003e\n\u003cp\u003e\\(t=1\\)时，\\(y_1=1\\)，有\\(\\delta_1(A)=0.6*1/6=0.1\\)，\\(\\delta_1(B)=0.4*0.1=0.04\\)。\u003c/p\u003e\n\u003cp\u003e\\(t=2\\)时，\\(y_2=3\\)，有：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e隐状态为A：a）A-\u0026gt;A有\\(\\delta_2(A)=(1/6)*0.1*0.8=1.33*10^{-2}\\)；b）B-\u0026gt;A有\\(\\delta_2(A)=(1/6)*0.04*0.1=6.6*10^{-4}\\)。所以A-\u0026gt;A，\\(\\psi_2(A)=A\\)。\u003c/li\u003e\n\u003cli\u003e隐状态为B：a）A-\u0026gt;B有\\(\\delta_2(B)=0.1*0.1*0.2=2*10^{-3}\\)；b）B-\u0026gt;B有\\(\\delta_2(B)=0.1*0.04*0.9=3.6*10^{-3}\\)。所以B-\u0026gt;B，\\(\\psi_2(B)=B\\)。\n如此计算下去，可以得到如下表：\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-7\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-7.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\\(t=10\\)时最大概率为\\(\\delta_{10}(B)\\)，经过回溯得到最佳隐状态为：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hmm-8\" loading=\"lazy\" src=\"/posts/2016-08-20-introduction-to-hmm-1/hmm-8.png\"\u003e\u003c/p\u003e\n\u003cp\u003e所以HMM很神奇吧，可以抓住韦小宝从第5次开始就一直在出千，而且出千之后，掷出的点数大部分为5和6。\u003c/p\u003e\n\u003cp\u003eViterbi算法还可用于解决语音识别或者拼音输入法。我们知道中文的一个拼音可以对应多个汉字，连续的一段拼音就能组成成千上万种可能的句子，哪一个句子才是最佳候选呢？我们可以把每个拼音当成观测状态，同音的汉字当成可能的隐状态。通过背景语料库统计得到每个汉字出现在词首的概率、汉字之间的转移概率和汉字与拼音之间的发射概率，这样我们就能得到模型参数，然后利用Viterbi算法求解出一个最佳的隐状态序列，这样就能完成一个简易的拼音输入法。\u003c/p\u003e\n\u003cp\u003eHMM在实际中主要有3个方面的应用，分别是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e从一段观测序列\\(\\mathbf{Y}\\)及已知模型\\(\\mathbf{\\lambda=(\\mu,A,B)}\\)出发，估计出隐状态\\(\\mathbf{X}\\)的最佳值，称为解码问题，这是状态估计问题。这篇博客讨论的就是这个问题。\u003c/li\u003e\n\u003cli\u003e从一段观测序列\\(\\mathbf{Y}\\)出发，估计模型参数组\\(\\mathbf{\\lambda=(\\mu,A,B)}\\)，称为学习问题，就是参数估计问题。\u003c/li\u003e\n\u003cli\u003e对于一个特定的观测链\\(\\mathbf{Y}\\)，已知它可能是由已经学习好的若干模型之一所得的观测，要决定此观测究竟是得自其中哪一个模型，这称为识别问题，就是分类问题。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e关于HMM的学习问题和识别问题，请听下回分解。\u003c/p\u003e","title":"隐马尔可夫模型及其应用（1）简介\u0026解码问题"},{"content":"半年的时光又过去了，圆满结束了一年的集中教学任务，离开了美丽的雁栖湖，回到闹市中关村。\n这半年基本上延续了研一上学期的高强度学习，四门硬课。《高级算法》这门课由四位大师级的老师授课，内容囊括了近似算法、计算复杂性、随机算法、局部搜索、全息规约等，完全是神一样的课。最后复习的时候，大家都生不如死啊，不过经过一个月的挑灯夜战，我还是取得了97分的好成绩，值了。\n《大数据系统与大规模数据分析》这门课的老师是一个年轻的海归，要求很严格，有专门的算法检查平时作业是否抄袭，真的有好几个同学因为抄袭而得0分。这门课的大作业是在GraphLite上实现SVD，我带领队员经过一个月的努力比较圆满的完成了大作业，感谢组里的编程大神。\n《机器学习方法与应用》是面向电子学院的课程，讲得太简单，考试基本是概念题，不建议选修。\n《生物信息学中的算法设计》这门课其实应该叫统计机器学习在生物信息领域的应用，讲的内容比《机器学习方法与应用》的内容更深更广。不过内容太多也难以消化，好好做大作业应该会有不少收获。\n集中教学一年，研一上的GPA是87分，研一下的GPA是89.3分，平均是88.1分。\n除了完成若干个课程大作业，这学期还完成了两个组内大作业，分别是倒排索引和蛋白质搜索引擎，也多谢XN和我一起查Bug、对答案。（天啊，我半年是做了多少个大作业啊…）\n这半年每周二回所和师姐交接任务，真是要感谢天真呆萌的JL师姐，当初保研的时候就被师姐的热情所感染，现在又有幸接替师姐的接力棒，好幸运。\n要说上半年最大的收获，应该是收获了一枚女朋友吧~没错，就是我这篇博客里提到的欣欣~真的没想到这么聊得来，一起吃饭、看电影、聊代码、骑行、游山玩水。这半年拍的照片，比我前22年拍的照片还多。和她在一起很开心，不过有时候也会很累，身体累（羸弱），有时候也心累，毕竟课程压力和组内压力摆在那里，白天去玩了，晚上还是要加班补回来的。有时候冷落了她，也会感到愧疚不安，特别是我在复习《高级算法》期间，两人都很少见面，那一次是真的惹欣欣生气了:-(\n总结一下在雁栖湖一年的收获，大致有如下图的四个方面：\n看看年初计划的完成情况：\n完成国科大下学期的课程任务：完成 接手pLink软件：完成 刷完LeetCode所有题目：上半年基本没刷题，下半年一定完成 读10本书：目前读了《数学之美》、《大话设计模式》、《我不知道该说什么，关于死亡还是爱情》、《男人来自火星、女人来自金星，卷I》，还差6本，下半年加油！ 去电影院看10场电影：目前看了《美人鱼》、《北京遇上西雅图之不二情书》、《忍者神龟2：破影而出》，还差好多… 改正坐姿：有一段时间刻意改正了，但是这东西貌似改不过来？ 下半年就进入实验室，开始科研实战了，做交联的师兄师姐都毕业了，留下我一个人，感觉好艰难，希望我能顺利进入角色，协助师兄把文章发了，维护好pLink2的软件，并且开发集群版。\n","permalink":"http://localhost:1313/posts/2016-08-20-2016-mid-year-summary/","summary":"\u003cp\u003e半年的时光又过去了，圆满结束了一年的集中教学任务，离开了美丽的雁栖湖，回到闹市中关村。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ucas-schedule-2016-spring\" loading=\"lazy\" src=\"/posts/2016-08-20-2016-mid-year-summary/ucas-schedule-2016-spring.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这半年基本上延续了研一上学期的高强度学习，四门硬课。《高级算法》这门课由四位大师级的老师授课，内容囊括了近似算法、计算复杂性、随机算法、局部搜索、全息规约等，完全是神一样的课。最后复习的时候，大家都生不如死啊，不过经过一个月的挑灯夜战，我还是取得了97分的好成绩，值了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"advanced-algorithm\" loading=\"lazy\" src=\"/posts/2016-08-20-2016-mid-year-summary/advanced-algorithm.png\"\u003e\u003c/p\u003e\n\u003cp\u003e《大数据系统与大规模数据分析》这门课的老师是一个年轻的海归，要求很严格，有专门的算法检查平时作业是否抄袭，真的有好几个同学因为抄袭而得0分。这门课的大作业是在GraphLite上实现SVD，我带领队员经过一个月的努力比较圆满的完成了大作业，感谢组里的编程大神。\u003c/p\u003e\n\u003cp\u003e《机器学习方法与应用》是面向电子学院的课程，讲得太简单，考试基本是概念题，不建议选修。\u003c/p\u003e\n\u003cp\u003e《生物信息学中的算法设计》这门课其实应该叫统计机器学习在生物信息领域的应用，讲的内容比《机器学习方法与应用》的内容更深更广。不过内容太多也难以消化，好好做大作业应该会有不少收获。\u003c/p\u003e\n\u003cp\u003e集中教学一年，研一上的GPA是87分，研一下的GPA是89.3分，平均是88.1分。\u003c/p\u003e\n\u003cp\u003e除了完成若干个课程大作业，这学期还完成了两个组内大作业，分别是倒排索引和蛋白质搜索引擎，也多谢XN和我一起查Bug、对答案。（天啊，我半年是做了多少个大作业啊…）\u003c/p\u003e\n\u003cp\u003e这半年每周二回所和师姐交接任务，真是要感谢天真呆萌的JL师姐，当初保研的时候就被师姐的热情所感染，现在又有幸接替师姐的接力棒，好幸运。\u003c/p\u003e\n\u003cp\u003e要说上半年最大的收获，应该是收获了一枚女朋友吧~没错，就是我\u003cdel\u003e这篇博客\u003c/del\u003e里提到的欣欣~真的没想到这么聊得来，一起吃饭、看电影、聊代码、骑行、游山玩水。这半年拍的照片，比我前22年拍的照片还多。和她在一起很开心，不过有时候也会很累，身体累（羸弱），有时候也心累，毕竟课程压力和组内压力摆在那里，白天去玩了，晚上还是要加班补回来的。有时候冷落了她，也会感到愧疚不安，特别是我在复习《高级算法》期间，两人都很少见面，那一次是真的惹欣欣生气了:-(\u003c/p\u003e\n\u003cp\u003e总结一下在雁栖湖一年的收获，大致有如下图的四个方面：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"mid-year 2016 summary\" loading=\"lazy\" src=\"/posts/2016-08-20-2016-mid-year-summary/mid-year-2016-summary.png\"\u003e\u003c/p\u003e\n\u003cp\u003e看看\u003ca href=\"https://bitjoy.net/posts/2016-01-03-2016-happy-new-year/\"\u003e年初计划\u003c/a\u003e的完成情况：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cdel\u003e完成国科大下学期的课程任务：完成\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e\u003cdel\u003e接手pLink软件：完成\u003c/del\u003e\u003c/li\u003e\n\u003cli\u003e刷完LeetCode所有题目：上半年基本没刷题，下半年一定完成\u003c/li\u003e\n\u003cli\u003e读10本书：目前读了《数学之美》、《大话设计模式》、《我不知道该说什么，关于死亡还是爱情》、《男人来自火星、女人来自金星，卷I》，还差6本，下半年加油！\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影：目前看了《美人鱼》、《北京遇上西雅图之不二情书》、《忍者神龟2：破影而出》，还差好多…\u003c/li\u003e\n\u003cli\u003e改正坐姿：有一段时间刻意改正了，但是这东西貌似改不过来？\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e下半年就进入实验室，开始科研实战了，做交联的师兄师姐都毕业了，留下我一个人，感觉好艰难，希望我能顺利进入角色，协助师兄把文章发了，维护好pLink2的软件，并且开发集群版。\u003c/p\u003e","title":"2016年中总结"},{"content":"我们知道常规的快速排序算法是一个不稳定的算法，也就是两个相等的数排序之后的顺序可能和在原序列中的顺序不同。这是因为当选定一个枢轴（pivot），要把其他数分到小于pivot和大于pivot的两边的时候，不同实现的分法不一样。\n下面我实现了一种稳定版快速排序算法，在Partition函数中保持了原序列中所有元素的相对顺序，只把pivot放到了它的正确位置。具体方法是三遍扫描原序列：1）第一遍先把小于pivot的元素按先后顺序放到tmp里，然后把pivot放到它的正确位置tmp[k]；2）第二遍把大于pivot的元素按先后顺序追加在tmp里，这样除了pivot以前的其他元素，都保持了和原序列中一样的顺序；3）第三遍把tmp赋值回原数组A。\n当排序算法稳定之后，就可以借此统计逆序数了，文件Q5.txt中共包含100000个不同的整数，每行一个数。我们可以使用稳定版快速排序算法对其排序，并统计出其中的逆序数个数。\n具体的Python 3实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Tue Oct 6 00:21:37 2015 @author: bitjoy \u0026#34;\u0026#34;\u0026#34; import time inversions = 0 def Partition(A, p, r): global inversions tmp = [0] * (r-p+1) pivot = A[p] k = 0 for i in range(p+1, r+1): # first if A[i] \u0026lt; pivot: tmp[k] = A[i] inversions = inversions + i – k – p k = k + 1 tmp[k] = pivot ans = k + p k = k + 1 for i in range(p+1, r+1): # second if A[i] \u0026gt; pivot: tmp[k] = A[i] k = k + 1 k = 0 for i in range(p, r+1): # third A[i] = tmp[k] k = k + 1 return ans def QuickSortAndCount(A, p, r): if p \u0026lt; r: q = Partition(A, p, r) QuickSortAndCount(A, p, q-1) QuickSortAndCount(A, q + 1, r) if __name__ == \u0026#34;__main__\u0026#34;: Q5 = open(\u0026#39;Q5.txt\u0026#39;, encoding = \u0026#39;utf-8\u0026#39;) data = [ int(x) for x in Q5 ] Q5.close() start = time.clock() QuickSortAndCount(data, 0, len(data) -1 ) end = time.clock() print(\u0026#34;number of inversions:%d\\ntime:%f s\u0026#34;%(inversions,end-start)) 虽然这种快排的时间复杂度还是O(nlgn)，但是在Partition函数中扫描了3次数组，并且借用了辅助数组tmp，不再是in-place排序算法，所以排序用时会比常规快排或者归并排序要慢。\n","permalink":"http://localhost:1313/posts/2016-08-18-the-stable-quick-sort/","summary":"\u003cp\u003e我们知道常规的快速排序算法是一个不稳定的算法，也就是两个相等的数排序之后的顺序可能和在原序列中的顺序不同。这是因为当选定一个枢轴（pivot），要把其他数分到小于pivot和大于pivot的两边的时候，不同实现的分法不一样。\u003c/p\u003e\n\u003cp\u003e下面我实现了一种稳定版快速排序算法，在Partition函数中保持了原序列中所有元素的相对顺序，只把pivot放到了它的正确位置。具体方法是三遍扫描原序列：1）第一遍先把小于pivot的元素按先后顺序放到tmp里，然后把pivot放到它的正确位置tmp[k]；2）第二遍把大于pivot的元素按先后顺序追加在tmp里，这样除了pivot以前的其他元素，都保持了和原序列中一样的顺序；3）第三遍把tmp赋值回原数组A。\u003c/p\u003e\n\u003cp\u003e当排序算法稳定之后，就可以借此统计逆序数了，文件\u003ca href=\"/posts/2016-08-18-the-stable-quick-sort/Q5.zip\"\u003eQ5.txt\u003c/a\u003e中共包含100000个\u003cstrong\u003e不同\u003c/strong\u003e的整数，每行一个数。我们可以使用稳定版快速排序算法对其排序，并统计出其中的逆序数个数。\u003c/p\u003e\n\u003cp\u003e具体的Python 3实现如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e46\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -*- coding: utf-8 -*-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eCreated on Tue Oct 6 00:21:37 2015\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e@author: bitjoy\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e time\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003einversions \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ePartition\u003c/span\u003e(A, p, r):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eglobal\u003c/span\u003e inversions\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    tmp \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e (r\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003ep\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    pivot \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e A[p]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(p\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, r\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e): \u003cspan style=\"color:#75715e\"\u003e# first\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e A[i] \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e pivot:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            tmp[k] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e A[i]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            inversions \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e inversions \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e k \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e p\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        tmp[k] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pivot\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ans \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e p\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(p\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, r\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e): \u003cspan style=\"color:#75715e\"\u003e# second\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e A[i] \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e pivot:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            tmp[k] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e A[i]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(p, r\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e): \u003cspan style=\"color:#75715e\"\u003e# third\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        A[i] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e tmp[k]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        k \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e k \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e ans\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eQuickSortAndCount\u003c/span\u003e(A, p, r):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e p \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e r:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    q \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Partition(A, p, r)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    QuickSortAndCount(A, p, q\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    QuickSortAndCount(A, q \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, r)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    Q5 \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e open(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Q5.txt\u0026#39;\u003c/span\u003e, encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;utf-8\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    data \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [ int(x) \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e x \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e Q5 ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    Q5\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    start \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e time\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclock()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    QuickSortAndCount(data, \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, len(data) \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    end \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e time\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclock()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;number of inversions:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003etime:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%f\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e s\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(inversions,end\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003estart))\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e虽然这种快排的时间复杂度还是O(nlgn)，但是在Partition函数中扫描了3次数组，并且借用了辅助数组tmp，不再是in-place排序算法，所以排序用时会比常规快排或者归并排序要慢。\u003c/p\u003e","title":"稳定版快速排序算法"},{"content":"哈弗曼编码是一个很经典的压缩算法，压缩率能达到50%，甚至更低。它的基本原理包括四个步骤：\n统计文件中每个字符出现的频率。 构建一个哈弗曼树。建树的过程是不断的合并频率最小的两个节点，父亲节点的频率为两个孩子节点的频率之和。如此循环直到合并成一个根节点。叶子节点为不同的字符及其频率。 生成哈弗曼编码。从树根开始对树进行编码，比如进入左孩子的边标记为0，进入右孩子的边标记为1，这里的0和1都是二进制位。这样之后，每个叶子节点都有一个唯一的二进制编码，这就是哈弗曼编码。频率越低的字符哈弗曼编码越长，频率越高的字符哈弗曼编码越短，这样就能起到压缩的效果。 第二遍扫描文件，把字符转换为对应的哈弗曼编码，保存成压缩文件。 解压缩的过程就是解析二进制位，然后查找哈弗曼树，每找到一个叶子节点，就解析出一个字符，直到解析完所有二进制位。下面详细解释我的C++实现。\n首先定义一个哈弗曼编码类，对外只提供压缩Compress和解压缩Decompress两个接口。值得注意的是有一个Node结构体，用于构成哈弗曼树的节点。此外count_node的key是字符频率，value是所在节点，且是multimap类型的，所以count_node会自动按字符频率有小到大排序，在构建哈弗曼树时，每次只需要取count_node的前两个节点进行合并即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class HuffmanCode { public: HuffmanCode(); void Compress(string src, string dest); void Decompress(string src, string dest); virtual ~HuffmanCode(); private: void CountLetter(string src); void ConstructHuffmanTree(); void GenerateHuffmanCode(); void WriteHuffmanCode(ofstream \u0026amp;os); void Compressing(string src, string dest); void InsertIntoHuffmanTree(char letter, string \u0026amp;code, int \u0026amp;k); void ConstructHuffmanTreeFromFile(ifstream \u0026amp;is); void Decompressing(ifstream \u0026amp;is, ofstream \u0026amp;os); map\u0026lt;char, int\u0026gt; letter_count; typedef struct Node { int id; bool is_leaf; char letter; int parent, lchild, rchild; Node() { } Node(int i, bool il, char lt, int p, int lc, int rc) : id(i), is_leaf(il), letter(lt), parent(p), lchild(lc), rchild(rc) { } }; multimap\u0026lt;int, Node\u0026gt; count_node; vector\u0026lt;Node\u0026gt; huffman_tree; map\u0026lt;char, vector\u0026lt;char\u0026gt;\u0026gt; letter_hcode; // hufman code for each letter }; 压缩函数Compress串起压缩的整个流程，包括统计字符频率、构建哈弗曼树、生成哈弗曼编码以及最后将原始文件转换成哈弗曼编码的二进制文件。\n1 2 3 4 5 6 void HuffmanCode::Compress(string src, string dest) { CountLetter(src); ConstructHuffmanTree(); GenerateHuffmanCode(); Compressing(src, dest); } Compress中的前三个函数不难，值得注意的是Compressing函数，它是真正进行压缩的函数。函数首先调用WriteHuffmanCode把每个字符的哈弗曼编码写入文件，作为文件头信息，以备后续解压使用。然后循环读取文件，把字符转换为哈弗曼二进制编码。每8 bit哈弗曼二进制位构成一个char byte，多个byte构成os_buf，当os_buf满时写入文件。\n在最后边界位置，需要小心处理。因为可能所有二进制位并不刚好是8的整数倍，所以在压缩文件的末尾用 1 byte作为标记。如果flag为0x0，则所有二进制位刚好是8的整数倍，无需特别处理。如果flag为0x01，则还剩小于8个二进制位需要单独放在一个byte里面，所以还需要一个byte存储剩余多少个二进制位。假设最后3个bytes分别为x,y,z，则如果z==0x0，则x,y常规解析；如果z==0x01，则只解析x中的前y个bits。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 void HuffmanCode::Compressing(string src, string dest) { ifstream is(src, ios::binary); ofstream os(dest, ios::binary); WriteHuffmanCode(os); char *is_buf = new char[MAX_LEN], *os_buf = new char[MAX_LEN]; list\u0026lt;char\u0026gt; tmp_hcode; int start_pos = 0, i, j, k, len, t; char c, flag = 0x0; // flag for the last byte list\u0026lt;char\u0026gt;::iterator it; while (is.peek() != EOF) { is.read(is_buf, MAX_LEN); len = is.gcount(); for (i = 0; i \u0026lt; len; i++) tmp_hcode.insert(tmp_hcode.end(), letter_hcode[is_buf[i]].begin(), letter_hcode[is_buf[i]].end()); k = tmp_hcode.size() / 8; t = 0; i = 0; it = tmp_hcode.begin(); while (i \u0026lt; 8 * k) { c = 0x0; for (j = i; j \u0026lt;= i + 7; j++) { c = (*it == \u0026#39;1\u0026#39;) ? (c | (1 \u0026lt;\u0026lt; (i + 7 – j))) : c; // char -\u0026gt; bit it++; } os_buf[t++] = c; i += 8; } os.write(os_buf, t * sizeof(char)); tmp_hcode.erase(tmp_hcode.begin(), it); } c = 0x0; i = 7; bool done = true; while (it != tmp_hcode.end()) { done = false; c = (*it == \u0026#39;1\u0026#39;) ? (c | (1 \u0026lt;\u0026lt; i)) : c; // left bits i–; it++; } if (!done) { os.write(\u0026amp;c, sizeof(char)); c = 7 – i; // only c bits used in the last byte os.write(\u0026amp;c, sizeof(char)); flag = 0x1; // the last byte is incomplete } os.write(\u0026amp;flag, sizeof(char)); is.close(); os.close(); delete[] is_buf; delete[] os_buf; } 函数Decompress串起解压缩的整个流程。首先调用ConstructHuffmanTreeFromFile读取压缩文件的头信息，也就是字符和哈弗曼编码的对应关系，然后构建哈弗曼树。同样Decompressing是实际的解压缩过程，它不断读取哈弗曼二进制位，然后从哈弗曼树根节点开始往下走，直到到达一个叶子节点，则解析出一个字符，如此循环，直到解析完所有二进制位。\n1 2 3 4 5 6 7 8 void HuffmanCode::Decompress(string src, string dest) { ifstream is(src, ios::binary); ofstream os(dest, ios::binary); ConstructHuffmanTreeFromFile(is); Decompressing(is, os); is.close(); os.close(); } 完整项目可以查看我的Github项目HZip，Windows版可执行程序请点此下载。\n压缩命令为：\n1 HZip.exe -c original_file_path compressed_file_path 解压缩命令为：\n1 HZip.exe -x compressed_file_path decompressed_file_path 下面是一些测试结果。\n//还没有统计好。。。\n//看来还是7Z道高一尺。\n我后面发现HZip甚至可以压缩/解压缩中文txt、pdf、图片、视频等（其实只要是ASCII编码的应该都可以吧？）。但是中文压缩效率较低，图片视频等压缩之后的大小几乎和没压缩是一样的:-(其实这很好理解，因为哈弗曼编码是根据字符频率的差异来编码的，英文只有26个字母加上一些符号，压缩效率肯定很高，而中文是以字为单位存储的，所以当以char读取来编码的时候，不同char的数量肯定更多，导致压缩效率较低。图片和视频就不得而知了。\n在测试的时候我发现压缩和解压缩大文件的时候，速度极其的慢，简直到了不能忍的地步，下一步我将分析性能瓶颈，争取把速度提高到可以接受的范围。\n","permalink":"http://localhost:1313/posts/2016-08-18-the-implementation-of-huffman-code/","summary":"\u003cp\u003e哈弗曼编码是一个很经典的压缩算法，压缩率能达到50%，甚至更低。它的基本原理包括四个步骤：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e统计文件中每个字符出现的频率。\u003c/li\u003e\n\u003cli\u003e构建一个哈弗曼树。建树的过程是不断的合并频率最小的两个节点，父亲节点的频率为两个孩子节点的频率之和。如此循环直到合并成一个根节点。叶子节点为不同的字符及其频率。\u003c/li\u003e\n\u003cli\u003e生成哈弗曼编码。从树根开始对树进行编码，比如进入左孩子的边标记为0，进入右孩子的边标记为1，这里的0和1都是二进制位。这样之后，每个叶子节点都有一个唯一的二进制编码，这就是哈弗曼编码。频率越低的字符哈弗曼编码越长，频率越高的字符哈弗曼编码越短，这样就能起到压缩的效果。\u003c/li\u003e\n\u003cli\u003e第二遍扫描文件，把字符转换为对应的哈弗曼编码，保存成压缩文件。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e解压缩的过程就是解析二进制位，然后查找哈弗曼树，每找到一个叶子节点，就解析出一个字符，直到解析完所有二进制位。下面详细解释我的C++实现。\u003c/p\u003e\n\u003cp\u003e首先定义一个哈弗曼编码类，对外只提供压缩Compress和解压缩Decompress两个接口。值得注意的是有一个Node结构体，用于构成哈弗曼树的节点。此外count_node的key是字符频率，value是所在节点，且是multimap类型的，所以count_node会自动按字符频率有小到大排序，在构建哈弗曼树时，每次只需要取count_node的前两个节点进行合并即可。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e37\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eHuffmanCode\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    HuffmanCode();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eCompress\u003c/span\u003e(string src, string dest);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eDecompress\u003c/span\u003e(string src, string dest);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evirtual\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e~\u003c/span\u003eHuffmanCode();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eprivate\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e CountLetter(string src);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eConstructHuffmanTree\u003c/span\u003e();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eGenerateHuffmanCode\u003c/span\u003e();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eWriteHuffmanCode\u003c/span\u003e(ofstream \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003eos);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eCompressing\u003c/span\u003e(string src, string dest);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eInsertIntoHuffmanTree\u003c/span\u003e(\u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e letter, string \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003ecode, \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003ek);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eConstructHuffmanTreeFromFile\u003c/span\u003e(ifstream \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003eis);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eDecompressing\u003c/span\u003e(ifstream \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003eis, ofstream \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003eos);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    map\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e, \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e letter_count;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003etypedef\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estruct\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eNode\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e id;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ebool\u003c/span\u003e is_leaf;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e letter;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e parent, lchild, rchild;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Node() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Node(\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e i, \u003cspan style=\"color:#66d9ef\"\u003ebool\u003c/span\u003e il, \u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e lt, \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e p, \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e lc, \u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e rc) \u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            id(i), is_leaf(il), letter(lt), parent(p), lchild(lc), rchild(rc) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    };\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    multimap\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e, Node\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e count_node;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    vector\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003eNode\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e huffman_tree;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    map\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e, vector\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003echar\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e letter_hcode; \u003cspan style=\"color:#75715e\"\u003e// hufman code for each letter\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e};\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e压缩函数Compress串起压缩的整个流程，包括统计字符频率、构建哈弗曼树、生成哈弗曼编码以及最后将原始文件转换成哈弗曼编码的二进制文件。\u003c/p\u003e","title":"Huffman编码压缩算法及其实现"},{"content":"之前写了七篇博客详细介绍了搜索引擎的工作原理。彼时的搜索引擎主要讲查询和网页的相关性匹配，是动态的、在线的、实时的。相关性匹配有一个问题，网页很容易作弊，比如可以在一个网页中写满诸如“免费”、“美容”之类的垃圾关键词，进而提升查询相关性。但是用户在查询时，一定希望返回的网页比较权威可信，比如同样搜索“苹果电脑”，排名第一的应该是Apple的官网，而不应该是中关村在线之类的第三方网站。\n权威性是一个静态的（或者说变化较慢的）衡量网页重要性的指标。但是应该怎样度量权威性呢，HITS算法使用authority来度量，即指向自身的网页数量越多，则自身的authority值越大。谷歌的PageRank算法是用PageRank值来衡量权威性的。HITS和PageRank一个比较大的区别是HITS和查询有关，而PageRank和查询无关，所以PageRank可以离线计算。下面主要介绍PageRank算法。\nPageRank’s thesis is that a webpage is important if it is pointed to by other important pages.\n我先不加解释的给出PageRank的公式，然后带领大家一步步推导出这个公式。\n$$\\pi^T=\\pi^T(\\alpha S+(1-\\alpha)E)$$我们首先明确目标：PageRank计算的是网页的静态权威度（PR值），也就是如果给定了一个网络结构，则每个网页的PR值就可以通过PageRank算法计算出。假设网页\\(P_i\\)的PR值为\\(r(P_i)\\)，则\\(r(P_i)\\)等于所有指向\\(P_i\\)的网页的PR值之和，即\n$$\\begin{equation}r(P_i)=\\sum\\limits_{P_j\\in B_{P_i}}\\frac{r(P_j)}{|P_j|}\\end{equation}$$其中\\(B_{P_i}\\)为指向\\(P_i\\)的网页集合，\\(|P_j|\\)为\\(P_j\\)的出边的数量。这个式子很好理解，包括两方面内容：1）\\(\\sum\\limits_{P_j\\in B_{P_i}}\\)表示如果指向\\(P_i\\)的网页数量越多，说明网页\\(P_i\\)越重要；2）\\(\\frac{r(P_j)}{|P_j|}\\)表示如果\\(P_j\\)指向的页面数量越少，但有一个指向了\\(P_i\\)，说明网页\\(P_i\\)越重要（如果一个大牛写了很多推荐信（\\(|P_j|\\)大），则这些推荐信的效力就下降了，如果大牛只给你写了推荐信（\\(|P_j|=1\\)），则这封推荐信的效力一定很高）。\n(1)式有一个问题，初始给定一个网络结构时，并不知道\\(r(P_i), r(P_j)\\)，如何计算呢？Brin和Page利用递归的思想求解，初始假设所有网页的PR值相等，都为\\(\\frac{1}{n}\\)，其中\\(n\\)为网络中网页的数量。则第\\(k+1\\)轮的PR计算公式为：\n$$\\begin{equation}r_{k+1}(P_i)=\\sum\\limits_{P_j\\in B_{P_i}}\\frac{r_k(P_j)}{|P_j|}\\end{equation}$$初始对所有网页\\(P_i\\)有\\(r_0(P_i)=\\frac{1}{n}\\)，迭代\\(k\\)步之后，可以计算出所有网页的PR值，然后按PR值从大到小排序，就可以知道每个网页的重要性了。\n对于上图的小网络，我们可以计算出其每一步的PR值：\n可以看到经过2次迭代之后，节点4的PR值最大，从图中也可以看出，节点4的出入边较多，它可能比较重要。\n注意到对于(2)式，当\\(i,j\\)之间有边时，\\(\\frac{1}{|P_j|}\\)相当于对\\(P_j\\)出度的归一化，设矩阵\\(H\\)为图的邻接矩阵的行归一化矩阵，对于上图，为\n设行向量\\(\\pi^{(k)T}\\)为第\\(k\\)轮迭代时所有网页的PR值，则式(2)可以转换为如下的矩阵形式：\n$$\\begin{equation}\\pi^{(k+1)T}=\\pi^{(k)T}H\\end{equation}$$初始有\\(\\pi^{(0)T}=\\frac{1}{n}e^T\\)，\\(e^T\\)为全1的行向量。我们可以从(3)式观测出几点信息：\n(3)式的每一轮计算涉及到向量和矩阵的乘法，复杂度为\\(O(n^2)\\)，\\(n\\)为矩阵\\(H\\)的大小 \\(H\\)是一个稀疏矩阵，因为大部分网页只和很少的网页有链接关系，所以上述向量和矩阵的乘法复杂度还可以降低 \\(H\\)有点像马尔科夫链中的随机转移矩阵，但又不完全是，因为如果有dangling nodes，则这一行就是全0，所以\\(H\\)被称为substochastic matrix 上图中的节点3就是一个dangling node，它只有入边，没有出边，也就是说，每一轮迭代，PR值只会流入3号节点，不会从3号节点流出，久而久之，3就像一个水槽(sink)一样，吸走了大部分的PR，导致PR值虚高。\n所以问题随之而来，怎样保证(3)式一定能够收敛到一个平稳概率分布\\(\\pi^T\\)，\\(\\pi^T\\)和\\(\\pi^{(0)T}\\)有关吗，怎样解决dangling nodes问题，等等。此时需要引入一点马尔科夫链理论的知识。\n在马尔科夫理论呢中，如果一个矩阵\\(P\\)是随机的（stochastic）、不可约的（irreducible）和非周期的（aperiodic），则对于任意的起始向量，都能收敛到一个唯一的平稳正向量。所以如果PageRank矩阵\\(H\\)满足上述三个条件，则可以用幂法（Power Method）找到一个平稳概率分布\\(\\pi^T\\)。幂法是用来计算最大特征值的特征向量。因为\\(H\\)的最大特征值为1，所以可以用幂法找到稳态时（\\(\\pi^T=\\pi^TH\\)）的概率分布\\(\\pi^T\\)。\n下面我们就将矩阵\\(H\\)调整为随机的（stochastic）、不可约的（irreducible）和非周期的（aperiodic）。\n行随机矩阵是指行和为1的非负矩阵。如果图中含有dangling nodes，则\\(H\\)不是随机的，比如上面的例子，第二行为全0。所以第一个调整是对于所有dangling nodes，都加上一个随机跳转向量\\(e^T/n\\)，含义就是如果进入死胡同（dangling nodes），则随机跳转到网络中的任意一个网页。定义向量\\(a\\)：\n$$\\begin{equation}a_i=\\begin{cases}1\\quad\\text{if page}~i\\text{ is a dangling node}\\\\0\\quad\\text{otherwise}\\end{cases}\\end{equation}$$则新的Google矩阵为：\n$$\\begin{equation}S=H+a\\frac{1}{n}e^T\\end{equation}$$新矩阵\\(S\\)就是一个行随机矩阵了。对于上图的例子，有\n为了保证矩阵\\(S\\)满足不可约性（irreducible）和非周期性（aperiodic），必须使\\(S\\)对应的图是强连通的且每个节点有自回路。所以再次调整为：\n$$\\begin{equation}G=\\alpha S+(1-\\alpha)\\frac{1}{n}ee^T\\end{equation}$$令\n$$\\begin{equation}E=\\frac{1}{n}ee^T\\end{equation}$$则得到本博客开头的Google矩阵公式：\n$$\\begin{equation}G=\\alpha S+(1-\\alpha)E\\end{equation}$$\\(E\\)即为随机平均游走矩阵。矩阵\\(G\\)也很好解释，大家上网的时候以\\(\\alpha\\)的概率沿着某个网页里面的链接一步步深入进去（\\(S\\)），当沿着链接走累的时候，以\\(1-\\alpha\\)的概率在地址栏输入一个新地址，随机跳走了（\\(E\\)）。\n此时的矩阵\\(G\\)满足随机性（stochastic）、不可约性（irreducible）和非周期性（aperiodic），所以可以根据幂法（Power Method）找到一个平稳概率分布\\(\\pi^T\\)，\\(\\pi^T_i\\)就衡量了网页\\(P_i\\)的重要性或者权威性。\n此时只剩下参数\\(\\alpha\\)了，\\(\\alpha\\)平衡了网络结构和随机游走。如果\\(\\alpha\\)很小，则\\(1-\\alpha\\)大，\\(G\\)就退化成一个人造随机网络，不能很好的反应真实的网络结构。如果\\(\\alpha\\)很大，则有可能不能得到一个稳态分布，或者幂法会失效。当\\(\\alpha\\approx 1\\)时，幂法失效，且\\(\\pi^T(\\alpha)\\)对\\(H\\)的微小扰动很敏感。Google的选择是\\(\\alpha=0.85\\)。\n将(5)式带入(6)式，得到\n$$\\begin{equation}G=\\alpha H+(\\alpha a+(1-\\alpha)e)\\frac{1}{n}e^T\\end{equation}$$(9)式就非常好计算了，只涉及到向量和矩阵的乘法，而且矩阵\\(H\\)还是稀疏矩阵，复杂度还可以降低。\n幂法（Power Method）求解PageRank稳态分布就是不断计算下面的等式：\n$$\\begin{equation}\\pi^{(k+1)T}=\\pi^{(k)T}G=\\alpha \\pi^{(k)T}H+(\\alpha\\pi^{(k)T}a+1-\\alpha)e^T/n\\end{equation}$$当前后两次的\\(\\pi^{(k+1)T}\\)和\\(\\pi^{(k)T}\\)变化小于某个阈值时，算法收敛，所以算法实现是非常容易的。Brin and Page在他们1998年的论文中提到，只需要50-100次迭代运算就可以收敛了。\n对于上图的例子，令\\(\\alpha=0.9\\)，解得\n利用幂法解得稳态分布为\n所以这6个网页的排名为4\u0026gt;6\u0026gt;5\u0026gt;2\u0026gt;3\u0026gt;1。\n真正的搜索引擎应该综合了网页的静态权威性（如PageRank值）和查询的相关性，每个网站都有一个PR值，具体可以点此查询。\n本博客主要内容参考Google’s PageRank and Beyond: The Science of Search Engine Rankings[1]，插图即为该书封面；如果想快速了解PageRank，可以参考[2]；[3]的讲解也很详细。\nhttp://geza.kzoo.edu/~erdi/patent/langvillebook.pdf http://www.cs.cmu.edu/~elaw/pagerank.pdf http://www.ams.org/samplings/feature-column/fcarc-pagerank ","permalink":"http://localhost:1313/posts/2016-08-04-googles-pagerank-and-beyond/","summary":"\u003cp\u003e之前写了\u003ca href=\"https://bitjoy.net/categories/%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%E6%9E%84%E5%BB%BA%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/\"\u003e七篇博客\u003c/a\u003e详细介绍了搜索引擎的工作原理。彼时的搜索引擎主要讲查询和网页的\u003cstrong\u003e相关性\u003c/strong\u003e匹配，是动态的、在线的、实时的。相关性匹配有一个问题，网页很容易作弊，比如可以在一个网页中写满诸如“免费”、“美容”之类的垃圾关键词，进而提升查询相关性。但是用户在查询时，一定希望返回的网页比较\u003cstrong\u003e权威可信\u003c/strong\u003e，比如同样搜索“苹果电脑”，排名第一的应该是Apple的官网，而不应该是中关村在线之类的第三方网站。\u003c/p\u003e\n\u003cp\u003e权威性是一个静态的（或者说变化较慢的）衡量网页重要性的指标。但是应该怎样度量权威性呢，\u003ca href=\"https://en.wikipedia.org/wiki/HITS_algorithm\"\u003eHITS算法\u003c/a\u003e使用authority来度量，即指向自身的网页数量越多，则自身的authority值越大。谷歌的\u003ca href=\"https://en.wikipedia.org/wiki/PageRank\"\u003ePageRank算法\u003c/a\u003e是用PageRank值来衡量权威性的。\u003ca href=\"http://blog.sina.com.cn/s/blog_72995dcc01013bkb.html\"\u003eHITS和PageRank一个比较大的区别是HITS和查询有关，而PageRank和查询无关，所以PageRank可以离线计算。\u003c/a\u003e下面主要介绍PageRank算法。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/press.princeton.edu/images/k8216.gif\"\u003e\u003c/p\u003e\n\u003cp\u003ePageRank’s thesis is that a webpage is important if it is pointed to by other important pages.\u003c/p\u003e\n\u003cp\u003e我先不加解释的给出PageRank的公式，然后带领大家一步步推导出这个公式。\u003c/p\u003e\n$$\\pi^T=\\pi^T(\\alpha S+(1-\\alpha)E)$$\u003cp\u003e我们首先明确目标：PageRank计算的是网页的静态权威度（PR值），也就是如果给定了一个网络结构，则每个网页的PR值就可以通过PageRank算法计算出。假设网页\\(P_i\\)的PR值为\\(r(P_i)\\)，则\\(r(P_i)\\)等于所有指向\\(P_i\\)的网页的PR值之和，即\u003c/p\u003e\n$$\\begin{equation}r(P_i)=\\sum\\limits_{P_j\\in B_{P_i}}\\frac{r(P_j)}{|P_j|}\\end{equation}$$\u003cp\u003e其中\\(B_{P_i}\\)为指向\\(P_i\\)的网页集合，\\(|P_j|\\)为\\(P_j\\)的出边的数量。这个式子很好理解，包括两方面内容：1）\\(\\sum\\limits_{P_j\\in B_{P_i}}\\)表示如果指向\\(P_i\\)的网页数量越多，说明网页\\(P_i\\)越重要；2）\\(\\frac{r(P_j)}{|P_j|}\\)表示如果\\(P_j\\)指向的页面数量越少，但有一个指向了\\(P_i\\)，说明网页\\(P_i\\)越重要（如果一个大牛写了很多推荐信（\\(|P_j|\\)大），则这些推荐信的效力就下降了，如果大牛只给你写了推荐信（\\(|P_j|=1\\)），则这封推荐信的效力一定很高）。\u003c/p\u003e\n\u003cp\u003e(1)式有一个问题，初始给定一个网络结构时，并不知道\\(r(P_i), r(P_j)\\)，如何计算呢？Brin和Page利用递归的思想求解，初始假设所有网页的PR值相等，都为\\(\\frac{1}{n}\\)，其中\\(n\\)为网络中网页的数量。则第\\(k+1\\)轮的PR计算公式为：\u003c/p\u003e\n$$\\begin{equation}r_{k+1}(P_i)=\\sum\\limits_{P_j\\in B_{P_i}}\\frac{r_k(P_j)}{|P_j|}\\end{equation}$$\u003cp\u003e初始对所有网页\\(P_i\\)有\\(r_0(P_i)=\\frac{1}{n}\\)，迭代\\(k\\)步之后，可以计算出所有网页的PR值，然后按PR值从大到小排序，就可以知道每个网页的重要性了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"pr-1\" loading=\"lazy\" src=\"/posts/2016-08-04-googles-pagerank-and-beyond/pr-1.png\"\u003e\n对于上图的小网络，我们可以计算出其每一步的PR值：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"pr-2\" loading=\"lazy\" src=\"/posts/2016-08-04-googles-pagerank-and-beyond/pr-2.png\"\u003e\n可以看到经过2次迭代之后，节点4的PR值最大，从图中也可以看出，节点4的出入边较多，它可能比较重要。\u003c/p\u003e\n\u003cp\u003e注意到对于(2)式，当\\(i,j\\)之间有边时，\\(\\frac{1}{|P_j|}\\)相当于对\\(P_j\\)出度的归一化，设矩阵\\(H\\)为图的邻接矩阵的行归一化矩阵，对于上图，为\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"pr-3\" loading=\"lazy\" src=\"/posts/2016-08-04-googles-pagerank-and-beyond/pr-3.png\"\u003e\n设行向量\\(\\pi^{(k)T}\\)为第\\(k\\)轮迭代时所有网页的PR值，则式(2)可以转换为如下的矩阵形式：\u003c/p\u003e\n$$\\begin{equation}\\pi^{(k+1)T}=\\pi^{(k)T}H\\end{equation}$$\u003cp\u003e初始有\\(\\pi^{(0)T}=\\frac{1}{n}e^T\\)，\\(e^T\\)为全1的行向量。我们可以从(3)式观测出几点信息：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(3)式的每一轮计算涉及到向量和矩阵的乘法，复杂度为\\(O(n^2)\\)，\\(n\\)为矩阵\\(H\\)的大小\u003c/li\u003e\n\u003cli\u003e\\(H\\)是一个稀疏矩阵，因为大部分网页只和很少的网页有链接关系，所以上述向量和矩阵的乘法复杂度还可以降低\u003c/li\u003e\n\u003cli\u003e\\(H\\)有点像马尔科夫链中的随机转移矩阵，但又不完全是，因为如果有dangling nodes，则这一行就是全0，所以\\(H\\)被称为substochastic matrix\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"pr-4\" loading=\"lazy\" src=\"/posts/2016-08-04-googles-pagerank-and-beyond/pr-4.png\"\u003e\n上图中的节点3就是一个dangling node，它只有入边，没有出边，也就是说，每一轮迭代，PR值只会流入3号节点，不会从3号节点流出，久而久之，3就像一个水槽(sink)一样，吸走了大部分的PR，导致PR值虚高。\u003c/p\u003e\n\u003cp\u003e所以问题随之而来，怎样保证(3)式一定能够收敛到一个平稳概率分布\\(\\pi^T\\)，\\(\\pi^T\\)和\\(\\pi^{(0)T}\\)有关吗，怎样解决dangling nodes问题，等等。此时需要引入一点马尔科夫链理论的知识。\u003c/p\u003e\n\u003cp\u003e在马尔科夫理论呢中，如果一个矩阵\\(P\\)是随机的（stochastic）、不可约的（irreducible）和非周期的（aperiodic），则对于任意的起始向量，都能收敛到一个唯一的平稳正向量。所以如果PageRank矩阵\\(H\\)满足上述三个条件，则可以用幂法（Power Method）找到一个平稳概率分布\\(\\pi^T\\)。\u003ca href=\"https://en.wikipedia.org/wiki/Power_iteration\"\u003e幂法\u003c/a\u003e是用来计算最大特征值的特征向量。因为\\(H\\)的最大特征值为1，所以可以用幂法找到稳态时（\\(\\pi^T=\\pi^TH\\)）的概率分布\\(\\pi^T\\)。\u003c/p\u003e\n\u003cp\u003e下面我们就将矩阵\\(H\\)调整为随机的（stochastic）、不可约的（irreducible）和非周期的（aperiodic）。\u003c/p\u003e\n\u003cp\u003e行随机矩阵是指行和为1的非负矩阵。如果图中含有dangling nodes，则\\(H\\)不是随机的，比如上面的例子，第二行为全0。所以第一个调整是对于所有dangling nodes，都加上一个随机跳转向量\\(e^T/n\\)，含义就是如果进入死胡同（dangling nodes），则随机跳转到网络中的任意一个网页。定义向量\\(a\\)：\u003c/p\u003e\n$$\\begin{equation}a_i=\\begin{cases}1\\quad\\text{if page}~i\\text{ is a dangling node}\\\\0\\quad\\text{otherwise}\\end{cases}\\end{equation}$$\u003cp\u003e则新的Google矩阵为：\u003c/p\u003e\n$$\\begin{equation}S=H+a\\frac{1}{n}e^T\\end{equation}$$\u003cp\u003e新矩阵\\(S\\)就是一个行随机矩阵了。对于上图的例子，有\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"pr-5\" loading=\"lazy\" src=\"/posts/2016-08-04-googles-pagerank-and-beyond/pr-5.png\"\u003e\n为了保证矩阵\\(S\\)满足\u003ca href=\"http://mathworld.wolfram.com/ReducibleMatrix.html\"\u003e不可约性（irreducible）\u003c/a\u003e和\u003ca href=\"https://en.wikipedia.org/wiki/Aperiodic_graph\"\u003e非周期性（aperiodic）\u003c/a\u003e，必须使\\(S\\)对应的图是强连通的且每个节点有自回路。所以再次调整为：\u003c/p\u003e\n$$\\begin{equation}G=\\alpha S+(1-\\alpha)\\frac{1}{n}ee^T\\end{equation}$$\u003cp\u003e令\u003c/p\u003e\n$$\\begin{equation}E=\\frac{1}{n}ee^T\\end{equation}$$\u003cp\u003e则得到本博客开头的Google矩阵公式：\u003c/p\u003e\n$$\\begin{equation}G=\\alpha S+(1-\\alpha)E\\end{equation}$$\u003cp\u003e\\(E\\)即为随机平均游走矩阵。矩阵\\(G\\)也很好解释，大家上网的时候以\\(\\alpha\\)的概率沿着某个网页里面的链接一步步深入进去（\\(S\\)），当沿着链接走累的时候，以\\(1-\\alpha\\)的概率在地址栏输入一个新地址，随机跳走了（\\(E\\)）。\u003c/p\u003e\n\u003cp\u003e此时的矩阵\\(G\\)满足随机性（stochastic）、不可约性（irreducible）和非周期性（aperiodic），所以可以根据幂法（Power Method）找到一个平稳概率分布\\(\\pi^T\\)，\\(\\pi^T_i\\)就衡量了网页\\(P_i\\)的重要性或者权威性。\u003c/p\u003e","title":"还原谷歌PageRank算法真相"},{"content":"$$\\begin{equation}Pr(|\\hat{p}-p|\\geq 5\\%)\\leq 5\\%\\end{equation}$$上一回我们讲到当\\(p\\)本身很小的时候，容易被5%（绝对误差）给淹没掉，导致结果的不可信。我们可以引入相对误差，把(1)式转换为如下的不等式\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq\\delta p)\\leq\\epsilon\\end{equation}$$同理，我们可以用\n$$\\begin{equation}\\hat{p}=\\frac{x_1+x_2+…+x_n}{n}\\end{equation}$$代替\\(\\hat{p}\\)（建议先看上一篇博客），转换为\n$$\\begin{equation}Pr(|X-np|\\geq\\delta np)\\end{equation}$$类似的，\\(X=x_1+x_2+…+x_n\\)，\\(E(X)=\\mu=np\\)，所以(4)式等价为\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\end{equation}$$这个时候，因为不等号右边和均值\\(\\mu\\)有关，不能再用切比雪夫不等式了，我们需要另外一个武器：Chernoff bound。它有两种形式：\n$$\\begin{equation}Pr(X\\geq (1+\\delta)\\mu)\\leq[\\frac{e^\\delta}{(1+\\delta)^{1+\\delta}}]^\\mu\\leq e^{-\\frac{\\mu}{3}\\delta^2}\\quad\\forall\\delta\u003e0\\end{equation}$$$$\\begin{equation}Pr(X\\leq (1-\\delta)\\mu)\\leq[\\frac{e^{-\\delta}}{(1-\\delta)^{1-\\delta}}]^\\mu\\leq e^{-\\frac{\\mu}{2}\\delta^2}\\quad\\forall 0\u003c\\delta\u003c1\\end{equation}$$Chernoff bound的证明需要用到马尔可夫不等式，有一点技巧。以上两种形式可以统一成\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\leq 2e^{-\\frac{\\mu}{3}\\delta^2}\\end{equation}$$也是一个很漂亮的不等式。\n利用Chernoff bound求解(5)式：\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\leq 2e^{-\\frac{\\mu}{3}\\delta^2}\\\\=2e^{-\\frac{np}{3}\\delta^2}\\leq\\epsilon\\end{equation}$$解得\n$$\\begin{equation}n\\geq\\left\\lceil\\frac{3ln\\frac{2}{\\epsilon}}{p\\delta^2}\\right\\rceil\\end{equation}$$这个结果看起来就很复杂了。也就是说，如果要设计调查问卷使满足(2)式的精度，抽样的样本数必须满足(10)式。从(10)式可知，当要求的精度越高（即\\(\\delta\\)和\\(\\epsilon\\)越小），所需的样本数越大。并且结果还和真实值\\(p\\)有关。\n","permalink":"http://localhost:1313/posts/2016-07-23-the-validity-of-the-questionnaire-2/","summary":"$$\\begin{equation}Pr(|\\hat{p}-p|\\geq 5\\%)\\leq 5\\%\\end{equation}$$\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-07-23-the-validity-of-the-questionnaire-1/\"\u003e上一回\u003c/a\u003e我们讲到当\\(p\\)本身很小的时候，容易被5%（绝对误差）给淹没掉，导致结果的不可信。我们可以引入相对误差，把(1)式转换为如下的不等式\u003c/p\u003e\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq\\delta p)\\leq\\epsilon\\end{equation}$$\u003cp\u003e同理，我们可以用\u003c/p\u003e\n$$\\begin{equation}\\hat{p}=\\frac{x_1+x_2+…+x_n}{n}\\end{equation}$$\u003cp\u003e代替\\(\\hat{p}\\)（建议先看\u003ca href=\"https://bitjoy.net/posts/2016-07-23-the-validity-of-the-questionnaire-1/\"\u003e上一篇博客\u003c/a\u003e），转换为\u003c/p\u003e\n$$\\begin{equation}Pr(|X-np|\\geq\\delta np)\\end{equation}$$\u003cp\u003e类似的，\\(X=x_1+x_2+…+x_n\\)，\\(E(X)=\\mu=np\\)，所以(4)式等价为\u003c/p\u003e\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\end{equation}$$\u003cp\u003e这个时候，因为不等号右边和均值\\(\\mu\\)有关，不能再用切比雪夫不等式了，我们需要另外一个武器：\u003ca href=\"https://en.wikipedia.org/wiki/Chernoff_bound\"\u003eChernoff bound\u003c/a\u003e。它有两种形式：\u003c/p\u003e\n$$\\begin{equation}Pr(X\\geq (1+\\delta)\\mu)\\leq[\\frac{e^\\delta}{(1+\\delta)^{1+\\delta}}]^\\mu\\leq e^{-\\frac{\\mu}{3}\\delta^2}\\quad\\forall\\delta\u003e0\\end{equation}$$$$\\begin{equation}Pr(X\\leq (1-\\delta)\\mu)\\leq[\\frac{e^{-\\delta}}{(1-\\delta)^{1-\\delta}}]^\\mu\\leq e^{-\\frac{\\mu}{2}\\delta^2}\\quad\\forall 0\u003c\\delta\u003c1\\end{equation}$$\u003cp\u003eChernoff bound的证明需要用到马尔可夫不等式，有一点技巧。以上两种形式可以统一成\u003c/p\u003e\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\leq 2e^{-\\frac{\\mu}{3}\\delta^2}\\end{equation}$$\u003cp\u003e也是一个很漂亮的不等式。\u003c/p\u003e\n\u003cp\u003e利用Chernoff bound求解(5)式：\u003c/p\u003e\n$$\\begin{equation}Pr(|X-\\mu|\\geq\\delta\\mu)\\leq 2e^{-\\frac{\\mu}{3}\\delta^2}\\\\=2e^{-\\frac{np}{3}\\delta^2}\\leq\\epsilon\\end{equation}$$\u003cp\u003e解得\u003c/p\u003e\n$$\\begin{equation}n\\geq\\left\\lceil\\frac{3ln\\frac{2}{\\epsilon}}{p\\delta^2}\\right\\rceil\\end{equation}$$\u003cp\u003e这个结果看起来就很复杂了。也就是说，如果要设计调查问卷使满足(2)式的精度，抽样的样本数必须满足(10)式。从(10)式可知，当要求的精度越高（即\\(\\delta\\)和\\(\\epsilon\\)越小），所需的样本数越大。并且结果还和真实值\\(p\\)有关。\u003c/p\u003e","title":"调查问卷的有效性（2）相对误差"},{"content":"每年春晚过后，央视又要吹嘘说今年春晚收视率创新高了，但是我们总感觉央视在骗我们，因为我是越长大越不看春晚了[笑cry]，所以收视率到底是怎么统计出来的，央视的说法是否靠谱呢？\n最近的美国大选真是热闹，很多机构都会发放一些调查问卷，然后统计出希拉里或者唐纳德的民众支持率是多少，但是我并没有收到调查问卷，凭什么就得出了民众支持率了，意思是把我排除在民众之外咯？所以引出这样一个问题，调查问卷是否可信，即调查问卷的有效性。\n其实，央视统计收视率并不要问全中国14亿人口有多少人看了春晚，他只需要从14亿人口里面随机抽\\(n\\)个人，问一下这\\(n\\)个人里有多少人看了春晚，然后把看的人数除以总数就大概估计出全国的收视率了。同理调查民众支持率也是一样，只需要随机调查\\(n\\)个人的意向，把支持希拉里的人数除以总数就大概得到了希拉里的支持率。\n但是你要问了，通过抽样调查出来的收视率和支持率靠谱吗，需要随机抽样多少人才能得到一个比较好的全局近似解呢？今天我们就来解决这个问题。\n假设我们随机抽样了\\(n\\)个人，分别是\\(x_1,x_2,…,x_n\\)。如果第\\(i\\)个人看了春晚，则\\(x_i=1\\)，否则\\(x_i=0\\)。那么通过这\\(n\\)个人的收视情况，我们可以估计出一个收视率\n$$\\begin{equation}\\hat{p}=\\frac{x_1+x_2+…+x_n}{n}\\end{equation}$$假设全国的真实收视率是\\(p\\)，那么平均到每一个人，他看了春晚的概率就是\\(p\\)，也即\\(Pr(x_i=1)=p\\)，所以有\n$$\\begin{equation}E(x_i)=p\\quad E(x_i^2)=p\\quad Var(x_i)=p(1-p)\\end{equation}$$我们的目的就是希望通过\\(n\\)个人估计出来的\\(\\hat{p}\\)和\\(p\\)越接近越好。换句话说，我们希望\\(\\hat{p}\\)和\\(p\\)相差大于5%的概率要小于5%。再换句话说就是有至少95%的概率，\\(\\hat{p}\\)和\\(p\\)相差在5%以内，即\\(\\hat{p}\\)和\\(p\\)很接近。注意这里的两个5%都是可以换成任意你想要的精度。用数学语言表示就是，\\(n\\)至少为多少时，以下不等式可以被满足。\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq 5\\%)\\leq 5\\%\\end{equation}$$把(1)式代入(3)式，用\\(\\frac{1}{20}\\)代替5%，得到等价形式：\n$$\\begin{equation}Pr(|(\\frac{x_1+x_2+…+x_n}{n})-p|\\geq\\frac{1}{20})\\\\ \\Longleftrightarrow~Pr(|X-np|\\geq\\frac{n}{20})\\end{equation}$$其中\\(X=x_1+x_2+…+x_n\\)。根据期望的线性可加性，有\n$$\\begin{equation}E(X)=E(x_1+x_2+…+x_n)=E(x_1)+E(x_2)+…+E(x_n)=np\\end{equation}$$所以(4)又等价于\n$$\\begin{equation}Pr(|X-E(X)|\\geq\\frac{n}{20})\\end{equation}$$我们需要利用著名的切比雪夫不等式来求解上式，切比雪夫不等式如下：\n$$\\begin{equation}Pr(|X-E(X)|\\geq~c)\\leq\\frac{Var(X)}{c^2}\\end{equation}$$切比雪夫不等式可以直接由马尔可夫不等式得到，马尔可夫不等式的证明也不难，略过。\n利用切比雪夫不等式求解(6)式\n$$\\begin{equation}Pr(|X-E(X)|\\geq\\frac{n}{20})\\leq\\frac{Var(X)}{n^2}*400\\\\ =\\frac{n*Var(x_i)}{n^2}*400\\\\ =\\frac{p(1-p)}{n}*400\\\\ \\leq\\frac{1/4}{n}*400=\\frac{100}{n} \\end{equation}$$第一个等号是因为\\(n\\)个变量是独立同分布的，所以方差也有类似于(5)式的线性性质。最后一个不等号是因为\\(p(1-p)\\)是一个开口向下的抛物线，在\\(p=1/2\\)时取到极值\\(1/4\\)。\n回到最初的不等式(3)，则(8)式要满足\\(\\frac{100}{n}\\leq 5\\%\\)，解得\\(n\\geq 2000\\)。注意到求出的\\(n\\)和总体人数是无关的，也就是说，虽然全中国有十几亿人口，但是央视只要随机抽样调查2000个人的收视情况，就能以比较高的概率准确估计出全国的收视率。\n这个结论还是很漂亮的，但是这种方法有两个限制条件：\n采样满足独立同分布，即这\\(n\\)个人是独立同分布的，不能针对某一特定人群调查 (3)式的5%是一个绝对误差，当\\(p\\)本身很小的时候，容易被5%淹没 对于第1个问题，稍微好处理一点，抽样的时候尽量随机一点。对于第2个问题，比较好的解决办法是引入相对误差，即把(3)式转换为如下的不等式\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq\\delta p)\\leq\\epsilon\\end{equation}$$(9)式的求解就比较复杂了，得出的结论也没有上面那么简单，具体的求解方法请听下回分解。\n","permalink":"http://localhost:1313/posts/2016-07-23-the-validity-of-the-questionnaire-1/","summary":"\u003cp\u003e每年春晚过后，央视又要吹嘘说今年春晚收视率创新高了，但是我们总感觉央视在骗我们，因为我是越长大越不看春晚了[笑cry]，所以收视率到底是怎么统计出来的，央视的说法是否靠谱呢？\u003c/p\u003e\n\u003cp\u003e最近的美国大选真是热闹，很多机构都会发放一些调查问卷，然后统计出希拉里或者唐纳德的民众支持率是多少，但是我并没有收到调查问卷，凭什么就得出了民众支持率了，意思是把我排除在民众之外咯？所以引出这样一个问题，调查问卷是否可信，即调查问卷的有效性。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/www.carp.ca/wp-content/uploads/2012/08/questionnaire1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e其实，央视统计收视率并不要问全中国14亿人口有多少人看了春晚，他只需要从14亿人口里面随机抽\\(n\\)个人，问一下这\\(n\\)个人里有多少人看了春晚，然后把看的人数除以总数就大概估计出全国的收视率了。同理调查民众支持率也是一样，只需要随机调查\\(n\\)个人的意向，把支持希拉里的人数除以总数就大概得到了希拉里的支持率。\u003c/p\u003e\n\u003cp\u003e但是你要问了，通过抽样调查出来的收视率和支持率靠谱吗，需要随机抽样多少人才能得到一个比较好的全局近似解呢？今天我们就来解决这个问题。\u003c/p\u003e\n\u003cp\u003e假设我们随机抽样了\\(n\\)个人，分别是\\(x_1,x_2,…,x_n\\)。如果第\\(i\\)个人看了春晚，则\\(x_i=1\\)，否则\\(x_i=0\\)。那么通过这\\(n\\)个人的收视情况，我们可以估计出一个收视率\u003c/p\u003e\n$$\\begin{equation}\\hat{p}=\\frac{x_1+x_2+…+x_n}{n}\\end{equation}$$\u003cp\u003e假设全国的真实收视率是\\(p\\)，那么平均到每一个人，他看了春晚的概率就是\\(p\\)，也即\\(Pr(x_i=1)=p\\)，所以有\u003c/p\u003e\n$$\\begin{equation}E(x_i)=p\\quad E(x_i^2)=p\\quad Var(x_i)=p(1-p)\\end{equation}$$\u003cp\u003e我们的目的就是希望通过\\(n\\)个人估计出来的\\(\\hat{p}\\)和\\(p\\)越接近越好。换句话说，我们希望\\(\\hat{p}\\)和\\(p\\)相差大于5%的概率要小于5%。再换句话说就是有至少95%的概率，\\(\\hat{p}\\)和\\(p\\)相差在5%以内，即\\(\\hat{p}\\)和\\(p\\)很接近。注意这里的两个5%都是可以换成任意你想要的精度。用数学语言表示就是，\\(n\\)至少为多少时，以下不等式可以被满足。\u003c/p\u003e\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq 5\\%)\\leq 5\\%\\end{equation}$$\u003cp\u003e把(1)式代入(3)式，用\\(\\frac{1}{20}\\)代替5%，得到等价形式：\u003c/p\u003e\n$$\\begin{equation}Pr(|(\\frac{x_1+x_2+…+x_n}{n})-p|\\geq\\frac{1}{20})\\\\ \\Longleftrightarrow~Pr(|X-np|\\geq\\frac{n}{20})\\end{equation}$$\u003cp\u003e其中\\(X=x_1+x_2+…+x_n\\)。根据期望的线性可加性，有\u003c/p\u003e\n$$\\begin{equation}E(X)=E(x_1+x_2+…+x_n)=E(x_1)+E(x_2)+…+E(x_n)=np\\end{equation}$$\u003cp\u003e所以(4)又等价于\u003c/p\u003e\n$$\\begin{equation}Pr(|X-E(X)|\\geq\\frac{n}{20})\\end{equation}$$\u003cp\u003e我们需要利用著名的\u003ca href=\"https://en.wikipedia.org/wiki/Chebyshev%27s_inequality\"\u003e切比雪夫不等式\u003c/a\u003e来求解上式，切比雪夫不等式如下：\u003c/p\u003e\n$$\\begin{equation}Pr(|X-E(X)|\\geq~c)\\leq\\frac{Var(X)}{c^2}\\end{equation}$$\u003cp\u003e切比雪夫不等式可以直接由\u003ca href=\"https://en.wikipedia.org/wiki/Markov%27s_inequality\"\u003e马尔可夫不等式\u003c/a\u003e得到，马尔可夫不等式的证明也不难，略过。\u003c/p\u003e\n\u003cp\u003e利用切比雪夫不等式求解(6)式\u003c/p\u003e\n$$\\begin{equation}Pr(|X-E(X)|\\geq\\frac{n}{20})\\leq\\frac{Var(X)}{n^2}*400\\\\ =\\frac{n*Var(x_i)}{n^2}*400\\\\ =\\frac{p(1-p)}{n}*400\\\\ \\leq\\frac{1/4}{n}*400=\\frac{100}{n} \\end{equation}$$\u003cp\u003e第一个等号是因为\\(n\\)个变量是独立同分布的，所以方差也有类似于(5)式的线性性质。最后一个不等号是因为\\(p(1-p)\\)是一个开口向下的抛物线，在\\(p=1/2\\)时取到极值\\(1/4\\)。\u003c/p\u003e\n\u003cp\u003e回到最初的不等式(3)，则(8)式要满足\\(\\frac{100}{n}\\leq 5\\%\\)，解得\\(n\\geq 2000\\)。注意到求出的\\(n\\)和总体人数是无关的，也就是说，虽然全中国有十几亿人口，但是央视只要随机抽样调查2000个人的收视情况，就能以比较高的概率准确估计出全国的收视率。\u003c/p\u003e\n\u003cp\u003e这个结论还是很漂亮的，但是这种方法有两个限制条件：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e采样满足独立同分布，即这\\(n\\)个人是独立同分布的，不能针对某一特定人群调查\u003c/li\u003e\n\u003cli\u003e(3)式的5%是一个绝对误差，当\\(p\\)本身很小的时候，容易被5%淹没\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e对于第1个问题，稍微好处理一点，抽样的时候尽量随机一点。对于第2个问题，比较好的解决办法是引入相对误差，即把(3)式转换为如下的不等式\u003c/p\u003e\n$$\\begin{equation}Pr(|\\hat{p}-p|\\geq\\delta p)\\leq\\epsilon\\end{equation}$$\u003cp\u003e(9)式的求解就比较复杂了，得出的结论也没有上面那么简单，具体的求解方法请听下回分解。\u003c/p\u003e","title":"调查问卷的有效性（1）绝对误差"},{"content":"你是否想过如下问题：怎样向色盲证明两只袜子的颜色是不一样的？怎样证明两个图是不同构的？怎样证明一个数是二次非剩余的？\n咋听起来觉得很有意思吧，色盲是区分不了颜色的，怎么能让他相信两只袜子的颜色不一样呢。图同构问题目前既没有被证明属于P，也没有被证明属于NP-Complete。二次非剩余问题也没有被证明属于NP。\n这些听起来很“难”的问题，却可以通过交互式证明进行证明，下面先通过“向色盲证明两只袜子的颜色不同”这个有趣的例子一窥交互式证明的强大。\n向色盲证明两只袜子的颜色不同 P有一只红袜子和黄袜子，她的一个色盲朋友V不相信P的袜子颜色不同，P如何才能让V相信这是真的呢？一个简单的办法如下：\nP把两只袜子给V，V每只手拿了一只袜子 P转过身背对V V抛一枚硬币，如果头面朝上，则保持两只袜子不动，否则交换左右手的袜子 P转过身，V问P是否交换过袜子 如果P回答错误，则V不相信；否则，重复100次实验，如果P都回答正确，则V相信这两只袜子是不同颜色的 如果两只袜子的颜色确实不一样，则P通过区分两只袜子的颜色能正确回答V有没有交换过袜子。但是如果两只袜子颜色一样，则不管V有没有交换过，P都无法分辨这两只袜子，所以只好猜V有没有交换，而猜对的概率只有1/2，重复100次，都猜对的概率只有\\((1/2)^{100}\\)，这是一个非常小的数，可以认为几乎不会发生，即出错的概率极低。\n这就是交互式证明的一个例子，上述证明有三个特点：1）交互过程，整个证明需要P和V进行交互才能完成；2）具有随机性，即V需要抛一枚硬币，来决定是否交换袜子；3）零知识，虽然V最终相信了这两只袜子是不同颜色的，但V还是不知道这两只袜子是什么颜色的。\n下面我们给出交互式证明的形式化定义。\n交互式证明（Interactive Proofs, IP） 令\\(k\\)是\\(N\\rightarrow N\\)的一个函数，我们称语言\\(L\\)属于\\(IP[k]\\)，如果存在一个\\(k(|x|)\\)多项式时间概率图灵机TM \\(V\\)，使得：\n$$ \\begin{equation} \\begin{cases} x\\in L \\Longrightarrow\\exists P\\quad Pr[V \\text{ accepts }x, V(x)=1]\\geq 2/3 \\\\ x\\notin L \\Longrightarrow\\forall P\\quad Pr[V \\text{ accepts }x, V(x)=1]\\leq 1/3 \\end{cases} \\end{equation} $$定义\n$$IP=\\underset{c}{\\bigcup} IP[n^c]$$上述定义的第一条称为“完备性”（Completeness）：如果\\(x\\in L\\)，则存在一个证明者P（prover），使得验证者V（verfier）能以多项式时间接受\\(x\\)，且接受的概率大于2/3；第二条称为“可靠性”（Soundness），如果\\(x\\notin L\\)，则对于所有证明者P，V接受\\(x\\)的概率都不会超过1/3。\n对应到上面的例子，完备性：当两只袜子的颜色确实不同时，V接受的概率为1\u0026gt;2/3；可靠性：当两只袜子的颜色相同时，重复100次实验，V接受的概率只有\\((1/2)^{100}\u003c1/3\\)。\nIP这个复杂性类就是所有IP[k]的并集。在IP中，P的能力是无穷的，但它不一定是诚实的；V能力较弱，只能进行多项式时间的计算。\n下面我们给出另外两个交互式证明的协议。\n图不同构（Graph Non-isomorphism, GNI）的交互式证明 如果图\\(G_1\\)和\\(G_2\\)可以通过对顶点进行恰当标记来将它们转换为同一个图，则称\\(G_1\\)和\\(G_2\\)同构，记为\\(G_1\\cong G_2\\)。换句话说，如果\\(G_1\\)和\\(G_2\\)同构，则在\\(G_1\\)的所有顶点标签上存在一个置换\\(\\pi\\)使得\\(\\pi (G_1)=G_2\\)，其中\\(\\pi (G_1)\\)是将\\(\\pi\\)作用到\\(G_1\\)的各个顶点上之后得到的图。下图就是两个同构图，右边给出了置换\\(\\pi\\)。\n图同构的补集为图不同构（Graph Non-isomorphism, GNI），即判定给定的两个图是否不同构。下面是GNI的一个交互式证明过程。\n给定两个图\\(G_1\\)和\\(G_2\\)，证明者P想要向验证者V证明\\(G_1\\ncong G_2\\)。\nV：随机选一个\\(i\\in \\{1,2\\}\\)，对\\(G_i\\)做一个随机的置换，得到新图\\(H\\)，则有\\(H\\cong G_i\\)，将\\(H\\)发送给P P：发送\\(j\\)给V V：如果\\(i\\neq j\\)，则拒绝；否则重复100次实验，都有\\(i==j\\)，则相信\\(G_1\\ncong G_2\\) 完备性：如果\\(G_1\\ncong G_2\\)，则\\(H\\)只和\\(G_1, G_2\\)中的一个图同构，P因为能力无穷，一定能找出和\\(H\\)同构的图\\(G_j\\)，且满足\\(j==i\\)。\n可靠性：如果\\(G_1\\cong G_2\\)，则\\(H\\)和\\(G_1, G_2\\)都同构，所以P无法区分，只好猜一个\\(j\\)，所以\\(j==i\\)的概率只有1/2，重复100次实验，P都猜对的概率只有\\((1/2)^{100}\u003c1/3\\)。\n零知识：虽然V相信了\\(G_1\\ncong G_2\\)，但V对于P怎样证出来的一无所知。\nP.S.\n有趣的是，关于图同构问题，芝加哥大学的科学家László Babai最近给出了一个伪多项式时间的算法，被称为是计算机理论界近10年最重要的成果。\nNew algorithm cracks graph problem A Quasipolynomial Time Algorithm for Graph Isomorphism: The Details Graph Isomorphism in Quasipolynomial Time 图同构在P/NP问题上重大突破，计算机理论10年最重要成果 二次非剩余（Quadratic non-residuosity, QNR）的交互式证明 如果存在整数\\(b\\)使得\\(a\\equiv b^2(\\mod p)\\)，则称整数\\(a\\)是模\\(p\\)的二次剩余，并称\\(b\\)是\\(a\\)模\\(p\\)的平方根。显然，\\(-b\\)是\\(a\\)模\\(p\\)的另一个平方根，而且\\(a\\)模\\(p\\)不存在其他平方根，因为\\(x^2-a=0\\)在域\\(GF(p)\\)上至多有两个解。\n类似的，如果不存在整数\\(b\\)使得\\(a\\equiv b^2(\\mod p)\\)，则称整数\\(a\\)是模\\(p\\)的二次非剩余（Quadratic non-residuosity, QNR），记为\\(\u003c a, p\u003e\\in QNR\\)。下面是QNR的一个交互式证明过程。\n给定一个素数\\(p\\)和另一个整数\\(a\\)，P要向V证明\\(\u003c a, p\u003e\\in QNR\\)。\nV：取模\\(p\\)的随机数\\(r(\\mod p)\\)和随机数\\(b\\in\\{0,1\\}\\)，如果\\(b=0\\)，发送\\(r^2(\\mod p)\\)给P；否则发送\\(a\\cdot r^2(\\mod p)\\)给P P：发送\\(b’\\)给V V：如果\\(b’\\neq b\\)，则拒绝；否则重复100次实验，都有\\(b’=b\\)，则相信\\(\u003c a, p\u003e\\in QNR\\) 完备性：如果\\(\u003c a, p\u003e\\in QNR\\)，则\\(\u003c a \\cdot r^2,p\u003e\\in QNR\\)，但\\(\u003c r^2, p\u003e\\notin QNR\\)，所以P能区分\\(a\\cdot r^2\\)和\\(r^2\\)，即总能回答正确使得\\(b’=b\\)。\n可靠性：\\(\u003c a, p\u003e\\notin QNR\\)，则\\(\u003c a \\cdot r^2,p\u003e\\notin QNR\\)，且\\(\u003c r^2, p\u003e\\notin QNR\\)，即\\(a\\cdot r^2\\)和\\(r^2\\)都是二次剩余，所以P无法区分，只能瞎猜，正确的概率为1/2，重复100次，都回答正确的概率只有\\((1/2)^{100}\u003c1/3\\)。\n零知识：虽然V相信了\\(\u003c a, p\u003e\\in QNR\\)，但V对于P怎样证出来的一无所知。\n交互式证明的零知识真是有趣，它是密码学中大量研究工作的基础。很多场合都可能会用到零知识证明，比如要向别人证明你有密码，但又不透露密码；要向别人证明你会解某道题，但又不透露解题过程；要让别人相信你知道怎样从甲地到乙地，但又不告诉别人从甲到乙的路……\n交互式证明是这学期选修《高级算法》时接触的，主要参考书目Computational Complexity: A Modern Approach\n","permalink":"http://localhost:1313/posts/2016-07-14-the-interesting-interactive-proofs/","summary":"\u003cp\u003e你是否想过如下问题：怎样向色盲证明两只袜子的颜色是不一样的？怎样证明两个图是不同构的？怎样证明一个数是二次非剩余的？\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/writtenbyrel.com/wp-content/uploads/2014/06/socks.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e咋听起来觉得很有意思吧，色盲是区分不了颜色的，怎么能让他相信两只袜子的颜色不一样呢。图同构问题目前既没有被证明属于P，也没有被证明属于NP-Complete。二次非剩余问题也没有被证明属于NP。\u003c/p\u003e\n\u003cp\u003e这些听起来很“难”的问题，却可以通过交互式证明进行证明，下面先通过“向色盲证明两只袜子的颜色不同”这个有趣的例子一窥交互式证明的强大。\u003c/p\u003e\n\u003ch1 id=\"向色盲证明两只袜子的颜色不同\"\u003e向色盲证明两只袜子的颜色不同\u003c/h1\u003e\n\u003cp\u003eP有一只红袜子和黄袜子，她的一个色盲朋友V不相信P的袜子颜色不同，P如何才能让V相信这是真的呢？一个简单的办法如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eP把两只袜子给V，V每只手拿了一只袜子\u003c/li\u003e\n\u003cli\u003eP转过身背对V\u003c/li\u003e\n\u003cli\u003eV抛一枚硬币，如果头面朝上，则保持两只袜子不动，否则交换左右手的袜子\u003c/li\u003e\n\u003cli\u003eP转过身，V问P是否交换过袜子\u003c/li\u003e\n\u003cli\u003e如果P回答错误，则V不相信；否则，重复100次实验，如果P都回答正确，则V相信这两只袜子是不同颜色的\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e如果两只袜子的颜色确实不一样，则P通过区分两只袜子的颜色能正确回答V有没有交换过袜子。但是如果两只袜子颜色一样，则不管V有没有交换过，P都无法分辨这两只袜子，所以只好猜V有没有交换，而猜对的概率只有1/2，重复100次，都猜对的概率只有\\((1/2)^{100}\\)，这是一个非常小的数，可以认为几乎不会发生，即出错的概率极低。\u003c/p\u003e\n\u003cp\u003e这就是交互式证明的一个例子，上述证明有三个特点：1）交互过程，整个证明需要P和V进行交互才能完成；2）具有随机性，即V需要抛一枚硬币，来决定是否交换袜子；3）零知识，虽然V最终相信了这两只袜子是不同颜色的，但V还是不知道这两只袜子是什么颜色的。\u003c/p\u003e\n\u003cp\u003e下面我们给出交互式证明的形式化定义。\u003c/p\u003e\n\u003ch1 id=\"交互式证明interactive-proofs-ip\"\u003e交互式证明（Interactive Proofs, IP）\u003c/h1\u003e\n\u003cp\u003e令\\(k\\)是\\(N\\rightarrow N\\)的一个函数，我们称语言\\(L\\)属于\\(IP[k]\\)，如果存在一个\\(k(|x|)\\)多项式时间概率图灵机TM \\(V\\)，使得：\u003c/p\u003e\n$$\n\\begin{equation}\n\\begin{cases}\nx\\in L \\Longrightarrow\\exists P\\quad Pr[V \\text{ accepts }x, V(x)=1]\\geq 2/3 \\\\\nx\\notin L \\Longrightarrow\\forall P\\quad Pr[V \\text{ accepts }x, V(x)=1]\\leq 1/3\n\\end{cases}\n\\end{equation}\n$$\u003cp\u003e定义\u003c/p\u003e\n$$IP=\\underset{c}{\\bigcup} IP[n^c]$$\u003cp\u003e上述定义的第一条称为“完备性”（Completeness）：如果\\(x\\in L\\)，则存在一个证明者P（prover），使得验证者V（verfier）能以多项式时间接受\\(x\\)，且接受的概率大于2/3；第二条称为“可靠性”（Soundness），如果\\(x\\notin L\\)，则对于所有证明者P，V接受\\(x\\)的概率都不会超过1/3。\u003c/p\u003e\n\u003cp\u003e对应到上面的例子，完备性：当两只袜子的颜色确实不同时，V接受的概率为1\u0026gt;2/3；可靠性：当两只袜子的颜色相同时，重复100次实验，V接受的概率只有\\((1/2)^{100}\u003c1/3\\)。\u003c/p\u003e\n\u003cp\u003eIP这个复杂性类就是所有IP[k]的并集。在IP中，P的能力是无穷的，但它不一定是诚实的；V能力较弱，只能进行多项式时间的计算。\u003c/p\u003e\n\u003cp\u003e下面我们给出另外两个交互式证明的协议。\u003c/p\u003e\n\u003ch1 id=\"图不同构graph-non-isomorphism-gni的交互式证明\"\u003e图不同构（Graph Non-isomorphism, GNI）的交互式证明\u003c/h1\u003e\n\u003cp\u003e如果图\\(G_1\\)和\\(G_2\\)可以通过对顶点进行恰当标记来将它们转换为同一个图，则称\\(G_1\\)和\\(G_2\\)同构，记为\\(G_1\\cong G_2\\)。换句话说，如果\\(G_1\\)和\\(G_2\\)同构，则在\\(G_1\\)的所有顶点标签上存在一个置换\\(\\pi\\)使得\\(\\pi (G_1)=G_2\\)，其中\\(\\pi (G_1)\\)是将\\(\\pi\\)作用到\\(G_1\\)的各个顶点上之后得到的图。下图就是两个同构图，右边给出了置换\\(\\pi\\)。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"isomorphism graph\" loading=\"lazy\" src=\"/posts/2016-07-14-the-interesting-interactive-proofs/isomorphism-graph.webp\"\u003e\u003c/p\u003e\n\u003cp\u003e图同构的补集为图不同构（Graph Non-isomorphism, GNI），即判定给定的两个图是否不同构。下面是GNI的一个交互式证明过程。\u003c/p\u003e\n\u003cp\u003e给定两个图\\(G_1\\)和\\(G_2\\)，证明者P想要向验证者V证明\\(G_1\\ncong G_2\\)。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eV：随机选一个\\(i\\in \\{1,2\\}\\)，对\\(G_i\\)做一个随机的置换，得到新图\\(H\\)，则有\\(H\\cong G_i\\)，将\\(H\\)发送给P\u003c/li\u003e\n\u003cli\u003eP：发送\\(j\\)给V\u003c/li\u003e\n\u003cli\u003eV：如果\\(i\\neq j\\)，则拒绝；否则重复100次实验，都有\\(i==j\\)，则相信\\(G_1\\ncong G_2\\)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e完备性：如果\\(G_1\\ncong G_2\\)，则\\(H\\)只和\\(G_1, G_2\\)中的一个图同构，P因为能力无穷，一定能找出和\\(H\\)同构的图\\(G_j\\)，且满足\\(j==i\\)。\u003c/p\u003e","title":"有趣的交互式证明"},{"content":"\n\\(\\LaTeX\\)的强大我就不赘述了，这里简单介绍一下怎样在Windows快速配置一个完美好用的\\(\\LaTeX\\)环境。\n我大学刚接触\\(\\LaTeX\\)的时候，使用的是CTeX，CTeX是一个大礼包，整合了编译器编辑器等，但是由于久不更新，很多宏包和语法都变了，而且CTeX附带的WinEdt是商业软件，30天之后需要收费，我又不想用盗版，所以就打算自己配置\\(\\LaTeX\\)环境。\n目前使用的是MiKTeX+Texmaker的完美组合！MiKTeX是\\(\\LaTeX\\)编译器，Texmaker是\\(\\LaTeX\\)编辑器。两者都是开源软件。\nMiKTeX非常棒的地方在于“MiKTeX has the ability to install needed packages automatically (on-the-fly)”，就是说，你用MiKTeX时，不需要担心某个宏包是否存在，你只管用就是了，MiKTeX会在你第一次用到某个宏包时，自动从网上下载，非常方便。正因为这样，MiKTeX的安装包很小，只有175MB。当然，因为是on-the-fly的，所以必须联网使用，而且MiKTeX只有Windows版本。\nMiKTeX自带了一个TeXworks编辑器的，但是这软件用户体验并不好。我以前一直都用WinEdt，很好用，但是它是商业软件，我又不想盗版（说到底是没钱…），所以换了Texmaker。Texmaker可以媲美WinEdt，软件布局合理，各种快捷键用起来也很方便。不过在上手之前要简单配置一下。\n如果是写英文文章，点击“快速构建”左边的箭头（或者F1快捷键），就能一键编译并刷新pdf视图。但是默认的快速构建使用的引擎是PdfLaTeX，如果你是中文用户，使用了xeCJK宏包，则必须使用XeLaTeX引擎编译，所以依次点击“选项-\u0026gt;配置Texmaker-\u0026gt;（左边）快速构建”，选择快速构建命令为”XeLaTeX + View PDF”。\n构建好的PDF默认是以弹窗的形式展现的，我们可以设置让代码和PDF并排显示，这样方便在PDF和源代码之间切换，配置如下：\nTexmaker自带了一个PDF阅读器，当然你也可以使用外部阅读器，比如非常棒的Sumatra PDF，只需填入Sumatra PDF的路径跟上%.pdf，并选中External Viewer。\nTexmaker还有一个很好用的功能是“正向/反向搜索”。反向搜索是点击PDF某个位置，会跳到tex源代码对应位置，快捷方式是ctrl+click。正向搜索是点击tex源代码某个位置，会跳到PDF对应的位置，默认快捷方式ctrl+space，但是这个快捷方式好像用不了，可以自行配置成其他快捷方式，比如ctrl+1，我当时是打开下图的快捷方式窗口才发现这个问题的。\n正反向搜索都可以通过鼠标右键菜单实现，但是快捷键还是更方便的。最重要的一点是，源文件*.tex所在路径不能有中文！！！要不然正反向搜索不能用，这点很重要，我当时郁闷了好久。\n另外还可以配置一下编辑器的字体，勾选”Backup documents every 10 min”之类的。\nOK，大功告成，这种三段式的界面、F1快速构建以及正向/反向搜索，用起来真是太顺手了，Just Enjoy \\(\\LaTeX\\)~\n下面是我常用的\\(\\LaTeX\\)中文模板：\nLaTeXDemo.pdf\nLaTeXDemo.tex\n","permalink":"http://localhost:1313/posts/2016-05-16-an-easy-to-use-latex-toolkit/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://www.ctan.org/lion/ctan_lion_350x350.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\\(\\LaTeX\\)的强大我就不赘述了，这里简单介绍一下怎样在Windows快速配置一个完美好用的\\(\\LaTeX\\)环境。\u003c/p\u003e\n\u003cp\u003e我大学刚接触\\(\\LaTeX\\)的时候，使用的是\u003ca href=\"http://www.ctex.org/HomePage\"\u003eCTeX\u003c/a\u003e，CTeX是一个大礼包，整合了编译器编辑器等，但是由于久不更新，很多宏包和语法都变了，而且CTeX附带的WinEdt是商业软件，30天之后需要收费，我又不想用盗版，所以就打算自己配置\\(\\LaTeX\\)环境。\u003c/p\u003e\n\u003cp\u003e目前使用的是\u003ca href=\"http://miktex.org/\"\u003eMiKTeX\u003c/a\u003e+\u003ca href=\"http://www.xm1math.net/texmaker/\"\u003eTexmaker\u003c/a\u003e的完美组合！MiKTeX是\\(\\LaTeX\\)编译器，Texmaker是\\(\\LaTeX\\)编辑器。两者都是开源软件。\u003c/p\u003e\n\u003cp\u003eMiKTeX非常棒的地方在于“MiKTeX has the ability to install needed packages automatically (on-the-fly)”，就是说，你用MiKTeX时，不需要担心某个宏包是否存在，你只管用就是了，MiKTeX会在你第一次用到某个宏包时，自动从网上下载，非常方便。正因为这样，MiKTeX的安装包很小，只有175MB。当然，因为是on-the-fly的，所以必须联网使用，而且MiKTeX只有Windows版本。\u003c/p\u003e\n\u003cp\u003eMiKTeX自带了一个\u003ca href=\"https://www.tug.org/texworks/\"\u003eTeXworks\u003c/a\u003e编辑器的，但是这软件用户体验并不好。我以前一直都用WinEdt，很好用，但是它是商业软件，我又不想盗版（说到底是没钱…），所以换了Texmaker。Texmaker可以媲美WinEdt，软件布局合理，各种快捷键用起来也很方便。不过在上手之前要简单配置一下。\u003c/p\u003e\n\u003cp\u003e如果是写英文文章，点击“快速构建”左边的箭头（或者F1快捷键），就能一键编译并刷新pdf视图。但是默认的快速构建使用的引擎是PdfLaTeX，如果你是中文用户，使用了xeCJK宏包，则必须使用XeLaTeX引擎编译，所以依次点击“选项-\u0026gt;配置Texmaker-\u0026gt;（左边）快速构建”，选择快速构建命令为”XeLaTeX + View PDF”。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Texmaker-1\" loading=\"lazy\" src=\"/posts/2016-05-16-an-easy-to-use-latex-toolkit/Texmaker-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e构建好的PDF默认是以弹窗的形式展现的，我们可以设置让代码和PDF并排显示，这样方便在PDF和源代码之间切换，配置如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Texmaker-2\" loading=\"lazy\" src=\"/posts/2016-05-16-an-easy-to-use-latex-toolkit/Texmaker-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003eTexmaker自带了一个PDF阅读器，当然你也可以使用外部阅读器，比如非常棒的\u003ca href=\"http://www.sumatrapdfreader.org/free-pdf-reader.html\"\u003eSumatra PDF\u003c/a\u003e，只需填入Sumatra PDF的路径跟上%.pdf，并选中External Viewer。\u003c/p\u003e\n\u003cp\u003eTexmaker还有一个很好用的功能是“正向/反向搜索”。反向搜索是点击PDF某个位置，会跳到tex源代码对应位置，快捷方式是ctrl+click。正向搜索是点击tex源代码某个位置，会跳到PDF对应的位置，默认快捷方式ctrl+space，但是这个快捷方式好像用不了，可以自行配置成其他快捷方式，比如ctrl+1，我当时是打开下图的快捷方式窗口才发现这个问题的。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Texmaker-3\" loading=\"lazy\" src=\"/posts/2016-05-16-an-easy-to-use-latex-toolkit/Texmaker-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e正反向搜索都可以通过鼠标右键菜单实现，但是快捷键还是更方便的。最重要的一点是，源文件*.tex所在路径不能有中文！！！要不然正反向搜索不能用，这点很重要，我当时郁闷了好久。\u003c/p\u003e\n\u003cp\u003e另外还可以配置一下编辑器的字体，勾选”Backup documents every 10 min”之类的。\u003c/p\u003e\n\u003cp\u003eOK，大功告成，这种三段式的界面、F1快速构建以及正向/反向搜索，用起来真是太顺手了，Just Enjoy \\(\\LaTeX\\)~\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/www.xm1math.net/texmaker/texmakertop_big.png\"\u003e\u003c/p\u003e\n\u003cp\u003e下面是我常用的\\(\\LaTeX\\)中文模板：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/2016-05-16-an-easy-to-use-latex-toolkit/LaTeXDemo.pdf\"\u003eLaTeXDemo.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/2016-05-16-an-easy-to-use-latex-toolkit/LaTeXDemo.tex\"\u003eLaTeXDemo.tex\u003c/a\u003e\u003c/p\u003e","title":"LaTeX写作完美解决方案"},{"content":"SVM回顾 支持向量机（SVM）的一大特点是最大化间距（max margin）。对于如上图的二分类问题，虽然有很多线可以将左右两部分分开，但是只有中间的红线效果是最好的，因为它的可活动范围（margin）是最大的，从直观上来说很好理解。\n对于线性二分类问题，假设分类面为\n$$\\begin{equation} u=\\vec w \\cdot \\vec x-b \\end{equation}$$则margin为\n$$\\begin{equation} m=\\frac{1}{||w||_2} \\end{equation}$$根据max margin规则和约束条件，得到如下优化问题，我们要求的就是参数\\(\\vec w\\)和\\(b\\)：\n$$\\begin{equation} \\min\\limits_{\\vec w,b}\\frac{1}{2}||\\vec w||^2 \\quad\\text{subject to}\\quad y_i(\\vec w\\cdot \\vec x_i-b) \\geq 1, \\forall i,\\end{equation}$$对于正样本，类标号\\(y_i\\)为+1，反之则为-1。根据拉格朗日对偶，(3)可以转换为如下的二次规划（QP）问题，其中\\(\\alpha_i\\)为拉格朗日乘子。\n$$\\begin{equation} \\min\\limits_{\\vec \\alpha}\\Psi(\\vec\\alpha)=\\min\\limits_{\\vec \\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^Ny_iy_j(\\vec x_i\\cdot\\vec x_j)\\alpha_i\\alpha_j-\\sum_{i=1}^N\\alpha_i,\\end{equation}$$其中N为样本数量。上式还需满足如下两个约束条件：\n$$\\begin{equation} \\alpha_i\\geq 0, \\forall i,\\end{equation}$$$$\\begin{equation} \\sum_{i=1}^Ny_i\\alpha_i=0.\\end{equation}$$一旦求解出所有的拉格朗日乘子，则我们可以通过如下的公式得到分类面参数\\(\\vec w\\)和\\(b\\)。\n$$\\begin{equation}\\vec w=\\sum_{i=1}^Ny_i\\alpha_i\\vec x_i,\\quad b=\\vec w\\cdot\\vec x_k-y_k\\quad\\text{for some}\\quad\\alpha_k\u003e0.\\end{equation}$$当然并不是所有的数据都可以完美的线性划分，可能有少量数据就是混在对方阵营，这时可以通过引入松弛变量\\(\\xi_i\\)得到软间隔形式的SVM：\n$$\\begin{equation} \\min\\limits_{\\vec w,b,\\vec\\xi}\\frac{1}{2}||\\vec w||^2+C\\sum_{i=1}^N\\xi_i \\quad\\text{subject to}\\quad y_i(\\vec w\\cdot \\vec x_i-b) \\geq 1-\\xi_i, \\forall i,\\end{equation}$$其中的\\(\\xi_i\\)为松弛变量，能假装把错的样本分对，\\(C\\)对max margin和margin failures的trades off。对于这个新的优化问题，约束变成了一个box constraint：\n$$\\begin{equation}0\\leq\\alpha_i \\leq C,\\forall i.\\end{equation}$$而松弛变量\\(\\xi_i\\)不再出现在对偶公式中了。\n对于线性不可分的数据，可以用核函数\\(K\\)将其投影到高维空间，这样就可分了，由此得到一般的分类面公式：\n$$\\begin{equation}u=\\sum_{j=1}^Ny_j\\alpha_jK(\\vec x_j,\\vec x)-b,\\end{equation}$$终极优化问题就变成了下面这个样子：\n$$\\begin{equation} \\min\\limits_{\\vec \\alpha}\\Psi(\\vec\\alpha)=\\min\\limits_{\\vec \\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^Ny_iy_jK(\\vec x_i,\\vec x_j)\\alpha_i\\alpha_j-\\sum_{i=1}^N\\alpha_i,\\\\0\\leq\\alpha_i \\leq C,\\forall i,\\\\ \\sum_{i=1}^Ny_i\\alpha_i=0.\\end{equation}$$要满足的KKT条件为：\n$$ \\begin{equation} \\begin{cases} \\alpha_i = 0 \\Leftrightarrow y_iu_i \\geq 1,\\\\ 0 \u003c \\alpha_i \u003c C \\Leftrightarrow y_iu_i = 1,\\\\ \\alpha_i = C \\Leftrightarrow y_iu_i \\leq 1.\\\\ \\end{cases} \\end{equation} $$SMO算法 为了求解式(11)，SMO算法通过启发式方法选择两个\\(\\alpha_i,\\alpha_j\\)当变量，固定其他\\(\\alpha_k\\)，然后用解析的方法求解两个变量的二次规划问题。关于这两个变量的解应该更接近原始的二次规划问题，更重要的是，这时子问题可以通过解析方法求解，避免了矩阵运算，大大提高了整个算法的计算速度。如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。\n不失一般性，假设选择的两个拉格朗日乘子是\\(\\alpha_1,\\alpha_2\\)，因为\\(\\alpha_i\\)和变量\\(x_i\\)是一一对应的，所以也可以说选择了变量\\(x_1,x_2\\)。固定其他变量\\(\\alpha_i (i=3,4,…,N)\\)\n此时，式(11)的优化问题转换为如下的优化问题：\n$$\\begin{equation} \\min\\limits_{\\alpha_1,\\alpha_2}W(\\alpha_1,\\alpha_2)=\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2\\\\-(\\alpha_1+\\alpha_2)+y_1\\alpha_1\\sum_{i=3}^Ny_i\\alpha_iK_{i1}+y_2\\alpha_2\\sum_{i=3}^Ny_i\\alpha_iK_{i2}\\end{equation}$$满足\n$$\\begin{equation}\\alpha_1y_1+\\alpha_2y_2=-\\sum_{i=3}^Ny_i\\alpha_i=\\zeta\\end{equation}$$因为只有\\(\\alpha_1,\\alpha_2\\)是变量，其他\\(\\alpha_i (i=3,4,…,N)\\)是固定的（可以认为是常数），所以式(13)省略了不含\\(\\alpha_1,\\alpha_2\\)的常数项。又因为\\(y_i=\\pm 1\\)，\\(\\alpha_i\\)有限制(9)，所以\\(\\alpha_1,\\alpha_2\\)被限制在\\([0,C]\\times[0,C]\\)的盒子里，且位于对角线上，如图Figure 1.\n由于\\(\\alpha_2^{new}\\)需满足(9)，所以最优值\\(\\alpha_2^{new}\\)的取值范围必须满足条件\n$$\\begin{equation}L\\leq\\alpha_2^{new}\\leq H\\end{equation}$$其中L与H是\\(\\alpha_2^{new}\\)所在的对角线段端点的界。如果\\(y_1\\neq y_2\\)（Figure 1.左图），则\n$$\\begin{equation}L=max(0,\\alpha_2^{old}-\\alpha_1^{old}),\\quad H=min(C,C+\\alpha_2^{old}-\\alpha_1^{old}).\\end{equation}$$如果\\(y_1= y_2\\)（Figure 1.右图），则\n$$\\begin{equation}L=max(0,\\alpha_2^{old}+\\alpha_1^{old}-C),\\quad H=min(C,\\alpha_2^{old}+\\alpha_1^{old}).\\end{equation}$$令\n$$\\begin{equation}\\eta=K(\\vec x_1,\\vec x_1)+K(\\vec x_2,\\vec x_2)-2K(\\vec x_1,\\vec x_2).\\end{equation}$$因为\\(y_i=\\pm 1\\)，所以\\(y_i^2=1\\)，式(14)同乘以\\(y_1\\)，用\\(\\alpha_2\\)表示\\(\\alpha_1\\)并代入式(13)，得到只含一个变量\\(\\alpha_2\\)的表达式，该表达式对\\(\\alpha_2\\)求偏导并等于0，求出\\(\\alpha_2\\)的更新公式为：\n$$\\begin{equation}\\alpha_2^{new}=\\alpha_2^{old}+\\frac{y_2(E_1-E_2)}{\\eta},\\end{equation}$$其中\\(E_i=u_i-y_i\\)是第\\(i\\)个样本的训练误差。经过截断之后的\\(\\alpha_2^{new}\\)为：\n$$ \\begin{equation} \\alpha_2^{new, clipped} = \\begin{cases} H, \u0026 \\mbox{if }\\alpha_2^{new}\\geq H;\\\\ \\alpha_2^{new}, \u0026 \\mbox{if }L \u003c \\alpha_2^{new} \u003c H;\\\\ L, \u0026 \\mbox{if }\\alpha_2^{new}\\leq L. \\end{cases} \\end{equation} $$将\\(\\alpha_2^{new,clipped}\\)的结果代入式(14)，得到\\(\\alpha_1\\)的更新公式：\n$$\\begin{equation}\\alpha_1^{new}=\\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new,clipped}).\\end{equation}$$当\\(0 \u003c \\alpha_1^{new} \u003c C\\)时，由式(12)中间的条件可以得到阈值\\(b_1\\)的更新公式：\n$$\\begin{equation}b_1^{new}=b^{old}-E_1-y_1K_{11}(\\alpha_1^{new}-\\alpha_1^{old})-y_2K_{21}(\\alpha_2^{new,clipped}-\\alpha_2^{old}),\\end{equation}$$同理，当\\(0 \u003c \\alpha_2^{new,clipped} \u003c C\\)时，得到阈值\\(b_2\\)的更新公式：\n$$\\begin{equation}b_2^{new}=b^{old}-E_2-y_1K_{12}(\\alpha_1^{new}-\\alpha_1^{old})-y_2K_{22}(\\alpha_2^{new,clipped}-\\alpha_2^{old}).\\end{equation}$$综合以上，阈值\\(b\\)的更新公式为：\n$$ \\begin{equation} b^{new}= \\begin{cases} b_1^{new}, \u0026 \\mbox{if } 0 \u003c \\alpha_1^{new} \u003c C; \\\\ b_2^{new}, \u0026 \\mbox{if } 0 \u003c \\alpha_2^{new,clipped} \u003c C;\\\\ (b_1^{new}+b_2^{new})/2, \u0026 \\mbox{otherwise}. \\end{cases} \\end{equation} $$至此，如果我们选定了两个变量\\(\\alpha_2,\\alpha_1\\)，则可以通过公式(19)和(21)来更新它们，并通过公式(24)更新阈值\\(b\\)，通过多次迭代逼近最优结果。但是怎样选择\\(\\alpha_2,\\alpha_1\\)呢，SMO通过启发式方法选择！\n对于第一个样本\\(\\alpha_2\\)，我们选择违反KKT条件式(12)（最严重）的样本\\(\\alpha_2\\)；由式(19)可知，\\(\\alpha_2^{new}\\)是依赖于\\(|E_1-E_2|\\)的，为了加快计算，一种简单的做法是选择\\(\\alpha_1\\)，使其对应的\\(|E_1-E_2|\\)最大。\n至此，我们有了启发式选择两个变量\\(\\alpha_2,\\alpha_1\\)的方法，以及求解这两个变量二次规划的解析方法，下面介绍SMO算法的具体实现。\nMATLAB代码实现 算法伪代码如下：\n输入：训练数据\\(T=\\{(\\vec x_1,y_1),(\\vec x_2,y_2),…,(\\vec x_N,y_N)\\}\\)，其中\\(\\vec x_i \\in R^n\\)，\\(y_i\\in\\{-1,+1\\}\\)，\\(i=1,2,…,N\\)，精度为\\(\\epsilon\\)。\n输出：拉格朗日乘子\\(\\vec\\alpha=\\{\\alpha_1,\\alpha_2,…,\\alpha_N\\}\\)和阈值\\(b\\)的近似解。\n初始化\\(\\vec \\alpha=\\vec 0, b=0\\) 选一个违反KKT条件的变量\\(\\alpha_2\\) 选一个使得\\(|E_1-E_2|\\)最大的变量\\(\\alpha_1\\) 根据公式(19)~(21)更新变量\\(\\alpha_2,\\alpha_1\\) 根据公式(22)~(24)更新阈值\\(b\\) 如果不满足结束条件，则转2；否则算法结束 算法的结束条件是：如果所有变量都已经检查过，且没有变量可以进一步优化时，算法结束。\n下面是SMO算法的MATLAB简化实现，省略了论文[1]的公式(19)。代码主流程参考论文[1]中的伪代码，部分实现细节参考[2]。\n本代码和MATLAB自带的seqminopt.m算法接口相同，可直接替换使用。下载代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 function [alphas, offset] = bitjoy_seqminopt(data, targetLabels, boxConstraints, ... kernelFunc, smoOptions) % https://bitjoy.net/posts/2016-05-02-svm-smo-algorithm/ % 参考文献: % [1] A Fast Algorithm for Training Support Vector Machines, % http://research.microsoft.com/pubs/69644/tr-98-14.pdf % [2] CSDN, http://blog.csdn.net/techq/article/details/6171688 N = length(data); % 样本数量 alphas = zeros([N,1]); % 所有拉格朗日乘子初始化为0 offset = 0.0; % 偏移量b也初始化为0 numChanged = 0; % 拉格朗日乘子改变的个数 examineAll = 1; % 是否需要检查所有拉格朗日乘子 % 主流程 while numChanged \u0026gt; 0 || examineAll numChanged = 0; if examineAll == 1 for i = 1 : N numChanged = numChanged + examineExample(i); end else for i = 1 : N if alphas(i) ~= 0 \u0026amp;\u0026amp; alphas(i) ~= boxConstraints(i) numChanged = numChanged + examineExample(i); end end end if examineAll == 1 examineAll = 0; elseif numChanged == 0 examineAll = 1; end end % 计算当前参数下第idx个样本的函数输出 function [svm_o] = wxpb(idx) svm_o = 0.0; for j = 1 : N svm_o = svm_o + alphas(j) * targetLabels(j) * kernelFunc(data(j,:),data(idx,:)); end svm_o = svm_o + offset; end % 选定第二个变量i2后，根据max|E1-E2|的启发式规则，选择i1； % 如果没有满足条件的i1，返回-1. function [i1] = selectSecondChoice(i2, E2) i1 = -1; maxDelta = -1; for j = 1 : N if j ~= i2 \u0026amp;\u0026amp; alphas(j) ~= 0 \u0026amp;\u0026amp; alphas(j) ~= boxConstraints(j) Ej = wxpb(j) - targetLabels(j); if abs(E2 - Ej) \u0026gt; maxDelta i1 = j; maxDelta = abs(E2 - Ej); end end end end % 根据选定的两个变量i1,i2，代入更新公式计算； % 最后还更新了偏移量offset，也就是y=wx+b中的b。 function [flag] = takeStep(i1, i2) alpha1 = alphas(i1); y1 = targetLabels(i1); E1 = wxpb(i1) - y1; alpha2 = alphas(i2); y2 = targetLabels(i2); E2 = wxpb(i2) - y2; s = y1 * y2; if y1 ~= y2 L = max(0, alpha2 - alpha1); H = min(boxConstraints(i2), boxConstraints(i1) + alpha2 - alpha1); else L = max(0, alpha2 + alpha1 - boxConstraints(i1)); H = min(boxConstraints(i2), alpha2 + alpha1); end if L == H flag = 0; return; end k11 = kernelFunc(data(i1,:),data(i1,:)); k12 = kernelFunc(data(i1,:),data(i2,:)); k22 = kernelFunc(data(i2,:),data(i2,:)); eta = k11 + k22 - 2 * k12; if eta \u0026gt; 0 a2 = alpha2 + y2 * (E1 - E2) / eta; %截断 if a2 \u0026lt; L a2 = L; elseif a2 \u0026gt; H a2 = H; end else % 并未处理该case，论文[1]公式(19)有更详细的方法 flag = 0; return; end if abs(a2 - alpha2) \u0026lt; eps * (a2 + alpha2 + eps) flag = 0; return; end a1 = alpha1 + s * (alpha2 - a2); alphas(i1) = a1; alphas(i2) = a2; % 更新offset b1 = offset - E1 - y1 * k11 * (a1 - alpha1) - y2 * k12 * (a2 - alpha2); b2 = offset - E2 - y1 * k12 * (a1 - alpha1) - y2 * k22 * (a2 - alpha2); if a1 \u0026gt; 0 \u0026amp;\u0026amp; a1 \u0026lt; boxConstraints(i1) offset = b1; elseif a2 \u0026gt; 0 \u0026amp;\u0026amp; a2 \u0026lt; boxConstraints(i2) offset = b2; else offset = (b1 + b2) / 2; end flag = 1; end % 检查样本。 % 首先判断i2是否满足KKT条件，如果不满足， % 则根据启发式规则再选择i1样本， % 然后更新i1和i2的拉格朗日乘子。 function [flag] = examineExample(i2) y2 = targetLabels(i2); alpha2 = alphas(i2); E2 = wxpb(i2) - y2; r2 = E2 * y2; if (r2 \u0026lt; -smoOptions.TolKKT \u0026amp;\u0026amp; alpha2 \u0026lt; boxConstraints(i2)) || (r2 \u0026gt; smoOptions.TolKKT \u0026amp;\u0026amp; alpha2 \u0026gt; 0) i1 = selectSecondChoice(i2, E2); if i1 == -1 i1 = floor(1 + rand() * N); % 随机选一个i1 while i1 == i2 i1 = floor(1 + rand() * N); end flag = takeStep(i1,i2); else flag = takeStep(i1,i2); end else flag = 0; end end end 参考资料\n[1]. J.C. Platt: A Fast Algorithm for Training Support Vector Machines\n[2]. CSDN, techq, SVM算法实现（一）\n[3]. 国科大叶齐祥老师机器学习方法与应用课程资料\n[4]. 支持向量机（SVM）的详细推导过程及注解（一）\n","permalink":"http://localhost:1313/posts/2016-05-02-svm-smo-algorithm/","summary":"\u003ch1 id=\"svm回顾\"\u003eSVM回顾\u003c/h1\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/20/SVM_Example_of_Hyperplanes.png\"\u003e\u003c/p\u003e\n\u003cp\u003e支持向量机（SVM）的一大特点是最大化间距（max margin）。对于如上图的二分类问题，虽然有很多线可以将左右两部分分开，但是只有中间的红线效果是最好的，因为它的可活动范围（margin）是最大的，从直观上来说很好理解。\u003c/p\u003e\n\u003cp\u003e对于线性二分类问题，假设分类面为\u003c/p\u003e\n$$\\begin{equation} u=\\vec w \\cdot \\vec x-b \\end{equation}$$\u003cp\u003e则margin为\u003c/p\u003e\n$$\\begin{equation} m=\\frac{1}{||w||_2} \\end{equation}$$\u003cp\u003e根据max margin规则和约束条件，得到如下优化问题，我们要求的就是参数\\(\\vec w\\)和\\(b\\)：\u003c/p\u003e\n$$\\begin{equation} \\min\\limits_{\\vec w,b}\\frac{1}{2}||\\vec w||^2 \\quad\\text{subject to}\\quad y_i(\\vec w\\cdot \\vec x_i-b) \\geq 1, \\forall i,\\end{equation}$$\u003cp\u003e对于正样本，类标号\\(y_i\\)为+1，反之则为-1。根据拉格朗日对偶，(3)可以转换为如下的二次规划（QP）问题，其中\\(\\alpha_i\\)为拉格朗日乘子。\u003c/p\u003e\n$$\\begin{equation} \\min\\limits_{\\vec \\alpha}\\Psi(\\vec\\alpha)=\\min\\limits_{\\vec \\alpha}\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^Ny_iy_j(\\vec x_i\\cdot\\vec x_j)\\alpha_i\\alpha_j-\\sum_{i=1}^N\\alpha_i,\\end{equation}$$\u003cp\u003e其中N为样本数量。上式还需满足如下两个约束条件：\u003c/p\u003e\n$$\\begin{equation} \\alpha_i\\geq 0, \\forall i,\\end{equation}$$$$\\begin{equation} \\sum_{i=1}^Ny_i\\alpha_i=0.\\end{equation}$$\u003cp\u003e一旦求解出所有的拉格朗日乘子，则我们可以通过如下的公式得到分类面参数\\(\\vec w\\)和\\(b\\)。\u003c/p\u003e\n$$\\begin{equation}\\vec w=\\sum_{i=1}^Ny_i\\alpha_i\\vec x_i,\\quad b=\\vec w\\cdot\\vec x_k-y_k\\quad\\text{for some}\\quad\\alpha_k\u003e0.\\end{equation}$$\u003cp\u003e当然并不是所有的数据都可以完美的线性划分，可能有少量数据就是混在对方阵营，这时可以通过引入松弛变量\\(\\xi_i\\)得到软间隔形式的SVM：\u003c/p\u003e\n$$\\begin{equation} \\min\\limits_{\\vec w,b,\\vec\\xi}\\frac{1}{2}||\\vec w||^2+C\\sum_{i=1}^N\\xi_i \\quad\\text{subject to}\\quad y_i(\\vec w\\cdot \\vec x_i-b) \\geq 1-\\xi_i, \\forall i,\\end{equation}$$\u003cp\u003e其中的\\(\\xi_i\\)为松弛变量，能假装把错的样本分对，\\(C\\)对max margin和margin failures的trades off。对于这个新的优化问题，约束变成了一个box constraint：\u003c/p\u003e\n$$\\begin{equation}0\\leq\\alpha_i \\leq C,\\forall i.\\end{equation}$$\u003cp\u003e而松弛变量\\(\\xi_i\\)不再出现在对偶公式中了。\u003c/p\u003e","title":"SVM之序列最小最优化算法（SMO算法）"},{"content":"\nyn说最近在备考GMAT和托福，把手机都清理了只为专心学习。xx说TCP/IP大作业要用Qt做一个网络监控的软件，问我Qt好不好学。\nGRE遇见Qt，会擦出怎样的火花呢~没错，我用Qt写了一个强化背诵GRE单词的软件——Cracking 3000\n当时的一本GRE单词书有3000个单词，用杨鹏17天刷过之后，很多都记不住，于是想有没有办法把记不住的单词筛选出来，集中力量强化记忆。网上已经有3000的Excel表，所以我很自然的想到了把表格导入软件，用软件快速测试，并把不认识的单词筛选到新的Excel表格中。这样就可以把不认识的单词表打印出来，记完之后再导入软件进行新一轮的测试筛选，直到不认识的单词数为零。\n有了软件需求，代码实现起来就很快了。由于当时用Qt库比较多，所以直接拿来用了。软件实现这一块，主要是Excel表格的导入和导出，需要查一些文档，其他的就很简单了。\n虽然和GRE纠缠了一个多月，最终却没有考，但是想起当初早上6点爬起来背单词，晚上回宿舍抹黑写代码的情景，心情还是有一点小小的激动！以后类似的体验估计不会太多吧。\n最后祝yn GT考试顺利，xx大作业圆满完成！\nCracking_3000软件安装包及说明\n","permalink":"http://localhost:1313/posts/2016-03-16-hello-gre-do-you-like-me/","summary":"\u003cp\u003e\u003cimg alt=\"GRE3000\" loading=\"lazy\" src=\"/posts/2016-03-16-hello-gre-do-you-like-me/GRE3000.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eyn说最近在备考GMAT和托福，把手机都清理了只为专心学习。xx说TCP/IP大作业要用Qt做一个网络监控的软件，问我Qt好不好学。\u003c/p\u003e\n\u003cp\u003eGRE遇见Qt，会擦出怎样的火花呢~没错，我用Qt写了一个强化背诵GRE单词的软件——Cracking 3000\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Cracking-3000-1\" loading=\"lazy\" src=\"/posts/2016-03-16-hello-gre-do-you-like-me/Cracking-3000-1.png\"\u003e \u003cimg alt=\"Cracking-3000-2\" loading=\"lazy\" src=\"/posts/2016-03-16-hello-gre-do-you-like-me/Cracking-3000-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e当时的一本GRE单词书有3000个单词，用杨鹏17天刷过之后，很多都记不住，于是想有没有办法把记不住的单词筛选出来，集中力量强化记忆。网上已经有3000的Excel表，所以我很自然的想到了把表格导入软件，用软件快速测试，并把不认识的单词筛选到新的Excel表格中。这样就可以把不认识的单词表打印出来，记完之后再导入软件进行新一轮的测试筛选，直到不认识的单词数为零。\u003c/p\u003e\n\u003cp\u003e有了软件需求，代码实现起来就很快了。由于当时用Qt库比较多，所以直接拿来用了。软件实现这一块，主要是Excel表格的导入和导出，需要查一些文档，其他的就很简单了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"GRE3000-2\" loading=\"lazy\" src=\"/posts/2016-03-16-hello-gre-do-you-like-me/GRE3000-2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e虽然和GRE纠缠了一个多月，最终却没有考，但是想起当初早上6点爬起来背单词，晚上回宿舍抹黑写代码的情景，心情还是有一点小小的激动！以后类似的体验估计不会太多吧。\u003c/p\u003e\n\u003cp\u003e最后祝yn GT考试顺利，xx大作业圆满完成！\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/posts/2016-03-16-hello-gre-do-you-like-me/Cracking_3000.zip\"\u003eCracking_3000软件安装包及说明\u003c/a\u003e\u003c/p\u003e","title":"和GRE纠缠的岁月"},{"content":"刚坐上去北京的列车，就收到了妈妈的微信语音：霖，早上收拾东西怎么忘了带上我给你洗好的鞋呀。我这才想起早上妈妈把洗好的鞋和叠好的衣服放在我房间，我却忘了带鞋。\n后来和爸妈在群里聊了起来。当我问爸爸什么时候返回学校时，他却说前天突然请假回家惹老板不高兴了，可能要被炒鱿鱼。是，老爸在那个学校当老师十几年了，我平时老数落他当老师工资那么低，为什么不改行，可突然听到这个消息，心里却不是滋味。\n其实老爸没必要请假回来的。前几天我发脾气，老爸好像真的决定转行搞种植业了，托我在淘宝买了好多枸杞树，自己带回了五十棵脐橙树苗，还准备去某个地方考察什么药材。\n离家前一天，妈妈特地跑到县城买了好多排骨回来，还煮了十个土鸡蛋要我带着路上吃。老爸买了好多苹果、香蕉、猕猴桃要我带着路上吃。今天早上收拾行李的时候，从来不动手的爸爸，也抢着往我包里塞各种牛奶和水果。\n二十多年了，经历了多少次的离家，从来没有像今天这样的不舍。二十多年了，我突然发现爸爸妈妈变矮了，爸爸的额头黑得发亮，妈妈的眼角也长出了好多鱼尾纹。\n今年难得有一个月的寒假，但我整天忙着看论文、书和电视剧，和爸妈的交流反而少了。有天吃过晚饭，发现妈妈独自坐在客厅戳着她的手机。我问过才发现原来妈妈想看哥哥和他女朋友的照片，但是怎么都弄不出来，我帮妈妈找出来之后，还教她怎么用微信和qq，妈妈说wifi图标像降落伞，我说你什么时候想上网就把降落伞打开，我说你如果想和哥哥聊天，就按住底部的按钮，等到出现小喇叭之后就可以说话了，说完放开手，听到“嗖“”的一声，说的话就发过去了，但是妈妈经常忘记打开降落伞，经常忘记按小喇叭。。。\n刚刷QQ空间的时候，看到一个同学的说说“马上又要去坐火车回武汉了，在家的时间越来越少了，没能好好陪陪父母，我不是称职的儿子。”\n坐在火车上，看着窗外闪过的霓虹灯，突然觉得这个世界好陌生好无情，每个人在时间面前是多么的渺小。\n2016年2月26日于z68列车上。\n","permalink":"http://localhost:1313/posts/2016-02-26-leave-home-again/","summary":"\u003cp\u003e刚坐上去北京的列车，就收到了妈妈的微信语音：霖，早上收拾东西怎么忘了带上我给你洗好的鞋呀。我这才想起早上妈妈把洗好的鞋和叠好的衣服放在我房间，我却忘了带鞋。\u003c/p\u003e\n\u003cp\u003e后来和爸妈在群里聊了起来。当我问爸爸什么时候返回学校时，他却说前天突然请假回家惹老板不高兴了，可能要被炒鱿鱼。是，老爸在那个学校当老师十几年了，我平时老数落他当老师工资那么低，为什么不改行，可突然听到这个消息，心里却不是滋味。\u003c/p\u003e\n\u003cp\u003e其实老爸没必要请假回来的。前几天我发脾气，老爸好像真的决定转行搞种植业了，托我在淘宝买了好多枸杞树，自己带回了五十棵脐橙树苗，还准备去某个地方考察什么药材。\u003c/p\u003e\n\u003cp\u003e离家前一天，妈妈特地跑到县城买了好多排骨回来，还煮了十个土鸡蛋要我带着路上吃。老爸买了好多苹果、香蕉、猕猴桃要我带着路上吃。今天早上收拾行李的时候，从来不动手的爸爸，也抢着往我包里塞各种牛奶和水果。\u003c/p\u003e\n\u003cp\u003e二十多年了，经历了多少次的离家，从来没有像今天这样的不舍。二十多年了，我突然发现爸爸妈妈变矮了，爸爸的额头黑得发亮，妈妈的眼角也长出了好多鱼尾纹。\u003c/p\u003e\n\u003cp\u003e今年难得有一个月的寒假，但我整天忙着看论文、书和电视剧，和爸妈的交流反而少了。有天吃过晚饭，发现妈妈独自坐在客厅戳着她的手机。我问过才发现原来妈妈想看哥哥和他女朋友的照片，但是怎么都弄不出来，我帮妈妈找出来之后，还教她怎么用微信和qq，妈妈说wifi图标像降落伞，我说你什么时候想上网就把降落伞打开，我说你如果想和哥哥聊天，就按住底部的按钮，等到出现小喇叭之后就可以说话了，说完放开手，听到“嗖“”的一声，说的话就发过去了，但是妈妈经常忘记打开降落伞，经常忘记按小喇叭。。。\u003c/p\u003e\n\u003cp\u003e刚刷QQ空间的时候，看到一个同学的说说“马上又要去坐火车回武汉了，在家的时间越来越少了，没能好好陪陪父母，我不是称职的儿子。”\u003c/p\u003e\n\u003cp\u003e坐在火车上，看着窗外闪过的霓虹灯，突然觉得这个世界好陌生好无情，每个人在时间面前是多么的渺小。\u003c/p\u003e\n\u003cp\u003e2016年2月26日于z68列车上。\u003c/p\u003e","title":"爸妈老了"},{"content":"现在是2016年2月4日，距离农历新年不到4天，结束了半年的国科大研一生活，躺在被窝里，松了一口气……\n来国科大之前，在贴吧上了解到国科大雁栖湖校区地处偏远农村，周边几乎没有娱乐场所；但同时学校的软硬件设施非常的棒：豪华单人间，研究员甚至院士亲自授课等等。所以对国科大雁栖湖校区满是憧憬。至今还清楚的记得坐校车从玉泉路过来时，沿途看到APEC主会场的鸟蛋、国科大桥、钟楼以及国科大正门几个大字时的激动心情~\n入住国科大，着实被UCAS的蓝天白云、青山绿水给迷住了。\n当然，凡事有利必有弊，因为这里远离市区，环境好，但正因为远离市区，几乎没有年轻人的娱乐活动，想要看个电影唱个歌少说也得跑城里，再要想感受下帝都奢靡的生活，必须各种倒车近2个小时到市里。\n研一这上学期，半年只进市里两次，一次是买山地车，一次是回所里开会。购物主要靠天猫超市。\n九十月份，大家都和大一新生似的，各种疯玩，野长城、雁栖湖、慕田峪、青龙峡、密云水库。进入十一月，新鲜劲过去了，又开始各种赶大小作业，复习考试。\n这是我这学期的课表，看着课好像不多，每天都有半天休息，但是真的感觉回到了大三呀！尤其数据挖掘、信息检索、矩阵论一周上两次课，当天上完的课如果没有及时复习，隔一天再学新内容完全跟不上啊，而且矩阵论每次课都有好多作业啊，这数学课不做练习完全消化不了呀。更神的课还要数周五的卜神算法，君不见，每到周四晚上，西A、西B两栋宿舍，灯火通明，大家都在熬夜赶算法作业啊，不熬个两三点都不好意思和别人说你熬夜了呀。大家可以感受一下我整理的卜神算法作业~~\n正是因为这奇葩的课程安排，这半年几乎没有12点前睡过觉，估计平均是1:30才睡觉，早上8点多才起，中午也没午休。想想大学的时候按时作息，真是惭愧。期间有一次听说搜狐一同届华科毕业生猝死，朋友圈传得沸沸扬扬，大伙都吓得要命，纷纷表示绝不熬夜，早睡早起，我那天也是吓坏了，决定早睡，11:30就爬床上了，但是不知道是因为紧张还是熬夜习惯了，辗转反侧，到12点多才睡着的。\n我们研一在国科大上课是有补助的，但是在帝都完全不够用啊，而且CS相关的几个所补助都比ICT高，so当时还公车上书，各种写联名信、起义，经过半年之久的持久战，所里终于答应从2016年开始给我们涨500块钱的工资。涨了之后差不多够吃饭了。\n虽然这半年课业繁重，但是也抽空锻炼了身体，天气不是很冷的时候，隔一天就会去夜跑；而且选了乒乓球课，从直拍转为了横拍，并且在课上结识了路路，打球好厉害的一个女生，每次老师来指导的时候，都叫路路温柔点 O__O “…\n另外花了一千多块钱买了一辆二手山地车，骑着到处转悠了一下。很有缘的是，认识了一位才女。本来我们骑行社一块去美利达准备买车，但是由于种种原因我和小欣都没买，然后我们一块坐小黑车回村，在车上聊着聊着就认识，没想到后来还成为了好朋友，小欣的台球和乒乓球都打得不错，琴棋书画样样精通。突然发现身边的学霸才子佳人好多，更加深刻感受到有些东西不是你努力就能够弥补的，天赋、眼界、才艺、品味、性格……\n来国科大的这半年，自我感觉变化最大的是自己变得爱说话了，而且带着一种zhuang bi气息，不知道是不是受某几个我一直崇拜的人的影响。有时候静下心来想想都不敢相信之前的话是我说的，和大学时的我完全判若两人。当然这种事情有利也有弊，还在慢慢找平衡点，可能正如CL说的“话怎么说是一回事，内心要知道自己想要的”。\n这半年突然害怕一个人上学，一个人吃饭，一个人自习了，更喜欢face-to-face的交谈，少了对网络的依赖，不知道是不是因为性格的变化、环境的变化、抑或是认识的人多了，有了念想。\n半年时光，认识了不多不少几个好朋友：良辰、发文章、牛牛、欣儿、路路，有你们真好，谢谢你们~\n2016猴赛雷，即将从研一的上课转入课题组工作，很关键的一年，加油！\n","permalink":"http://localhost:1313/posts/2016-02-04-half-year-experience-report-in-ucas/","summary":"\u003cp\u003e现在是2016年2月4日，距离农历新年不到4天，结束了半年的国科大研一生活，躺在被窝里，松了一口气……\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ucas-2015-1\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e来国科大之前，在贴吧上了解到国科大雁栖湖校区地处偏远农村，周边几乎没有娱乐场所；但同时学校的软硬件设施非常的棒：豪华单人间，研究员甚至院士亲自授课等等。所以对国科大雁栖湖校区满是憧憬。至今还清楚的记得坐校车从玉泉路过来时，沿途看到APEC主会场的鸟蛋、国科大桥、钟楼以及国科大正门几个大字时的激动心情~\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ucas-2015-7\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-7.jpg\"\u003e \u003cimg alt=\"ucas-2015-3\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-3.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e入住国科大，着实被UCAS的蓝天白云、青山绿水给迷住了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ucas-2015-2\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-2.jpg\"\u003e \u003cimg alt=\"ucas-2015-4\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-4.jpg\"\u003e \u003cimg alt=\"ucas-2015-5\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-5.jpg\"\u003e \u003cimg alt=\"ucas-2015-6\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-2015-6.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e当然，凡事有利必有弊，因为这里远离市区，环境好，但正因为远离市区，几乎没有年轻人的娱乐活动，想要看个电影唱个歌少说也得跑城里，再要想感受下帝都奢靡的生活，必须各种倒车近2个小时到市里。\u003c/p\u003e\n\u003cp\u003e研一这上学期，半年只进市里两次，一次是买山地车，一次是回所里开会。购物主要靠天猫超市。\u003c/p\u003e\n\u003cp\u003e九十月份，大家都和大一新生似的，各种疯玩，野长城、雁栖湖、慕田峪、青龙峡、密云水库。进入十一月，新鲜劲过去了，又开始各种赶大小作业，复习考试。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ucas-schedule-2015-fall\" loading=\"lazy\" src=\"/posts/2016-02-04-half-year-experience-report-in-ucas/ucas-schedule-2015-fall.png\"\u003e\u003c/p\u003e\n\u003cp\u003e这是我这学期的课表，看着课好像不多，每天都有半天休息，但是真的感觉回到了大三呀！尤其数据挖掘、信息检索、矩阵论一周上两次课，当天上完的课如果没有及时复习，隔一天再学新内容完全跟不上啊，而且矩阵论每次课都有好多作业啊，这数学课不做练习完全消化不了呀。更神的课还要数周五的卜神算法，君不见，每到周四晚上，西A、西B两栋宿舍，灯火通明，大家都在熬夜赶算法作业啊，不熬个两三点都不好意思和别人说你熬夜了呀。\u003ca href=\"https://bitjoy.net/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/\"\u003e大家可以感受一下我整理的卜神算法作业~~\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e正是因为这奇葩的课程安排，这半年几乎没有12点前睡过觉，估计平均是1:30才睡觉，早上8点多才起，中午也没午休。想想大学的时候按时作息，真是惭愧。期间有一次听说搜狐一同届华科毕业生猝死，朋友圈传得沸沸扬扬，大伙都吓得要命，纷纷表示绝不熬夜，早睡早起，我那天也是吓坏了，决定早睡，11:30就爬床上了，但是不知道是因为紧张还是熬夜习惯了，辗转反侧，到12点多才睡着的。\u003c/p\u003e\n\u003cp\u003e我们研一在国科大上课是有补助的，但是在帝都完全不够用啊，而且CS相关的几个所补助都比ICT高，so当时还公车上书，各种写联名信、起义，经过半年之久的持久战，所里终于答应从2016年开始给我们涨500块钱的工资。涨了之后差不多够吃饭了。\u003c/p\u003e\n\u003cp\u003e虽然这半年课业繁重，但是也抽空锻炼了身体，天气不是很冷的时候，隔一天就会去夜跑；而且选了乒乓球课，从直拍转为了横拍，并且在课上结识了路路，打球好厉害的一个女生，每次老师来指导的时候，都叫路路温柔点 O__O “…\u003c/p\u003e\n\u003cp\u003e另外花了一千多块钱买了一辆二手山地车，骑着到处转悠了一下。很有缘的是，认识了一位才女。本来我们骑行社一块去美利达准备买车，但是由于种种原因我和小欣都没买，然后我们一块坐小黑车回村，在车上聊着聊着就认识，没想到后来还成为了好朋友，小欣的台球和乒乓球都打得不错，琴棋书画样样精通。突然发现身边的学霸才子佳人好多，更加深刻感受到有些东西不是你努力就能够弥补的，天赋、眼界、才艺、品味、性格……\u003c/p\u003e\n\u003cp\u003e来国科大的这半年，自我感觉变化最大的是自己变得爱说话了，而且带着一种zhuang bi气息，不知道是不是受某几个我一直崇拜的人的影响。有时候静下心来想想都不敢相信之前的话是我说的，和大学时的我完全判若两人。当然这种事情有利也有弊，还在慢慢找平衡点，可能正如CL说的“话怎么说是一回事，内心要知道自己想要的”。\u003c/p\u003e\n\u003cp\u003e这半年突然害怕一个人上学，一个人吃饭，一个人自习了，更喜欢face-to-face的交谈，少了对网络的依赖，不知道是不是因为性格的变化、环境的变化、抑或是认识的人多了，有了念想。\u003c/p\u003e\n\u003cp\u003e半年时光，认识了不多不少几个好朋友：良辰、发文章、牛牛、欣儿、路路，有你们真好，谢谢你们~\u003c/p\u003e\n\u003cp\u003e2016猴赛雷，即将从研一的上课转入课题组工作，很关键的一年，加油！\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/image.thepaper.cn/www/image/4/753/520.jpg\"\u003e\u003c/p\u003e","title":"国科大半年体验报告"},{"content":"这学期选修了卜老师的算法课，都说这课是神课，上过之后果然是神课。同样是算法课，别人12月底就考完了，我们要1月底才考试。\n本课程主要讲了以下几个专题：\nDivide-and-conquer Dynamic programming Greedy Linear programming Linear programming: duality Network flow Problem hardness: Polynomial-time reduction NP-Completeness Approximation algorithm 前三个专题的算法大多数本科时学过的，但是经卜老师讲一遍还会有新的收获。后六个专题接触较少，学到了很多新算法。\n下图是卜老师每节课必讲的问题求解思路图：\n（待我回家把图画出来…）\n本课程最神的要数课后作业了，一般deadline是周五，每到周四晚上，大家都做好熬通宵赶作业的准备，没熬到两三点都不好意思睡觉，我同学有一次甚至熬到了第二天六点！\n每次作业大概有10题，前7题是算法设计，后3题是算法实现，每题都不是省油的灯，不过如果把每道题都理解消化，算法及编程能力会有很大的提高。\n下面是我整理出来的算法题目和个人解答，大家感受一下。（仅供完成作业之后交流使用，拒绝抄袭！）\nAssignment1_DandC.zip A1sol.pdf | A1sol.tex A1sol_supplement.pdf | A1sol_supplement.tex_.zip Assignment2_DP.zip A2sol.pdf | A2sol.tex_.zip A2sol_supplement.pdf | A2sol_supplement.tex Assignment3_Greedy.zip A3sol.pdf | A3sol.tex_.zip A3sol_supplement.pdf | A3sol_supplement.tex Assignment4_LP.zip A4sol.pdf | A4sol.tex A4sol_supplement.pdf | A4sol_supplement.tex Assignment5_NF.zip A5sol.pdf | A5sol.tex A5sol_supplement.pdf | A5sol_supplement.tex Assignment6_NP.pdf A6sol.pdf | A6sol.tex Assignment7_App.pdf A7sol.pdf | A7sol.tex ","permalink":"http://localhost:1313/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/","summary":"\u003cp\u003e这学期选修了\u003ca href=\"http://bioinfo.ict.ac.cn/~dbu/AlgorithmCourses/CS711008Z/CS711008Z_2015.html\"\u003e卜老师的算法课\u003c/a\u003e，都说这课是神课，上过之后果然是神课。同样是算法课，别人12月底就考完了，我们要1月底才考试。\u003c/p\u003e\n\u003cp\u003e本课程主要讲了以下几个专题：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDivide-and-conquer\u003c/li\u003e\n\u003cli\u003eDynamic programming\u003c/li\u003e\n\u003cli\u003eGreedy\u003c/li\u003e\n\u003cli\u003eLinear programming\u003c/li\u003e\n\u003cli\u003eLinear programming: duality\u003c/li\u003e\n\u003cli\u003eNetwork flow\u003c/li\u003e\n\u003cli\u003eProblem hardness: Polynomial-time reduction\u003c/li\u003e\n\u003cli\u003eNP-Completeness\u003c/li\u003e\n\u003cli\u003eApproximation algorithm\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e前三个专题的算法大多数本科时学过的，但是经卜老师讲一遍还会有新的收获。后六个专题接触较少，学到了很多新算法。\u003c/p\u003e\n\u003cp\u003e下图是卜老师每节课必讲的问题求解思路图：\u003c/p\u003e\n\u003cp\u003e（待我回家把图画出来…）\u003c/p\u003e\n\u003cp\u003e本课程最神的要数课后作业了，一般deadline是周五，每到周四晚上，大家都做好熬通宵赶作业的准备，没熬到两三点都不好意思睡觉，我同学有一次甚至熬到了第二天六点！\u003c/p\u003e\n\u003cp\u003e每次作业大概有10题，前7题是算法设计，后3题是算法实现，每题都不是省油的灯，不过如果把每道题都理解消化，算法及编程能力会有很大的提高。\u003c/p\u003e\n\u003cp\u003e下面是我整理出来的算法题目和个人解答，大家感受一下。（\u003cstrong\u003e仅供完成作业之后交流使用，拒绝抄袭！\u003c/strong\u003e）\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment1_DandC.zip\"\u003eAssignment1_DandC.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A1sol.pdf\"\u003eA1sol.pdf\u003c/a\u003e   |  \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A1sol.tex\"\u003eA1sol.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A1sol_supplement.pdf\"\u003eA1sol_supplement.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A1sol_supplement.tex_.zip\"\u003eA1sol_supplement.tex_.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment2_DP.zip\"\u003eAssignment2_DP.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A2sol.pdf\"\u003eA2sol.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A2sol.tex_.zip\"\u003eA2sol.tex_.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A2sol_supplement.pdf\"\u003eA2sol_supplement.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A2sol_supplement.tex\"\u003eA2sol_supplement.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment3_Greedy.zip\"\u003eAssignment3_Greedy.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A3sol.pdf\"\u003eA3sol.pdf\u003c/a\u003e  |  \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A3sol.tex_.zip\"\u003eA3sol.tex_.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A3sol_supplement.pdf\"\u003eA3sol_supplement.pdf\u003c/a\u003e  |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A3sol_supplement.tex\"\u003eA3sol_supplement.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment4_LP.zip\"\u003eAssignment4_LP.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A4sol.pdf\"\u003eA4sol.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A4sol.tex\"\u003eA4sol.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A4sol_supplement.pdf\"\u003eA4sol_supplement.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A4sol_supplement.tex\"\u003eA4sol_supplement.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment5_NF.zip\"\u003eAssignment5_NF.zip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A5sol.pdf\"\u003eA5sol.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A5sol.tex\"\u003eA5sol.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A5sol_supplement.pdf\"\u003eA5sol_supplement.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A5sol_supplement.tex\"\u003eA5sol_supplement.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment6_NP.pdf\"\u003eAssignment6_NP.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A6sol.pdf\"\u003eA6sol.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A6sol.tex\"\u003eA6sol.tex\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/Assignment7_App.pdf\"\u003eAssignment7_App.pdf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A7sol.pdf\"\u003eA7sol.pdf\u003c/a\u003e   |   \u003ca href=\"/posts/2016-01-29-algorithm-design-and-analysis-by-dbu/A7sol.tex\"\u003eA7sol.tex\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"卜神算法作业整理"},{"content":"至此，整个新闻搜索引擎构建完毕，总体效果令人满意，不过还是有很多可以改进的地方。下面总结一下本系统的优点和不足。\n优点\n倒排索引存储方式。因为不同词项的倒排记录表长度一般不同，所以没办法以常规的方式存入关系型数据库。通过将一个词项的倒排记录表序列化成一个字符串再存入数据库，读取的时候通过反序列化获得相关数据，整个结构类似于邻接表的形式。\n推荐阅读实现方式。利用特征提取的方法，用25个关键词表示一篇新闻，大大减小了文档词项矩阵规模，提高计算效率的同时不影响推荐新闻相关性。\n借用了Reddit的热度公式，融合了时间因素。\n不足\n构建索引时，为了降低索引规模，提高算法速度，我们将纯数字词项过滤了，同时忽略了词项大小写。虽然索引规模下降了，但是牺牲了搜索引擎的正确率。\n构建索引时，采用了jieba的精确分词模式，比如句子“我来到北京清华大学”的分词结果为“我/ 来到/ 北京/ 清华大学”，“清华大学”作为一个整体被当作一个词项，如果搜索关键词是“清华”，则该句子不能匹配，但显然这个句子和“清华”相关。所以后续可以采用结巴的搜索引擎分词模式，虽然索引规模增加了，但能提升搜索引擎的召回率。\n在推荐阅读模块，虽然进行了维度约减，但是当数据量较大时（数十万条新闻），得到的文档词项矩阵也是巨大的，会远远超过现有PC的内存大小。所以可以先对新闻进行粗略的聚类，再在类内计算两两cosine相似度，得到值得推荐的新闻。\n在热度公式中，虽然借用了Reddit的公式，大的方向是正确的，但是引入了参数\\(k_1\\)和\\(k_2\\)，而且将其简单的设置为1。如果能够由专家给出或者经过机器学习训练得到，则热度公式的效果会更好。\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-09-introduction-to-building-a-search-engine-7/","summary":"\u003cp\u003e至此，整个新闻搜索引擎构建完毕，总体效果令人满意，不过还是有很多可以改进的地方。下面总结一下本系统的优点和不足。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e优点\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e倒排索引存储方式。因为不同词项的倒排记录表长度一般不同，所以没办法以常规的方式存入关系型数据库。通过将一个词项的倒排记录表序列化成一个字符串再存入数据库，读取的时候通过反序列化获得相关数据，整个结构类似于邻接表的形式。\u003c/p\u003e\n\u003cp\u003e推荐阅读实现方式。利用特征提取的方法，用25个关键词表示一篇新闻，大大减小了文档词项矩阵规模，提高计算效率的同时不影响推荐新闻相关性。\u003c/p\u003e\n\u003cp\u003e借用了Reddit的热度公式，融合了时间因素。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e不足\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e构建索引时，为了降低索引规模，提高算法速度，我们将纯数字词项过滤了，同时忽略了词项大小写。虽然索引规模下降了，但是牺牲了搜索引擎的正确率。\u003c/p\u003e\n\u003cp\u003e构建索引时，采用了jieba的精确分词模式，比如句子“我来到北京清华大学”的分词结果为“我/ 来到/ 北京/ 清华大学”，“清华大学”作为一个整体被当作一个词项，如果搜索关键词是“清华”，则该句子不能匹配，但显然这个句子和“清华”相关。所以后续可以采用结巴的搜索引擎分词模式，虽然索引规模增加了，但能提升搜索引擎的召回率。\u003c/p\u003e\n\u003cp\u003e在推荐阅读模块，虽然进行了维度约减，但是当数据量较大时（数十万条新闻），得到的文档词项矩阵也是巨大的，会远远超过现有PC的内存大小。所以可以先对新闻进行粗略的聚类，再在类内计算两两cosine相似度，得到值得推荐的新闻。\u003c/p\u003e\n\u003cp\u003e在热度公式中，虽然借用了Reddit的公式，大的方向是正确的，但是引入了参数\\(k_1\\)和\\(k_2\\)，而且将其简单的设置为1。如果能够由专家给出或者经过机器学习训练得到，则热度公式的效果会更好。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e完整可运行的新闻搜索引擎Demo请看我的Github项目\u003ca href=\"https://github.com/01joy/news_search_engine\"\u003enews_search_engine\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e以下是系列博客：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-1/\"\u003e和我一起构建搜索引擎（一）简介\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-2/\"\u003e和我一起构建搜索引擎（二）网络爬虫\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-3/\"\u003e和我一起构建搜索引擎（三）构建索引\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-4/\"\u003e和我一起构建搜索引擎（四）检索模型\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-5/\"\u003e和我一起构建搜索引擎（五）推荐阅读\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-6/\"\u003e和我一起构建搜索引擎（六）系统展示\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-7/\"\u003e和我一起构建搜索引擎（七）总结展望\u003c/a\u003e\u003c/p\u003e","title":"和我一起构建搜索引擎（七）总结展望"},{"content":"前几个博客已经介绍完搜索引擎的所有功能，为了实现更好的用户体验，需要一个web界面。这一部分是另一个队员做的，我这里借用他的代码。\n我们利用开源的Flask Web框架搭建了展示系统，搜索引擎只需要两个界面，一个是搜索界面，另一个是展示详细新闻的页面（实际搜索引擎没有这个页面）。编写好这两个模板页面并调用前面给出的接口，得到数据，展示出来就可以。\n这一部分没有太多需要讲解的算法，直接上效果图（点击图片可以查看大图）。\n图1. 搜索页面\n图2. 新闻详情页面\n由于数据量不大，只有1000条新闻，所以第一页中后面几个结果相关度就不是很高了。但是经过测试，在大数据量的情况下，不论是搜索的速度、准确率、召回率以及推荐阅读的相关度，都达到了不错的效果。\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-09-introduction-to-building-a-search-engine-6/","summary":"\u003cp\u003e前几个博客已经介绍完搜索引擎的所有功能，为了实现更好的用户体验，需要一个web界面。这一部分是另一个队员做的，我这里借用他的代码。\u003c/p\u003e\n\u003cp\u003e我们利用开源的\u003ca href=\"http://flask.pocoo.org/\"\u003eFlask Web框架\u003c/a\u003e搭建了展示系统，搜索引擎只需要两个界面，一个是搜索界面，另一个是展示详细新闻的页面（实际搜索引擎没有这个页面）。编写好这两个模板页面并调用前面给出的接口，得到数据，展示出来就可以。\u003c/p\u003e\n\u003cp\u003e这一部分没有太多需要讲解的算法，直接上效果图（点击图片可以查看大图）。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图1. 搜索页面\" loading=\"lazy\" src=\"/posts/2016-01-09-introduction-to-building-a-search-engine-6/News-Search-Engine1.webp\"\u003e\n图1. 搜索页面\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图2. 新闻详情页面\" loading=\"lazy\" src=\"/posts/2016-01-09-introduction-to-building-a-search-engine-6/News-Search-Engine2.webp\"\u003e\n图2. 新闻详情页面\u003c/p\u003e\n\u003cp\u003e由于数据量不大，只有1000条新闻，所以第一页中后面几个结果相关度就不是很高了。但是经过测试，在大数据量的情况下，不论是搜索的速度、准确率、召回率以及推荐阅读的相关度，都达到了不错的效果。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e完整可运行的新闻搜索引擎Demo请看我的Github项目\u003ca href=\"https://github.com/01joy/news_search_engine\"\u003enews_search_engine\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e以下是系列博客：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-1/\"\u003e和我一起构建搜索引擎（一）简介\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-2/\"\u003e和我一起构建搜索引擎（二）网络爬虫\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-3/\"\u003e和我一起构建搜索引擎（三）构建索引\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-4/\"\u003e和我一起构建搜索引擎（四）检索模型\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-5/\"\u003e和我一起构建搜索引擎（五）推荐阅读\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-6/\"\u003e和我一起构建搜索引擎（六）系统展示\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-7/\"\u003e和我一起构建搜索引擎（七）总结展望\u003c/a\u003e\u003c/p\u003e","title":"和我一起构建搜索引擎（六）系统展示"},{"content":"虽然主要的检索功能实现了，但是我们还需要一个“推荐阅读”的功能。当用户浏览某条具体新闻时，我们在页面底端给出5条和该新闻相关的新闻，也就是一个最简单的推荐系统。\n搜狐新闻“相关新闻”模块\n推荐模块的思路是度量两两新闻之间的相似度，取相似度最高的前5篇新闻作为推荐阅读的新闻。\n我们前面讲过，一篇文档可以用一个向量表示，向量中的每个值是不同词项t在该文档d中的词频tf。但是一篇较短的文档（如新闻）的关键词并不多，所以我们可以提取每篇新闻的关键词，用这些关键词的tfidf值构成文档的向量表示，这样能够大大减少相似度计算量，同时保持较好的推荐效果。\njieba分词组件自带关键词提取功能，并能返回关键词的tfidf值。所以对每篇新闻，我们先提取tfidf得分最高的前25个关键词，用这25个关键词的tfidf值作为文档的向量表示。由此能够得到一个1000*m的文档词项矩阵M，矩阵每行表示一个文档，每列表示一个词项，m为1000个文档的所有互异的关键词（大概10000个）。矩阵M当然也是稀疏矩阵。\n得到文档词项矩阵M之后，我们利用sklearn的pairwise_distances函数计算M中行向量之间的cosine相似度，对每个文档，得到与其最相似的前5篇新闻id，并把结果写入数据库。\n推荐阅读模块的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Wed Dec 23 14:06:10 2015 @author: bitjoy.net \u0026#34;\u0026#34;\u0026#34; from os import listdir import xml.etree.ElementTree as ET import jieba import jieba.analyse import sqlite3 import configparser from datetime import * import math import pandas as pd import numpy as np from sklearn.metrics import pairwise_distances class RecommendationModule: stop_words = set() k_nearest = [] config_path = \u0026#39;\u0026#39; config_encoding = \u0026#39;\u0026#39; doc_dir_path = \u0026#39;\u0026#39; doc_encoding = \u0026#39;\u0026#39; stop_words_path = \u0026#39;\u0026#39; stop_words_encoding = \u0026#39;\u0026#39; idf_path = \u0026#39;\u0026#39; db_path = \u0026#39;\u0026#39; def __init__(self, config_path, config_encoding): self.config_path = config_path self.config_encoding = config_encoding config = configparser.ConfigParser() config.read(config_path, config_encoding) self.doc_dir_path = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_dir_path\u0026#39;] self.doc_encoding = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_encoding\u0026#39;] self.stop_words_path = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;stop_words_path\u0026#39;] self.stop_words_encoding = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;stop_words_encoding\u0026#39;] self.idf_path = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;idf_path\u0026#39;] self.db_path = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;db_path\u0026#39;] f = open(self.stop_words_path, encoding = self.stop_words_encoding) words = f.read() self.stop_words = set(words.split(\u0026#39;\\n\u0026#39;)) def write_k_nearest_matrix_to_db(self): conn = sqlite3.connect(self.db_path) c = conn.cursor() c.execute(\u0026#34;\u0026#39;DROP TABLE IF EXISTS knearest\u0026#39;\u0026#34;) c.execute(\u0026#34;\u0026#39;CREATE TABLE knearest(id INTEGER PRIMARY KEY, first INTEGER, second INTEGER, third INTEGER, fourth INTEGER, fifth INTEGER)\u0026#39;\u0026#34;) for docid, doclist in self.k_nearest: c.execute(\u0026#34;INSERT INTO knearest VALUES (?, ?, ?, ?, ?, ?)\u0026#34;, tuple([docid] + doclist)) conn.commit() conn.close() def is_number(self, s): try: float(s) return True except ValueError: return False def construct_dt_matrix(self, files, topK = 200): jieba.analyse.set_stop_words(self.stop_words_path) jieba.analyse.set_idf_path(self.idf_path) M = len(files) N = 1 terms = {} dt = [] for i in files: root = ET.parse(self.doc_dir_path + i).getroot() title = root.find(\u0026#39;title\u0026#39;).text body = root.find(\u0026#39;body\u0026#39;).text docid = int(root.find(\u0026#39;id\u0026#39;).text) tags = jieba.analyse.extract_tags(title + \u0026#39;。\u0026#39; + body, topK=topK, withWeight=True) #tags = jieba.analyse.extract_tags(title, topK=topK, withWeight=True) cleaned_dict = {} for word, tfidf in tags: word = word.strip().lower() if word == \u0026#39;\u0026#39; or self.is_number(word): continue cleaned_dict[word] = tfidf if word not in terms: terms[word] = N N += 1 dt.append([docid, cleaned_dict]) dt_matrix = [[0 for i in range(N)] for j in range(M)] i =0 for docid, t_tfidf in dt: dt_matrix[i][0] = docid for term, tfidf in t_tfidf.items(): dt_matrix[i][terms[term]] = tfidf i += 1 dt_matrix = pd.DataFrame(dt_matrix) dt_matrix.index = dt_matrix[0] print(\u0026#39;dt_matrix shape:(%d %d)\u0026#39;%(dt_matrix.shape)) return dt_matrix def construct_k_nearest_matrix(self, dt_matrix, k): tmp = np.array(1 – pairwise_distances(dt_matrix[dt_matrix.columns[1:]], metric = \u0026#34;cosine\u0026#34;)) similarity_matrix = pd.DataFrame(tmp, index = dt_matrix.index.tolist(), columns = dt_matrix.index.tolist()) for i in similarity_matrix.index: tmp = [int(i),[]] j = 0 while j \u0026lt;= k: max_col = similarity_matrix.loc[i].idxmax(axis = 1) similarity_matrix.loc[i][max_col] = -1 if max_col != i: tmp[1].append(int(max_col)) #max column name j += 1 self.k_nearest.append(tmp) def gen_idf_file(self): files = listdir(self.doc_dir_path) n = float(len(files)) idf = {} for i in files: root = ET.parse(self.doc_dir_path + i).getroot() title = root.find(\u0026#39;title\u0026#39;).text body = root.find(\u0026#39;body\u0026#39;).text seg_list = jieba.lcut(title + \u0026#39;。\u0026#39; + body, cut_all=False) seg_list = set(seg_list) – self.stop_words for word in seg_list: word = word.strip().lower() if word == \u0026#39;\u0026#39; or self.is_number(word): continue if word not in idf: idf[word] = 1 else: idf[word] = idf[word] + 1 idf_file = open(self.idf_path, \u0026#39;w\u0026#39;, encoding = \u0026#39;utf-8\u0026#39;) for word, df in idf.items(): idf_file.write(\u0026#39;%s %.9f\\n\u0026#39;%(word, math.log(n / df))) idf_file.close() def find_k_nearest(self, k, topK): self.gen_idf_file() files = listdir(self.doc_dir_path) dt_matrix = self.construct_dt_matrix(files, topK) self.construct_k_nearest_matrix(dt_matrix, k) self.write_k_nearest_matrix_to_db() if __name__ == \u0026#34;__main__\u0026#34;: print(\u0026#39;—–start time: %s—–\u0026#39;%(datetime.today())) rm = RecommendationModule(\u0026#39;../config.ini\u0026#39;, \u0026#39;utf-8\u0026#39;) rm.find_k_nearest(5, 25) print(\u0026#39;—–finish time: %s—–\u0026#39;%(datetime.today())) 这个模块的代码量最多，主要原因是需要构建文档词项矩阵，并且计算k邻居矩阵。矩阵数据结构的设计需要特别注意，否则会严重影响系统的效率。我刚开始把任务都扔给了pandas.DataFrame，后来发现当两个文档向量合并时，需要join连接操作，当数据量很大时，非常耗时，所以改成了先用python原始的list存储，最后一次性构造一个完整的pandas.DataFrame，速度比之前快了不少。\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-09-introduction-to-building-a-search-engine-5/","summary":"\u003cp\u003e虽然主要的检索功能实现了，但是我们还需要一个“推荐阅读”的功能。当用户浏览某条具体新闻时，我们在页面底端给出5条和该新闻相关的新闻，也就是一个最简单的推荐系统。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"搜狐新闻“相关新闻”模块\" loading=\"lazy\" src=\"/posts/2016-01-09-introduction-to-building-a-search-engine-5/sohu-news3.webp\"\u003e\n搜狐新闻“相关新闻”模块\u003c/p\u003e\n\u003cp\u003e推荐模块的思路是度量两两新闻之间的相似度，取相似度最高的前5篇新闻作为推荐阅读的新闻。\u003c/p\u003e\n\u003cp\u003e我们前面讲过，一篇文档可以用一个向量表示，向量中的每个值是不同词项t在该文档d中的词频tf。但是一篇较短的文档（如新闻）的关键词并不多，所以我们可以提取每篇新闻的关键词，用这些关键词的tfidf值构成文档的向量表示，这样能够大大减少相似度计算量，同时保持较好的推荐效果。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/fxsjy/jieba#3-%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96\"\u003ejieba分词组件自带关键词提取功能\u003c/a\u003e，并能返回关键词的tfidf值。所以对每篇新闻，我们先提取tfidf得分最高的前25个关键词，用这25个关键词的tfidf值作为文档的向量表示。由此能够得到一个1000*m的文档词项矩阵M，矩阵每行表示一个文档，每列表示一个词项，m为1000个文档的所有互异的关键词（大概10000个）。矩阵M当然也是稀疏矩阵。\u003c/p\u003e\n\u003cp\u003e得到文档词项矩阵M之后，我们利用\u003ca href=\"http://sklearn.metrics.pairwise.pairwise_distances/\"\u003esklearn的pairwise_distances函数\u003c/a\u003e计算M中行向量之间的cosine相似度，对每个文档，得到与其最相似的前5篇新闻id，并把结果写入数据库。\u003c/p\u003e\n\u003cp\u003e推荐阅读模块的代码如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 46\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 47\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 48\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 49\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 50\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 51\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 52\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 53\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 54\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 55\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 56\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 57\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 58\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 59\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 60\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 61\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 62\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 63\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 64\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 65\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 66\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 67\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 68\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 69\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 70\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 71\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 72\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 73\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 74\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 75\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 76\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 77\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 78\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 79\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 80\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 81\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 82\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 83\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 84\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 85\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 86\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 87\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 88\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 89\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 90\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 91\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 92\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 93\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 94\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 95\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 96\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 97\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 98\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 99\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e100\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e101\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e102\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e103\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e104\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e105\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e106\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e107\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e108\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e109\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e110\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e111\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e112\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e113\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e114\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e115\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e116\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e117\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e118\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e119\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e120\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e121\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e122\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e123\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e124\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e125\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e126\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e127\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e128\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e129\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e130\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e131\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e132\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e133\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e134\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e135\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e136\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e137\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e138\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e139\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e140\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e141\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e142\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e143\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e144\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e145\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e146\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e147\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e148\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e149\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e150\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e151\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e152\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e153\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e154\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e155\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e156\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e157\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e158\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -*- coding: utf-8 -*-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eCreated on Wed Dec 23 14:06:10 2015\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e@author: bitjoy.net\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e os \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e listdir\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e xml.etree.ElementTree \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e ET\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e jieba\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e jieba.analyse\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e sqlite3\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e configparser\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e datetime \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e math\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pandas \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e pd\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e sklearn.metrics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pairwise_distances\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eRecommendationModule\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    stop_words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    k_nearest \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    doc_dir_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    doc_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    stop_words_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    stop_words_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    idf_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    db_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e(self, config_path, config_encoding):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config_path\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config_encoding\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e configparser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConfigParser()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread(config_path, config_encoding)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_dir_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_dir_path\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_encoding\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;stop_words_path\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;stop_words_encoding\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eidf_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;idf_path\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edb_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;db_path\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        f \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e open(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words_path, encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words_encoding)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e f\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set(words\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ewrite_k_nearest_matrix_to_db\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sqlite3\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econnect(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edb_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecursor()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#39;DROP TABLE IF EXISTS knearest\u0026#39;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#39;CREATE TABLE knearest(id INTEGER PRIMARY KEY, first INTEGER, second INTEGER, third INTEGER, fourth INTEGER, fifth INTEGER)\u0026#39;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e docid, doclist \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ek_nearest:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;INSERT INTO knearest VALUES (?, ?, ?, ?, ?, ?)\u0026#34;\u003c/span\u003e, tuple([docid] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e doclist))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecommit()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eis_number\u003c/span\u003e(self, s):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003etry\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            float(s)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eexcept\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eValueError\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003econstruct_dt_matrix\u003c/span\u003e(self, files, topK \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e200\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eanalyse\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset_stop_words(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eanalyse\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset_idf_path(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eidf_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        M \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e len(files)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        terms \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        dt \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e files:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            root \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eparse(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_dir_path \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egetroot()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            title \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;body\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            docid \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;id\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            tags \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eanalyse\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eextract_tags(title \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;。\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e body, topK\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003etopK, withWeight\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e#tags = jieba.analyse.extract_tags(title, topK=topK, withWeight=True)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            cleaned_dict \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e word, tfidf \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e tags:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                word \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e word\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estrip()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eor\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eis_number(word):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                cleaned_dict[word] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e tfidf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e terms:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    terms[word] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e N\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    N \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    dt\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend([docid, cleaned_dict])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    dt_matrix \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(N)] \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e j \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(M)]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        i \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e docid, t_tfidf \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e dt:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            dt_matrix[i][\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e docid\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e term, tfidf \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e t_tfidf\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                dt_matrix[i][terms[term]] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e tfidf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                i \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        dt_matrix \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataFrame(dt_matrix)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        dt_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eindex \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e dt_matrix[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;dt_matrix shape:(\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e)\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(dt_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e dt_matrix\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003econstruct_k_nearest_matrix\u003c/span\u003e(self, dt_matrix, k):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        tmp \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003earray(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e pairwise_distances(dt_matrix[dt_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecolumns[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e:]], metric \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;cosine\u0026#34;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        similarity_matrix \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDataFrame(tmp, index \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e dt_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eindex\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etolist(), columns \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e dt_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eindex\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etolist())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e similarity_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eindex:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            tmp \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [int(i),[]]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            j \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e j \u003cspan style=\"color:#f92672\"\u003e\u0026lt;=\u003c/span\u003e k:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                max_col \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e similarity_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eloc[i]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eidxmax(axis \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                similarity_matrix\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eloc[i][max_col] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e max_col \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e i:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    tmp[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(int(max_col)) \u003cspan style=\"color:#75715e\"\u003e#max column name\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    j \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ek_nearest\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(tmp)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egen_idf_file\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        files \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e listdir(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_dir_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        n \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e float(len(files))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        idf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e files:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            root \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eparse(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_dir_path \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egetroot()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            title \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;body\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            seg_list \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elcut(title \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;。\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e body, cut_all\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            seg_list \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set(seg_list) \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e–\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e seg_list:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                word \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e word\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estrip()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eor\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eis_number(word):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e word \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e idf:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    idf[word] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    idf[word] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e idf[word] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                idf_file \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e open(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eidf_path, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;w\u0026#39;\u003c/span\u003e, encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;utf-8\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e word, df \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e idf\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    idf_file\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%.9f\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(word, math\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elog(n \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e df)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                idf_file\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003efind_k_nearest\u003c/span\u003e(self, k, topK):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egen_idf_file()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        files \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e listdir(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edoc_dir_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        dt_matrix \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econstruct_dt_matrix(files, topK)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econstruct_k_nearest_matrix(dt_matrix, k)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite_k_nearest_matrix_to_db()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;—–start time: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e—–\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(datetime\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etoday()))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e RecommendationModule(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../config.ini\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;utf-8\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind_k_nearest(\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e25\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;—–finish time: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e—–\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(datetime\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etoday()))\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e这个模块的代码量最多，主要原因是需要构建文档词项矩阵，并且计算k邻居矩阵。矩阵数据结构的设计需要特别注意，否则会严重影响系统的效率。我刚开始把任务都扔给了pandas.DataFrame，后来发现当两个文档向量合并时，需要join连接操作，当数据量很大时，非常耗时，所以改成了先用python原始的list存储，最后一次性构造一个完整的pandas.DataFrame，速度比之前快了不少。\u003c/p\u003e","title":"和我一起构建搜索引擎（五）推荐阅读"},{"content":"构建好倒排索引之后，就可以开始检索了。\n检索模型有很多，比如向量空间模型、概率模型、语言模型等。其中最有名的、检索效果最好的是基于概率的BM25模型。\n给定一个查询Q和一篇文档d，d对Q的BM25得分公式为\n$$BM25_{score}(Q,d)=\\sum_{t\\in Q}w(t,d)$$$$w(t,d)=\\frac{qtf}{k_3+qtf}\\times \\frac{k_1\\times tf}{tf+k_1(1-b+b\\times l_d/avg\\_l)}\\times log_2\\frac{N-df+0.5}{df+0.5}$$公式中变量含义如下：\n\\(qtf\\)：查询中的词频 \\(tf\\)：文档中的词频 \\(l_d\\)：文档长度 \\(avg\\_l\\)：平均文档长度 \\(N\\)：文档数量 \\(df\\)：文档频率 \\(b,k_1,k_3\\)：可调参数 这个公式看起来很复杂，我们把它分解一下，其实很容易理解。第一个公式是外部公式，一个查询Q可能包含多个词项，比如“苹果手机”就包含“苹果”和“手机”两个词项，我们需要分别计算“苹果”和“手机”对某个文档d的贡献分数w(t,d)，然后将他们加起来就是整个文档d相对于查询Q的得分。\n第二个公式就是计算某个词项t在文档d中的得分，它包括三个部分。第一个部分是词项t在查询Q中的得分，比如查询“中国人说中国话”中“中国”出现了两次，此时qtf=2，说明这个查询希望找到的文档和“中国”更相关，“中国”的权重应该更大，但是通常情况下，查询Q都很短，而且不太可能包含相同的词项，所以这个因子是一个常数，我们在实现的时候可以忽略。\n第二部分类似于TFIDF模型中的TF项。也就是说某个词项t在文档d中出现次数越多，则t越重要，但是文档长度越长，tf也倾向于变大，所以使用文档长度除以平均长度\\(l_d/avg\\_l\\)起到某种归一化的效果，\\(k_1\\)和\\(b\\)是可调参数。\n第三部分类似于TFIDF模型中的IDF项。也就是说虽然“的”、“地”、“得”等停用词在某文档d中出现的次数很多，但是他们在很多文档中都出现过，所以这些词对d的贡献分并不高，接近于0；反而那些很稀有的词如”糖尿病“能够很好的区分不同文档，这些词对文档的贡献分应该较高。\n所以根据BM25公式，我们可以很快计算出不同文档t对查询Q的得分情况，然后按得分高低排序给出结果。\n下面是给定一个查询句子sentence，根据BM25公式给出文档排名的函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def result_by_BM25(self, sentence): seg_list = jieba.lcut(sentence, cut_all=False) n, cleaned_dict = self.clean_list(seg_list) BM25_scores = {} for term in cleaned_dict.keys(): r = self.fetch_from_db(term) if r is None: continue df = r[1] w = math.log2((self.N - df + 0.5) / (df + 0.5)) docs = r[2].split(\u0026#39;\\n\u0026#39;) for doc in docs: docid, date_time, tf, ld = doc.split(\u0026#39;\\t\u0026#39;) docid = int(docid) tf = int(tf) ld = int(ld) s = (self.K1 * tf * w) / (tf + self.K1 * (1 - self.B + self.B * ld / self.AVG_L)) if docid in BM25_scores: BM25_scores[docid] = BM25_scores[docid] + s else: BM25_scores[docid] = s BM25_scores = sorted(BM25_scores.items(), key = operator.itemgetter(1)) BM25_scores.reverse() if len(BM25_scores) == 0: return 0, [] else: return 1, BM25_scores 首先将句子分词得到所有查询词项，然后从数据库中取出词项对应的倒排记录表，对记录表中的所有文档，计算其BM25得分，最后按得分高低排序作为查询结果。\n类似的，我们还可以对所有文档按时间先后顺序排序，越新鲜的新闻排名越高；还可以按新闻的热度排序，越热门的新闻排名越高。\n关于热度公式，我们认为一方面要兼顾相关度，另一方面也要考虑时间因素，所以是BM25打分和时间打分的一个综合。\n比较有名的热度公式有两个，一个是Hacker News的，另一个是Reddit的，他们的公式分别为：\n图1. hacker news ranking algorithm [1]\n图2. reddit ranking algorithm [2]\n可以看出，他们都是将新闻/评论的一个原始得分和时间组合起来，只是一个用除法，一个用加法。所以我们也依葫芦画瓢，”自创“了一个简单的热度公式：\n$$hot_{score}=k_1log(BM25_{score})+\\frac{k_2}{t_{now}-t_{news}}$$用BM25得分加上新闻时间和当前时间的差值的倒数，\\(k_1\\)和\\(k_2\\)也是可调参数。\n按时间排序和按热度排序的函数和按BM25打分排序的函数类似，这里就不贴出来了，详细情况可以看我的Github项目News_IR_Demo。\n至此，搜索引擎的搜索功能已经实现了，你可以试着修改./web/search_engine.py的第167行的关键词，看看搜索结果是否和你预想的排序是一样的。不过由于我们的数据量只有1000个新闻，并不能涵盖所有关键词，更多的测试可以留给大家线下完成。\n[1]. http://amix.dk/blog/post/19574\n[2]. http://amix.dk/blog/post/19588\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-07-introduction-to-building-a-search-engine-4/","summary":"\u003cp\u003e构建好倒排索引之后，就可以开始检索了。\u003c/p\u003e\n\u003cp\u003e检索模型有很多，比如向量空间模型、概率模型、语言模型等。其中最有名的、检索效果最好的是基于概率的BM25模型。\u003c/p\u003e\n\u003cp\u003e给定一个查询Q和一篇文档d，d对Q的BM25得分公式为\u003c/p\u003e\n$$BM25_{score}(Q,d)=\\sum_{t\\in Q}w(t,d)$$$$w(t,d)=\\frac{qtf}{k_3+qtf}\\times \\frac{k_1\\times tf}{tf+k_1(1-b+b\\times l_d/avg\\_l)}\\times log_2\\frac{N-df+0.5}{df+0.5}$$\u003cp\u003e公式中变量含义如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\\(qtf\\)：查询中的词频\u003c/li\u003e\n\u003cli\u003e\\(tf\\)：文档中的词频\u003c/li\u003e\n\u003cli\u003e\\(l_d\\)：文档长度\u003c/li\u003e\n\u003cli\u003e\\(avg\\_l\\)：平均文档长度\u003c/li\u003e\n\u003cli\u003e\\(N\\)：文档数量\u003c/li\u003e\n\u003cli\u003e\\(df\\)：文档频率\u003c/li\u003e\n\u003cli\u003e\\(b,k_1,k_3\\)：可调参数\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这个公式看起来很复杂，我们把它分解一下，其实很容易理解。第一个公式是外部公式，一个查询Q可能包含多个词项，比如“苹果手机”就包含“苹果”和“手机”两个词项，我们需要分别计算“苹果”和“手机”对某个文档d的贡献分数w(t,d)，然后将他们加起来就是整个文档d相对于查询Q的得分。\u003c/p\u003e\n\u003cp\u003e第二个公式就是计算某个词项t在文档d中的得分，它包括三个部分。第一个部分是词项t在查询Q中的得分，比如查询“中国人说中国话”中“中国”出现了两次，此时qtf=2，说明这个查询希望找到的文档和“中国”\u003cstrong\u003e更\u003c/strong\u003e相关，“中国”的权重应该更大，但是通常情况下，查询Q都很短，而且不太可能包含相同的词项，所以这个因子是一个常数，我们在实现的时候可以忽略。\u003c/p\u003e\n\u003cp\u003e第二部分类似于\u003ca href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\"\u003eTFIDF模型\u003c/a\u003e中的TF项。也就是说某个词项t在文档d中出现次数越多，则t越重要，但是文档长度越长，tf也倾向于变大，所以使用文档长度除以平均长度\\(l_d/avg\\_l\\)起到某种归一化的效果，\\(k_1\\)和\\(b\\)是可调参数。\u003c/p\u003e\n\u003cp\u003e第三部分类似于\u003ca href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\"\u003eTFIDF模型\u003c/a\u003e中的IDF项。也就是说虽然“的”、“地”、“得”等停用词在某文档d中出现的次数很多，但是他们在很多文档中都出现过，所以这些词对d的贡献分并不高，接近于0；反而那些很稀有的词如”糖尿病“能够很好的区分不同文档，这些词对文档的贡献分应该较高。\u003c/p\u003e\n\u003cp\u003e所以根据BM25公式，我们可以很快计算出不同文档t对查询Q的得分情况，然后按得分高低排序给出结果。\u003c/p\u003e\n\u003cp\u003e下面是给定一个查询句子sentence，根据BM25公式给出文档排名的函数\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e27\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eresult_by_BM25\u003c/span\u003e(self, sentence):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tseg_list \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elcut(sentence, cut_all\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tn, cleaned_dict \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclean_list(seg_list)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tBM25_scores \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e term \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cleaned_dict\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ekeys():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\tr \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efetch_from_db(term)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e r \u003cspan style=\"color:#f92672\"\u003eis\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eNone\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\tdf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e r[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\tw \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e math\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elog2((self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eN \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e df \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e (df \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\tdocs \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e r[\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e doc \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e docs:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\tdocid, date_time, tf, ld \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e doc\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\tdocid \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(docid)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\ttf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(tf)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\tld \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(ld)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\ts \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eK1 \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e tf \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e w) \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e (tf \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eK1 \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e (\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eB \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eB \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e ld \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eAVG_L))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e docid \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e BM25_scores:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\tBM25_scores[docid] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e BM25_scores[docid] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e s\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\tBM25_scores[docid] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e s\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tBM25_scores \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sorted(BM25_scores\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems(), key \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e operator\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitemgetter(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tBM25_scores\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ereverse()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e len(BM25_scores) \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, BM25_scores\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e首先将句子分词得到所有查询词项，然后从数据库中取出词项对应的倒排记录表，对记录表中的所有文档，计算其BM25得分，最后按得分高低排序作为查询结果。\u003c/p\u003e","title":"和我一起构建搜索引擎（四）检索模型"},{"content":"目前正是所谓的“大数据”时代，数据量多到难以计数，怎样结构化的存储以便于分析计算，是当前的一大难题。上一篇博客我们简单抓取了1000个搜狐新闻数据，搜索的过程就是从这1000个新闻中找出和关键词相关的新闻来，那么怎样快速搜索呢，总不可能依次打开xml文件一个字一个字的找吧，这时就需要借助倒排索引这个强大的数据结构。\n在讲倒排索引之前，我们先介绍一下布尔检索。布尔检索只是简单返回包含某个关键词的文档，比如查询“苹果手机”，则返回所有包含“苹果”和“手机”关键词的文档，布尔检索并不对返回结果排序，所以有可能返回的第一个文档是“某个男孩边吃苹果边玩手机…“。\n实现布尔检索并不难，我们需要构建一个如下图的词项文档矩阵：\n图1. 布尔检索中的词项文档矩阵\n每行对应一个词项，每列对应一个文档，如果该值为1，表示该行词项出现在该列文档中。比如词项”苹果“出现在doc1和doc3文档中，如果我们要找同时出现”苹果“和”手机“的文档，只需把他们对应的向量取出来进行”与“操作，此为101\u0026amp;011=001，所以doc3同时出现了”苹果“和”手机“两个关键词，我们将其返回。\n布尔检索虽然很快，但是它也有很多缺陷，比如不能对结果排序，词项只有出现和不出现两种状态，但是一篇文档中出现10次“苹果“和只出现1次”苹果“，他们的相关度肯定是不相同的。所以需要对布尔检索进行改进。\n在扫描文档时，不但记录某词项出现与否，还记录该词项出现的次数，即词项频率(tf)；同时我们记录该文档的长度(ld)，以及某词项在不同文档中出现的次数，即文档频率(df)。\n图2. 倒排索引结构图\n这样我们就得到了如上图的倒排索引。左边部分被称为词典，存储的是1000个新闻中所有不同的词项；右边部分被称为倒排记录表，存储的是出现Term_i的那些文档信息。倒排索引中存储的变量都是为了给后续检索模型使用。\n讲到这里，我们需要解决如下几个问题。\n怎样得到一篇文档中的所有词项。给我们一篇新闻稿子，人类很容易分辨出”苹果“和”手机“是两个不同的词项，但是计算机怎么知道是这两个词呢？为什么不是”苹”、”国手“和”机“呢？这就需要进行中文分词，我们可以借助开源的jieba中文分词组件来完成，jieba分词能够将一个中文句子切成一个个词项，这样我们就可以统计tf, df了。 有些词，如”的“、”地“、”得“、”如果“等，几乎每篇文档都会出现，他们起不到很好的区分文档的效果，这类词被称为”停用词“，我们需要把他们去掉。去停词的步骤可以在jieba分词之后完成。 怎样存储倒排记录表。假设1000个文档共有20000个不同的词项，如果用类似图1的矩阵形式存储，需要耗费100020000=210^7个存储单元，但是图1往往是一个稀疏矩阵，因为一个文档中可能只出现了200个不同的词项，剩余的19800个词项都是空的。用矩阵方式存储时空效率都不高。所以我们可以采用图2的方式，词典用B-树或hash存储，倒排记录表用邻接链表存储方式，这样能大大减少存储空间。如果我们要将图2保存到数据库，可以对倒排记录表序列化成一个长的字符串，写入到一个单元格，读取的时候再反序列化。比如每个Doc内部用’\\t’连接，Doc之间用’\\n’连接，读取的时候split即可。 倒排索引构建算法使用内存式单遍扫描索引构建方法（SPIMI），其实就是依次对每篇新闻进行分词，如果出现新的词项则插入到词典中，否则将该文档的信息追加到词项对应的倒排记录表中。SPIMI的伪代码如下：\n图3. SPIMI算法伪代码\n下面是构建索引的所有代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Sat Dec 5 23:31:22 2015 @author: bitjoy.net \u0026#34;\u0026#34;\u0026#34; from os import listdir import xml.etree.ElementTree as ET import jieba import sqlite3 import configparser class Doc: docid = 0 date_time = \u0026#39;\u0026#39; tf = 0 ld = 0 def __init__(self, docid, date_time, tf, ld): self.docid = docid self.date_time = date_time self.tf = tf self.ld = ld def __repr__(self): return(str(self.docid) + \u0026#39;\\t\u0026#39; + self.date_time + \u0026#39;\\t\u0026#39; + str(self.tf) + \u0026#39;\\t\u0026#39; + str(self.ld)) def __str__(self): return(str(self.docid) + \u0026#39;\\t\u0026#39; + self.date_time + \u0026#39;\\t\u0026#39; + str(self.tf) + \u0026#39;\\t\u0026#39; + str(self.ld)) class IndexModule: stop_words = set() postings_lists = {} config_path = \u0026#39;\u0026#39; config_encoding = \u0026#39;\u0026#39; def __init__(self, config_path, config_encoding): self.config_path = config_path self.config_encoding = config_encoding config = configparser.ConfigParser() config.read(config_path, config_encoding) f = open(config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;stop_words_path\u0026#39;], encoding = config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;stop_words_encoding\u0026#39;]) words = f.read() self.stop_words = set(words.split(\u0026#39;\\n\u0026#39;)) def is_number(self, s): try: float(s) return True except ValueError: return False def clean_list(self, seg_list): cleaned_dict = {} n = 0 for i in seg_list: i = i.strip().lower() if i != \u0026#39;\u0026#39; and not self.is_number(i) and i not in self.stop_words: n = n + 1 if i in cleaned_dict: cleaned_dict[i] = cleaned_dict[i] + 1 else: cleaned_dict[i] = 1 return n, cleaned_dict def write_postings_to_db(self, db_path): conn = sqlite3.connect(db_path) c = conn.cursor() c.execute(\u0026#34;\u0026#39;DROP TABLE IF EXISTS postings\u0026#39;\u0026#34;) c.execute(\u0026#34;\u0026#39;CREATE TABLE postings(term TEXT PRIMARY KEY, df INTEGER, docs TEXT)\u0026#39;\u0026#34;) for key, value in self.postings_lists.items(): doc_list = \u0026#39;\\n\u0026#39;.join(map(str,value[1])) t = (key, value[0], doc_list) c.execute(\u0026#34;INSERT INTO postings VALUES (?, ?, ?)\u0026#34;, t) conn.commit() conn.close() def construct_postings_lists(self): config = configparser.ConfigParser() config.read(self.config_path, self.config_encoding) files = listdir(config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_dir_path\u0026#39;]) AVG_L = 0 for i in files: root = ET.parse(config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_dir_path\u0026#39;] + i).getroot() title = root.find(\u0026#39;title\u0026#39;).text body = root.find(\u0026#39;body\u0026#39;).text docid = int(root.find(\u0026#39;id\u0026#39;).text) date_time = root.find(\u0026#39;datetime\u0026#39;).text seg_list = jieba.lcut(title + \u0026#39;。\u0026#39; + body, cut_all=False) ld, cleaned_dict = self.clean_list(seg_list) AVG_L = AVG_L + ld for key, value in cleaned_dict.items(): d = Doc(docid, date_time, value, ld) if key in self.postings_lists: self.postings_lists[key][0] = self.postings_lists[key][0] + 1 # df++ self.postings_lists[key][1].append(d) else: self.postings_lists[key] = [1, [d]] # [df, [Doc]] AVG_L = AVG_L / len(files) config.set(\u0026#39;DEFAULT\u0026#39;, \u0026#39;N\u0026#39;, str(len(files))) config.set(\u0026#39;DEFAULT\u0026#39;, \u0026#39;avg_l\u0026#39;, str(AVG_L)) with open(self.config_path, ‘w’, encoding = self.config_encoding) as configfile: config.write(configfile) self.write_postings_to_db(config[‘DEFAULT’][‘db_path’]) if __name__ == \u0026#34;__main__\u0026#34;: im = IndexModule(\u0026#39;../config.ini\u0026#39;, \u0026#39;utf-8\u0026#39;) im.construct_postings_lists() 运行之后会在./data/下生成一个ir.db数据库文件，这就是构建好的索引数据库。\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-07-introduction-to-building-a-search-engine-3/","summary":"\u003cp\u003e目前正是所谓的“大数据”时代，数据量多到难以计数，怎样结构化的存储以便于分析计算，是当前的一大难题。上一篇博客我们简单抓取了1000个搜狐新闻数据，搜索的过程就是从这1000个新闻中找出和关键词相关的新闻来，那么怎样快速搜索呢，总不可能依次打开xml文件一个字一个字的找吧，这时就需要借助倒排索引这个强大的数据结构。\u003c/p\u003e\n\u003cp\u003e在讲倒排索引之前，我们先介绍一下布尔检索。布尔检索只是简单返回包含某个关键词的文档，比如查询“苹果手机”，则返回所有包含“苹果”和“手机”关键词的文档，布尔检索并不对返回结果排序，所以有可能返回的第一个文档是“某个男孩边吃苹果边玩手机…“。\u003c/p\u003e\n\u003cp\u003e实现布尔检索并不难，我们需要构建一个如下图的词项文档矩阵：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图1. 布尔检索中的词项文档矩阵\" loading=\"lazy\" src=\"/posts/2016-01-07-introduction-to-building-a-search-engine-3/td_matrix.png\"\u003e\n图1. 布尔检索中的词项文档矩阵\u003c/p\u003e\n\u003cp\u003e每行对应一个词项，每列对应一个文档，如果该值为1，表示该行词项出现在该列文档中。比如词项”苹果“出现在doc1和doc3文档中，如果我们要找同时出现”苹果“和”手机“的文档，只需把他们对应的向量取出来进行”与“操作，此为101\u0026amp;011=001，所以doc3同时出现了”苹果“和”手机“两个关键词，我们将其返回。\u003c/p\u003e\n\u003cp\u003e布尔检索虽然很快，但是它也有很多缺陷，比如不能对结果排序，词项只有出现和不出现两种状态，但是一篇文档中出现10次“苹果“和只出现1次”苹果“，他们的相关度肯定是不相同的。所以需要对布尔检索进行改进。\u003c/p\u003e\n\u003cp\u003e在扫描文档时，不但记录某词项出现与否，还记录该词项出现的次数，即词项频率(tf)；同时我们记录该文档的长度(ld)，以及某词项在不同文档中出现的次数，即文档频率(df)。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图2. 倒排索引结构图\" loading=\"lazy\" src=\"/posts/2016-01-07-introduction-to-building-a-search-engine-3/inverted-index.png\"\u003e\n图2. 倒排索引结构图\u003c/p\u003e\n\u003cp\u003e这样我们就得到了如上图的倒排索引。左边部分被称为词典，存储的是1000个新闻中所有不同的词项；右边部分被称为倒排记录表，存储的是出现Term_i的那些文档信息。倒排索引中存储的变量都是为了给后续检索模型使用。\u003c/p\u003e\n\u003cp\u003e讲到这里，我们需要解决如下几个问题。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e怎样得到一篇文档中的所有词项。给我们一篇新闻稿子，人类很容易分辨出”苹果“和”手机“是两个不同的词项，但是计算机怎么知道是这两个词呢？为什么不是”苹”、”国手“和”机“呢？这就需要进行中文分词，我们可以借助开源的\u003ca href=\"https://github.com/fxsjy/jieba\"\u003ejieba中文分词组件\u003c/a\u003e来完成，jieba分词能够将一个中文句子切成一个个词项，这样我们就可以统计tf, df了。\u003c/li\u003e\n\u003cli\u003e有些词，如”的“、”地“、”得“、”如果“等，几乎每篇文档都会出现，他们起不到很好的区分文档的效果，这类词被称为”停用词“，我们需要把他们去掉。去停词的步骤可以在jieba分词之后完成。\u003c/li\u003e\n\u003cli\u003e怎样存储倒排记录表。假设1000个文档共有20000个不同的词项，如果用类似图1的矩阵形式存储，需要耗费1000\u003cem\u003e20000=2\u003c/em\u003e10^7个存储单元，但是图1往往是一个稀疏矩阵，因为一个文档中可能只出现了200个不同的词项，剩余的19800个词项都是空的。用矩阵方式存储时空效率都不高。所以我们可以采用图2的方式，词典用B-树或hash存储，倒排记录表用邻接链表存储方式，这样能大大减少存储空间。如果我们要将图2保存到数据库，可以对倒排记录表序列化成一个长的字符串，写入到一个单元格，读取的时候再反序列化。比如每个Doc内部用’\\t’连接，Doc之间用’\\n’连接，读取的时候split即可。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e倒排索引构建算法使用内存式单遍扫描索引构建方法（SPIMI），其实就是依次对每篇新闻进行分词，如果出现新的词项则插入到词典中，否则将该文档的信息追加到词项对应的倒排记录表中。SPIMI的伪代码如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图3. SPIMI算法伪代码\" loading=\"lazy\" src=\"/posts/2016-01-07-introduction-to-building-a-search-engine-3/SPIMI-algo.png\"\u003e\n图3. SPIMI算法伪代码\u003c/p\u003e\n\u003cp\u003e下面是构建索引的所有代码：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e  9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 46\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 47\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 48\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 49\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 50\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 51\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 52\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 53\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 54\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 55\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 56\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 57\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 58\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 59\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 60\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 61\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 62\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 63\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 64\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 65\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 66\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 67\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 68\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 69\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 70\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 71\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 72\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 73\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 74\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 75\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 76\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 77\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 78\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 79\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 80\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 81\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 82\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 83\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 84\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 85\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 86\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 87\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 88\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 89\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 90\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 91\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 92\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 93\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 94\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 95\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 96\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 97\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 98\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 99\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e100\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e101\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e102\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e103\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e104\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e105\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e106\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e107\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e108\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e109\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e110\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e111\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e112\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e113\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e114\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -*- coding: utf-8 -*-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eCreated on Sat Dec 5 23:31:22 2015\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e@author: bitjoy.net\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e os \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e listdir\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e xml.etree.ElementTree \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e ET\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e jieba\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e sqlite3\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e configparser\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eDoc\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    docid \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    date_time \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    tf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ld \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e(self, docid, date_time, tf, ld):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edocid \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e docid\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edate_time \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e date_time\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etf \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e tf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eld \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ld\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__repr__\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e(str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edocid) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edate_time \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etf) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eld))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__str__\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e(str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edocid) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edate_time \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etf) \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\t\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e str(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eld))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eIndexModule\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    stop_words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    postings_lists \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e__init__\u003c/span\u003e(self, config_path, config_encoding):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_path \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config_path\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config_encoding\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e configparser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConfigParser()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread(config_path, config_encoding)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        f \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e open(config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;stop_words_path\u0026#39;\u003c/span\u003e], encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;stop_words_encoding\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e f\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e set(words\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esplit(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eis_number\u003c/span\u003e(self, s):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003etry\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            float(s)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eexcept\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eValueError\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eclean_list\u003c/span\u003e(self, seg_list):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cleaned_dict \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        n \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e seg_list:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            i \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e i\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estrip()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elower()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eis_number(i) \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estop_words:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                n \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e n \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cleaned_dict:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                cleaned_dict[i] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cleaned_dict[i] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                cleaned_dict[i] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e n, cleaned_dict\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ewrite_postings_to_db\u003c/span\u003e(self, db_path):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sqlite3\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econnect(db_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecursor()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#39;DROP TABLE IF EXISTS postings\u0026#39;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#39;CREATE TABLE postings(term TEXT PRIMARY KEY, df INTEGER, docs TEXT)\u0026#39;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e key, value \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            doc_list \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ejoin(map(str,value[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            t \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (key, value[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e], doc_list)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            c\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eexecute(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;INSERT INTO postings VALUES (?, ?, ?)\u0026#34;\u003c/span\u003e, t)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecommit()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        conn\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003econstruct_postings_lists\u003c/span\u003e(self):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e configparser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConfigParser()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_path, self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_encoding)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        files \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e listdir(config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_dir_path\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        AVG_L \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e files:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            root \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eparse(config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_dir_path\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e i)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egetroot()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            title \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;title\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;body\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            docid \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;id\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            date_time \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;datetime\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            seg_list \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e jieba\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003elcut(title \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;。\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e body, cut_all\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            ld, cleaned_dict \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclean_list(seg_list)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            AVG_L \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e AVG_L \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e ld\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e key, value \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e cleaned_dict\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitems():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                d \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Doc(docid, date_time, value, ld)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e key \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists[key][\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists[key][\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# df++\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists[key][\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(d)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epostings_lists[key] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, [d]] \u003cspan style=\"color:#75715e\"\u003e# [df, [Doc]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    AVG_L \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e AVG_L \u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e len(files)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;N\u0026#39;\u003c/span\u003e, str(len(files)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;avg_l\u0026#39;\u003c/span\u003e, str(AVG_L))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e open(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_path, \u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003ew\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e, encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econfig_encoding) \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e configfile:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite(configfile)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite_postings_to_db(config[\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003eDEFAULT\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e][\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e‘\u003c/span\u003edb_path\u003cspan style=\"color:#960050;background-color:#1e0010\"\u003e’\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    im \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e IndexModule(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../config.ini\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;utf-8\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    im\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econstruct_postings_lists()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e运行之后会在./data/下生成一个ir.db数据库文件，这就是构建好的索引数据库。\u003c/p\u003e","title":"和我一起构建搜索引擎（三）构建索引"},{"content":"网络爬虫又称网络蜘蛛、Web采集器等，它是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。\n我们在设计网络爬虫的时候需要注意两点：\n鲁棒性。Web中有些服务器会制造采集器陷阱（spider traps），这些陷阱服务器实际上是Web页面的生成器，它能在某个域下生成无数网页，从而使采集器陷入到一个无限的采集循环中去。采集器必须能从这些陷阱中跳出来。当然，这些陷阱倒不一定都是恶意的，有时可能是网站设计疏忽所导致的结果。\n礼貌性。Web服务器具有一些隐式或显式的政策来控制采集器访问它们的频率。设计采集器时必须要遵守这些代表礼貌性的访问政策。\n采集器的基本架构如下图所示。\n基本上是“抓取→分析→得到新的URL→再抓取→再分析”这样一个死循环过程。\n以上内容摘自王斌老师翻译的《信息检索导论》课本。\n由于我们要做的是一个新闻搜索引擎，所以抓取的是新闻数据，对于爬虫，网上也有很多的开源程序，如nutch等，Github上还有人专门开发了抓取新闻的组件newspaper，可以很方便的提取新闻标题、正文、时间等信息。不过用python写爬虫也是分分钟的事情，下面我们一起来试一试。\n首先找一个新闻网站，为简单起见，要找那种结构清晰、html代码便于解析的门户网站，比如搜狐新闻、参考消息等。\n搜狐新闻的国内要闻列表如下：\n结构非常清楚，左边是带URL的标题，右边括号里有新闻时间。这一页列表就有200条新闻，如果我们要获取1000条，只要不断模拟点击下一页即可。下一页的URL也只是在首页的基础上加上_xxx.shtml，xxx就是不同的页码。\n查看列表的html源码，得知列表都在类名为newsblue1的td中，所以只需要解析html源码就可以得到新闻标题、URL和时间，python解析html可以用BeautifulSoup包，非常方便。\n进入到新闻详细页面，正文部分如下：\n查看html源码，正文位于类名为text clear的div中，据此可以很方便的提取新闻正文。\n得到一条新闻的所有数据之后，我们需要将之结构化成xml文件，借助相应的xml包可以很方便的完成这项工作。xml格式定义如下：\n注意爬虫需要访问网络，难免会出现一些异常，所以捕获异常是非常有必要的。另外，搜狐每篇新闻正文后面都会有一段’//’开始的注释，这个需要过滤掉，短于140个字的新闻我也过滤掉了。整个搜索系统的配置参数都存储在config.ini文件中。\n下面是完整的python 3.4+代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Created on Sat Dec 19 11:57:01 2015 @author: bitjoy.net \u0026#34;\u0026#34;\u0026#34; from bs4 import BeautifulSoup import urllib.request import xml.etree.ElementTree as ET import configparser def get_news_pool(root, start, end): news_pool = [] for i in range(start,end,-1): page_url = \u0026#39;\u0026#39; if i != start: page_url = root +\u0026#39;_%d.shtml\u0026#39;%(i) else: page_url = root + \u0026#39;.shtml\u0026#39; try: response = urllib.request.urlopen(page_url) except Exception as e: print(\u0026#34;—–%s: %s—–\u0026#34;%(type(e), page_url)) continue html = response.read() soup = BeautifulSoup(html) td = soup.find(\u0026#39;td\u0026#39;, class_ = \u0026#34;newsblue1\u0026#34;) a = td.find_all(\u0026#39;a\u0026#39;) span = td.find_all(\u0026#39;span\u0026#39;) for i in range(len(a)): date_time = span[i].string url = a[i].get(\u0026#39;href\u0026#39;) title = a[i].string news_info = [\u0026#39;2016-\u0026#39;+date_time[1:3]+\u0026#39;-\u0026#39;+date_time[4:-1]+\u0026#39;:00\u0026#39;,url,title] news_pool.append(news_info) return(news_pool) def crawl_news(news_pool, min_body_len, doc_dir_path, doc_encoding): i = 1 for news in news_pool: try: response = urllib.request.urlopen(news[1]) except Exception as e: print(\u0026#34;—–%s: %s—–\u0026#34;%(type(e), news[1])) continue html = response.read() soup = BeautifulSoup(html) try: body = soup.find(\u0026#39;div\u0026#39;, class_ = \u0026#34;text clear\u0026#34;).find(\u0026#39;div\u0026#39;).get_text() except Exception as e: print(\u0026#34;—–%s: %s—–\u0026#34;%(type(e), news[1])) continue if \u0026#39;//\u0026#39; in body: body = body[:body.index(\u0026#39;//\u0026#39;)] body = body.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;) if len(body) \u0026lt;= min_body_len: continue doc = ET.Element(\u0026#34;doc\u0026#34;) ET.SubElement(doc, \u0026#34;id\u0026#34;).text = \u0026#34;%d\u0026#34;%(i) ET.SubElement(doc, \u0026#34;url\u0026#34;).text = news[1] ET.SubElement(doc, \u0026#34;title\u0026#34;).text = news[2] ET.SubElement(doc, \u0026#34;datetime\u0026#34;).text = news[0] ET.SubElement(doc, \u0026#34;body\u0026#34;).text = body tree = ET.ElementTree(doc) tree.write(doc_dir_path + \u0026#34;%d.xml\u0026#34;%(i), encoding = doc_encoding, xml_declaration = True) i += 1 if __name__ == \u0026#39;__main__\u0026#39;: config = configparser.ConfigParser() config.read(\u0026#39;../config.ini\u0026#39;, \u0026#39;utf-8\u0026#39;) root = \u0026#39;http://news.sohu.com/1/0903/61/subject212846158\u0026#39; news_pool = get_news_pool(root, 854, 849) crawl_news(news_pool, 140, config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_dir_path\u0026#39;], config[\u0026#39;DEFAULT\u0026#39;][\u0026#39;doc_encoding\u0026#39;]) print(\u0026#39;done!\u0026#39;) 完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-04-introduction-to-building-a-search-engine-2/","summary":"\u003cp\u003e网络爬虫又称网络蜘蛛、Web采集器等，它是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。\u003c/p\u003e\n\u003cp\u003e我们在设计网络爬虫的时候需要注意两点：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e鲁棒性\u003c/strong\u003e。Web中有些服务器会制造采集器陷阱（spider traps），这些陷阱服务器实际上是Web页面的生成器，它能在某个域下生成无数网页，从而使采集器陷入到一个无限的采集循环中去。采集器必须能从这些陷阱中跳出来。当然，这些陷阱倒不一定都是恶意的，有时可能是网站设计疏忽所导致的结果。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e礼貌性\u003c/strong\u003e。Web服务器具有一些隐式或显式的政策来控制采集器访问它们的频率。设计采集器时必须要遵守这些代表礼貌性的访问政策。\u003c/p\u003e\n\u003cp\u003e采集器的基本架构如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"the basic crawler architecture\" loading=\"lazy\" src=\"/posts/2016-01-04-introduction-to-building-a-search-engine-2/the-basic-crawler-architecture.png\"\u003e\u003c/p\u003e\n\u003cp\u003e基本上是“抓取→分析→得到新的URL→再抓取→再分析”这样一个死循环过程。\u003c/p\u003e\n\u003cp\u003e以上内容摘自王斌老师翻译的《信息检索导论》课本。\u003c/p\u003e\n\u003cp\u003e由于我们要做的是一个新闻搜索引擎，所以抓取的是新闻数据，对于爬虫，网上也有很多的开源程序，如nutch等，Github上还有人专门开发了抓取新闻的组件\u003ca href=\"https://github.com/codelucas/newspaper\"\u003enewspaper\u003c/a\u003e，可以很方便的提取新闻标题、正文、时间等信息。不过用python写爬虫也是分分钟的事情，下面我们一起来试一试。\u003c/p\u003e\n\u003cp\u003e首先找一个新闻网站，为简单起见，要找那种结构清晰、html代码便于解析的门户网站，比如\u003ca href=\"http://news.sohu.com/1/0903/61/subject212846158.shtml\"\u003e搜狐新闻\u003c/a\u003e、\u003ca href=\"http://www.cankaoxiaoxi.com/china/szyw/\"\u003e参考消息\u003c/a\u003e等。\u003c/p\u003e\n\u003cp\u003e搜狐新闻的国内要闻列表如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"sohu news1\" loading=\"lazy\" src=\"/posts/2016-01-04-introduction-to-building-a-search-engine-2/sohu-news1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e结构非常清楚，左边是带URL的标题，右边括号里有新闻时间。这一页列表就有200条新闻，如果我们要获取1000条，只要不断模拟点击下一页即可。下一页的URL也只是在首页的基础上加上_xxx.shtml，xxx就是不同的页码。\u003c/p\u003e\n\u003cp\u003e查看列表的html源码，得知列表都在类名为newsblue1的td中，所以只需要解析html源码就可以得到新闻标题、URL和时间，python解析html可以用BeautifulSoup包，非常方便。\u003c/p\u003e\n\u003cp\u003e进入到新闻详细页面，正文部分如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"sohu news2\" loading=\"lazy\" src=\"/posts/2016-01-04-introduction-to-building-a-search-engine-2/sohu-news2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e查看html源码，正文位于类名为text clear的div中，据此可以很方便的提取新闻正文。\u003c/p\u003e\n\u003cp\u003e得到一条新闻的所有数据之后，我们需要将之结构化成xml文件，借助相应的xml包可以很方便的完成这项工作。xml格式定义如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"xml format\" loading=\"lazy\" src=\"/posts/2016-01-04-introduction-to-building-a-search-engine-2/xml-format.png\"\u003e\u003c/p\u003e\n\u003cp\u003e注意爬虫需要访问网络，难免会出现一些异常，所以捕获异常是非常有必要的。另外，搜狐每篇新闻正文后面都会有一段’//’开始的注释，这个需要过滤掉，短于140个字的新闻我也过滤掉了。整个搜索系统的配置参数都存储在config.ini文件中。\u003c/p\u003e\n\u003cp\u003e下面是完整的python 3.4+代码。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e12\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e13\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e14\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e15\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e16\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e17\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e18\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e19\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e20\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e21\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e22\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e23\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e24\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e25\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e26\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e27\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e28\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e29\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e30\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e31\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e32\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e33\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e34\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e35\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e36\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e37\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e38\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e39\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e40\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e41\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e42\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e43\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e44\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e45\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e46\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e47\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e48\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e49\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e50\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e51\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e52\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e53\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e54\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e55\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e56\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e57\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e58\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e59\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e60\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e61\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e62\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e63\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e64\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e65\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e66\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e67\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e68\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e69\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e70\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e71\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e72\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e73\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e74\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e75\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# -*- coding: utf-8 -*-\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003eCreated on Sat Dec 19 11:57:01 2015\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e@author: bitjoy.net\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e bs4 \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e BeautifulSoup\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e urllib.request\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e xml.etree.ElementTree \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e ET\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e configparser\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eget_news_pool\u003c/span\u003e(root, start, end):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    news_pool \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(start,end,\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        page_url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e start:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            page_url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;_\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e.shtml\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(i)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            page_url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e root \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;.shtml\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003etry\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            response \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e urllib\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erequest\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eurlopen(page_url)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eexcept\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eException\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;—–\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e—–\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(type(e), page_url))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        html \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e response\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        soup \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e BeautifulSoup(html)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        td \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e soup\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;td\u0026#39;\u003c/span\u003e, class_ \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;newsblue1\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        a \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e td\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind_all(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;a\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        span \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e td\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind_all(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;span\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(len(a)):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            date_time \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e span[i]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estring\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e a[i]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eget(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;href\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            title \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e a[i]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estring\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            news_info \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;2016-\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003edate_time[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e:\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;-\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003edate_time[\u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e:\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;:00\u0026#39;\u003c/span\u003e,url,title]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            news_pool\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(news_info)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e(news_pool)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecrawl_news\u003c/span\u003e(news_pool, min_body_len, doc_dir_path, doc_encoding):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    i \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e news \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e news_pool:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003etry\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            response \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e urllib\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erequest\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eurlopen(news[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eexcept\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eException\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;—–\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e—–\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(type(e), news[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        html \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e response\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        soup \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e BeautifulSoup(html)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003etry\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e soup\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;div\u0026#39;\u003c/span\u003e, class_ \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text clear\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efind(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;div\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eget_text()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eexcept\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eException\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;—–\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e—–\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(type(e), news[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;//\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e body:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e body[:body\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eindex(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;//\u0026#39;\u003c/span\u003e)]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            body \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e body\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ereplace(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34; \u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e len(body) \u003cspan style=\"color:#f92672\"\u003e\u0026lt;=\u003c/span\u003e min_body_len:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        doc \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eElement(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;doc\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSubElement(doc, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(i)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSubElement(doc, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;url\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e news[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSubElement(doc, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;title\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e news[\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSubElement(doc, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;datetime\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e news[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSubElement(doc, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;body\u0026#34;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etext \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e body\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        tree \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e ET\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eElementTree(doc)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        tree\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite(doc_dir_path \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e.xml\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(i), encoding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e doc_encoding, xml_declaration \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        i \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;__main__\u0026#39;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e configparser\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eConfigParser()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;../config.ini\u0026#39;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;utf-8\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    root \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;http://news.sohu.com/1/0903/61/subject212846158\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    news_pool \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e get_news_pool(root, \u003cspan style=\"color:#ae81ff\"\u003e854\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e849\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    crawl_news(news_pool, \u003cspan style=\"color:#ae81ff\"\u003e140\u003c/span\u003e, config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_dir_path\u0026#39;\u003c/span\u003e], config[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;DEFAULT\u0026#39;\u003c/span\u003e][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;doc_encoding\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;done!\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003chr\u003e\n\u003cp\u003e完整可运行的新闻搜索引擎Demo请看我的Github项目\u003ca href=\"https://github.com/01joy/news_search_engine\"\u003enews_search_engine\u003c/a\u003e。\u003c/p\u003e","title":"和我一起构建搜索引擎（二）网络爬虫"},{"content":"我们上网用得最多的一项服务应该是搜索，不管大事小情，都喜欢百度一下或谷歌一下，那么百度和谷歌是怎样从浩瀚的网络世界中快速找到你想要的信息呢，这就是搜索引擎的艺术，属于信息检索的范畴。\n这学期学习了《现代信息检索》课程，使用的是Stanford的教材Introduction to Information Retrieval，网上有电子版，大家可以参考。\n本课程的大作业是完成一个新闻搜索引擎，要求是这样的：定向采集3-4个新闻网站，实现这些网站信息的抽取、索引和检索。网页数目不少于10万条。能按相关度、时间和热度（需要自己定义）进行排序，能实现相似新闻的自动聚类。\n截止日期12月31日，我们已经在规定时间完成了该系统，自认为检索效果不错，所以在此把过程记录如下，欢迎讨论。\n网上有很多开源的全文搜索引擎，比如Lucene、Sphinx、Whoosh等，都提供了很好的API，开发者只需要调用相关接口就可以实现一个全功能的搜索引擎。不过既然学习了IR这门课，自然要把相关技术实践一下，所以我们打算自己实现一个搜索引擎。\n这是简介部分，主要介绍整个搜索引擎的思路和框架。\n上图为本搜索引擎的框架图。首先爬虫程序从特定的几个新闻网站抓取新闻数据，然后过滤网页中的图片、视频、广告等无关元素，抽取新闻的主体内容，得到结构化的xml数据。然后一方面使用内存式单遍扫描索引构建方法（SPIMI）构建倒排索引，供检索模型使用；另一方面根据向量空间模型计算两两新闻之间的余弦相似度，供推荐模块使用。最后利用概率检索模型中的BM25公式计算给定关键词下的文档相关性评分，BM25打分结合时间因素得到热度评分，根据评分给出排序结果。\n在后续博文中，我会详细介绍每个部分的实现。\n完整可运行的新闻搜索引擎Demo请看我的Github项目news_search_engine。\n以下是系列博客：\n和我一起构建搜索引擎（一）简介\n和我一起构建搜索引擎（二）网络爬虫\n和我一起构建搜索引擎（三）构建索引\n和我一起构建搜索引擎（四）检索模型\n和我一起构建搜索引擎（五）推荐阅读\n和我一起构建搜索引擎（六）系统展示\n和我一起构建搜索引擎（七）总结展望\n","permalink":"http://localhost:1313/posts/2016-01-04-introduction-to-building-a-search-engine-1/","summary":"\u003cp\u003e我们上网用得最多的一项服务应该是搜索，不管大事小情，都喜欢百度一下或谷歌一下，那么百度和谷歌是怎样从浩瀚的网络世界中快速找到你想要的信息呢，这就是搜索引擎的艺术，属于信息检索的范畴。\u003c/p\u003e\n\u003cp\u003e这学期学习了《现代信息检索》课程，使用的是Stanford的教材\u003ca href=\"http://nlp.stanford.edu/IR-book/\"\u003eIntroduction to Information Retrieval\u003c/a\u003e，网上有电子版，大家可以参考。\u003c/p\u003e\n\u003cp\u003e本课程的大作业是完成一个新闻搜索引擎，要求是这样的：定向采集3-4个新闻网站，实现这些网站信息的抽取、索引和检索。网页数目不少于10万条。能按相关度、时间和热度（需要自己定义）进行排序，能实现相似新闻的自动聚类。\u003c/p\u003e\n\u003cp\u003e截止日期12月31日，我们已经在规定时间完成了该系统，自认为检索效果不错，所以在此把过程记录如下，欢迎讨论。\u003c/p\u003e\n\u003cp\u003e网上有很多开源的全文搜索引擎，比如Lucene、Sphinx、Whoosh等，都提供了很好的API，开发者只需要调用相关接口就可以实现一个全功能的搜索引擎。不过既然学习了IR这门课，自然要把相关技术实践一下，所以我们打算自己实现一个搜索引擎。\u003c/p\u003e\n\u003cp\u003e这是简介部分，主要介绍整个搜索引擎的思路和框架。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"search engine outline\" loading=\"lazy\" src=\"/posts/2016-01-04-introduction-to-building-a-search-engine-1/search-engine-architecture.png\"\u003e\u003c/p\u003e\n\u003cp\u003e上图为本搜索引擎的框架图。首先爬虫程序从特定的几个新闻网站抓取新闻数据，然后过滤网页中的图片、视频、广告等无关元素，抽取新闻的主体内容，得到结构化的xml数据。然后一方面使用内存式单遍扫描索引构建方法（SPIMI）构建倒排索引，供检索模型使用；另一方面根据向量空间模型计算两两新闻之间的余弦相似度，供推荐模块使用。最后利用概率检索模型中的BM25公式计算给定关键词下的文档相关性评分，BM25打分结合时间因素得到热度评分，根据评分给出排序结果。\u003c/p\u003e\n\u003cp\u003e在后续博文中，我会详细介绍每个部分的实现。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e完整可运行的新闻搜索引擎Demo请看我的Github项目\u003ca href=\"https://github.com/01joy/news_search_engine\"\u003enews_search_engine\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e以下是系列博客：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-1/\"\u003e和我一起构建搜索引擎（一）简介\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-04-introduction-to-building-a-search-engine-2/\"\u003e和我一起构建搜索引擎（二）网络爬虫\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-3/\"\u003e和我一起构建搜索引擎（三）构建索引\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-07-introduction-to-building-a-search-engine-4/\"\u003e和我一起构建搜索引擎（四）检索模型\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-5/\"\u003e和我一起构建搜索引擎（五）推荐阅读\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-6/\"\u003e和我一起构建搜索引擎（六）系统展示\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://bitjoy.net/posts/2016-01-09-introduction-to-building-a-search-engine-7/\"\u003e和我一起构建搜索引擎（七）总结展望\u003c/a\u003e\u003c/p\u003e","title":"和我一起构建搜索引擎（一）简介"},{"content":"突然发现，从小到大，自己做事都做得很慢，别人一会做完的作业，我可能要花好几个小时。但拿作业一对比，明显能看出差距，自己精雕细琢的作品不是别人随随便便就能比的。\n最近几次和同学合作完成大作业也遇到了类似的情况，数据抓取的同学给我的数据，不是格式不对就是内容缺胳膊少腿，质量极其差，还不愿修改，曰：只是做一个演示系统，有数据就行了。他不知道他这样的数据给我，我们后面做得再好，最终的演示效果也不会好，他这样的随意，后面的人不知要多花多少时间来弥补。我也无意跟他多费口舌，自己挽起袖子重做了他的工作。\n类似的事情，我遇到的不在少数，和别人沟通的时间远远超过了自己完成任务的时间。所以往往一个很简单的工作，我要花比别人多两到三倍的时间。这个过程就像工匠在雕琢自己的作品，是不计时间的，直到自己认为完美为止。这大概就是老罗所说的工匠精神吧。\n在一个完美主义者的眼里，这是一个千疮百孔的世界。\n糟糕的文档排版，错别字和错误标点一堆，一群人并排走挡了后面或对面的人，开水房离宿舍十万八千里，蚊香的设计，电脑接口位置的设计，U盘接口的设计，凸出的摄像头，插队，说脏话。。。\n当然也有同学劝我，这些东西差不多就行了，何必花这么多时间做这么好干什么，还不如去看个电影打个球。也经常听人说Take it easy，别太认真，认真你就输了。\n但是我始终相信，态度决定一切。你一天认真做了，别人不一定看得到，但坚持一个月甚至一年，总会有志同道合的人发现你，而你的坚持也将一点点的改变这个行业这个世界。就像老罗做手机，虽然销量不怎么样，但他的工匠精神、他的情怀，值得每一个人尊敬。T2统一听筒和各种传感器的位置、消失的电源键、消失的SIM卡插槽、消失的金属中框断点完全是超出iPhone的美好设计。希望老罗的情怀之路能够坚持下去、越走越远。\n","permalink":"http://localhost:1313/posts/2016-01-04-attitude-is-everything/","summary":"\u003cp\u003e突然发现，从小到大，自己做事都做得很慢，别人一会做完的作业，我可能要花好几个小时。但拿作业一对比，明显能看出差距，自己精雕细琢的作品不是别人随随便便就能比的。\u003c/p\u003e\n\u003cp\u003e最近几次和同学合作完成大作业也遇到了类似的情况，数据抓取的同学给我的数据，不是格式不对就是内容缺胳膊少腿，质量极其差，还不愿修改，曰：只是做一个演示系统，有数据就行了。他不知道他这样的数据给我，我们后面做得再好，最终的演示效果也不会好，他这样的随意，后面的人不知要多花多少时间来弥补。我也无意跟他多费口舌，自己挽起袖子重做了他的工作。\u003c/p\u003e\n\u003cp\u003e类似的事情，我遇到的不在少数，和别人沟通的时间远远超过了自己完成任务的时间。所以往往一个很简单的工作，我要花比别人多两到三倍的时间。这个过程就像工匠在雕琢自己的作品，是不计时间的，直到自己认为完美为止。这大概就是老罗所说的工匠精神吧。\u003c/p\u003e\n\u003cp\u003e在一个完美主义者的眼里，这是一个千疮百孔的世界。\u003c/p\u003e\n\u003cp\u003e糟糕的文档排版，错别字和错误标点一堆，一群人并排走挡了后面或对面的人，开水房离宿舍十万八千里，蚊香的设计，电脑接口位置的设计，U盘接口的设计，凸出的摄像头，插队，说脏话。。。\u003c/p\u003e\n\u003cp\u003e当然也有同学劝我，这些东西差不多就行了，何必花这么多时间做这么好干什么，还不如去看个电影打个球。也经常听人说Take it easy，别太认真，认真你就输了。\u003c/p\u003e\n\u003cp\u003e但是我始终相信，态度决定一切。你一天认真做了，别人不一定看得到，但坚持一个月甚至一年，总会有志同道合的人发现你，而你的坚持也将一点点的改变这个行业这个世界。就像老罗做手机，虽然销量不怎么样，但他的工匠精神、他的情怀，值得每一个人尊敬。T2统一听筒和各种传感器的位置、消失的电源键、消失的SIM卡插槽、消失的金属中框断点完全是超出iPhone的美好设计。希望老罗的情怀之路能够坚持下去、越走越远。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"smartisan-T2-2015\" loading=\"lazy\" src=\"/posts/2016-01-04-attitude-is-everything/smartisan-T2-2015.jpg\"\u003e\u003c/p\u003e","title":"认真你就赢了"},{"content":"2015年过得好快，梳理一下，2015年的时间线大概是这样的：\n3月来北京计算所做毕设→5月返回武大修改论文→5月30公开答辩→6月毕业季→7月回北京计算所→8月回家陪父母→9月国科大开学→持续高强度的学习→2016元旦还在图书馆研究NPC问题。\n2015年给我的总体感受是很忙，但忙的事情都很琐碎，并没有什么大的里程碑事件，不过以下三件事情我认为值得一提。\n本科四年修成正果，研究生三年新的起航 买了一辆属于自己的山地车，1k2，虽然是二手的，但足够我骑着它去看世界了:-) 也许是在城市里待久了，我特别享受这种亲近大自然的感觉，蓝天、白云、草原、大海这些美景永远也看不够。\n在国科大认识了两个好基友，虽然都是单身汪，但至少想看电影吃火锅的时候还可以有个伴。（此处居然少了三人合照） 2015年共写了14篇博客，包含3篇技术博客，bitjoy.net 历史累计PV1039，UV520，IP502。\n展望2016年，大的方向基本都确定了，目标如下：\n完成国科大下学期的课程任务 接手pLink软件 刷完LeetCode所有题目 读10本书 去电影院看10场电影（2015下半年在怀柔村里没看一部电影/(ㄒoㄒ)/~~） 改正坐姿 大家一起见证！\n","permalink":"http://localhost:1313/posts/2016-01-03-2016-happy-new-year/","summary":"\u003cp\u003e2015年过得好快，梳理一下，2015年的时间线大概是这样的：\u003c/p\u003e\n\u003cp\u003e3月来北京计算所做毕设→5月返回武大修改论文→5月30公开答辩→6月毕业季→7月回北京计算所→8月回家陪父母→9月国科大开学→持续高强度的学习→2016元旦还在图书馆研究NPC问题。\u003c/p\u003e\n\u003cp\u003e2015年给我的总体感受是很忙，但忙的事情都很琐碎，并没有什么大的里程碑事件，不过以下三件事情我认为值得一提。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e本科四年修成正果，研究生三年新的起航\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"whu_certificate\" loading=\"lazy\" src=\"/posts/2016-01-03-2016-happy-new-year/whu_certificate.jpg\"\u003e\n\u003cimg alt=\"ucas_admission\" loading=\"lazy\" src=\"/posts/2016-01-03-2016-happy-new-year/ucas_admission.jpg\"\u003e\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e买了一辆属于自己的山地车，1k2，虽然是二手的，但足够我骑着它去看世界了:-)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"bike\" loading=\"lazy\" src=\"/posts/2016-01-03-2016-happy-new-year/bike.jpg\"\u003e\n\u003cimg alt=\"2015_cycling_1\" loading=\"lazy\" src=\"/posts/2016-01-03-2016-happy-new-year/2015_cycling_1.jpg\"\u003e\n\u003cimg alt=\"2015_cycling_2\" loading=\"lazy\" src=\"/posts/2016-01-03-2016-happy-new-year/2015_cycling_2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e也许是在城市里待久了，我特别享受这种亲近大自然的感觉，蓝天、白云、草原、大海这些美景永远也看不够。\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e在国科大认识了两个好基友，虽然都是单身汪，但至少想看电影吃火锅的时候还可以有个伴。（此处居然少了三人合照）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e2015年共写了14篇博客，包含3篇技术博客，bitjoy.net 历史累计PV1039，UV520，IP502。\u003c/p\u003e\n\u003cp\u003e展望2016年，大的方向基本都确定了，目标如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e完成国科大下学期的课程任务\u003c/li\u003e\n\u003cli\u003e接手pLink软件\u003c/li\u003e\n\u003cli\u003e刷完LeetCode所有题目\u003c/li\u003e\n\u003cli\u003e读10本书\u003c/li\u003e\n\u003cli\u003e去电影院看10场电影（2015下半年在怀柔村里没看一部电影/(ㄒoㄒ)/~~）\u003c/li\u003e\n\u003cli\u003e改正坐姿\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e大家一起见证！\u003c/p\u003e","title":"2016新年快乐"},{"content":"\n第一次在北方过冬，今年北京11月6日就下雪了，然而我在广州的小伙伴还穿着短袖吃着冰棍呢。。。\n北京2015年的第一场雪，比以往时候来的更早一些\n今天又下起了第二场雪，下了整整两天的大雪，然而我房间的暖气却不暖了，大叔来修了两次，无功而返，说是一楼的宿舍暖气都有问题，当初设计有缺陷(╯‵□′)╯︵┻━┻\n这样也好，给了自己去图书馆的理由❉\n","permalink":"http://localhost:1313/posts/2015-11-22-snow-in-beijing/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/Von_Koch_curve.gif\"\u003e\u003c/p\u003e\n\u003cp\u003e第一次在北方过冬，今年北京11月6日就下雪了，然而我在广州的小伙伴还穿着短袖吃着冰棍呢。。。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"2015_11_06_beijing_snow\" loading=\"lazy\" src=\"/posts/2015-11-22-snow-in-beijing/2015_11_06_beijing_snow.jpg\"\u003e\n北京2015年的第一场雪，比以往时候来的更早一些\u003c/p\u003e\n\u003cp\u003e今天又下起了第二场雪，下了整整两天的大雪，然而我房间的暖气却不暖了，大叔来修了两次，无功而返，说是一楼的宿舍暖气都有问题，当初设计有缺陷(╯‵□′)╯︵┻━┻\u003c/p\u003e\n\u003cp\u003e这样也好，给了自己去图书馆的理由❉\u003c/p\u003e","title":"北国的雪"},{"content":"安装WIN10一个月以来，校园有线网经常间歇性断网，通常是20分钟不到就断了，需要重启或者把有线连接关闭再打开才可以。在微博上问过微软客服也无果，后来Google到某国外的解决办法，现记录如下。\n说到底WIN10断网的问题还是和驱动有关，先看一下我的有线网卡Broadcom NetLink (TM) Gigabit Ethernet，驱动信息是这样的：\n还是13年的驱动，版本号是15.6.0.14，于是第一想到的是更新驱动。点击驱动右键选更新-\u0026gt;自动搜索更新的驱动程序软件-\u0026gt;提示“已安装适合设备的最佳驱动程序软件”，但这明明不是最新的驱动啊！\n于是在Broadcom的官网上找到了最新驱动win_b57_x64-17.2.0.2，版本号是17.2.0.2，更新日期2015-10-27，原来这才是最新的驱动。\n（2018.1.25更新：上面的地址已失效，最新地址请点击此处，并选择DOWNLOADS→Software→NetLink®/NetXtreme® I Desktop/Mobile/Server (x64)，也可以从本站下载。）\n在安装最新驱动之前，我们需要关闭WIN10的自动更新驱动功能，因为WIN10会认为它的15.6.0.14版本是最新的，在windows update时把实际最新的17.2.0.2版本替换掉。具体做法是在Cortana中搜索“更改设备安装设置”并打开，选择否，从不安装来自Windows更新的驱动程序软件，如下。\n然后重启进入安全模式，再次在设备管理器中右键点击网卡驱动，选择更新-\u0026gt;浏览计算机以查找驱动程序软件-\u0026gt;从计算机的设备驱动程序列表中选取-\u0026gt;点击从磁盘安装按钮-\u0026gt;浏览找到你之前在网上下载的最新驱动（*.inf格式）-\u0026gt;选中-\u0026gt;依次确定。刷新之后再次查看驱动信息如下：\n可以看到驱动已经更新到最新的版本了。再次重启进入正常模式，目前用了两天了也没有再断过网。\n其他WIN10驱动问题应该也可以用类似的方法解决。\n（话说我的WIN10偶尔会死机，就是用着用着突然鼠标和键盘完全动不了了，只能强制重启，有谁知道这是怎么回事吗？）\n","permalink":"http://localhost:1313/posts/2015-11-13-solution-to-win10s-network-problem/","summary":"\u003cp\u003e安装WIN10一个月以来，校园有线网经常间歇性断网，通常是20分钟不到就断了，需要重启或者把有线连接关闭再打开才可以。在微博上问过微软客服也无果，\u003ca href=\"http://www.pcadvisor.co.uk/forum/windows-29/windows-10-no-internet-trough-ethernet-4540238/\"\u003e后来Google到某国外的解决办法\u003c/a\u003e，现记录如下。\u003c/p\u003e\n\u003cp\u003e说到底WIN10断网的问题还是和驱动有关，先看一下我的有线网卡Broadcom NetLink (TM) Gigabit Ethernet，驱动信息是这样的：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"bcm-driver-before\" loading=\"lazy\" src=\"/posts/2015-11-13-solution-to-win10s-network-problem/bcm-driver-before.png\"\u003e\u003c/p\u003e\n\u003cp\u003e还是13年的驱动，版本号是15.6.0.14，于是第一想到的是更新驱动。点击驱动右键选更新-\u0026gt;自动搜索更新的驱动程序软件-\u0026gt;提示“已安装适合设备的最佳驱动程序软件”，但这明明不是最新的驱动啊！\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.broadcom.com/support/ethernet-nic-netxtreme-i-desktop-mobile\"\u003e于是在Broadcom的官网上找到了最新驱动win_b57_x64-17.2.0.2\u003c/a\u003e，版本号是17.2.0.2，更新日期2015-10-27，原来这才是最新的驱动。\u003c/p\u003e\n\u003cp\u003e（\u003cstrong\u003e2018.1.25\u003c/strong\u003e更新：上面的地址已失效，\u003ca href=\"https://www.broadcom.cn/products/ethernet-connectivity/controllers/bcm5720#downloads\"\u003e最新地址请点击此处\u003c/a\u003e，并选择DOWNLOADS→Software→NetLink®/NetXtreme® I Desktop/Mobile/Server (x64)，\u003ca href=\"/posts/2015-11-13-solution-to-win10s-network-problem/win_b57_x64-17.2.0.2.zip\"\u003e也可以从本站下载\u003c/a\u003e。）\u003c/p\u003e\n\u003cp\u003e在安装最新驱动之前，我们需要关闭WIN10的自动更新驱动功能，因为WIN10会认为它的15.6.0.14版本是最新的，在windows update时把实际最新的17.2.0.2版本替换掉。具体做法是在Cortana中搜索“\u003cstrong\u003e更改设备安装设置\u003c/strong\u003e”并打开，选择否，从不安装来自Windows更新的驱动程序软件，如下。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"change-device-installation\" loading=\"lazy\" src=\"/posts/2015-11-13-solution-to-win10s-network-problem/change-device-installation.png\"\u003e\u003c/p\u003e\n\u003cp\u003e然后\u003ca href=\"http://jingyan.baidu.com/article/fea4511a72cb38f7ba912543.html\"\u003e重启进入安全模式\u003c/a\u003e，再次在设备管理器中右键点击网卡驱动，选择更新-\u0026gt;浏览计算机以查找驱动程序软件-\u0026gt;从计算机的设备驱动程序列表中选取-\u0026gt;点击从磁盘安装按钮-\u0026gt;浏览找到你之前在网上下载的最新驱动（*.inf格式）-\u0026gt;选中-\u0026gt;依次确定。刷新之后再次查看驱动信息如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"bcm-driver-after\" loading=\"lazy\" src=\"/posts/2015-11-13-solution-to-win10s-network-problem/bcm-driver-after.png\"\u003e\u003c/p\u003e\n\u003cp\u003e可以看到驱动已经更新到最新的版本了。再次重启进入正常模式，目前用了两天了也没有再断过网。\u003c/p\u003e\n\u003cp\u003e其他WIN10驱动问题应该也可以用类似的方法解决。\u003c/p\u003e\n\u003cp\u003e（话说我的WIN10偶尔会死机，就是用着用着突然鼠标和键盘完全动不了了，只能强制重启，有谁知道这是怎么回事吗？）\u003c/p\u003e","title":"解决Win10间歇性断网的问题"},{"content":"大家好，施一公老师的26篇博客大概可以分为四类：1）讲述个人生活经历2）评论社会问题3）讨论国家科技和人才引进政策4）介绍学习方法。这些博客比较全面地反应了施一公的求学经历、由学生到教授的转变过程以及回国之后为中国人才引进所做出的努力。\n给我感触最深的有3点：1）环境对人的影响很大2）坚持总会有所收获3）做一个有担当、有社会责任感的科研人。\n1）环境对人的影响很大 《从\u0026lt;高考1977\u0026gt;说起》这篇博客详细介绍了施一公高考前的家庭情况，施一公的父亲是哈工大毕业，母亲是北京矿业学院（今中国矿业大学）毕业，在上世纪五六十年代，父母都是名校大学毕业，可谓是少有的知识分子家庭。施一公还有两个姐姐、一个哥哥、一个表哥和一个表姐，哥哥姐姐们刻苦的学习、父亲悉心的辅导以及不错的高考成绩对施一公产生了很大的影响，争强好胜的施一公自然不甘示弱，以84年全国数学联赛省第一名的成绩保送清华。诚然，施一公的成绩和他自己的刻苦努力分不开，但是从小良好的家庭氛围也功不可没。\n2）坚持总会有所收获 这可以从施一公的两个例子中看出。\n《今天3000米》讲到施一公从82年跑步的“倒数第一“、“颜面尽失”之后开始坚持长跑，直到89年从未间断，在85年的清华新生运动会3000米竞走中，以16分10秒轻松获得第一名。\n跑步这件事我也深有体会，我高中几乎没有体育锻炼，大一刚入学的体能测试中，1000米项目跑了4’15’’，小组倒数第一，跑完全程脸都发白，当时真的担心大学因体育挂科毕不了业。后来大三下的时候，开始坚持跑步，一开始每晚跑3圈，一个月后加一圈，最后稳定在每晚跑5圈，一直坚持到毕业。在毕业体能测试中，还是1000米项目，我居然跑了3’42’’，小组顺数第一名，跑完之后虽然有点累，但并不感觉难受，连我自己都不太相信。\n《如何做一名优秀的博士生：（一）时间的付出》中，施一公讲到他在留学期间的时间付出。“留学的第一年，我情绪波动很大，内心浮躁而迷茫，根本无心念书。”“第二年，每周五天、每天从上午9点做实验到晚上7、8点，周末也会去两个半天。””到了第三年，晚上常常干到11点多，赶最后一班校车从霍普金斯医学院回Homewood campus（我住在附近）。””研究生阶段后期，我的刻苦在实验室是出了名的。每天晚上做实验到半夜三点左右，回到住处躺下来睡觉时常常已是四点以后；但每天早晨八点都会被窗外纽约第一大道(First Avenue)上的汽车喧闹声吵醒，九点左右又回到实验室开始了新的一天…”\n施一公几年如一日的坚持没有白费，他顺利毕业并获得名校终生教职席位。\n其实正如H老师所说“以大多数人努力的程度，根本还没到拼智商的时候。” 坚持做一件事，点滴积累，做到极致。无论什么事情，坚持做下去，一定能有所收获，对于体力活更是如此。\n3）做一个有担当、有社会责任感的科研人 在读施一公博客的时候，心潮澎湃，热血沸腾，无论是施一公自己排除万难坚持回国的行动，还是施一公回国之后号召海龟回国的倡议，亦或是施一公为人才引进，千人计划建言献策的付出，都真真切切的体现了他的强烈的爱国热情。\n施一公回国后的去私心、敢担当、有作为，坚持职业操守，“我申请基金的时候一定不和评委在评审前或评审后做任何形式的私下沟通；我当评委的时候一定不和申请人在评审前或评审后做任何形式的私下沟通”等都在用切身行动一点点改善国内的科研环境。\n在pFind组，H老师也时常教导我们要对学术保留一点敬畏之心，做好科研，尽自己一份力改善国际社会对中国学术界的看法。\n总的来说，施一公老师的博客内容丰富，让我受益匪浅，也给了我很多启发，关于如何做一名合格的研究生，我还完全是门外汉，前面的师兄师姐都给出了很多方法论的解读，我也把施一公关于如何做一名优秀的博士生的几个要点罗列如下，希望用此标准来要求自己。\n如何一名优秀的博士生：\n时间的付出 方法论的转变 正确分析负面结果 耗费时间的完美主义阻碍创新进取 科研文献与学术讲座的取与舍 挑战传统思维 祝大家工作顺利！\n-bitJoy\n","permalink":"http://localhost:1313/posts/2015-10-31-review-about-shiyigongs-blogs/","summary":"\u003cp\u003e大家好，\u003ca href=\"http://blog.sciencenet.cn/home.php?mod=space\u0026amp;uid=46212\u0026amp;do=blog\u0026amp;view=me\u0026amp;from=space\"\u003e施一公老师的26篇博客\u003c/a\u003e大概可以分为四类：1）讲述个人生活经历2）评论社会问题3）讨论国家科技和人才引进政策4）介绍学习方法。这些博客比较全面地反应了施一公的求学经历、由学生到教授的转变过程以及回国之后为中国人才引进所做出的努力。\u003c/p\u003e\n\u003cp\u003e给我感触最深的有3点：1）环境对人的影响很大2）坚持总会有所收获3）做一个有担当、有社会责任感的科研人。\u003c/p\u003e\n\u003ch1 id=\"1环境对人的影响很大\"\u003e1）环境对人的影响很大\u003c/h1\u003e\n\u003cp\u003e《从\u0026lt;高考1977\u0026gt;说起》这篇博客详细介绍了施一公高考前的家庭情况，施一公的父亲是哈工大毕业，母亲是北京矿业学院（今中国矿业大学）毕业，在上世纪五六十年代，父母都是名校大学毕业，可谓是少有的知识分子家庭。施一公还有两个姐姐、一个哥哥、一个表哥和一个表姐，哥哥姐姐们刻苦的学习、父亲悉心的辅导以及不错的高考成绩对施一公产生了很大的影响，争强好胜的施一公自然不甘示弱，以84年全国数学联赛省第一名的成绩保送清华。诚然，施一公的成绩和他自己的刻苦努力分不开，但是从小良好的家庭氛围也功不可没。\u003c/p\u003e\n\u003ch1 id=\"2坚持总会有所收获\"\u003e2）坚持总会有所收获\u003c/h1\u003e\n\u003cp\u003e这可以从施一公的两个例子中看出。\u003c/p\u003e\n\u003cp\u003e《今天3000米》讲到施一公从82年跑步的“倒数第一“、“颜面尽失”之后开始坚持长跑，直到89年从未间断，在85年的清华新生运动会3000米竞走中，以16分10秒轻松获得第一名。\u003c/p\u003e\n\u003cp\u003e跑步这件事我也深有体会，我高中几乎没有体育锻炼，大一刚入学的体能测试中，1000米项目跑了4’15’’，小组倒数第一，跑完全程脸都发白，当时真的担心大学因体育挂科毕不了业。后来大三下的时候，开始坚持跑步，一开始每晚跑3圈，一个月后加一圈，最后稳定在每晚跑5圈，一直坚持到毕业。在毕业体能测试中，还是1000米项目，我居然跑了3’42’’，小组顺数第一名，跑完之后虽然有点累，但并不感觉难受，连我自己都不太相信。\u003c/p\u003e\n\u003cp\u003e《如何做一名优秀的博士生：（一）时间的付出》中，施一公讲到他在留学期间的时间付出。“留学的第一年，我情绪波动很大，内心浮躁而迷茫，根本无心念书。”“第二年，每周五天、每天从上午9点做实验到晚上7、8点，周末也会去两个半天。””到了第三年，晚上常常干到11点多，赶最后一班校车从霍普金斯医学院回Homewood campus（我住在附近）。””研究生阶段后期，我的刻苦在实验室是出了名的。每天晚上做实验到半夜三点左右，回到住处躺下来睡觉时常常已是四点以后；但每天早晨八点都会被窗外纽约第一大道(First Avenue)上的汽车喧闹声吵醒，九点左右又回到实验室开始了新的一天…”\u003c/p\u003e\n\u003cp\u003e施一公几年如一日的坚持没有白费，他顺利毕业并获得名校终生教职席位。\u003c/p\u003e\n\u003cp\u003e其实正如H老师所说“\u003cstrong\u003e以大多数人努力的程度，根本还没到拼智商的时候。\u003c/strong\u003e” 坚持做一件事，点滴积累，做到极致。无论什么事情，坚持做下去，一定能有所收获，对于体力活更是如此。\u003c/p\u003e\n\u003ch1 id=\"3做一个有担当有社会责任感的科研人\"\u003e3）做一个有担当、有社会责任感的科研人\u003c/h1\u003e\n\u003cp\u003e在读施一公博客的时候，心潮澎湃，热血沸腾，无论是施一公自己排除万难坚持回国的行动，还是施一公回国之后号召海龟回国的倡议，亦或是施一公为人才引进，千人计划建言献策的付出，都真真切切的体现了他的强烈的爱国热情。\u003c/p\u003e\n\u003cp\u003e施一公回国后的去私心、敢担当、有作为，坚持职业操守，“我申请基金的时候一定不和评委在评审前或评审后做任何形式的私下沟通；我当评委的时候一定不和申请人在评审前或评审后做任何形式的私下沟通”等都在用切身行动一点点改善国内的科研环境。\u003c/p\u003e\n\u003cp\u003e在pFind组，H老师也时常教导我们要\u003cstrong\u003e对学术保留一点敬畏之心\u003c/strong\u003e，做好科研，尽自己一份力改善国际社会对中国学术界的看法。\u003c/p\u003e\n\u003cp\u003e总的来说，施一公老师的博客内容丰富，让我受益匪浅，也给了我很多启发，关于如何做一名合格的研究生，我还完全是门外汉，前面的师兄师姐都给出了很多方法论的解读，我也把施一公关于如何做一名优秀的博士生的几个要点罗列如下，希望用此标准来要求自己。\u003c/p\u003e\n\u003cp\u003e如何一名优秀的博士生：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e时间的付出\u003c/li\u003e\n\u003cli\u003e方法论的转变\n\u003col\u003e\n\u003cli\u003e正确分析负面结果\u003c/li\u003e\n\u003cli\u003e耗费时间的完美主义阻碍创新进取\u003c/li\u003e\n\u003cli\u003e科研文献与学术讲座的取与舍\u003c/li\u003e\n\u003cli\u003e挑战传统思维\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e祝大家工作顺利！\u003c/p\u003e\n\u003cp\u003e-bitJoy\u003c/p\u003e","title":"读施一公博客有感"},{"content":"进入研究生生涯完成的第一个新生培训作业是“2.5亿个浮点数的外部排序算法”，前后折腾了将近一个月，结果是在i7处理器上，限制512MB内存，排序用时250秒左右。\n这个作业的常规思路大部分人都能想到，按块读取文件-\u0026gt;atof转换为double-\u0026gt;内部快速排序或基数排序-\u0026gt;dtoa转换为char*-\u0026gt;按块写入文件。这里面中间的三个过程都很耗时，特别是atof和dtoa，因为精度只要求保留9位小数，所以可以自己实现atof和dtoa来加速，也可以使用多线程加速。\n整个作业都是基于对IEEE754浮点数的深刻理解展开的，所以下面详细讲解浮点数的一些知识。\nIEEE754双精度浮点数 目前大多数CPU内浮点数的表示都遵循IEEE754标准，IEEE754双精度浮点数（double）表示如下图所示。\nIEEE754 double在内存中的形式[1]\nsign bit：符号位，1位，用来表示正负号，0表示非负；1表示负 exponent：指数位，11位，用来表示次方数，是一个无符号数 fraction：尾数位，52位，用来表示精确度，也是一个无符号数，有些资料也叫做mantissa或significand 这种表示形式有两点需要注意。\n第一，既然exponent是无符号的，那么怎样表示负指数呢？\nIEEE754规定，二进制串中算得的e需要减去一个偏移量bias，对于double，bias=1023，即e’=e-bias。因为\\(e\\in[0,2^{11}-1]\\)，所以最终\\(e’\\in[-2^{10}+1,2^{10}]\\)。但是如果把e本身看作有符号数e”，则\\(e”\\in[-2^{10},2^{10}-1]\\)，既然e”和e’相差微小，为什么不直接把e看成有符号数，而非要把它看成无符号数，再减去一个偏移量bias呢？\n这是因为如果把e看成无符号数再减偏移量，浮点数大小比较速度更快。引用维基百科的一段话：\nBy arranging the fields so that the sign bit is in the most significant bit position, the biased exponent in the middle, then the mantissa in the least significant bits, the resulting value will be ordered properly, whether it’s interpreted as a floating point or integer value. This allows high speed comparisons of floating point numbers using fixed point hardware.\n对于两个正浮点数a和b，如果a\u0026gt;b，则a的二进制字符串的字典序也相应的在b的后面；对于负数则正好相反。也就是说，无论是把这个数看成浮点数还是整数，都可以通过只比较两个数的二进制串得出大小关系，而不需要通过公式计算其十进制值再比较大小，这显然加快了比较速度。\n浮点数的这个特性使得对浮点数排序也可以使用基数排序！很神奇吧，具体是这样的：先对二进制串进行分组，按先低位组后高位组对其进行基数排序；当到最高位组时，把负数放到正数的前面逆序排列，正数常规排列，得到的就是有序的排列。\n比如，假设把数看成无符号数时，会得到下面的基数排序结果，此时需要调整顺序，把正数统一移到后面，就是代码第50行：index += negatives；把负数移到前面的同时逆序排列，相当于在0的上面画一条线，然后-1,-2,-7以这条线做一个翻折对称，所以新的负数的下标变成了第48行的：index = n – index – 1。\n0　:　0000 1　:　0001 2　:　0010 4　:　0100 -1　:　1001 -2　:　1010 -7　:　1111　变成：\n-7　:　1111　-2　:　1010 -1　:　1001 0　:　0000 1　:　0001 2　:　0010 4　:　0100 具体的实现可以看这个讨论，下面是我实现的C++版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 void RadixSort(std::vector\u0026lt;double\u0026gt; \u0026amp;nums) { int n = nums.size(); vector\u0026lt;LL\u0026gt; t(n), a(n); for (int i = 0; i \u0026lt; n; i++) a[i] = *(LL*)(\u0026amp;nums[i]); //将double的二进制转换为long long int groupLength = 16; //可自定义 int bitLength = 64; int len = 1 \u0026lt;\u0026lt; groupLength; vector\u0026lt;int\u0026gt; count(len), pref(len); int groups = bitLength / groupLength; int mask = len - 1; int negatives = 0, positives = 0; for (int c = 0, shift = 0; c \u0026lt; groups; c++, shift += groupLength) { // reset count array fill(count.begin(), count.end(), 0); // counting elements of the c-th group for (int i = 0; i \u0026lt; n; i++) { ++count[(a[i] \u0026gt;\u0026gt; shift) \u0026amp; mask]; // additionally count all negative // values in first round if (c == 0 \u0026amp;\u0026amp; a[i] \u0026lt; 0) ++negatives; } if (c == 0) positives = n - negatives; // calculating prefixes pref[0] = 0; for (int i = 1; i \u0026lt; len; i++) pref[i] = pref[i - 1] + count[i - 1]; // from a[] to t[] elements ordered by c-th group for (int i = 0; i \u0026lt; n; i++) { // Get the right index to sort the number in int index = pref[(a[i] \u0026gt;\u0026gt; shift) \u0026amp; mask]++; if (c == groups - 1) { // We\u0026#39;re in the last (most significant) group, if the // number is negative, order them inversely in front // of the array, pushing positive ones back. if (a[i] \u0026lt; 0) index = n - index - 1; else index += negatives; } t[index] = a[i]; } // a[]=t[] and start again until the last group if (c != groups - 1) { for (int j = 0; j \u0026lt; n; j++) a[j] = t[j]; } } // Convert back the ints to the double array for (int i = 0; i \u0026lt; n; i++) nums[i] = *(double*)(\u0026amp;t[i]); //重新把long long 的二进制转换为double } 浮点数的基数排序肯定会比快速排序快，至于快多少我就没有测试了。\n二，IEEE754浮点数都是规格化浮点数。\n规格化（normalized）浮点数是指尾数f的最高位非0。如果指数e的范围是无限的，则可以通过对尾数f移位并调整指数e的大小对v进行规格化；如果e的范围是有限的，则有些数并不能被规格化（f移位过多，导致调整后的e超出其范围）。有最小指数的不可规格化浮点数称为非规格化数（denormals）。[3]\n当所有的数都是以规格化数或非规格化数表示时，他们是唯一的。[3]\nIEEE754也能表示非规格化浮点数，但不在本文的讨论范围。\n既然IEEE754浮点数都是规格化浮点数，则他们的f最高位都是1（非0），所以可以省略这一位。也就是说IEEE754双精度浮点数的尾数实际上有53位，只是最高位都是1，所以都省略掉了。\n了解了这两点之后，我们就可以理解为什么将IEEE754双精度浮点数转换为十进制数的公式是下面这个样子了。\n[1]\n或者\n[1]\n在接下来的讨论中，假设一个浮点数为v，其尾数为\\(f_v\\)，指数为\\(e_v\\)，基为b（通常为2），则有\\(v=f_v\\times b^{e_v}\\)。对于IEEE754 double，因为尾数省略了最高位1，所以有\\(hidden=2^{52}\\)，二进制串中的尾数为\\(f_{IEEE}\\)，真正的尾数为\\(f_v=hidden + f_{IEEE}\\)，真正的指数为\\(e_v=e_{IEEE}-bias\\)，所以有\\(v=f_v\\times 2^{e_v}\\)。\n舍入机制 因为浮点数并不能表示所有的实数，所以将实数映射到浮点数的时候，需要一个舍入机制，有两种舍入机制：\nup：向上进位，使用\\([x]^\\uparrow\\)表示； even：选择偶数，使用\\([x]^\\Box\\)表示，比如在十进制中，1.5→2、0.5→0；这是IEEE的默认策略。 当四舍五入的策略不重要时，使用\\([x]^*\\)表示。我们使用\\(\\widetilde x=[x]_p^s\\)来表示规格化浮点数\\(\\widetilde x\\)（x上一根波浪线）的尾数位数（精度）为p，在规格化的过程中使用了s的四舍五入策略。\nULP ULP的全称为unit in the last place，可以理解为尾数相差一个单位时，浮点数的差值。因为x被四舍五入到最接近x的值\\(\\widetilde x\\)，所以有\\(|\\widetilde x-x|\\leqslant 0.5\\times b^e=0.5ulp\\)。\n邻居和边界 令\\(v=f_v\\times b^{e_v}\\)是一个正浮点数，则v的前驱节点\\(v^-\\)是v的上一个可以表示的浮点数；v的后继节点\\(v^+\\)是v的下一个可以表示的浮点数。如果v是最小值，则\\(v^-=0\\)；如果v是最大值，则\\(v^+=v+(v-v^-)\\)。\\(v^-\\)和\\(v^+\\)都是v的邻居，他们和v具有相同的距离。\n两个相邻的数\\(v_1\\)和\\(v_2\\)的边界为他们的算术平均\\(m=(v_1+v_2)/2\\)。根据定义，边界值是不能被表示的。每个浮点数v都有2个边界：\\(m^-=(v^-+v)/2\\)、\\(m^+=(v^++v)/2\\)。明显的，任何实数\\(m^- \u003c w \u003c m^+\\)都将四舍五入到v，也就是说在\\((m^-,m^+)\\)之间的实数是无法用计算机表示的。\nGrisu原是1970年代意大利动画片中的主角，它是一条想成为消防员的小龙，配图是动画片VCD的封面[2]\n下面开始介绍Grisu算法，参考论文Printing Floating-Point Numbers Quickly and Accurately with Integers[3]。\nGrisu算法 自定义数据结构 本文使用整数来实现浮点数的转换，数据结构如下：\n1 2 3 4 typedef struct diy_fp{ uint64_t f; int e; }diy_fp; diy_fp中的f表示尾数，e表示指数。f的精度为q=64，高于IEEE754双精度浮点数的精度p=53。\ndiy_fp有两种运算，减法和乘法。减法为指数相等，尾数相减，结果可能没有规格化；乘法如下：\n$$x\\otimes y=[(f_x\\times f_y)/2^q ]^\\uparrow \\times 2^{e_x+e_y+q}$$乘法结果要四舍五入到64位，所以会有一些错误，但是错误不超过0.5ulp，结果可能没有规格化。\n预计算10的幂 Grisu算法需要用到10的幂的规格化结果，提前计算好这些结果能加速Grisu运行。函数diy_fp cached_power(int k);能够直接返回\\(10^k\\)的规格化浮点数。\n假设输入浮点数为v，其指数为e，需要寻找的10的幂即为\\(\\widetilde{c_k}=f_{c_k }\\times 2^{e_{c_k}}=[10^k ]_q^*\\)，且指数满足\\(\\alpha \\leqslant e_{c_k}+e \\leqslant \\gamma\\)。推导过程为：同时对\\(\\widetilde{c_k}\\)两边取\\(log_{10}\\)得到\\(k=log_{10}(f_{c_k }\\times 2^{e_{c_k}})\\)，将\\(e_{c_k}\\)的下界\\(e_{c_k}\\geqslant\\alpha-e\\)代入，得到\\(k=\\lceil log_{10}(f_{c_k}\\times 2^{\\alpha -e})\\rceil\\)，又因为\\(\\widetilde{c_k}\\)所表示的10的幂的尾数精度为\\(q\\)，所以\\(f_{c_k}\\)的下界为\\(2^{q-1}\\)，代入前一个式子得到下式：\n$$k=\\lceil log_{10}2^{\\alpha -e+q-1}\\rceil=\\lceil (\\alpha -e+q-1)\\times 1/log_{2}10\\rceil$$Grisu算法描述 假设浮点数v的指数是负数，则v可表示为\\(\\frac{f_v}{2^{-e_v}}\\)，如果能找到一个t，使得\\(1\\leqslant \\frac{f_v\\times 10^t}{2^{-e_v}}\u003c10\\)，则v的十进制尾数为\\(\\frac{f_v\\times 10^t}{2^{-e_v}}\\)，十进制指数为-t；我们可以很容易获取\\(\\frac{f_v\\times 10^t}{2^{-e_v}}\\)的各位数字。\n所以Grisu要解决的问题就是怎样快速的将\\(f_v\\times 2^{e_v}\\)转换为\\(D\\times 10^k\\)的形式，并且D尽量小。这样我们可以很容易的获取D的各位数字，和其十进制指数k。问题进一步转换为求k，使得\\(D=v\\times 10^{-k}=f_v\\times 2^{e_v}\\times f_{c_{-k}}\\times 2^{e_{c_{-k}}}\\)尽量的小，所以要让\\(e_{c_{-k}}\\)和\\(e_v\\)能尽量的抵消掉，这就是为什么在预计算10的幂中要求\\(\\alpha \\leqslant e_{c_k}+e \\leqslant \\gamma\\)，且\\(\\alpha\\)和\\(\\gamma\\)都比较小，但是论文表明\\(\\alpha\\)和\\(\\gamma\\)并不是越小越好。\n下面是Grisu算法的具体描述\n输入：精度为p的正浮点数v 前提：diy_fp的精度q≥p+2，且\\(\\widetilde{c_k}=[10^k ]_q^*\\)已经提前计算好了。 输出：十进制字符串V，且满足\\([V]_p^\\Box=v\\)，也就是说再次读取字符串V时，能四舍五入成浮点数v。 过程：\n求v的规格化浮点数表示diy_fp w 寻找满足\\(\\alpha \\leqslant e_c+e_w+q\\leqslant \\gamma\\)的10的幂\\(\\widetilde{c_{-k}}=f_c\\times 2^{e_c}=[10^{-k}]_q^*\\) 计算乘积\\(\\widetilde D=f_D\\times 2^{e_D}=w\\otimes\\widetilde{c_{-k}}\\) 定义\\(V=\\widetilde D\\times 10^k\\)，输出\\(\\widetilde D\\)的十进制表示，字符\u0026rsquo;e\u0026rsquo;和k的十进制表示。 Grisu算法中，\\(\\widetilde D\\)相当于v的十进制尾数，k相当于v的十进制指数。\nGrisu2算法 Grisu算法虽然快，但是得到的结果并不是最短的，比如Grisu可能会把1.0打印成10000000000000000000e-19。Grisu2算法使用了额外的二进制位使得对于99%的输入都能输出最短的字符串表示。\n主要长度 令v是一个正实数，n, l和s是整数，有\\(l\\geqslant 1, 10^{l-1}\\leqslant s\\leqslant 10^l, v=s\\times 10^{n-l}\\)，并且l越小越好，则s的前l个数字是v的主要数字（leading digits），l就是v的主要长度（leading length）。\n通俗的说就是把V的不必要的前导和后尾0去掉后的长度，比如\\(1.23=123\\times 10^{-2}\\)=\\(1230\\times 10^{-3}\\)，但是\\(1230\\times 10^{-3}\\)就多了一个不必要的后尾0，所以1.23的主要长度是3。\n定理6.2 令x和y是2个实数，且\\(x\\leqslant y\\)。令k是满足y mod \\(10^k\\leqslant y-x\\)的最大整数，则有\\(V=\\lfloor \\frac{y}{10^k}\\rfloor \\times 10^k\\)满足\\(x\\leqslant V\\leqslant y\\)。并且V的主要长度（leading length）是所有在[x,y]范围内最小的。\nGrisu2算法正是在Grisu的基础上，利用定理6.2输出了最短的长度。\nGrisu2算法描述 输入：同Grisu算法 前提：diy_fp的精度q≥p+3，且\\(\\widetilde {c_k}=[10^k ]_q^*\\)已经提前计算好了。 输出：十进制数字\\(d_i\\)，i属于[0,n]，和整数\\(\\kappa\\)使得\\(V=d_0 … d_n\\times 10^\\kappa\\)满足\\([V]_p^\\Box =v\\) 过程：\n计算v的边界\\(m^-\\)和\\(m^+\\)（\\(v\\mp 0.5ulp\\)） diy_fp \\(w^+=m^+\\); diy_fp \\(w^-=m^-;\\) 且\\(e_w^-=e_w^+\\) 寻找满足\\(\\alpha \\leqslant e_c+e_w^++q\\leqslant \\gamma\\)的10的幂\\(\\widetilde{c_{-k}}=f_c\\times 2^{e_c}=[10^{-k}]_q^*\\) 计算\\(\\widetilde{M^-}=w^- \\otimes \\widetilde{c_{-k}}\\)；\\(\\widetilde{M^+}=w^+ \\otimes \\widetilde{c_{-k}}\\)；\\(M_{\\uparrow }^-=\\widetilde{M^-}+1ulp\\)；\\(M_{\\downarrow }^+=\\widetilde{M^+}-1ulp\\)；\\(\\delta =M_{\\downarrow }^+-M_{\\uparrow }^-\\) 找到最大的\\(\\kappa\\)使得\\(M_{\\downarrow }^+ mod 10^{\\kappa }\\leqslant \\delta\\)，并且定义\\(P=\\lfloor \\frac{M_{\\downarrow }^+}{10^{\\kappa }}\\rfloor\\) 定义\\(V=P\\times 10^{k+\\kappa }\\)，输出P的十进制数字和\\(K=k+\\kappa\\) Grisu2算法的前三步工作和Grisu算法类似，4,5步的工作近似求Grisu算法的\\(\\widetilde D\\)的最短表示，利用的正是定理6.2，所以最终的指数是由k和\\(\\kappa\\)两部分构成的。\n该论文表示，Grisu2算法比sprintf快四倍左右，根据milo的测试[4] ，经过优化的Grisu2算法milo比sprintf快九倍，据说Google的V8 JavaScript引擎就是使用了Grisu算法，速度才很快的。\ndtoa-benchmark，grisu2是sprintf的5.7倍，milo（优化过的grisu2算法）是sprintf的9.1倍[4]\n至于milo是怎么优化Grisu的，可以参考他的博客，完整的代码可以参考他的Github项目[4]。\n关于完整全面的浮点数介绍，可以参考文献[5]。\n参考：\n[1]. https://en.wikipedia.org/wiki/Double-precision_floating-point_format\n[2]. http://miloyip.com/images/grisu.jpg\n[3]. Printing Floating-Point Numbers Quickly and Accurately with Integers第2节\n[4]. dtoa-benchmark\n[5]. What Every Computer Scientist Should Know About Floating-Point Arithmetic\n","permalink":"http://localhost:1313/posts/2015-08-30-introduction-to-floating-point-numbers-and-grisu-algorithm/","summary":"\u003cp\u003e进入研究生生涯完成的第一个新生培训作业是“2.5亿个浮点数的外部排序算法”，前后折腾了将近一个月，结果是在i7处理器上，限制512MB内存，排序用时250秒左右。\u003c/p\u003e\n\u003cp\u003e这个作业的常规思路大部分人都能想到，按块读取文件-\u0026gt;atof转换为double-\u0026gt;内部快速排序或基数排序-\u0026gt;dtoa转换为char*-\u0026gt;按块写入文件。这里面中间的三个过程都很耗时，特别是atof和dtoa，因为精度只要求保留9位小数，所以可以自己实现atof和dtoa来加速，也可以使用多线程加速。\u003c/p\u003e\n\u003cp\u003e整个作业都是基于对IEEE754浮点数的深刻理解展开的，所以下面详细讲解浮点数的一些知识。\u003c/p\u003e\n\u003ch1 id=\"ieee754双精度浮点数\"\u003eIEEE754双精度浮点数\u003c/h1\u003e\n\u003cp\u003e目前大多数CPU内浮点数的表示都遵循IEEE754标准，IEEE754双精度浮点数（double）表示如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"IEEE754 double在内存中的形式[1]\" loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/a/a9/IEEE_754_Double_Floating_Point_Format.svg\"\u003e\nIEEE754 double在内存中的形式[1]\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esign bit：符号位，1位，用来表示正负号，0表示非负；1表示负\u003c/li\u003e\n\u003cli\u003eexponent：指数位，11位，用来表示次方数，是一个无符号数\u003c/li\u003e\n\u003cli\u003efraction：尾数位，52位，用来表示精确度，也是一个无符号数，有些资料也叫做mantissa或significand\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这种表示形式有两点需要注意。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e第一，既然exponent是无符号的，那么怎样表示负指数呢？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIEEE754规定，二进制串中算得的e需要减去一个偏移量bias，对于double，bias=1023，即e’=e-bias。因为\\(e\\in[0,2^{11}-1]\\)，所以最终\\(e’\\in[-2^{10}+1,2^{10}]\\)。但是如果把e本身看作有符号数e”，则\\(e”\\in[-2^{10},2^{10}-1]\\)，既然e”和e’相差微小，为什么不直接把e看成有符号数，而非要把它看成无符号数，再减去一个偏移量bias呢？\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://stackoverflow.com/questions/2612775/why-ieee-floating-point-number-calculate-exponent-using-a-biased-form\"\u003e这是因为如果把e看成无符号数再减偏移量，浮点数大小比较速度更快。\u003c/a\u003e引用\u003ca href=\"https://en.wikipedia.org/wiki/Exponent_bias\"\u003e维基百科\u003c/a\u003e的一段话：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBy arranging the fields so that the sign bit is in the most significant bit position, the biased exponent in the middle, then the mantissa in the least significant bits, the resulting value will be ordered properly, whether it’s interpreted as a floating point or integer value. This allows high speed comparisons of floating point numbers using fixed point hardware.\u003c/p\u003e","title":"浮点数知识及Grisu算法介绍"},{"content":"去年暑假在北大计算所实习的时候，任务之一就是批量下载百度图片。当时没学python，用c#实现了一个简易版本的批量下载器，如下图。\nC#版本百度图片批量下载器（抓的是百度的wap站点，现在好像不能用了）\n当时“时间紧，任务重“，既没仔细研究百度图片API，也没处理好界面线程阻塞的问题。这个问题其实很有意思，趁着暑假在家，实现了一个比较完美的python版本，先上效果图。\npython3版本百度图片批量下载器\n新版使用了python-3.4.3.amd64.msi + PyQt5-5.5-gpl-Py3.4-Qt5.5.0-x64.exe + eric6-6.0.8.zip + cx_Freeze-4.3.4-cp34-none-win_amd64.whl，完整项目在我的GitHub上。大致有如下几点工作：\n研究百度图片API，获取原始图片URL列表 使用python3进行多线程下载 利用pyqt5实现界面 利用cx_Freeze4打包整个程序 下面记录每个步骤的要点，供后人参考。\n百度图片API 正常使用百度图片搜索的时候，URL是这样的：\nhttp://image.baidu.com/search/index?ct=201326592\u0026z=0\u0026tn=baiduimage\u0026ipn=r\u0026word=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6\u0026pn=0\u0026istype=2\u0026ie=utf-8\u0026oe=utf-8\u0026cl=2\u0026lm=-1\u0026st=-1\u0026fr=\u0026fmq=1439374041843_R\u0026ic=0\u0026se=\u0026sme=\u0026width=0\u0026height=0\u0026face=0\n里面有很多参数，有些我们并不需要，精简之后大概是这样的：\nhttp://image.baidu.com/i?tn=baiduimage\u0026ie=utf-8\u0026word=%E7%BE%8E%E5%A5%B3\u0026pn=\u0026rn=\u0026z=\nword为搜索关键词；pn为page number当前是第几页，实际含义是image id，表示第几张图片，从0开始；rn为每一页的图片数量，最大为60；z表示图片尺寸，z=9特大尺寸，z=3大尺寸，z=2中等尺寸，z=1小尺寸，z=0所有尺寸。\n但是这个URL是给”人“看的，下一页的图片是动态加载的，其html代码的图片URL数量固定。一番查询之后发现，将tn=baiduimage换成tn=resultjson_com能获取到所有图片URL的json，json当然是给”猴“看的，这样就能轻松获取到所有图片的URL。\n慢着，仔细看看json中的objURL，是一串连”猴“都看不懂的字符串，原来百度把图片真实URL加密了，好在加密方法是简单的字符映射，参考这篇博客成功解密。\n更新：tn=resultjson_com的objURL是加密了，但是tn=resultjson的objURL并没有加密，所以采用tn=resultjson最佳。\n通过控制pn和rn就能获取指定数量的图片URL，但是我发现rn最大只能为60，并且不同的pn可能会有相同的图片url（比如pn=0和pn=1都有ippr_z2C$qAzdH3FAzdH3Fooo_z\u0026amp;e3Bd8vs7k_z\u0026amp;e3Bv54_z\u0026amp;e3BvgAzdH3F7rs5w1utsjAzdH3Fda8nAzdH3Fa080AzdH3Fda8na080aldm9bb8m_z\u0026amp;e3B3r2这个objURL），所以使用python的集合（set）去重。\n更新：pn实际上指图片的id，pn=0、rn=60能获取到从0~59这60个URL列表，pn=1、rn=60能获取到从1~60这60个URL列表，所以pn=0和pn=1的列表中当然有59个是重复的。正确的做法是pn=0、rn=60获取0~59这60个URL列表，然后pn=60、rn=60获取60~119这60个列表，以此类推，这样获取到的URL就不会有重复的了。\n获取图片URL列表的简要代码如下：\n1 2 3 4 5 6 7 8 9 10 11 def ParseJSON(self, pn, rn, st): url = \u0026#39;http://image.baidu.com/i?tn=resultjson_com\u0026amp;amp;amp;ie=utf-8\u0026amp;amp;amp;word=%s\u0026amp;amp;amp;pn=%d\u0026amp;amp;amp;rn=%d\u0026amp;amp;amp;z=%d\u0026#39;%(self.word, pn, rn, self.size) #print(url) request = urllib.request.Request(url = url, headers = my_header) html = urllib.request.urlopen(request).read() hjson = json.loads(html.decode(\u0026#39;gbk\u0026#39;)) for i in range(0, len(hjson[\u0026#39;data\u0026#39;])-1):#最后一个数据为空 img_url = self.DecodeURL(hjson[\u0026#39;data\u0026#39;][i][\u0026#39;objURL\u0026#39;]) if img_url not in st: st.add(img_url)#去重 self.progressBar_updated_signal.emit()#更新进度条 DecodeURL是解密函数。很奇怪，json最后一个数据是空的。\n更新：文章末尾的最新代码已经不需要set去重和DecodeURL解密了。\npython3多线程下载 多线程下载图片主要参考了这个例子，只是将其转换为python3的形式，不得不感叹python的易用性，创建线程和下载图片一两行代码就可以完成，太方便了。这个例子有点像单生产者多消费者模型，创建了4个线程之后，并不需要告诉a线程下载哪几张图片，这4个线程会自定从队列里获取，互斥变量的访问也不会出错，减轻了程序员很多任务。\n关于python抓取网络资源的介绍，这篇博客介绍得很全面。有些URL并不是图片的真正地址，访问之后还会进行跳转，这种情况在使用urlretrieve下载图片时可能会抛出URLError异常。其他还可能遇到timeout、HTTPError、OSError等异常，可以使用下面的方式一次性捕获所有异常。\n1 2 3 4 try: urllib.request.urlretrieve(img_url, self.dir + \u0026#39;/\u0026#39; + img_url.split(\u0026#39;/\u0026#39;)[-1]) except Exception as e: print(\u0026#34;—–%s: %s—–n\u0026#34;%(type(e), img_url)) pyqt5实现界面 大二大三的时候接触过c++ qt4 gui编程，现在改python了，不过基本思想差不多，pyqt和c++qt的api基本相同，所以借助eric6实现了python和qt的联接。\n关于eric的使用教程，网上很多，这个讲解很详细，不过如果在windows上，安装没那么复杂，安装eric之前先安装好python3和pyqt5就行了，不用任何配置。eric熟练之后很方便了，直接拖拽控件画界面。\n在使用pyqt5的时候有一些坑需要注意，尤其是我使用的是最新版的python3和qt5，网上的资料不是很多。我遇到的两个主要坑是qt的信号和槽以及界面线程阻塞的问题。\n1 2 3 4 5 6 def on_download_pushButton_clicked(self): if self.check_option() == 1: de = DownloadEngine(self.word_lineEdit.text(), self.size_radio_group.checkedId(), self.num_spinBox.value(), self.dir_lineEdit.text(), self.thread_spinBox.value(),self.progressBar) de.Run() msg_box = QMessageBox(QMessageBox.Information, \u0026#34;提示\u0026#34;, \u0026#34;下载完毕\u0026#34;) msg_box.exec_() 上面这一段是我最开始的按钮槽函数，点击下载按钮之后，实例化一个DownloadEngine，然后de.Run()开始下载，要等到下载完毕de.Run()返回后，流程才会往下走，弹出提示框。但是下载过程耗时较长，这样整个界面线程阻塞，程序出现假死状态。\n后来参考A、B两个例子，令DownloadEngine继承QThread，用de.start()开启一个新的下载线程，界面线程继续往下走，当de下载完毕后，发送download_done_signal信号，界面接收到该信号后弹出提示框。第二版程序大概如下：\n1 2 3 4 5 def on_download_pushButton_clicked(self): if self.check_option() == 1: de = DownloadEngine(self.word_lineEdit.text(), self.size_radio_group.checkedId(), self.num_spinBox.value(), self.dir_lineEdit.text(), self.thread_spinBox.value(), self.progressBar) de.start() de.download_done_signal.connect(self.download_done_slot) 一切似乎很正确，但是点击下载按钮后程序崩溃，提示pythonw.exe已停止工作，我开始以为是python或者pyqt5的问题，重装一遍问题依旧。百思不得其解之后，用命令行运行py程序，cmd提示QThread: Destroyed while thread is still running，原来de是该函数的局部变量，当de.start()后，函数继续往下走直到结束，但是de的下载任务可能还没完成，所以导致上面的错误。解决办法就是将de变为该类的成员变量，将de改为self.de。\n有了这一个编写信号和槽函数的经验之后，准备实现进度条的功能，同样要保证界面线程不阻塞，所以创建了progressBar_updated_signal信号和相应槽函数。\n但是因为界面是直接和DownloadEngine联系的，但真正的下载工作是在ImageDownloadThread完成，所以progressBar_updated_signal信号最原始的发出地是ImageDownloadThread。这里面的信号传递关系如下图所示：\n“进度条更新”信号传递关系\npython3多线程下载代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 # -*- coding: utf-8 -*- import urllib.request import json import socket import queue from PyQt5.QtCore import * global my_header my_header = {\u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0\u0026#39;} global bad bad = 0 class ImageDownloadThread(QThread): sub_progressBar_updated_signal = pyqtSignal() def __init__(self,queue_in, dir_in): #进程间通过队列通信，所以每个进程需要用到同一个队列初始化 super(ImageDownloadThread,self).__init__() self.my_queue=queue_in self.dir = dir_in #self.setDaemon(True) #守护线程 self.start() #启动线程 #使用队列实现进程间通信 def run(self): while (True): global bad img_url = self.my_queue.get() socket.setdefaulttimeout(5)#这里对整个socket层设置超时时间。后续连接中如果再使用到socket，不必再设置 try: urllib.request.urlretrieve(img_url, self.dir + \u0026#39;/\u0026#39; + img_url.split(\u0026#39;/\u0026#39;)[-1]) except Exception as e: print(\u0026#34;—–%s: %s—–n\u0026#34;%(type(e), img_url)) bad += 1 self.sub_progressBar_updated_signal.emit() if self.my_queue.empty(): break self.my_queue.task_done() #当使用者线程调用 task_done() 以表示检索了该项目、并完成了所有的工作时，那么未完成的任务的总数就会减少。 class DownloadEngine(QThread): download_done_signal = pyqtSignal(int) status_changed_signal = pyqtSignal(str) progressBar_updated_signal = pyqtSignal() def __init__(self, word_in, size_in, num_in, dir_in, thread_num_in): super(DownloadEngine,self).__init__() self.word = urllib.parse.quote(word_in) self.size = size_in self.num = num_in self.dir = dir_in self.thread_num = thread_num_in def ParseJSON(self, pn, rn, qe): url = \u0026#39;http://image.baidu.com/i?tn=resultjson\u0026amp;ie=utf-8\u0026amp;word=%s\u0026amp;pn=%d\u0026amp;rn=%d\u0026amp;z=%d\u0026#39;%(self.word, pn, rn, self.size) #print(url) request = urllib.request.Request(url = url, headers = my_header) html = urllib.request.urlopen(request).read() hjson = json.loads(html.decode(\u0026#39;gbk\u0026#39;)) for i in range(0, len(hjson[‘data’])-1):#最后一个数据为空 qe.put(hjson[\u0026#39;data\u0026#39;][i][\u0026#39;objURL\u0026#39;]) self.progressBar_updated_signal.emit()#更新进度条 def GetImgUrlQueue(self): img_url_queue = queue.Queue(0) if self.num \u0026lt;= 60: self.ParseJSON(0, self.num, img_url_queue) else: n = self.num / 60 n = int(n) for i in range(n): self.ParseJSON(i * 60, 60, img_url_queue) self.ParseJSON(n * 60, self.num – n * 60, img_url_queue) return img_url_queue def sub_update_progressBar(self): self.progressBar_updated_signal.emit() def run(self): global bad bad = 0 self.status_changed_signal.emit(\u0026#39;获取URL\u0026#39;) img_url_queue = self.GetImgUrlQueue() threads = [] self.status_changed_signal.emit(‘下载图片’) #多线程爬去图片 for i in range(self.thread_num): thread=ImageDownloadThread(img_url_queue, self.dir) thread.sub_progressBar_updated_signal.connect(self.sub_update_progressBar) threads.append(thread) #合并进程，当子进程结束时，主进程才可以执行 for thread in threads: thread.wait() self.status_changed_signal.emit(\u0026#39;下载完成\u0026#39;) self.download_done_signal.emit(bad) pyqt5界面代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34; Module implementing MainDialog. \u0026#34;\u0026#34;\u0026#34; from PyQt5.QtCore import * from PyQt5.QtWidgets import * from Ui_main import Ui_Dialog from DownloadEngine import DownloadEngine import webbrowser class MainDialog(QDialog, Ui_Dialog): \u0026#34;\u0026#34;\u0026#34; Class documentation goes here. \u0026#34;\u0026#34;\u0026#34; def __init__(self, parent=None): \u0026#34;\u0026#34;\u0026#34; Constructor @param parent reference to the parent widget (QWidget) \u0026#34;\u0026#34;\u0026#34; super(MainDialog, self).__init__(parent) self.setupUi(self) self.size_radio_group = QButtonGroup() self.size_radio_group.addButton(self.total_radioButton, 0) self.size_radio_group.addButton(self.XL_radioButton, 9) self.size_radio_group.addButton(self.L_radioButton, 3) self.size_radio_group.addButton(self.M_radioButton, 2) self.size_radio_group.addButton(self.S_radioButton, 1) self.count = 0 def check_option(self): if self.word_lineEdit.text() == \u0026#34;\u0026#34;: msg_box = QMessageBox(QMessageBox.Warning, \u0026#34;警告\u0026#34;, \u0026#34;请输入搜索关键词！\u0026#34;) msg_box.exec_() return 0 if self.dir_lineEdit.text() == \u0026#34;\u0026#34;: msg_box = QMessageBox(QMessageBox.Warning, \u0026#34;警告\u0026#34;, \u0026#34;请选择图片存储目录！\u0026#34;) msg_box.exec_() return 0 return 1 @pyqtSlot() def on_dir_pushButton_clicked(self): \u0026#34;\u0026#34;\u0026#34; Slot documentation goes here. \u0026#34;\u0026#34;\u0026#34; # TODO: not implemented yet dir = QFileDialog.getExistingDirectory(self, \u0026#34;选择图片存储目录\u0026#34;,\u0026#34;.\u0026#34;) self.dir_lineEdit.setText(dir) @pyqtSlot() def on_download_pushButton_clicked(self): \u0026#34;\u0026#34;\u0026#34; Slot documentation goes here. \u0026#34;\u0026#34;\u0026#34; # TODO: not implemented yet self.progressBar.setValue(0) if self.check_option() == 1: self.progressBar.setMaximum(self.num_spinBox.value()) self.download_pushButton.setEnabled(False) self.de = DownloadEngine(self.word_lineEdit.text(), self.size_radio_group.checkedId(), self.num_spinBox.value(), self.dir_lineEdit.text(), self.thread_spinBox.value()) self.de.start() self.de.status_changed_signal.connect(self.status_changed_slot) self.de.download_done_signal.connect(self.download_done_slot) self.de.progressBar_updated_signal.connect(self.progressBar_updated_slot) @pyqtSlot() def on_src_pushButton_clicked(self): \u0026#34;\u0026#34;\u0026#34; Slot documentation goes here. \u0026#34;\u0026#34;\u0026#34; # TODO: not implemented yet webbrowser.open(\u0026#34;https://github.com/Beeder/BaiduImageDownloader\u0026#34;) def progressBar_updated_slot(self): self.count += 1 self.progressBar.setValue(self.count) def status_changed_slot(self, tip): self.status_label.setText(tip) self.count = 0 if tip != \u0026#39;下载完成\u0026#39;: self.progressBar.setValue(0) def download_done_slot(self, bad): msg_box = QMessageBox(QMessageBox.Information, \u0026#34;提示\u0026#34;, \u0026#34;下载完毕n成功%d,失败%d\u0026#34;%(self.num_spinBox.value() – bad, bad)) msg_box.exec_() self.download_pushButton.setEnabled(True) if __name__ == \u0026#34;__main__\u0026#34;: import sys app = QApplication(sys.argv) Dialog = MainDialog() Dialog.show() sys.exit(app.exec_()) cx_Freeze4打包 目前有好几个python打包程序，但是只有cx_Freeze明确表示支持python3，所以非他莫属了。\n在sourceforge下载cx_Freeze-4.3.3.win-amd64-py3.4.msi，安装；根据官方指南编写setup.py代码，将setup.py放到工程根目录下，执行python setup.py bdist_msi；报错`AttributeError:’module’object has no attribute ‘fix_up_module’。原来这是cx_Freeze-4.3.3版本的一个bug，利用替换的方法，安装4.3.4版本，问题解决。\n打包完成之后生成一个BaiduImageDownloader-0.1-amd64.msi文件，拷贝到其他电脑上也可正常运行。Winows7 64位用户可以点击下载BaiduImageDownloader-0.1-amd64.msi。\n（完）\n","permalink":"http://localhost:1313/posts/2015-08-13-baidu-image-downloader-python3-pyqt5-eric6-cx_freeze4/","summary":"\u003cp\u003e去年暑假在北大计算所实习的时候，任务之一就是批量下载百度图片。当时没学python，用c#实现了一个简易版本的批量下载器，如下图。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"C#版本百度图片批量下载器（抓的是百度的wap站点，现在好像不能用了）\" loading=\"lazy\" src=\"/posts/2015-08-13-baidu-image-downloader-python3-pyqt5-eric6-cx_freeze4/BaiduImageDownloader1.png\"\u003e\nC#版本百度图片批量下载器（抓的是百度的wap站点，现在好像不能用了）\u003c/p\u003e\n\u003cp\u003e当时“时间紧，任务重“，既没仔细研究百度图片API，也没处理好界面线程阻塞的问题。这个问题其实很有意思，趁着暑假在家，实现了一个比较完美的python版本，先上效果图。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"python3版本百度图片批量下载器\" loading=\"lazy\" src=\"/posts/2015-08-13-baidu-image-downloader-python3-pyqt5-eric6-cx_freeze4/BaiduImageDownloader2.png\"\u003e\npython3版本百度图片批量下载器\u003c/p\u003e\n\u003cp\u003e新版使用了\u003ca href=\"https://www.python.org/ftp/python/3.4.3/python-3.4.3.amd64.msi\"\u003epython-3.4.3.amd64.msi\u003c/a\u003e + \u003ca href=\"http://sourceforge.net/projects/pyqt/files/PyQt5/PyQt-5.5/PyQt5-5.5-gpl-Py3.4-Qt5.5.0-x64.exe\"\u003ePyQt5-5.5-gpl-Py3.4-Qt5.5.0-x64.exe\u003c/a\u003e + \u003ca href=\"http://downloads.sourceforge.net/project/eric-ide/eric6/stable/6.0.8/eric6-6.0.8.zip?r=http%3A%2F%2Fsourceforge.net%2Fprojects%2Feric-ide%2Ffiles%2Feric6%2Fstable%2F\u0026amp;ts=1439435222\u0026amp;use_mirror=nchc\"\u003eeric6-6.0.8.zip\u003c/a\u003e + \u003ca href=\"http://www.lfd.uci.edu/~gohlke/pythonlibs/3i673h27/cx_Freeze-4.3.4-cp34-none-win_amd64.whl\"\u003ecx_Freeze-4.3.4-cp34-none-win_amd64.whl\u003c/a\u003e，完整项目在\u003ca href=\"https://github.com/01joy/BaiduImageDownloader\"\u003e我的GitHub上\u003c/a\u003e。大致有如下几点工作：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e研究百度图片API，获取原始图片URL列表\u003c/li\u003e\n\u003cli\u003e使用python3进行多线程下载\u003c/li\u003e\n\u003cli\u003e利用pyqt5实现界面\u003c/li\u003e\n\u003cli\u003e利用cx_Freeze4打包整个程序\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e下面记录每个步骤的要点，供后人参考。\u003c/p\u003e\n\u003ch1 id=\"百度图片api\"\u003e百度图片API\u003c/h1\u003e\n\u003cp\u003e正常使用百度图片搜索的时候，URL是这样的：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://image.baidu.com/search/index?ct=201326592\u0026amp;z=0\u0026amp;tn=baiduimage\u0026amp;ipn=r\u0026amp;word=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6\u0026amp;pn=0\u0026amp;istype=2\u0026amp;ie=utf-8\u0026amp;oe=utf-8\u0026amp;cl=2\u0026amp;lm=-1\u0026amp;st=-1\u0026amp;fr=\u0026amp;fmq=1439374041843_R\u0026amp;ic=0\u0026amp;se=\u0026amp;sme=\u0026amp;width=0\u0026amp;height=0\u0026amp;face=0\"\u003ehttp://image.baidu.com/search/index?ct=201326592\u0026z=0\u0026tn=baiduimage\u0026ipn=r\u0026word=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6\u0026pn=0\u0026istype=2\u0026ie=utf-8\u0026oe=utf-8\u0026cl=2\u0026lm=-1\u0026st=-1\u0026fr=\u0026fmq=1439374041843_R\u0026ic=0\u0026se=\u0026sme=\u0026width=0\u0026height=0\u0026face=0\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e里面有很多参数，有些我们并不需要，精简之后大概是这样的：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://image.baidu.com/i?tn=baiduimage\u0026amp;ie=utf-8\u0026amp;word=%E7%BE%8E%E5%A5%B3\u0026amp;pn=\u0026amp;rn=\u0026amp;z=\"\u003ehttp://image.baidu.com/i?tn=baiduimage\u0026ie=utf-8\u0026word=%E7%BE%8E%E5%A5%B3\u0026pn=\u0026rn=\u0026z=\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eword为搜索关键词；pn为page number\u003cdel\u003e当前是第几页，\u003c/del\u003e\u003cstrong\u003e实际含义是image id\u003c/strong\u003e，表示第几张图片，从0开始；rn为每一页的图片数量，最大为60；z表示图片尺寸，z=9特大尺寸，z=3大尺寸，z=2中等尺寸，z=1小尺寸，z=0所有尺寸。\u003c/p\u003e\n\u003cp\u003e但是这个URL是给”人“看的，下一页的图片是动态加载的，其html代码的图片URL数量固定。一番查询之后发现，将tn=baiduimage换成tn=resultjson_com能获取到所有图片URL的json，json当然是给”猴“看的，这样就能轻松获取到所有图片的URL。\u003c/p\u003e\n\u003cp\u003e慢着，仔细看看json中的objURL，是一串连”猴“都看不懂的字符串，原来百度把图片真实URL加密了，好在加密方法是简单的字符映射，参考\u003ca href=\"http://blog.csdn.net/hbuxiaoshe/article/details/44780653\"\u003e这篇博客\u003c/a\u003e成功解密。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e更新：tn=resultjson_com的objURL是加密了，但是tn=resultjson的objURL并没有加密，所以采用tn=resultjson最佳。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e通过控制pn和rn就能获取指定数量的图片URL，但是我发现rn最大只能为60，并且不同的pn可能会有相同的图片url（比如\u003ca href=\"http://image.baidu.com/i?tn=resultjson_com\u0026amp;ie=utf-8\u0026amp;word=mit\u0026amp;pn=0\u0026amp;rn=60\u0026amp;z=9\"\u003epn=0\u003c/a\u003e和\u003ca href=\"http://image.baidu.com/i?tn=resultjson_com\u0026amp;ie=utf-8\u0026amp;word=mit\u0026amp;pn=1\u0026amp;rn=60\u0026amp;z=9\"\u003epn=1\u003c/a\u003e都有ippr_z2C$qAzdH3FAzdH3Fooo_z\u0026amp;e3Bd8vs7k_z\u0026amp;e3Bv54_z\u0026amp;e3BvgAzdH3F7rs5w1utsjAzdH3Fda8nAzdH3Fa080AzdH3Fda8na080aldm9bb8m_z\u0026amp;e3B3r2这个objURL），所以使用python的集合（set）去重。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e更新：pn实际上指图片的id，pn=0、rn=60能获取到从0~59这60个URL列表，pn=1、rn=60能获取到从1~60这60个URL列表，所以pn=0和pn=1的列表中当然有59个是重复的。正确的做法是pn=0、rn=60获取0~59这60个URL列表，然后pn=60、rn=60获取60~119这60个列表，以此类推，这样获取到的URL就不会有重复的了。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e获取图片URL列表的简要代码如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 6\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 7\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 8\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e 9\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e10\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e11\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eParseJSON\u003c/span\u003e(self, pn, rn, st):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;http://image.baidu.com/i?tn=resultjson_com\u0026amp;amp;amp;ie=utf-8\u0026amp;amp;amp;word=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%s\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026amp;amp;amp;pn=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026amp;amp;amp;rn=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026amp;amp;amp;z=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e%d\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e(self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eword, pn, rn, self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esize)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e#print(url)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    request \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e urllib\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erequest\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eRequest(url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e url, headers \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e my_header)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    html \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e urllib\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erequest\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eurlopen(request)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    hjson \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e json\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eloads(html\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edecode(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;gbk\u0026#39;\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, len(hjson[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;data\u0026#39;\u003c/span\u003e])\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e):\u003cspan style=\"color:#75715e\"\u003e#最后一个数据为空\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        img_url \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eDecodeURL(hjson[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;data\u0026#39;\u003c/span\u003e][i][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;objURL\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e img_url \u003cspan style=\"color:#f92672\"\u003enot\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e st:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            st\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eadd(img_url)\u003cspan style=\"color:#75715e\"\u003e#去重\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eprogressBar_updated_signal\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eemit()\u003cspan style=\"color:#75715e\"\u003e#更新进度条\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eDecodeURL是解密函数。很奇怪，json最后一个数据是空的。\u003c/p\u003e","title":"百度图片批量下载器（python3 + pyqt5 + eric6 + cx_Freeze4）"},{"content":"2015年6月27日，武汉大学在梅园操场举办了2015年毕业典礼。\n武汉大学2015年毕业典礼，正在发言的是雷军学长[1]\n武汉大学2015年毕业典礼[2]\n坐在台下，顿感恍惚，四年前同样在梅操，同样是这些人，我们举办开学典礼。\n武汉大学2011年开学典礼[3]\n原来梅操到梅操的距离只有四年。\n毕业典礼结束，下午集体收拾行李，大家都默默无语。晚上去红牛—-大一第一次聚餐的地方—-吃最后的晚餐，这次聚餐喝了两箱啤酒，6瓶白酒！白酒下肚，前一口酒落地又向上翻滚，和后一口酒相互撞击，四年的往事喷涌而出。离别之际，每个人都把自己的心声说出来了，说出了自己的家境、对某某的感情、一个宿舍的兄弟情，说出了自己的抱负、未来的理想，再互相拥抱、道一声珍重。\n山水一程，三生有幸[4]\n回到宿舍，所有人吐得一塌糊涂，昏睡过去。也许这就是离别的滋味，折磨着你，让你难受，只有把它吐出来，离开了，平静了，一切就好了。\n第二天醒来，发现隔壁宿舍的几个哥们已经走了，宿舍冷清了许多。去小卖部买了一些非必需品，只为把卡里的几十块钱用掉。打包行李，准备出发。\n毕业了，离开了，那些大一的迷茫、兼职，大二的信息安全竞赛，大三繁重的课业、为保研奋斗的数学建模竞赛，大四悠闲的生活也将躲藏在记忆的某个角落，不再被轻易的发现。武大的樱花、牌坊、樱顶、新图、青楼、梅操电影、珞珈之声、每天晚上在奥场穿着17号球衣跑步的女生、一起练笛的同学、在梅园食堂吃饭的一对情侣、幽默装逼的室友，这一幅幅画面，也将随着时间的车轮，慢慢消散。\n天下没有不散的筵席，我们来到这个世上，就注定要历经悲欢离合。在中国最美丽的大学，度过了我人生中最美好的年华，山水一程，三生有幸，感谢有你。\n别怕，梦的方向叫做闯，青春还没散场！\n参考：\n[1]. 武汉大学官方微博\n[2]. 武大新闻网：http://news.whu.edu.cn/info/1002/43788.htm\n[3]. 守望珞珈的新浪博客：http://blog.sina.com.cn/s/blog_4da93d1f0100t6v9.html\n[4]. 珞珈山水bbs毕业封面：http://bbs.whu.edu.cn/\n","permalink":"http://localhost:1313/posts/2015-06-28-farewell-to-whu/","summary":"\u003cp\u003e2015年6月27日，武汉大学在梅园操场举办了2015年毕业典礼。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"武汉大学2015年毕业典礼，正在发言的是雷军学长[1]\" loading=\"lazy\" src=\"https://i0.wp.com/ww1.sinaimg.cn/large/634fd979jw1etiecww24cj20hs0dctbj.jpg\"\u003e\n武汉大学2015年毕业典礼，正在发言的是雷军学长[1]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"武汉大学2015年毕业典礼[2]\" loading=\"lazy\" src=\"https://i0.wp.com/news.whu.edu.cn/_mediafile/whu_news/2015/06/27/29kc177qza.jpg\"\u003e\n武汉大学2015年毕业典礼[2]\u003c/p\u003e\n\u003cp\u003e坐在台下，顿感恍惚，四年前同样在梅操，同样是这些人，我们举办开学典礼。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"武汉大学2011年开学典礼[3]\" loading=\"lazy\" src=\"https://i0.wp.com/ww3.sinaimg.cn/large/005Muw7zjw1etjl1cs943j30j60bpgpm.jpg\"\u003e\n武汉大学2011年开学典礼[3]\u003c/p\u003e\n\u003cp\u003e原来梅操到梅操的距离只有四年。\u003c/p\u003e\n\u003cp\u003e毕业典礼结束，下午集体收拾行李，大家都默默无语。晚上去红牛—-大一第一次聚餐的地方—-吃最后的晚餐，这次聚餐喝了两箱啤酒，6瓶白酒！白酒下肚，前一口酒落地又向上翻滚，和后一口酒相互撞击，四年的往事喷涌而出。离别之际，每个人都把自己的心声说出来了，说出了自己的家境、对某某的感情、一个宿舍的兄弟情，说出了自己的抱负、未来的理想，再互相拥抱、道一声珍重。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"whu_bbs_graduation_2015\" loading=\"lazy\" src=\"/posts/2015-06-28-farewell-to-whu/whu_bbs_graduation_2015.jpg\"\u003e\n山水一程，三生有幸[4]\u003c/p\u003e\n\u003cp\u003e回到宿舍，所有人吐得一塌糊涂，昏睡过去。也许这就是离别的滋味，折磨着你，让你难受，只有把它吐出来，离开了，平静了，一切就好了。\u003c/p\u003e\n\u003cp\u003e第二天醒来，发现隔壁宿舍的几个哥们已经走了，宿舍冷清了许多。去小卖部买了一些非必需品，只为把卡里的几十块钱用掉。打包行李，准备出发。\u003c/p\u003e\n\u003cp\u003e毕业了，离开了，那些大一的迷茫、兼职，大二的信息安全竞赛，大三繁重的课业、为保研奋斗的数学建模竞赛，大四悠闲的生活也将躲藏在记忆的某个角落，不再被轻易的发现。武大的樱花、牌坊、樱顶、新图、青楼、梅操电影、珞珈之声、每天晚上在奥场穿着17号球衣跑步的女生、一起练笛的同学、在梅园食堂吃饭的一对情侣、幽默装逼的室友，这一幅幅画面，也将随着时间的车轮，慢慢消散。\u003c/p\u003e\n\u003cp\u003e天下没有不散的筵席，我们来到这个世上，就注定要历经悲欢离合。在中国最美丽的大学，度过了我人生中最美好的年华，山水一程，三生有幸，感谢有你。\u003c/p\u003e\n\u003cp\u003e别怕，梦的方向叫做闯，青春还没散场！\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. 武汉大学官方微博\u003c/p\u003e\n\u003cp\u003e[2]. 武大新闻网：\u003ca href=\"http://news.whu.edu.cn/info/1002/43788.htm\"\u003ehttp://news.whu.edu.cn/info/1002/43788.htm\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[3]. 守望珞珈的新浪博客：\u003ca href=\"http://blog.sina.com.cn/s/blog_4da93d1f0100t6v9.html\"\u003ehttp://blog.sina.com.cn/s/blog_4da93d1f0100t6v9.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[4]. 珞珈山水bbs毕业封面：\u003ca href=\"http://bbs.whu.edu.cn/\"\u003ehttp://bbs.whu.edu.cn/\u003c/a\u003e\u003c/p\u003e","title":"再见武大"},{"content":"今天参加了【珞珈阅读广场第89期】《礼物》（影像阅读），感触很多，收获也很多，其中最大的收获就是体会到了交流的乐趣。\n【珞珈阅读广场第89期】《礼物》（影像阅读）[1]\n影片《礼物》讲述了一位功成名就的大叔和身为小偷的女生相互救赎的故事。大叔白手起家，一路打拼，最后坐拥万贯家财，却抛弃了妻子和女儿，导致妻子自杀；女生的父亲早逝，母亲生活又不检点，女生曾差点一刀把母亲捅死，为了偿还男朋友的债务，女生甚至当起了小偷。在一次行窃过程中被大叔抓住，大叔要求女生当他的司机和搬运工，带他去东京，给女儿送一个礼物。在去东京的路上，大叔走访了自己曾经到过的很多地方，但大多数都是激情满怀而去，失望而归。到达东京，当女生知道大叔要送给女儿的礼物是自己的心脏，自己的生命的时候，女生陷入了两难的境地。但是女生最终答应的大叔的请求，帮其送出了礼物，大叔得到了救赎，女生也因为大叔的一句“要好好活着”而坚强乐观的面对生活。\n90分钟的观影结束之后，主持人抛出了如下几个问题。\n关于《礼物》这部电影，主持人提出的若干问题\n因为上学期我也参加过珞珈阅读广场的观影活动，当时也体会到了与他人交流的乐趣，现在马上要毕业了，所以跃跃欲试，想和同学们交流一下。正好第一个问题主持人点名叫我谈一谈。我当时谈了一下我对大叔这种献出自己生命拯救外孙女的行为表示了理解，并表示自己也会做出类似的事情。在场的另外一个老师就表达了他的观点，他对日本这种“野蛮粗暴”的拯救方式不太理解，也不太赞同，大叔最后的死亡过程类似于日本武士的剖腹自尽。\n关于第三个问题，大家也畅所欲言，从很多个方面谈了自己的想法，大部分还是认为大叔想要给外孙女抽一个好彩头的观点。其实这个观点要到最后大叔把这个“大吉”签绑到外孙女的病床上才能感觉得到，在对后面内容不知道的情况下，我认为最合理的解释应该是这一行为体现了大叔好强甚至“蛮横”的性格，因为他抽签的时候说自己从白手起家到现在亿万富翁，就像中了头彩一样，那么我现在抽签，也要像我经商一样，取得最好的结果。有的同学甚至解读出了大叔“执念”这一层含义。\n对于第四个问题，好几个同学分享了自己的经历或者想法。我当时表达了“虽然你现在面临不幸，请不要过多的抱怨，珍惜当下，因为你现在所遭遇的，正是你将来所怀念的；当你再故地重游的时候，也许像这位大叔一样，再也找不到当初那种美好的感觉了。“，并顺带告诉学弟学妹们，珍惜在武大的美好时光，自己马上要毕业了，对武大的一花一木都非常的不舍。\n最后一个问题，主持人给出了很好的解答，并且阐明了要拯救一个身处绝望的人的困难性，很精彩。\n交流过程中很有意思的一件事情是，主持人给出了这样一个观点”婚姻或家庭不幸的人，其子女的性格往往也会偏离常态，并且子女的婚姻或家庭也很可能会不幸。”，对于这个观点，大家的反应比较激烈，特别是在场的那位老师，表达了他的反对意见。我因为自己的家庭环境原因，反而表示了积极的一面，就是父母婚姻不幸的人，其孩子有可能反而更加珍惜婚姻，珍惜家庭，所以家庭有可能比一般家庭更加幸福。当然也有同学表示对爱的人抓得太紧，有可能适得其反，导致婚姻的破裂。主持人讲了这样一段话，很好的表达了这个观点：\n让爱恰到好处－不让疯长的孤独烧毁世界，也不让泛滥的博爱窒息自由。\n电影从晚上7:00到8:30，讨论从8:30到10:00。讨论结束的时候，主持人把本期两本书分别赠送给了我和另一位硕士毕业生，我的赠书是《那一天》。\n赠书《那一天》[2]\n讨论结束，临走的时候大家还意犹未尽，主持人对大家的讨论表示感谢，此时有一位同学表示主持人的发言也很不错。确实，整个讨论环节，主持人很好的带动起了大家发言的兴趣和积极性，包括问题的设置，主持人的点名提问以及主持人自身精彩的解说，都非常好的带动了现场的气氛，打开了观众的话匣子。不久前我刚好参加过这位主持人主持的”周末艺苑·外院专场“演出，当时主持人随机应变的能力和绝妙的口才给我留下了深刻的印象，学弟不错，加油！\n这次观影交流达到了真正交流的目的，观众中有大一大二的新生，有即将毕业的本科生和研究生，也有已经成家的中年老师，大家基于各自的背景，表达自己的想法，聆听他人的观点，达到了很好的思维发散、观点碰撞的目的。此时我想到了高中背的萧伯纳讲过的一句话：\n两个人各有一个苹果，交换之后，每个人还是只有一个苹果；然而，当两个人各有一种思想，交换之后，每个人却拥有了两种思想。\n前几天观看了踪点剧社的两部毕业大戏《理想》和《禁闭》，两部很有深意的话剧，话剧结束的时候，也有一个短暂的交流会，也很精彩。\n人很多时候会沉浸在自我的世界中，产生很多偏见，此时不妨听一听他人的观点，也许会豁然开朗或眼前一亮，觉得世界真奇妙。最后用H老师的一句话结束：技术上要多钻研，技术外要多沟通，生存两个法则。\n希望每个人都能发现并享受交流的乐趣。\n参考：\n[1]. 珞珈阅读广场第89期公告：http://www.lib.whu.edu.cn/news/view.asp?id=3354\n[2]. 豆瓣读书《那一天》：http://book.douban.com/subject/25904481/\n","permalink":"http://localhost:1313/posts/2015-06-12-the-joy-of-communication/","summary":"\u003cp\u003e今天参加了\u003ca href=\"http://www.lib.whu.edu.cn/news/view.asp?id=3354\"\u003e【珞珈阅读广场第89期】《礼物》（影像阅读）\u003c/a\u003e，感触很多，收获也很多，其中最大的收获就是体会到了交流的乐趣。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"【珞珈阅读广场第89期】《礼物》（影像阅读）[1]\" loading=\"lazy\" src=\"https://i0.wp.com/www.lib.whu.edu.cn/news/tc/readSqua89.jpg\"\u003e\n【珞珈阅读广场第89期】《礼物》（影像阅读）[1]\u003c/p\u003e\n\u003cp\u003e影片\u003ca href=\"http://movie.douban.com/subject/25878911/\"\u003e《礼物》\u003c/a\u003e讲述了一位功成名就的大叔和身为小偷的女生相互救赎的故事。大叔白手起家，一路打拼，最后坐拥万贯家财，却抛弃了妻子和女儿，导致妻子自杀；女生的父亲早逝，母亲生活又不检点，女生曾差点一刀把母亲捅死，为了偿还男朋友的债务，女生甚至当起了小偷。在一次行窃过程中被大叔抓住，大叔要求女生当他的司机和搬运工，带他去东京，给女儿送一个礼物。在去东京的路上，大叔走访了自己曾经到过的很多地方，但大多数都是激情满怀而去，失望而归。到达东京，当女生知道大叔要送给女儿的礼物是自己的心脏，自己的生命的时候，女生陷入了两难的境地。但是女生最终答应的大叔的请求，帮其送出了礼物，大叔得到了救赎，女生也因为大叔的一句“要好好活着”而坚强乐观的面对生活。\u003c/p\u003e\n\u003cp\u003e90分钟的观影结束之后，主持人抛出了如下几个问题。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"关于《礼物》这部电影，主持人提出的若干问题\" loading=\"lazy\" src=\"/posts/2015-06-12-the-joy-of-communication/questions-about-gift.jpg\"\u003e\n关于《礼物》这部电影，主持人提出的若干问题\u003c/p\u003e\n\u003cp\u003e因为上学期我也参加过珞珈阅读广场的观影活动，当时也体会到了与他人交流的乐趣，现在马上要毕业了，所以跃跃欲试，想和同学们交流一下。正好第一个问题主持人点名叫我谈一谈。我当时谈了一下我对大叔这种献出自己生命拯救外孙女的行为表示了理解，并表示自己也会做出类似的事情。在场的另外一个老师就表达了他的观点，他对日本这种“野蛮粗暴”的拯救方式不太理解，也不太赞同，大叔最后的死亡过程类似于日本武士的剖腹自尽。\u003c/p\u003e\n\u003cp\u003e关于第三个问题，大家也畅所欲言，从很多个方面谈了自己的想法，大部分还是认为大叔想要给外孙女抽一个好彩头的观点。其实这个观点要到最后大叔把这个“大吉”签绑到外孙女的病床上才能感觉得到，在对后面内容不知道的情况下，我认为最合理的解释应该是这一行为体现了大叔好强甚至“蛮横”的性格，因为他抽签的时候说自己从白手起家到现在亿万富翁，就像中了头彩一样，那么我现在抽签，也要像我经商一样，取得最好的结果。有的同学甚至解读出了大叔“执念”这一层含义。\u003c/p\u003e\n\u003cp\u003e对于第四个问题，好几个同学分享了自己的经历或者想法。我当时表达了“虽然你现在面临不幸，请不要过多的抱怨，珍惜当下，因为你现在所遭遇的，正是你将来所怀念的；当你再故地重游的时候，也许像这位大叔一样，再也找不到当初那种美好的感觉了。“，并顺带告诉学弟学妹们，珍惜在武大的美好时光，自己马上要毕业了，对武大的一花一木都非常的不舍。\u003c/p\u003e\n\u003cp\u003e最后一个问题，主持人给出了很好的解答，并且阐明了要拯救一个身处绝望的人的困难性，很精彩。\u003c/p\u003e\n\u003cp\u003e交流过程中很有意思的一件事情是，主持人给出了这样一个观点”婚姻或家庭不幸的人，其子女的性格往往也会偏离常态，并且子女的婚姻或家庭也很可能会不幸。”，对于这个观点，大家的反应比较激烈，特别是在场的那位老师，表达了他的反对意见。我因为自己的家庭环境原因，反而表示了积极的一面，就是父母婚姻不幸的人，其孩子有可能反而更加珍惜婚姻，珍惜家庭，所以家庭有可能比一般家庭更加幸福。当然也有同学表示对爱的人抓得太紧，有可能适得其反，导致婚姻的破裂。主持人讲了这样一段话，很好的表达了这个观点：\u003c/p\u003e\n\u003cp\u003e让爱恰到好处－不让疯长的孤独烧毁世界，也不让泛滥的博爱窒息自由。\u003c/p\u003e\n\u003cp\u003e电影从晚上7:00到8:30，讨论从8:30到10:00。讨论结束的时候，主持人把本期两本书分别赠送给了我和另一位硕士毕业生，我的赠书是\u003ca href=\"http://book.douban.com/subject/25904481/\"\u003e《那一天》\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"赠书《那一天》[2]\" loading=\"lazy\" src=\"https://img1.doubanio.com/lpic/s27950209.jpg\"\u003e\n赠书《那一天》[2]\u003c/p\u003e\n\u003cp\u003e讨论结束，临走的时候大家还意犹未尽，主持人对大家的讨论表示感谢，此时有一位同学表示主持人的发言也很不错。确实，整个讨论环节，主持人很好的带动起了大家发言的兴趣和积极性，包括问题的设置，主持人的点名提问以及主持人自身精彩的解说，都非常好的带动了现场的气氛，打开了观众的话匣子。不久前我刚好参加过这位主持人主持的”周末艺苑·外院专场“演出，当时主持人随机应变的能力和绝妙的口才给我留下了深刻的印象，学弟不错，加油！\u003c/p\u003e\n\u003cp\u003e这次观影交流达到了真正交流的目的，观众中有大一大二的新生，有即将毕业的本科生和研究生，也有已经成家的中年老师，大家基于各自的背景，表达自己的想法，聆听他人的观点，达到了很好的思维发散、观点碰撞的目的。此时我想到了高中背的萧伯纳讲过的一句话：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e两个人各有一个苹果，交换之后，每个人还是只有一个苹果；然而，当两个人各有一种思想，交换之后，每个人却拥有了两种思想。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e前几天观看了踪点剧社的两部毕业大戏《理想》和《禁闭》，两部很有深意的话剧，话剧结束的时候，也有一个短暂的交流会，也很精彩。\u003c/p\u003e\n\u003cp\u003e人很多时候会沉浸在自我的世界中，产生很多偏见，此时不妨听一听他人的观点，也许会豁然开朗或眼前一亮，觉得世界真奇妙。最后用H老师的一句话结束：\u003cstrong\u003e技术上要多钻研，技术外要多沟通，生存两个法则。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e希望每个人都能发现并享受交流的乐趣。\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. 珞珈阅读广场第89期公告：\u003ca href=\"http://www.lib.whu.edu.cn/news/view.asp?id=3354\"\u003ehttp://www.lib.whu.edu.cn/news/view.asp?id=3354\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[2]. 豆瓣读书《那一天》：\u003ca href=\"http://book.douban.com/subject/25904481/\"\u003ehttp://book.douban.com/subject/25904481/\u003c/a\u003e\u003c/p\u003e","title":"交流的乐趣"},{"content":"前几天给毕设指导老师发邮件，麻烦老师写申优推荐理由，老师回复我说她的儿子这几天高考，她不在学院，最晚要9号才能帮我写。我才意识到又是一年高考时，距离自己参加高考已经过去了四个春秋，但高中生活的场景在脑海中却依稀可见，想来是要写一篇文章追忆那平凡或不平凡的高中生活。\n在我读初中的时候，县里只有一所高中－县一中，每年中考之前，县一中都会组织一次提前批考试，如果提前批考试被录取了，正式中考的时候只要过线就能上，和现在的大学自主招生有点类似。我当时考试成绩不错，大概是全县十名左右，被分到所谓的奥赛班了。正式中考完之后，县五中开始正式招生，五中是抚州市一个老板办的私立学校，据说办学模式借鉴临川一中。当时县五中为了抢占优质生源，承诺只要去五中读书，除了3年学费全免之外，还额外奖励3000块钱，而且如果高考考上清华北大，奖励10万，考上其他十大名牌大学，奖励3万。当时考虑到五中刚开始办学，又借鉴临川一中的办学模式，老师大多是抚州的“名师”，教学质量应该不错，而且去五中读书能省不少钱，爸爸建议我去五中。我当时其实是不太情愿的，毕竟一中奥赛班聚集了全县最优秀的老师和同学，不论是环境还是各项措施，都比刚办学的五中要好。不过我还是以“只要自己认真读，在哪个学校都能取得好成绩”的理由安慰自己，去了五中。\n说实话，五中学生的层次确实要比一中学生差。不过好在学校把几十个成绩比较好的安排在了一个班里，配备了更好的老师，实行特殊管理。\n在我的印象中，高中的三年过得都一样，高三并没有比高一高二累多少，或者说高一高二并没有比高三轻松多少。每天早上6点准时起床，洗漱完之后6:20做早操，高三的时候不用做早操，改早自习了。大概6:50吃早餐，7:10开始早读。因为是理科，早读的内容不外乎英语单词、英语作文、语文背诵诗词、语文作文。8:05开始正式上课，上午四节课一直上到11:40。下课之后回家吃饭，因为妈妈在校内陪读，所以午餐能在15分钟之内解决，然后马上回到教室做几道题或者看一两个作文素材。下午1点准时午休半个小时，2:05正式上课，下午三节课一直上到4:40，好像高三的时候改成了四节课，记不清了。和中午一样，快速解决晚餐，马上回到教室，首先复习或学习语文字词，包括拼音和常考成语，当时基本把《现代汉语词典》翻熟了；然后正式晚自习，一直到晚上11点才回家洗漱睡觉。当时学校规定其他班学生10点之后就必须离开教室，但是我们班特殊规定可以自习到11点。回家洗漱完之后大概11:30了，高三的时候，我还经常在睡之前打着小电筒复习一下白天学习的内容。\n学校每两个星期放1天假，再过两个星期放2天半假，大多数同学只有在2天半假的时候才回家一趟。放假期间，除了做一两套卷子，大多数时间是在看电视，另外会去书店逛一次，不过买的大多数是高考复习资料，仅有的算得上是课外读物的就是《疯狂阅读》或者《读者》之类的了，小县城的书店也没有其他的“闲书”。高考要求阅读的几篇经典名著，几乎没有完整阅读过纸质版，高三的时候为了应付高考，时间紧，任务重，直接从机房下载了《巴黎圣母院》、《堂吉诃德》等改编电影，这才稍微了解了一下主要内容。学校也没有像样的图书馆，在石城那个小县城，不可能买到这些“高大上”的书。我相信大城市的很多高中生肯定看过很多这类世界名著，周末或者放假的时候也是在忙着学琴棋书画。这可能就是所谓的城乡教育差别吧，虽然这种差别在高考的时候体现得并不明显，农村的孩子在高中稍微刻苦一下，也能上不错的大学；但是一旦到了大学，大城市的孩子和我这种从农村走出来的孩子的差别一下就能看出来。大城市的孩子不论是在交际、口才、学识、才艺等方面都能轻松碾压农村的孩子，农村孩子虽然你很刻苦，卷面成绩不错，但是知识面不够宽广，格局比较小，几乎没有才艺；并不是城里人歧视你，不和你玩，但是和你聊美术，你懂吗，和你聊莎士比亚剧作，你看过吗，和你排练音乐舞蹈，你会吗。来到武大之后，我对这种城乡读书孩子之间的差别真的深有体会，无论我多么努力，好像总达不到他们的高度，总是无法融入他们的生活。\n高中不像大学，每个班有固定的教师，每个人有固定的座位，有自己的“左邻右舍”，坐在座位上，真的感觉很温暖。每到下课的时候，班上都闹哄哄的，同班同学之间的交流也很多，班集体的荣誉感以及个人的归属感也很强。每次打扫卫生的时候，几乎要经过每个同学的座位，问一问有没有垃圾要处理的；每次发考卷的时候，也会左顾右盼，相互逗个乐。一年中要数元旦晚会最为热闹，犹记得高三那年元旦，我为了演唱“海阔天空”，每天回家吃饭的时候就听mp3，走在路上也会小声哼唱，当然对于五音不全的我，演唱效果并不是很好:-) 晚会当天下午开始布置场地，所有人把书搬回宿舍，清空教室，在玻璃窗上贴上气球，圣诞树贴纸，或者某个小画家直接在上面画一幅画，电风扇和墙上都会挂上彩带；在教室四周摆上课桌，课桌上摆上事先买好的瓜子、花生、糖果、饮料等；同学们借来音响话筒，老师也把自己的笔记本搬到教室，一场简朴晚会现场就布置好了。晚会的所有工作人员、演员、主持人都是自己班上的同学，大家欢聚一堂，过着小集体的节日，有时候在同学和主持人的怂恿下，老师们也会激情献唱一首。现在回想起来，这种小集体归属感真的很美好，高中毕业之后，我大概再也没有过这种感觉了。\n春节过后，就是高考的紧要关头了，每周一的班会课上，老师都会给我们加油鼓劲，告诉我们大学有多轻松美好。百日会战那天，班主任甚至亲自泼墨，写下“辛苦数日，幸福一生”的对联，贴在教室的后墙上。高三，每天就是不停的做卷子、刷题，日考、周考、月考，不断的考试，往往上一张考卷还没有讲评，下一次考试又到了。考得多了，对成绩也不那么看重了，不过也基本稳定在前三。\n图片来自[1]\n距离高考只剩一个星期的时候，题量开始下降，老师也变得温柔起来，开始提醒我们注意饮食，调整生物钟，保持充足睡眠等。考前3天，学校放假，让我们回家吃好喝好，放松心情。考前1天，看考场，当时坐我前面的一个同学找到我，叫我给他抄，并威胁我如果不给他抄，则影响我考试，碰到这样一个人渣，对我的心情还是有一定影响的，我也没敢把这件事告诉我妈。当天晚上英语老师找我谈话，宽慰我，跟我说考试的时候不要遮住试卷，他能不能抄到是他的事了，况且他最多只能抄到选择填空题，主观题还得靠真才实学。高考那两天，全校其他年级放假，为的是给所有考生创造一个安静舒适的校园环境，这一点给学校点赞。高考第一天和第二天上午还算顺利，正常发挥，第二天午休没有睡着，下午考英语的时候，听力几乎没有进入状态，哈欠连连，英语是我考得最差的一科了，当然英语本来就不是我的强项。\n考完之后，回到家中，妈妈给我和哥哥洗了两个甜瓜吃，寓意我们苦尽甘来:-) 高考完的那个暑假，妈妈说让我们好好在家待着休养生息，所以基本过着猪一样的生活。6月底高考成绩出来了，六百多吧，和估分差不多，纠结的是填志愿。当时也不像现在，互联网这么发达，基本上是通过《全国普通高等学校报考指南》了解每个学校，对提前批的情况也不甚了解。后来根据往年分数线以及自己的兴趣，报了武大、吉大、川大这几个学校，很幸运，录取了第一志愿第一专业WHUCS。说实话，在填志愿之前，我只知道清华北大这两个学校，对武大这所“全国最美丽的大学”一无所知，我不是狂妄自大，而是孤陋寡闻。\n高中真的很累、很辛苦，要想坚持下去，一定要找一个可靠的精神支柱，不论是做什么事都是这样，一直以来，支持我勇往直前的都是我的亲人和我想要改变命运的决心！要说高中3年的收获，那就是它磨砺了我的意志，增强了我忍受孤独的能力，当然高中并不是我最孤独的时候，至少有我前面提到的小集体归属感；高中三年，也认识了很要好的同学兼老乡WQ、WS和MZ。\n对了，高中那会每个人都会有一个座右铭，我也不例外，很大众化，汪国真的“既然选择了远方，便只顾风雨兼程”。惊闻汪老师于2015年4月26日逝世，令人嘘唏。\n参考：\n[1]. 永不过时的高考记忆\n","permalink":"http://localhost:1313/posts/2015-06-08-my-high-school-life/","summary":"\u003cp\u003e前几天给毕设指导老师发邮件，麻烦老师写申优推荐理由，老师回复我说她的儿子这几天高考，她不在学院，最晚要9号才能帮我写。我才意识到又是一年高考时，距离自己参加高考已经过去了四个春秋，但高中生活的场景在脑海中却依稀可见，想来是要写一篇文章追忆那平凡或不平凡的高中生活。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"classroom\" loading=\"lazy\" src=\"/posts/2015-06-08-my-high-school-life/classroom.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e在我读初中的时候，县里只有一所高中－县一中，每年中考之前，县一中都会组织一次提前批考试，如果提前批考试被录取了，正式中考的时候只要过线就能上，和现在的大学自主招生有点类似。我当时考试成绩不错，大概是全县十名左右，被分到所谓的奥赛班了。正式中考完之后，县五中开始正式招生，五中是抚州市一个老板办的私立学校，据说办学模式借鉴临川一中。当时县五中为了抢占优质生源，承诺只要去五中读书，除了3年学费全免之外，还额外奖励3000块钱，而且如果高考考上清华北大，奖励10万，考上其他十大名牌大学，奖励3万。当时考虑到五中刚开始办学，又借鉴临川一中的办学模式，老师大多是抚州的“名师”，教学质量应该不错，而且去五中读书能省不少钱，爸爸建议我去五中。我当时其实是不太情愿的，毕竟一中奥赛班聚集了全县最优秀的老师和同学，不论是环境还是各项措施，都比刚办学的五中要好。不过我还是以“只要自己认真读，在哪个学校都能取得好成绩”的理由安慰自己，去了五中。\u003c/p\u003e\n\u003cp\u003e说实话，五中学生的层次确实要比一中学生差。不过好在学校把几十个成绩比较好的安排在了一个班里，配备了更好的老师，实行特殊管理。\u003c/p\u003e\n\u003cp\u003e在我的印象中，高中的三年过得都一样，高三并没有比高一高二累多少，或者说高一高二并没有比高三轻松多少。每天早上6点准时起床，洗漱完之后6:20做早操，高三的时候不用做早操，改早自习了。大概6:50吃早餐，7:10开始早读。因为是理科，早读的内容不外乎英语单词、英语作文、语文背诵诗词、语文作文。8:05开始正式上课，上午四节课一直上到11:40。下课之后回家吃饭，因为妈妈在校内陪读，所以午餐能在15分钟之内解决，然后马上回到教室做几道题或者看一两个作文素材。下午1点准时午休半个小时，2:05正式上课，下午三节课一直上到4:40，好像高三的时候改成了四节课，记不清了。和中午一样，快速解决晚餐，马上回到教室，首先复习或学习语文字词，包括拼音和常考成语，当时基本把《现代汉语词典》翻熟了；然后正式晚自习，一直到晚上11点才回家洗漱睡觉。当时学校规定其他班学生10点之后就必须离开教室，但是我们班特殊规定可以自习到11点。回家洗漱完之后大概11:30了，高三的时候，我还经常在睡之前打着小电筒复习一下白天学习的内容。\u003c/p\u003e\n\u003cp\u003e学校每两个星期放1天假，再过两个星期放2天半假，大多数同学只有在2天半假的时候才回家一趟。放假期间，除了做一两套卷子，大多数时间是在看电视，另外会去书店逛一次，不过买的大多数是高考复习资料，仅有的算得上是课外读物的就是《疯狂阅读》或者《读者》之类的了，小县城的书店也没有其他的“闲书”。高考要求阅读的几篇经典名著，几乎没有完整阅读过纸质版，高三的时候为了应付高考，时间紧，任务重，直接从机房下载了《巴黎圣母院》、《堂吉诃德》等改编电影，这才稍微了解了一下主要内容。学校也没有像样的图书馆，在石城那个小县城，不可能买到这些“高大上”的书。我相信大城市的很多高中生肯定看过很多这类世界名著，周末或者放假的时候也是在忙着学琴棋书画。这可能就是所谓的城乡教育差别吧，虽然这种差别在高考的时候体现得并不明显，农村的孩子在高中稍微刻苦一下，也能上不错的大学；但是一旦到了大学，大城市的孩子和我这种从农村走出来的孩子的差别一下就能看出来。大城市的孩子不论是在交际、口才、学识、才艺等方面都能轻松碾压农村的孩子，农村孩子虽然你很刻苦，卷面成绩不错，但是知识面不够宽广，格局比较小，几乎没有才艺；并不是城里人歧视你，不和你玩，但是和你聊美术，你懂吗，和你聊莎士比亚剧作，你看过吗，和你排练音乐舞蹈，你会吗。来到武大之后，我对这种城乡读书孩子之间的差别真的深有体会，无论我多么努力，好像总达不到他们的高度，总是无法融入他们的生活。\u003c/p\u003e\n\u003cp\u003e高中不像大学，每个班有固定的教师，每个人有固定的座位，有自己的“左邻右舍”，坐在座位上，真的感觉很温暖。每到下课的时候，班上都闹哄哄的，同班同学之间的交流也很多，班集体的荣誉感以及个人的归属感也很强。每次打扫卫生的时候，几乎要经过每个同学的座位，问一问有没有垃圾要处理的；每次发考卷的时候，也会左顾右盼，相互逗个乐。一年中要数元旦晚会最为热闹，犹记得高三那年元旦，我为了演唱“海阔天空”，每天回家吃饭的时候就听mp3，走在路上也会小声哼唱，当然对于五音不全的我，演唱效果并不是很好:-) 晚会当天下午开始布置场地，所有人把书搬回宿舍，清空教室，在玻璃窗上贴上气球，圣诞树贴纸，或者某个小画家直接在上面画一幅画，电风扇和墙上都会挂上彩带；在教室四周摆上课桌，课桌上摆上事先买好的瓜子、花生、糖果、饮料等；同学们借来音响话筒，老师也把自己的笔记本搬到教室，一场简朴晚会现场就布置好了。晚会的所有工作人员、演员、主持人都是自己班上的同学，大家欢聚一堂，过着小集体的节日，有时候在同学和主持人的怂恿下，老师们也会激情献唱一首。现在回想起来，这种小集体归属感真的很美好，高中毕业之后，我大概再也没有过这种感觉了。\u003c/p\u003e\n\u003cp\u003e春节过后，就是高考的紧要关头了，每周一的班会课上，老师都会给我们加油鼓劲，告诉我们大学有多轻松美好。百日会战那天，班主任甚至亲自泼墨，写下“辛苦数日，幸福一生”的对联，贴在教室的后墙上。高三，每天就是不停的做卷子、刷题，日考、周考、月考，不断的考试，往往上一张考卷还没有讲评，下一次考试又到了。考得多了，对成绩也不那么看重了，不过也基本稳定在前三。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"图片来自[1]\" loading=\"lazy\" src=\"https://i0.wp.com/a3.att.hudong.com/66/83/300000908235130708833431958_950.jpg\"\u003e\n图片来自[1]\u003c/p\u003e\n\u003cp\u003e距离高考只剩一个星期的时候，题量开始下降，老师也变得温柔起来，开始提醒我们注意饮食，调整生物钟，保持充足睡眠等。考前3天，学校放假，让我们回家吃好喝好，放松心情。考前1天，看考场，当时坐我前面的一个同学找到我，叫我给他抄，并威胁我如果不给他抄，则影响我考试，碰到这样一个人渣，对我的心情还是有一定影响的，我也没敢把这件事告诉我妈。当天晚上英语老师找我谈话，宽慰我，跟我说考试的时候不要遮住试卷，他能不能抄到是他的事了，况且他最多只能抄到选择填空题，主观题还得靠真才实学。高考那两天，全校其他年级放假，为的是给所有考生创造一个安静舒适的校园环境，这一点给学校点赞。高考第一天和第二天上午还算顺利，正常发挥，第二天午休没有睡着，下午考英语的时候，听力几乎没有进入状态，哈欠连连，英语是我考得最差的一科了，当然英语本来就不是我的强项。\u003c/p\u003e\n\u003cp\u003e考完之后，回到家中，妈妈给我和哥哥洗了两个甜瓜吃，寓意我们苦尽甘来:-) 高考完的那个暑假，妈妈说让我们好好在家待着休养生息，所以基本过着猪一样的生活。6月底高考成绩出来了，六百多吧，和估分差不多，纠结的是填志愿。当时也不像现在，互联网这么发达，基本上是通过《全国普通高等学校报考指南》了解每个学校，对提前批的情况也不甚了解。后来根据往年分数线以及自己的兴趣，报了武大、吉大、川大这几个学校，很幸运，录取了第一志愿第一专业WHUCS。说实话，在填志愿之前，我只知道清华北大这两个学校，对武大这所“全国最美丽的大学”一无所知，我不是狂妄自大，而是孤陋寡闻。\u003c/p\u003e\n\u003cp\u003e高中真的很累、很辛苦，要想坚持下去，一定要找一个可靠的精神支柱，不论是做什么事都是这样，一直以来，支持我勇往直前的都是我的亲人和我想要改变命运的决心！要说高中3年的收获，那就是它磨砺了我的意志，增强了我忍受孤独的能力，当然高中并不是我最孤独的时候，至少有我前面提到的小集体归属感；高中三年，也认识了很要好的同学兼老乡WQ、WS和MZ。\u003c/p\u003e\n\u003cp\u003e对了，高中那会每个人都会有一个座右铭，我也不例外，很大众化，汪国真的“既然选择了远方，便只顾风雨兼程”。惊闻汪老师于2015年4月26日逝世，令人嘘唏。\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. \u003ca href=\"http://tupian.baike.com/84178/2.html\"\u003e永不过时的高考记忆\u003c/a\u003e\u003c/p\u003e","title":"我的高中生活"},{"content":"5月19号的中午吃完饭后随手刷了一下朋友圈，发现MS表哥分享了一个链接，说家乡发生了十多年未遇洪灾。仔细看了一下，发现这次洪灾真的很严重，然后就给妈妈打了个电话，妈妈说从昨天下午开始下大雨，到晚上下暴雨，我家后院有一座小山，也出现了滑坡；附近的一座桥也淹了，家门口的一片农田（不是我家的）也全被淹了。妈妈说她也是头一回看到这么大而持久的暴雨。\n后来看QQ空间，全是关于家乡灾情的状态，很多新闻媒体也报道了。县的下游地区受灾比较严重，隔壁有个叫横江的村镇，这个镇因为坐落在横江水旁边，受灾最严重，听说整个镇断水断电，都快成孤岛了。我有一个亲戚在横江，他们家一层楼就被淹了，很多人家养的鸡鸭鱼猪等都被冲走了，几乎所有的农田被淹，损失真的很严重。\n横江镇政府被淹情形[1]\n这是通往横江镇的公路，下坡后就是横江镇，可以看到整个村镇已经被淹[2]\n停在路边的小汽车都被冲走了！[2]\n整个村子一片汪洋[2]\n特写[2]\n估计我家后山的滑坡比这严重很多[1]\n我记得我小的时候（大概五六岁？），可能是1998年吧，也发生过一次特大洪水，当时也是晚上，爷爷把我和哥哥叫起来，爬到后山上去避难了。当时虽然洪水也大，但也就一晚上，第二天很快就退了，不像这次持续的特大暴雨。\n晚些时候我又打电话给妈妈，听妈妈说因为怕山体滑坡，已经去半山腰上的烤烟房里过夜了。那个烤烟房是我还没出生（或者我很小）的时候盖的土坯房，专门用来烤烟的。 后来一家人常年在外，也没怎么管它，很多瓦都碎了。前几年我爸回家补漏，现在是不会漏水了。但是毕竟年代久远了，而且是土坯房，又下那么大雨，还是不放心。想到妈妈一个人在家，晚上要住在一个没有水没有电的土坯房里，心里真不是滋味。以后有钱了应该把烤烟房改建成砖房，万一出现什么灾害，也有个落脚的地方。\n刚看了下天气预报，现在还在下雨，周六周一还有雷阵雨，希望不要再滑坡了。\n石城5月份历史天气，从15号开始连下了6天的大雨，以18,19,20为最[4]\n在这次灾害中，也要感谢当今发达的互联网，让全国各地的人实时了解家乡的情况，很多救援队也纷纷赶赴家乡展开救援。希望洪水快快退去，还家乡人民安宁的生活。\n此次特大灾害直播：http://mp.weixin.qq.com/s?__biz=MjM5MzEzODgyMA==\u0026amp;mid=206189476\u0026amp;idx=1\u0026amp;sn=3e2cc4f01b7ff42f933420063f4e5a74#rd\n520，对石城说“我爱你”：http://v.qq.com/boke/page/z/0/2/z0154dvf6d2.html\n灾后场景：http://mp.weixin.qq.com/s?__biz=MzAwNDUzMjg5OA==\u0026amp;mid=208435542\u0026amp;idx=1\u0026amp;sn=346f8e17448bf3a976c55591636b6e02\u0026amp;scene=1\u0026amp;from=singlemessage\u0026amp;isappinstalled=0#rd\n2015年5月21号石城新闻：http://v.qq.com/boke/page/w/0/4/w015442vnu4.html\n相关媒体报道：\nhttp://pic.people.com.cn/n/2015/0520/c1016-27031534.html?k=1\nhttp://news.163.com/15/0521/05/AQ48GSQU00014Q4P.html\nhttp://news.163.com/15/0521/06/AQ4CVUIF00014AEE.html\n参考：\n[1]. 赣江源头微信公众号文章\n[2]. 石城热线微信公众号文章\n[3]. 百度搜索“石城天气”，数据来源中国天气网\n[4]. 数据来源：http://15tianqi.cn/shicheng5yuetianqi/\n","permalink":"http://localhost:1313/posts/2015-05-21-severe-flooding-hits-my-hometown/","summary":"\u003cp\u003e5月19号的中午吃完饭后随手刷了一下朋友圈，发现MS表哥分享了一个\u003ca href=\"http://mp.weixin.qq.com/s?__biz=MzAwNDUzMjg5OA==\u0026amp;mid=208222165\u0026amp;idx=1\u0026amp;sn=8586e0f5d9830cfb4cf51921bfe1269a\u0026amp;scene=2\u0026amp;from=timeline\u0026amp;isappinstalled=0#rd\"\u003e链接\u003c/a\u003e，说家乡发生了十多年未遇洪灾。仔细看了一下，发现这次洪灾真的很严重，然后就给妈妈打了个电话，妈妈说从昨天下午开始下大雨，到晚上下暴雨，我家后院有一座小山，也出现了滑坡；附近的一座桥也淹了，家门口的一片农田（不是我家的）也全被淹了。妈妈说她也是头一回看到这么大而持久的暴雨。\u003c/p\u003e\n\u003cp\u003e后来看QQ空间，全是关于家乡灾情的状态，很多新闻媒体也报道了。县的下游地区受灾比较严重，隔壁有个叫横江的村镇，这个镇因为坐落在横江水旁边，受灾最严重，听说整个镇断水断电，都快成孤岛了。我有一个亲戚在横江，他们家一层楼就被淹了，很多人家养的鸡鸭鱼猪等都被冲走了，几乎所有的农田被淹，损失真的很严重。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"横江镇政府被淹情形[1]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-1.jpg\"\u003e\n横江镇政府被淹情形[1]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"这是通往横江镇的公路，下坡后就是横江镇，可以看到整个村镇已经被淹[2]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-2.jpg\"\u003e\n这是通往横江镇的公路，下坡后就是横江镇，可以看到整个村镇已经被淹[2]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"停在路边的小汽车都被冲走了！[2]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-3.jpg\"\u003e\n停在路边的小汽车都被冲走了！[2]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"整个村子一片汪洋[2]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-4.jpg\"\u003e\n整个村子一片汪洋[2]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"特写[2]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-5.jpg\"\u003e\n特写[2]\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"估计我家后山的滑坡比这严重很多[1]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/flood201505-6.jpg\"\u003e\n估计我家后山的滑坡比这严重很多[1]\u003c/p\u003e\n\u003cp\u003e我记得我小的时候（大概五六岁？），可能是1998年吧，也发生过一次特大洪水，当时也是晚上，爷爷把我和哥哥叫起来，爬到后山上去避难了。当时虽然洪水也大，但也就一晚上，第二天很快就退了，不像这次持续的特大暴雨。\u003c/p\u003e\n\u003cp\u003e晚些时候我又打电话给妈妈，听妈妈说因为怕山体滑坡，已经去半山腰上的烤烟房里过夜了。那个烤烟房是我还没出生（或者我很小）的时候盖的土坯房，专门用来烤烟的。 后来一家人常年在外，也没怎么管它，很多瓦都碎了。前几年我爸回家补漏，现在是不会漏水了。但是毕竟年代久远了，而且是土坯房，又下那么大雨，还是不放心。想到妈妈一个人在家，晚上要住在一个没有水没有电的土坯房里，心里真不是滋味。以后有钱了应该把烤烟房改建成砖房，万一出现什么灾害，也有个落脚的地方。\u003c/p\u003e\n\u003cp\u003e刚看了下天气预报，现在还在下雨，周六周一还有雷阵雨，希望不要再滑坡了。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"石城5月份历史天气，从15号开始连下了6天的大雨，以18,19,20为最[4]\" loading=\"lazy\" src=\"/posts/2015-05-21-severe-flooding-hits-my-hometown/shicheng_history_weather.png\"\u003e\n石城5月份历史天气，从15号开始连下了6天的大雨，以18,19,20为最[4]\u003c/p\u003e\n\u003cp\u003e在这次灾害中，也要感谢当今发达的互联网，让全国各地的人实时了解家乡的情况，很多救援队也纷纷赶赴家乡展开救援。希望洪水快快退去，还家乡人民安宁的生活。\u003c/p\u003e\n\u003cp\u003e此次特大灾害直播：\u003ca href=\"http://mp.weixin.qq.com/s?__biz=MjM5MzEzODgyMA==\u0026amp;mid=206189476\u0026amp;idx=1\u0026amp;sn=3e2cc4f01b7ff42f933420063f4e5a74#rd\"\u003ehttp://mp.weixin.qq.com/s?__biz=MjM5MzEzODgyMA==\u0026amp;mid=206189476\u0026amp;idx=1\u0026amp;sn=3e2cc4f01b7ff42f933420063f4e5a74#rd\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e520，对石城说“我爱你”：\u003ca href=\"http://v.qq.com/boke/page/z/0/2/z0154dvf6d2.html\"\u003ehttp://v.qq.com/boke/page/z/0/2/z0154dvf6d2.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e灾后场景：\u003ca href=\"http://mp.weixin.qq.com/s?__biz=MzAwNDUzMjg5OA==\u0026amp;mid=208435542\u0026amp;idx=1\u0026amp;sn=346f8e17448bf3a976c55591636b6e02\u0026amp;scene=1\u0026amp;from=singlemessage\u0026amp;isappinstalled=0#rd\"\u003ehttp://mp.weixin.qq.com/s?__biz=MzAwNDUzMjg5OA==\u0026amp;mid=208435542\u0026amp;idx=1\u0026amp;sn=346f8e17448bf3a976c55591636b6e02\u0026amp;scene=1\u0026amp;from=singlemessage\u0026amp;isappinstalled=0#rd\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e2015年5月21号石城新闻：\u003ca href=\"http://v.qq.com/boke/page/w/0/4/w015442vnu4.html\"\u003ehttp://v.qq.com/boke/page/w/0/4/w015442vnu4.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e相关媒体报道：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://pic.people.com.cn/n/2015/0520/c1016-27031534.html?k=1\"\u003ehttp://pic.people.com.cn/n/2015/0520/c1016-27031534.html?k=1\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://news.163.com/15/0521/05/AQ48GSQU00014Q4P.html\"\u003ehttp://news.163.com/15/0521/05/AQ48GSQU00014Q4P.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://news.163.com/15/0521/06/AQ4CVUIF00014AEE.html\"\u003ehttp://news.163.com/15/0521/06/AQ4CVUIF00014AEE.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. 赣江源头微信公众号文章\u003c/p\u003e\n\u003cp\u003e[2]. 石城热线微信公众号文章\u003c/p\u003e\n\u003cp\u003e[3]. 百度搜索“石城天气”，数据来源中国天气网\u003c/p\u003e\n\u003cp\u003e[4]. 数据来源：\u003ca href=\"http://15tianqi.cn/shicheng5yuetianqi/\"\u003ehttp://15tianqi.cn/shicheng5yuetianqi/\u003c/a\u003e\u003c/p\u003e","title":"家乡遭受几十年一遇的洪灾"},{"content":"其实在中学的时候就遇到过“刺猬困境”，只是那时候不知道这个名词，直接促使我了解这方面内容的原因是和XN的一次不愉快的交谈。\n那是半个多月以前的事情了，有一天中午我们一起吃完午饭回来，我们身后突然有一辆小车要启动，XN就一个劲叫我靠边走；但与此同时我边上迎面来了一辆电动车，于是我就和XN说我这边也有车，但是XN回我一句“叫你靠边走你不靠边走”，当时我听了很不舒服，还是告诉她我这边也有车，但是语气稍微重了点，我以为XN会到此为止，没想到她还是回了一句“你那么大声干嘛呀”……因为那天心情就不太好，还被她这么一说，那天我就没理她了。\n本来以为以XN的性格，这件事会很快过去，没想到XN一气之下不和我一起上下班了，也不和我一起打球吃饭了。我觉得事情还是有那么一点“严重”，这都还没正式入组，就和最熟悉的组员闹僵了，以后三年该怎么办呀，所以第二天晚上我还是诚恳的向XN道歉了。\n后来回想起这件事，确实是我的错。XN当时叫我靠边是一番好意，她多说了几句，我一个男生反倒先急了，确实是气度不够大。\n不过说实话，XN也并不是完全没有错，在我和她解释之后，她还不依不饶，似乎有点过了。她之前也跟我讲过自己有一次在班群里开某同学的玩笑，导致那个同学和她理论的事情。\n写到这的时候又让我想起了有一次教XN打乒乓球的时候，在公共场合批评她打球不够认真，哪里哪里打得不好之类的，当时我自己觉得不以为然，教她打球，当然要指出她的不对之处，但是后来XN告诉我她很生气，哪有我这样一直批评学生的老师啊。我仔细一想，对呀，我为什么在公共场合批评她打得不好呢，我是她的教练吗，我和她熟到能无话不谈吗，好像没有，所以这就是我没有把握好人际交往的一个度。我和XN只是同届同门师弟妹，我们之间应该保持一定的距离，距离太近了，说话做事无所顾忌，迟早会对某一方产生伤害，导致矛盾。\n刺猬困境是由叔本华提出的一个概念。寒冷的冬天，刺猬本想拥作一团、互相取暖，但一靠近便被彼此刺伤了；想分开避免扎伤，又觉得寒冷而想再彼此靠近。几个反复后，刺猬发现它们最好保持一点距离。\n与人交往也是这样，当两个人关系逐渐亲密起来，成为所谓的好基友、好闺蜜的时候，往往容易忽视对方的感受，说出一些伤人的话。\n高中的时候因为成绩还可以，班上的LL同学经常向我请教问题，我每次都会很耐心很热情的帮他解答，慢慢的关系比较好，他就经常和我一起上下课，课间跑到我的位置上和我聊天开玩笑，甚至左拍一下我右捏一下我。这让我感到很不舒服，感觉个人空间被入侵，后来我就慢慢的和他疏远，保持一个合理的距离。\n我和XN的那次不愉快交谈，就相当于刺猬间相互取暖导致彼此受伤的过程，不过我相信，经过几个回合，我们能慢慢的找到合适的距离，既相互取暖，又不至于受伤。\nEdward T. Hall’s personal reaction bubbles, showing radius in feet and meters[1]\n上图是爱德华·霍尔提出的人际交往的四个距离，从内到外依次是亲密距离-\u0026gt;个人距离-\u0026gt;社交距离-\u0026gt;公共距离。和朋友同事之间的距离应该保持在个人距离0.45米。\nP.S.希望能够找到那个和我共享亲密距离的人-:)\n参考：\n[1]. 维基百科“Personal space”条目：http://en.wikipedia.org/wiki/Personal_space\n","permalink":"http://localhost:1313/posts/2015-05-02-hedgehogs-dilemma-and-interpersonal-distance/","summary":"\u003cp\u003e其实在中学的时候就遇到过“刺猬困境”，只是那时候不知道这个名词，直接促使我了解这方面内容的原因是和XN的一次不愉快的交谈。\u003c/p\u003e\n\u003cp\u003e那是半个多月以前的事情了，有一天中午我们一起吃完午饭回来，我们身后突然有一辆小车要启动，XN就一个劲叫我靠边走；但与此同时我边上迎面来了一辆电动车，于是我就和XN说我这边也有车，但是XN回我一句“叫你靠边走你不靠边走”，当时我听了很不舒服，还是告诉她我这边也有车，但是语气稍微重了点，我以为XN会到此为止，没想到她还是回了一句“你那么大声干嘛呀”……因为那天心情就不太好，还被她这么一说，那天我就没理她了。\u003c/p\u003e\n\u003cp\u003e本来以为以XN的性格，这件事会很快过去，没想到XN一气之下不和我一起上下班了，也不和我一起打球吃饭了。我觉得事情还是有那么一点“严重”，这都还没正式入组，就和最熟悉的组员闹僵了，以后三年该怎么办呀，所以第二天晚上我还是诚恳的向XN道歉了。\u003c/p\u003e\n\u003cp\u003e后来回想起这件事，确实是我的错。XN当时叫我靠边是一番好意，她多说了几句，我一个男生反倒先急了，确实是气度不够大。\u003c/p\u003e\n\u003cp\u003e不过说实话，XN也并不是完全没有错，在我和她解释之后，她还不依不饶，似乎有点过了。她之前也跟我讲过自己有一次在班群里开某同学的玩笑，导致那个同学和她理论的事情。\u003c/p\u003e\n\u003cp\u003e写到这的时候又让我想起了有一次教XN打乒乓球的时候，在公共场合批评她打球不够认真，哪里哪里打得不好之类的，当时我自己觉得不以为然，教她打球，当然要指出她的不对之处，但是后来XN告诉我她很生气，哪有我这样一直批评学生的老师啊。我仔细一想，对呀，我为什么在公共场合批评她打得不好呢，我是她的教练吗，我和她熟到能无话不谈吗，好像没有，所以这就是我没有把握好人际交往的一个度。我和XN只是同届同门师弟妹，我们之间应该保持一定的距离，距离太近了，说话做事无所顾忌，迟早会对某一方产生伤害，导致矛盾。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"hedgehogs dilemma\" loading=\"lazy\" src=\"/posts/2015-05-02-hedgehogs-dilemma-and-interpersonal-distance/hedgehogs-dilemma.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e刺猬困境是由叔本华提出的一个概念。寒冷的冬天，刺猬本想拥作一团、互相取暖，但一靠近便被彼此刺伤了；想分开避免扎伤，又觉得寒冷而想再彼此靠近。几个反复后，刺猬发现它们最好保持一点距离。\u003c/p\u003e\n\u003cp\u003e与人交往也是这样，当两个人关系逐渐亲密起来，成为所谓的好基友、好闺蜜的时候，往往容易忽视对方的感受，说出一些伤人的话。\u003c/p\u003e\n\u003cp\u003e高中的时候因为成绩还可以，班上的LL同学经常向我请教问题，我每次都会很耐心很热情的帮他解答，慢慢的关系比较好，他就经常和我一起上下课，课间跑到我的位置上和我聊天开玩笑，甚至左拍一下我右捏一下我。这让我感到很不舒服，感觉个人空间被入侵，后来我就慢慢的和他疏远，保持一个合理的距离。\u003c/p\u003e\n\u003cp\u003e我和XN的那次不愉快交谈，就相当于刺猬间相互取暖导致彼此受伤的过程，不过我相信，经过几个回合，我们能慢慢的找到合适的距离，既相互取暖，又不至于受伤。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://i0.wp.com/upload.wikimedia.org/wikipedia/commons/thumb/3/35/Personal_Space.svg/640px-Personal_Space.svg.png\"\u003e\nEdward T. Hall’s personal reaction bubbles, showing radius in feet and meters[1]\u003c/p\u003e\n\u003cp\u003e上图是爱德华·霍尔提出的人际交往的四个距离，从内到外依次是亲密距离-\u0026gt;个人距离-\u0026gt;社交距离-\u0026gt;公共距离。和朋友同事之间的距离应该保持在个人距离0.45米。\u003c/p\u003e\n\u003cp\u003eP.S.希望能够找到那个和我共享亲密距离的人-:)\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. 维基百科“Personal space”条目：\u003ca href=\"http://en.wikipedia.org/wiki/Personal_space\"\u003ehttp://en.wikipedia.org/wiki/Personal_space\u003c/a\u003e\u003c/p\u003e","title":"“刺猬困境”与人际交往距离"},{"content":"其实很早之前就打算学习繁体字了，但是直接驱动我开始行动的还是上周末的一件事。\n上周末组内一起去天津蓟县盘山景区游玩，进入景区看到的第一个“景点”就是下面的乾隆御笔\n乾隆《游盘山记》\n当时JL师姐念了一遍，有几个繁体字不认识，在场的其他人也都模棱两可。我当时就后悔为什么不早点把繁体字学了呢。所以回所之后马上买了下面的这本《繁简字对照字典》，决定每天看一两页。\n《繁简字对照字典》[1]\n网上也有《游盘山记》的简体版，如下\n连太行，拱神京，放碣石，距沧溟，走蓟野，枕长城，盖蓟州之天作，俯临重壑，如众星拱北而莫敢与争者也。—-乾隆御笔\n对照图片中的繁体字，学习一下。\n有些繁体字和简体字不是一一对应的，比如同样是“汇”字，“汇聚”对应的繁体字为“匯聚”，而“词汇”对应的繁体字为“詞彙”，这一点需要注意，网上有开源的繁简字转换工具，可以看这里。\n关于繁简字的争论，网上已经很多了，我也不想评论，我只想说，学习繁体字完全是个人兴趣，我觉得繁体字很美，很有意思，一个字可以研究半天，外出游玩的时候也能顺带“和古人交流交流”，所以就学了-:)\n参考：\n[1]. 豆瓣读书《繁简字对照字典》：http://book.douban.com/subject/2234412/\n","permalink":"http://localhost:1313/posts/2015-04-21-learning-traditional-chinese-characters/","summary":"\u003cp\u003e其实很早之前就打算学习繁体字了，但是直接驱动我开始行动的还是上周末的一件事。\u003c/p\u003e\n\u003cp\u003e上周末组内一起去天津蓟县盘山景区游玩，进入景区看到的第一个“景点”就是下面的乾隆御笔\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"乾隆《游盘山记》\" loading=\"lazy\" src=\"/posts/2015-04-21-learning-traditional-chinese-characters/panshan.jpg\"\u003e\n乾隆《游盘山记》\u003c/p\u003e\n\u003cp\u003e当时JL师姐念了一遍，有几个繁体字不认识，在场的其他人也都模棱两可。我当时就后悔为什么不早点把繁体字学了呢。所以回所之后马上买了下面的这本《繁简字对照字典》，决定每天看一两页。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://img3.doubanio.com/lpic/s2988301.jpg\"\u003e\n《繁简字对照字典》[1]\u003c/p\u003e\n\u003cp\u003e网上也有《游盘山记》的简体版，如下\u003c/p\u003e\n\u003cp\u003e连太行，拱神京，放碣石，距沧溟，走蓟野，枕长城，盖蓟州之天作，俯临重壑，如众星拱北而莫敢与争者也。—-乾隆御笔\u003c/p\u003e\n\u003cp\u003e对照图片中的繁体字，学习一下。\u003c/p\u003e\n\u003cp\u003e有些繁体字和简体字不是一一对应的，比如同样是“汇”字，“汇聚”对应的繁体字为“匯聚”，而“词汇”对应的繁体字为“詞彙”，这一点需要注意，网上有开源的繁简字转换工具，可以看\u003ca href=\"http://opencc.byvoid.com/\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.zhihu.com/question/25389359\"\u003e关于繁简字的争论，网上已经很多了\u003c/a\u003e，我也不想评论，我只想说，学习繁体字完全是个人兴趣，我觉得繁体字很美，很有意思，一个字可以研究半天，外出游玩的时候也能顺带“和古人交流交流”，所以就学了-:)\u003c/p\u003e\n\u003cp\u003e参考：\u003c/p\u003e\n\u003cp\u003e[1]. 豆瓣读书《繁简字对照字典》：\u003ca href=\"http://book.douban.com/subject/2234412/\"\u003ehttp://book.douban.com/subject/2234412/\u003c/a\u003e\u003c/p\u003e","title":"我学繁体字"},{"content":"师姐推荐了最新一期《人物周刊》上的一篇文章–“不求助的人”。这篇文章讲到，在上月末德国空难事件中，副驾驶卢比茨选择了坠毁飞机。调查人员随后了解到，他可能患有抑郁症，同时，他的弱视或许在持续恶化。视力和心理问题给卢比茨的职业前景笼上阴影，而他没有选择求助。\n“求助”似乎从来不是首选项。遇到难题自己解决，不行就上网搜搜方法，还不行，则“咬紧牙关熬过去”。不求助的表面理由是不麻烦别人，但更真实的担忧大概是“如果开口求助，别人会认为我能力低下，我会因此丧失各种机会”。本质原因在于，给自己定的目标是“在他人面前表现出众”。\n目标可细分为两种，“精熟型目标（mastery goal）”和“绩效型目标（performancegoal）”。精熟型目标更重视过程而非结果，认为目的是自我提升，不是获得肯定。哪怕现在还“做不到”，但通过不断努力也能有所进步。既然目标是“成为更好的自己”，那么遇到困难时，自然会寻求帮助。而绩效型目标只看最终结果，你要么“能做到”，要么“不能做到”，要么力压众人表现出色，要么挑战失败沦为笑柄。既然目的是“从他人那里获得肯定”，感觉上像是“示弱”的求助就不会被列入选项。\n“不求助之人”并不少见。不过，“绩效型目标”者不知道的是，求助他人时，其实会提升此人对你的评价。每个人都觉得自己智慧过人，可以为别人授业解惑，而“懂得向聪明的我询问智慧建言的人，一定也是聪明人”。沃顿商学院的研究者发现，脑力竞赛中接到“搭档”求助的人，赛后给搭档打了更高的能力分。2010年，美国西北大学研究发现，老板其实更喜欢那些遇到困境会主动求援的下属，某种意义上，“求指点迷津”可能是对老板最好的恭维。\n说到底，不管目标是获得成长还是赞赏，求助都是帮助达成目标的大道。越早寻求帮助，越有机会让自己成长，也越有可能掌握技能、成功解决问题，周遭人对你的评价也会因此上升。反倒是不求助的人，万一拖到事情无法收拾，自己的自信和风评都会落到极低。\n这篇文章讲得很有道理，我发现身边就有很多不求助的人。他们从小到大很少向别人求助，自己能做的事尽量自己做，遇到难题也尽量自己扛，给人一种能力很强、很自信的印象；同时他们也很鄙视那些经常向别人求助的人，认为这些人“就知道问别人，这么简单的都不会”。你可以称赞“不求助的人”独立自主、坚韧刻苦，但是从某种程度上这恰恰反映了他们内心的不自信，他们害怕自己的求助暴露了自己的智商，显得自己水平不够。他们属于绩效型人群，只看重结果，不看重过程，如果当上领导，下属的压力肯定不小，久而久之，就会产生类似上面的案例，宁愿选择坠毁，也不愿向他人求助。\n不求助的人因为很少向他人求助，他们的交际圈也很窄，他们经常把自己封锁起来，甚至把主动伸出援手的人拒之门外。长期的封闭往往导致一些心理和精神疾病，以至于做出一些病态的选择。\n其实，和”不求助的人“的想法相反，文中沃顿商学院的研究结果很有道理，遇到问题喜欢求助的人，反而会受到别人较高的评价，因为被求助者会潜意识的认为“懂得向聪明的我询问智慧建言的人，一定也是聪明人”，说不定双方还能由此发展出一段不错的关系；而且越早求助，就能越早解决问题。这一箭多雕的事情，恐怕是不求助的人没有想到的吧。\n从某种程度上来说，我自己也是一个“不求助的人”，H老师估计早就猜透了，我希望能够在读研期间“收获一点成就感、一点自信心”，从绩效型人群转移到精熟型人群。\n","permalink":"http://localhost:1313/posts/2015-04-18-someone-who-doesnt-ask-for-help/","summary":"\u003cp\u003e师姐推荐了最新一期《人物周刊》上的一篇文章–\u003ca href=\"https://mp.weixin.qq.com/s?__biz=MTY0MzI5NDcwMQ==\u0026amp;mid=206411797\u0026amp;idx=1\u0026amp;sn=c0da9db2ed5a6a5bdedbfb871e45c8e6#rd\"\u003e“不求助的人”\u003c/a\u003e。这篇文章讲到，在上月末德国空难事件中，副驾驶卢比茨选择了坠毁飞机。调查人员随后了解到，他可能患有抑郁症，同时，他的弱视或许在持续恶化。视力和心理问题给卢比茨的职业前景笼上阴影，\u003cstrong\u003e而他没有选择求助\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e“求助”似乎从来不是首选项。遇到难题自己解决，不行就上网搜搜方法，还不行，则“咬紧牙关熬过去”。不求助的表面理由是不麻烦别人，但更真实的担忧大概是“如果开口求助，别人会认为我能力低下，我会因此丧失各种机会”。本质原因在于，给自己定的目标是“在他人面前表现出众”。\u003c/p\u003e\n\u003cp\u003e目标可细分为两种，“精熟型目标（mastery goal）”和“绩效型目标（performancegoal）”。精熟型目标更重视过程而非结果，认为目的是自我提升，不是获得肯定。哪怕现在还“做不到”，但通过不断努力也能有所进步。既然目标是“成为更好的自己”，那么遇到困难时，自然会寻求帮助。而绩效型目标只看最终结果，你要么“能做到”，要么“不能做到”，要么力压众人表现出色，要么挑战失败沦为笑柄。既然目的是“从他人那里获得肯定”，感觉上像是“示弱”的求助就不会被列入选项。\u003c/p\u003e\n\u003cp\u003e“不求助之人”并不少见。不过，“绩效型目标”者不知道的是，求助他人时，其实会提升此人对你的评价。每个人都觉得自己智慧过人，可以为别人授业解惑，而“懂得向聪明的我询问智慧建言的人，一定也是聪明人”。沃顿商学院的研究者发现，脑力竞赛中接到“搭档”求助的人，赛后给搭档打了更高的能力分。2010年，美国西北大学研究发现，老板其实更喜欢那些遇到困境会主动求援的下属，某种意义上，“求指点迷津”可能是对老板最好的恭维。\u003c/p\u003e\n\u003cp\u003e说到底，不管目标是获得成长还是赞赏，求助都是帮助达成目标的大道。越早寻求帮助，越有机会让自己成长，也越有可能掌握技能、成功解决问题，周遭人对你的评价也会因此上升。反倒是不求助的人，万一拖到事情无法收拾，自己的自信和风评都会落到极低。\u003c/p\u003e\n\u003cp\u003e这篇文章讲得很有道理，我发现身边就有很多不求助的人。他们从小到大很少向别人求助，自己能做的事尽量自己做，遇到难题也尽量自己扛，给人一种能力很强、很自信的印象；同时他们也很鄙视那些经常向别人求助的人，认为这些人“就知道问别人，这么简单的都不会”。你可以称赞“不求助的人”独立自主、坚韧刻苦，但是从某种程度上这恰恰反映了他们内心的不自信，他们害怕自己的求助暴露了自己的智商，显得自己水平不够。他们属于绩效型人群，只看重结果，不看重过程，如果当上领导，下属的压力肯定不小，久而久之，就会产生类似上面的案例，宁愿选择坠毁，也不愿向他人求助。\u003c/p\u003e\n\u003cp\u003e不求助的人因为很少向他人求助，他们的交际圈也很窄，他们经常把自己封锁起来，甚至把主动伸出援手的人拒之门外。长期的封闭往往导致一些心理和精神疾病，以至于做出一些病态的选择。\u003c/p\u003e\n\u003cp\u003e其实，和”不求助的人“的想法相反，文中沃顿商学院的研究结果很有道理，遇到问题喜欢求助的人，反而会受到别人较高的评价，因为被求助者会潜意识的认为“懂得向聪明的我询问智慧建言的人，一定也是聪明人”，说不定双方还能由此发展出一段不错的关系；而且越早求助，就能越早解决问题。这一箭多雕的事情，恐怕是不求助的人没有想到的吧。\u003c/p\u003e\n\u003cp\u003e从某种程度上来说，我自己也是一个“不求助的人”，H老师估计早就猜透了，我希望能够在读研期间\u003ca href=\"http://pfind.net/people/hesimin/Chinese/Favorite%20Books.htm\"\u003e“收获一点成就感、一点自信心”\u003c/a\u003e，从绩效型人群转移到精熟型人群。\u003c/p\u003e","title":"不求助的人"},{"content":"趁着周末，看了韩寒导演的处女作《后会无期》，说来奇怪，看的过程中没有丝毫感觉，情节松散，直到听到了片尾曲”平凡之路“，内心为之一颤，想来应该写点什么纪念一下。\n影片中三个年轻人离开家乡小岛，一路向西，横穿中国大陆，路上落下了胡生，错过了假装”小姐“的”骗子“，告别了一直”恋爱“着的笔友，遇到了善恶莫测的奇怪旅人，送走了最好的朋友，只有流浪的小狗留在了身边。几番告白，几番告别，勾勒出几段截然不同的平凡人生之路。\n突然间，我从影片中看到了萧瑟冷漠的世界，看到了饱经沧桑的老人挣扎着，反抗着，但最终离开了。\n影片给我影响最深的两句话是：\n你连世界都没观过，你哪来的世界观。 如果要告别，一定要用力一点,因为任何多看一眼,都有可能成为最后一眼,多说一句,都可能是最后一句。 经常有人惊讶于我小小年纪就表现得如此成熟，不知道是不是因为从小跟着父母外出闯荡，经历得多了，世界观不一样了。\n很小的时候就跟着父母去了广东JY，当时父亲帮别人挖煤，后来当过老师，开过早餐店，开过出租车。挖煤的时候每天都要在臭气熏天的河里挖半天煤，然后用船运回去，又要顶着炎炎夏日做半天的煤。每天完事之后脚乌漆墨黑，老茧长得跟树皮一样。开早餐店的时候，每天凌晨三四点就要起床开始和面，做包子，熬豆浆，炸油条。忙完了早餐还要去学校上班。\n那一年有天晚上，爸爸妈妈正在收拾店铺，准备第二天早晨的面料，YT睡到半夜突然KTBM，脚一直在发抖，我被吵醒之后马上告诉了爸爸妈妈。当时都已经很晚了，地段也比较偏僻，路上少有行人，幸好隔壁开茶叶店的老板还没有走，他用摩托车把YT送到了医院。那一天晚上格外的冷，我只记得妈妈站在医院门口不停的祈祷着什么。后来几经折腾，转院到汕头的大医院，病床好像在走廊里，医院的快餐比家里的还好吃。\n一家人出门在外，父母的工资很低，在外读书，一学期的学费要几百块钱，再加上YT的那场大病，家里的经济压力着实不小。父母经常为一些事大吵大闹，有几次还大打出手，作为小孩子的我只能哭着求着他们不要再吵不要再打了，过往的事件历历在目，那都是血和泪的记忆。\n在外漂泊的日子过得很辛苦，对于大人如此，对于我们这些青少年也一样。走在路上经常会被一些本地的小混混打，晚自习回家一定要结伴而行，不要走人少的路。我记得有一次我和表哥一起走在街上，一个骑自行车的小混混从我们背后踢了我们两脚，当时表哥正要反击，我把他拉住了，因为我知道，外地人在这势单力薄，根本不是这些人的对手，自己吃点亏，能不惹麻烦尽量不惹麻烦。但是这个小事给我的印象很深。\n也许是在外打拼的日子太苦，初一下学期，我、YT和妈妈回老家了，爸爸继续在JY打拼着。回到家之后，去了一个稍微好一点的初中，妈妈在我们身边陪读。\n因为在JY的时候，家里很穷，但是过年的时候，父母总还是会给我们买新衣服，所以每年就给我们买便宜又得体的西装。回到老家之后，城里的同学看我们经常穿西装，索性给我们取了一个外号”西装“，这导致我后来对西装厌恶至极。\n有一天晚上，晚自习回家，我和YT刚走出校门的时候，有一群小混混和我们逆行，他们跑的时候不小心把水溅到我们身上了，我想想也就算了，但是YT不服，故意把水溅到他们身上。我当时暗想坏了，他们会不会回来找我们算账啊，果不其然，没过多久，一帮人就追着我们打，幸好当时有一个老师路过，我向她”求救“才得以脱险回家。\n这些小事反应到我的性格上来就是忍气吞声，处世中庸，”吃亏是福“。这种性格在某种程度上也是一件好事，高中三年给我省了很多麻烦，也能让我沉下心来，埋头苦干，高考的时候考了全县第十一名，考取WHU也算是对我那几年的一个回报。其实农村孩子这种”两耳不闻窗外事，一心只读圣贤书“的单一发展，也给我视野狭窄、其他技能缺失埋下了伏笔，这里暂时按下不表。\n我还在JY的时候，有一天妈妈说我们要马上回家看外公，外公病了。回到老家之后，我和YT在院子里玩耍，后来妈妈拉我去见了外公最后一面。那大概是我记忆中第一次亲人离别。后来妈妈告诉我，外公当时还怪我到家之后为什么没有马上去看他呢，对呀，我当时为什么没有马上去看外公而是在院子里玩耍呢，也许那时候还不知道什么是离别吧。\n后来又经历了曾祖母的离别。记忆最深的是爷爷的离别。那大概是一年前吧，我当时正在图书馆准备保研的事情，突然爷爷给我打了一个电话，爷爷很少给我打电话的，而且那时候我们好像还不知道对方的手机号码，爷爷说是从YT那知道我的号码的。他问了我一些近况，叫我要好好照顾自己的身体，不要担心他；他还说他给YT也打了电话，给CY打电话但是没有打通；末了，他说这个电话没别的意思，就这样吧，挂了。我可以明显的感受到电话那头爷爷凄凉孤独的心，这通电话听起来很怪，我马上给爸爸打了个电话，告诉了他情况，爸爸说爷爷一个人在家，也许是太孤单了，或者是犯了老年痴呆症，爸爸还说爷爷也经常打类似的电话给他。是啊，奶奶在我很小的时候就去世了，爷爷一个人孤苦伶仃生活了将近二十年，纵然有三个儿子一个女儿，但是几个儿子儿媳之间为了老人的赡养问题竟成陌路，小儿子老大不小了也还没有成家，是孤独的在这个世界苟活着，给儿子儿媳带来更多的麻烦还是默默的离开，给年轻人省去一个包袱，爷爷心里恐怕早已有了答案。\n几天之后，噩耗传来，没想到那竟成了我和爷爷最后的通话。\n给爷爷办后事的时候，几个叔叔姑姑都回来了，这竟是我记忆中唯一一次看到大家坐在一张桌子上吃饭。\n很多人都说我冷漠、沉默寡言，其实我小时候不是这样的，可能是这些年经历的事太多了，我对很多事情漠不关心，很多不必要的、无意义的话也不讲了，很多点头之交的朋友也不联系了，也变得不喜欢和人争辩了；我开始喜欢独处，喜欢一个人走在路上，看过往匆匆行人，喜欢看《文化苦旅》、《一九八四》、《百年孤独》、《活着》……\n身边很多同学喜欢三五成群出门，经常见他们和各种各样的朋友打招呼。有一次我和一个夏令营认识的同学打招呼，XN居然诧异的告诉我这是她第一次在ICT发现我也有认识的人。为什么要有那么多朋友呢，可能你会告诉我你的QQ好友都上千了，但是真正在你社交圈里的朋友，能够和你交心的朋友，超过10个吗？逢年过节，为了维系那990个你都不记得他/她的模样的朋友，群发着各种短信，朋友圈、QQ空间、微博里不停的给别人点赞，有必要吗？我已经厌倦了这些虚情假意。\n令我很感动的是，前几天，我正在实验室敲代码的时候，接到了WQ的一个电话，我跟他抱怨了一下在北京的各种不顺，他跟我说一个人出门在外，要好好照顾自己；同时另一个好友WS也经常跟我说想和我聊聊。这让我感到非常温暖，虽然我现在一无所有，但是有一两个至交，足以。\n我已经走过了二十多年，是时候走出去看看世界，说不准世界观就形成了呢。\n","permalink":"http://localhost:1313/posts/2015-04-12-my-ordinary-life/","summary":"\u003cp\u003e趁着周末，看了韩寒导演的处女作《后会无期》，说来奇怪，看的过程中没有丝毫感觉，情节松散，直到听到了片尾曲”平凡之路“，内心为之一颤，想来应该写点什么纪念一下。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"The-Continent1.jpg\" loading=\"lazy\" src=\"/posts/2015-04-12-my-ordinary-life/The-Continent1.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e影片中三个年轻人离开家乡小岛，一路向西，横穿中国大陆，路上落下了胡生，错过了假装”小姐“的”骗子“，告别了一直”恋爱“着的笔友，遇到了善恶莫测的奇怪旅人，送走了最好的朋友，只有流浪的小狗留在了身边。几番告白，几番告别，勾勒出几段截然不同的平凡人生之路。\u003c/p\u003e\n\u003cp\u003e突然间，我从影片中看到了萧瑟冷漠的世界，看到了饱经沧桑的老人挣扎着，反抗着，但最终离开了。\u003c/p\u003e\n\u003cp\u003e影片给我影响最深的两句话是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e你连世界都没观过，你哪来的世界观。\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e如果要告别，一定要用力一点,因为任何多看一眼,都有可能成为最后一眼,多说一句,都可能是最后一句。\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e经常有人惊讶于我小小年纪就表现得如此成熟，不知道是不是因为从小跟着父母外出闯荡，经历得多了，世界观不一样了。\u003c/p\u003e\n\u003cp\u003e很小的时候就跟着父母去了广东JY，当时父亲帮别人挖煤，后来当过老师，开过早餐店，开过出租车。挖煤的时候每天都要在臭气熏天的河里挖半天煤，然后用船运回去，又要顶着炎炎夏日做半天的煤。每天完事之后脚乌漆墨黑，老茧长得跟树皮一样。开早餐店的时候，每天凌晨三四点就要起床开始和面，做包子，熬豆浆，炸油条。忙完了早餐还要去学校上班。\u003c/p\u003e\n\u003cp\u003e那一年有天晚上，爸爸妈妈正在收拾店铺，准备第二天早晨的面料，YT睡到半夜突然KTBM，脚一直在发抖，我被吵醒之后马上告诉了爸爸妈妈。当时都已经很晚了，地段也比较偏僻，路上少有行人，幸好隔壁开茶叶店的老板还没有走，他用摩托车把YT送到了医院。那一天晚上格外的冷，我只记得妈妈站在医院门口不停的祈祷着什么。后来几经折腾，转院到汕头的大医院，病床好像在走廊里，医院的快餐比家里的还好吃。\u003c/p\u003e\n\u003cp\u003e一家人出门在外，父母的工资很低，在外读书，一学期的学费要几百块钱，再加上YT的那场大病，家里的经济压力着实不小。父母经常为一些事大吵大闹，有几次还大打出手，作为小孩子的我只能哭着求着他们不要再吵不要再打了，过往的事件历历在目，那都是血和泪的记忆。\u003c/p\u003e\n\u003cp\u003e在外漂泊的日子过得很辛苦，对于大人如此，对于我们这些青少年也一样。走在路上经常会被一些本地的小混混打，晚自习回家一定要结伴而行，不要走人少的路。我记得有一次我和表哥一起走在街上，一个骑自行车的小混混从我们背后踢了我们两脚，当时表哥正要反击，我把他拉住了，因为我知道，外地人在这势单力薄，根本不是这些人的对手，自己吃点亏，能不惹麻烦尽量不惹麻烦。但是这个小事给我的印象很深。\u003c/p\u003e\n\u003cp\u003e也许是在外打拼的日子太苦，初一下学期，我、YT和妈妈回老家了，爸爸继续在JY打拼着。回到家之后，去了一个稍微好一点的初中，妈妈在我们身边陪读。\u003c/p\u003e\n\u003cp\u003e因为在JY的时候，家里很穷，但是过年的时候，父母总还是会给我们买新衣服，所以每年就给我们买便宜又得体的西装。回到老家之后，城里的同学看我们经常穿西装，索性给我们取了一个外号”西装“，这导致我后来对西装厌恶至极。\u003c/p\u003e\n\u003cp\u003e有一天晚上，晚自习回家，我和YT刚走出校门的时候，有一群小混混和我们逆行，他们跑的时候不小心把水溅到我们身上了，我想想也就算了，但是YT不服，故意把水溅到他们身上。我当时暗想坏了，他们会不会回来找我们算账啊，果不其然，没过多久，一帮人就追着我们打，幸好当时有一个老师路过，我向她”求救“才得以脱险回家。\u003c/p\u003e\n\u003cp\u003e这些小事反应到我的性格上来就是忍气吞声，处世中庸，”吃亏是福“。这种性格在某种程度上也是一件好事，高中三年给我省了很多麻烦，也能让我沉下心来，埋头苦干，高考的时候考了全县第十一名，考取WHU也算是对我那几年的一个回报。其实农村孩子这种”两耳不闻窗外事，一心只读圣贤书“的单一发展，也给我视野狭窄、其他技能缺失埋下了伏笔，这里暂时按下不表。\u003c/p\u003e\n\u003cp\u003e我还在JY的时候，有一天妈妈说我们要马上回家看外公，外公病了。回到老家之后，我和YT在院子里玩耍，后来妈妈拉我去见了外公最后一面。那大概是我记忆中第一次亲人离别。后来妈妈告诉我，外公当时还怪我到家之后为什么没有马上去看他呢，对呀，我当时为什么没有马上去看外公而是在院子里玩耍呢，也许那时候还不知道什么是离别吧。\u003c/p\u003e\n\u003cp\u003e后来又经历了曾祖母的离别。记忆最深的是爷爷的离别。那大概是一年前吧，我当时正在图书馆准备保研的事情，突然爷爷给我打了一个电话，爷爷很少给我打电话的，而且那时候我们好像还不知道对方的手机号码，爷爷说是从YT那知道我的号码的。他问了我一些近况，叫我要好好照顾自己的身体，不要担心他；他还说他给YT也打了电话，给CY打电话但是没有打通；末了，他说这个电话没别的意思，就这样吧，挂了。我可以明显的感受到电话那头爷爷凄凉孤独的心，这通电话听起来很怪，我马上给爸爸打了个电话，告诉了他情况，爸爸说爷爷一个人在家，也许是太孤单了，或者是犯了老年痴呆症，爸爸还说爷爷也经常打类似的电话给他。是啊，奶奶在我很小的时候就去世了，爷爷一个人孤苦伶仃生活了将近二十年，纵然有三个儿子一个女儿，但是几个儿子儿媳之间为了老人的赡养问题竟成陌路，小儿子老大不小了也还没有成家，是孤独的在这个世界苟活着，给儿子儿媳带来更多的麻烦还是默默的离开，给年轻人省去一个包袱，爷爷心里恐怕早已有了答案。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"The-Continent2.jpg\" loading=\"lazy\" src=\"/posts/2015-04-12-my-ordinary-life/The-Continent2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e几天之后，噩耗传来，没想到那竟成了我和爷爷最后的通话。\u003c/p\u003e\n\u003cp\u003e给爷爷办后事的时候，几个叔叔姑姑都回来了，这竟是我记忆中唯一一次看到大家坐在一张桌子上吃饭。\u003c/p\u003e\n\u003cp\u003e很多人都说我冷漠、沉默寡言，其实我小时候不是这样的，可能是这些年经历的事太多了，我对很多事情漠不关心，很多不必要的、无意义的话也不讲了，很多点头之交的朋友也不联系了，也变得不喜欢和人争辩了；我开始喜欢独处，喜欢一个人走在路上，看过往匆匆行人，喜欢看《文化苦旅》、《一九八四》、《百年孤独》、《活着》……\u003c/p\u003e\n\u003cp\u003e身边很多同学喜欢三五成群出门，经常见他们和各种各样的朋友打招呼。有一次我和一个夏令营认识的同学打招呼，XN居然诧异的告诉我这是她第一次在ICT发现我也有认识的人。为什么要有那么多朋友呢，可能你会告诉我你的QQ好友都上千了，但是真正在你社交圈里的朋友，能够和你交心的朋友，超过10个吗？逢年过节，为了维系那990个你都不记得他/她的模样的朋友，群发着各种短信，朋友圈、QQ空间、微博里不停的给别人点赞，有必要吗？我已经厌倦了这些虚情假意。\u003c/p\u003e\n\u003cp\u003e令我很感动的是，前几天，我正在实验室敲代码的时候，接到了WQ的一个电话，我跟他抱怨了一下在北京的各种不顺，他跟我说一个人出门在外，要好好照顾自己；同时另一个好友WS也经常跟我说想和我聊聊。这让我感到非常温暖，虽然我现在一无所有，但是有一两个至交，足以。\u003c/p\u003e\n\u003cp\u003e我已经走过了二十多年，是时候走出去看看世界，说不准世界观就形成了呢。\u003c/p\u003e","title":"人生，平凡之路"},{"content":"2015年2月28日到达北京，到现在一月有余，是时候月度总结了。\n这是我第三次来北京 2014年的6月份第一次来北京，去北大计算所实习十天。感受了一下北大计算所p老师及其博士生z的冷漠。我还记得刚到的那天晚上一个人孤零零的走在北京的大街上寻找北大方正员工宿舍的场景。每天重复着朝九晚十的固定模式，每个人都像机器一样的工作，实验室只有敲击键盘的声音，偶尔看到p的时候他也是阴沉着脸，临走的时候还被z批评了。虽然p承诺说只要我通过夏令营面试就会要我，但是这明显就是废话呀；而且这段不算愉快的实习经历让我对p的实验室产生了厌恶。\n回学校之后开始准备期末考试和7月份的夏令营，并于2014年7月9日到达北京，这次的行程包括北大计算所和中科院计算所的夏令营。北大计算所因为机试和综合排名不占优势，败下阵来，不过现在想想当初幸好没有进p的实验室，要不然天天对着p的臭脸，估计要疯。\n中科院计算所的风格完全不像北大，中科院真正做到了海纳百川，一视同仁，不像北大看不起校外学生。来北京之前和ict的h老师沟通过很多次，发现我和h老师的性格很相似，我非常崇拜h老师严谨的处事风格，经过认真的准备，我也顺利通过了ict的夏令营，然后跑去深圳siat玩耍了。\n北京的物价真是贵 这次是来ict完成我的本科毕业设计的，大概要待到5月初。一来从家里不方便带太多东西，二来想想要在北京待三年，所以准备在北京重新置办生活用品。\n2月28刚到北京，去了师兄推荐的家乐福大超市。超市很大，有两层，不过里面的东西真是贵。也就买了被子、三件套、桶、盆，花了四五百，很可恨的是，很薄的春秋单人被子，居然要169，这还是最便宜的，床垫比被子还贵，要199。\n没必要的用品可以不买，但是饭不能不吃。以前在学校一天也就十几块，到北京后发现，一顿饭就十多块了。早餐一个鸡蛋要2块，一个小包子要1.5块，哎，想想以前自己家做早餐的时候1块钱4个大肉包子啊，当初怎么不多吃几个呢。\n餐厅里的饭菜确实贵，不过国科大食堂的饭菜既便宜又好吃，赞一个。\n北京的雾霾 来北京之前对北京的雾霾也略有耳闻，觉得那是北京人的小题大做，武汉也是扬尘满天飞，我也活得好好的啊；不过真来北京之后，确实受不了，放眼望去，街道上灰蒙蒙的一片，路上的特大广告牌都看不清。也许是雾霾加干燥的原因，刚来北京那一个星期，嗓子特别不舒服，每天早晨刷牙的时候都恶心干呕，不过后来慢慢好了。\n3月28那天，北京还遭遇了沙尘暴，那场景真像外星人袭击地球。所以来北京，口罩是必需品。\n3月28日北京的沙尘暴[1]\n北京的风景 来北京的前3个星期，忙于组内布置的任务，也是过着朝九晚十的生活，后来想想也不能天天这样，我应该出去透透气，于是选择了颐和园，3月份属于颐和园景区的淡季，加上学生证，门票不贵。颐和园不愧是皇家园林，里面的景色确实很漂亮，有万寿山、昆明湖，湖水还算干净。\n我去的那天天气很好，空气质量也不错；人虽多，但不算拥挤，部分原因是颐和园真是太大了。我在游玩的过程中看到了很多外国旅游团，听到过英语、法语、日语、韩语、粤语、中文，让我默默联想起了八国联军侵略中国的场景。\n在游玩的过程中遇到了一个很温馨的“外国小家庭”，妈妈是外国人，爸爸应该也是外国人，爸爸和妈妈聊天的时候是用英语，但是爸爸和儿子却是用中文，他们还有一个坐在婴儿车里的小女儿，儿子坐在湖边玩ipad，小女儿在车里哭个不停…\n虽然颐和园的风景很好，但是里面居然有很多蚊子（小虫子）！期间有一个虫子撞到我的嘴巴，还有一个钻进了我的耳朵。可能是春天到了，万物复苏，加上颐和园内潮湿，植物丰富，给虫子提供了很好的环境。\n下面放几张游玩时拍的照片。\n谁能告诉我这是什么乐器\n学太极的外国人\n这也是迎春花？\n这个角度的颐和园很美\n长廊\n准备回去的时候看到的，各种长枪大炮，据说是在抓拍一种珍稀鸟类。。。\n颐和园的风景虽好，但就自然风光来说，比不上南方早已复苏的大自然。\n家乡的油菜花，比颐和园的迎春花漂亮多了-:)\n武大樱花已经争奇斗艳了，北方还是萧瑟一片[2]\n后记 没来北京之前，我想着以后一定要在北京工作，还要争取在北京安家落户。虽然北京生活压力很大，但是机会和挑战也多，这不就是我要的生活吗；而且北京是全国的政治、经济和文化中心，可以很方便的去感受中国几千年的文化和历史。在北京生活一个月之后，我的想法变了，很重要的原因是北京的雾霾和气候。雾霾不是一个人一天两天能够解决的，你必须尽量减少不必要的出行，跑步健身什么的就更别提了；北京属于北温带季风气候，冬天很干燥，而且放眼望去看不到绿色，ict旁边种的树，光秃秃的只有树干。这种钢筋混凝土构筑起来的城市，我真的不喜欢。\n和南方的一些城市相比，北京真的不适宜居住，尤其对于南方人！\n参考 [1]. 新京报网：http://www.bjnews.com.cn/news/2015/03/28/358122.html\n[2]. 中新网：http://www.chinanews.com/tp/hd2011/2015/03-21/496005.shtml\n","permalink":"http://localhost:1313/posts/2015-03-30-one-month-in-beijing/","summary":"\u003cp\u003e2015年2月28日到达北京，到现在一月有余，是时候月度总结了。\u003c/p\u003e\n\u003ch1 id=\"这是我第三次来北京\"\u003e这是我第三次来北京\u003c/h1\u003e\n\u003cp\u003e2014年的6月份第一次来北京，去北大计算所实习十天。感受了一下北大计算所p老师及其博士生z的冷漠。我还记得刚到的那天晚上一个人孤零零的走在北京的大街上寻找北大方正员工宿舍的场景。每天重复着朝九晚十的固定模式，每个人都像机器一样的工作，实验室只有敲击键盘的声音，偶尔看到p的时候他也是阴沉着脸，临走的时候还被z批评了。虽然p承诺说只要我通过夏令营面试就会要我，但是这明显就是废话呀；而且这段不算愉快的实习经历让我对p的实验室产生了厌恶。\u003c/p\u003e\n\u003cp\u003e回学校之后开始准备期末考试和7月份的夏令营，并于2014年7月9日到达北京，这次的行程包括北大计算所和中科院计算所的夏令营。北大计算所因为机试和综合排名不占优势，败下阵来，不过现在想想当初幸好没有进p的实验室，要不然天天对着p的臭脸，估计要疯。\u003c/p\u003e\n\u003cp\u003e中科院计算所的风格完全不像北大，中科院真正做到了海纳百川，一视同仁，不像北大看不起校外学生。来北京之前和ict的h老师沟通过很多次，发现我和h老师的性格很相似，我非常崇拜h老师严谨的处事风格，经过认真的准备，我也顺利通过了ict的夏令营，然后跑去深圳siat玩耍了。\u003c/p\u003e\n\u003ch1 id=\"北京的物价真是贵\"\u003e北京的物价真是贵\u003c/h1\u003e\n\u003cp\u003e这次是来ict完成我的本科毕业设计的，大概要待到5月初。一来从家里不方便带太多东西，二来想想要在北京待三年，所以准备在北京重新置办生活用品。\u003c/p\u003e\n\u003cp\u003e2月28刚到北京，去了师兄推荐的家乐福大超市。超市很大，有两层，不过里面的东西真是贵。也就买了被子、三件套、桶、盆，花了四五百，很可恨的是，很薄的春秋单人被子，居然要169，这还是最便宜的，床垫比被子还贵，要199。\u003c/p\u003e\n\u003cp\u003e没必要的用品可以不买，但是饭不能不吃。以前在学校一天也就十几块，到北京后发现，一顿饭就十多块了。早餐一个鸡蛋要2块，一个小包子要1.5块，哎，想想以前自己家做早餐的时候1块钱4个大肉包子啊，当初怎么不多吃几个呢。\u003c/p\u003e\n\u003cp\u003e餐厅里的饭菜确实贵，不过国科大食堂的饭菜既便宜又好吃，赞一个。\u003c/p\u003e\n\u003ch1 id=\"北京的雾霾\"\u003e北京的雾霾\u003c/h1\u003e\n\u003cp\u003e来北京之前对北京的雾霾也略有耳闻，觉得那是北京人的小题大做，武汉也是扬尘满天飞，我也活得好好的啊；不过真来北京之后，确实受不了，放眼望去，街道上灰蒙蒙的一片，路上的特大广告牌都看不清。也许是雾霾加干燥的原因，刚来北京那一个星期，嗓子特别不舒服，每天早晨刷牙的时候都恶心干呕，不过后来慢慢好了。\u003c/p\u003e\n\u003cp\u003e3月28那天，北京还遭遇了沙尘暴，那场景真像外星人袭击地球。所以来北京，口罩是必需品。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"3月28日北京的沙尘暴\" loading=\"lazy\" src=\"https://i0.wp.com/y0.ifengimg.com/cmpp/2015/03/28/14/1120a1ac-4317-49ca-8114-5962823270e1_size48_w600_h400.jpg\"\u003e\n3月28日北京的沙尘暴[1]\u003c/p\u003e\n\u003ch1 id=\"北京的风景\"\u003e北京的风景\u003c/h1\u003e\n\u003cp\u003e来北京的前3个星期，忙于组内布置的任务，也是过着朝九晚十的生活，后来想想也不能天天这样，我应该出去透透气，于是选择了颐和园，3月份属于颐和园景区的淡季，加上学生证，门票不贵。颐和园不愧是皇家园林，里面的景色确实很漂亮，有万寿山、昆明湖，湖水还算干净。\u003c/p\u003e\n\u003cp\u003e我去的那天天气很好，空气质量也不错；人虽多，但不算拥挤，部分原因是颐和园真是太大了。我在游玩的过程中看到了很多外国旅游团，听到过英语、法语、日语、韩语、粤语、中文，让我默默联想起了八国联军侵略中国的场景。\u003c/p\u003e\n\u003cp\u003e在游玩的过程中遇到了一个很温馨的“外国小家庭”，妈妈是外国人，爸爸应该也是外国人，爸爸和妈妈聊天的时候是用英语，但是爸爸和儿子却是用中文，他们还有一个坐在婴儿车里的小女儿，儿子坐在湖边玩ipad，小女儿在车里哭个不停…\u003c/p\u003e\n\u003cp\u003e虽然颐和园的风景很好，但是里面居然有很多蚊子（小虫子）！期间有一个虫子撞到我的嘴巴，还有一个钻进了我的耳朵。可能是春天到了，万物复苏，加上颐和园内潮湿，植物丰富，给虫子提供了很好的环境。\u003c/p\u003e\n\u003cp\u003e下面放几张游玩时拍的照片。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"谁能告诉我这是什么乐器\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace1.jpg\"\u003e\n谁能告诉我这是什么乐器\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"学太极的外国人\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace2.jpg\"\u003e\n学太极的外国人\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"这也是迎春花？\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace3.jpg\"\u003e\n这也是迎春花？\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"这个角度的颐和园很美\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace4.jpg\"\u003e\n这个角度的颐和园很美\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"长廊\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace5.jpg\"\u003e\n长廊\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"准备回去的时候看到的，各种长枪大炮，据说是在抓拍一种珍稀鸟类。。。\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/Summer-Palace6.jpg\"\u003e\n准备回去的时候看到的，各种长枪大炮，据说是在抓拍一种珍稀鸟类。。。\u003c/p\u003e\n\u003cp\u003e颐和园的风景虽好，但就自然风光来说，比不上南方早已复苏的大自然。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"家乡的油菜花，比颐和园的迎春花漂亮多了-:)\" loading=\"lazy\" src=\"/posts/2015-03-30-one-month-in-beijing/rape-flowers.jpg\"\u003e\n家乡的油菜花，比颐和园的迎春花漂亮多了-:)\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"武大樱花已经争奇斗艳了，北方还是萧瑟一片\" loading=\"lazy\" src=\"https://i0.wp.com/i2.chinanews.com/simg/hd/2015/03/21/7e9527ca232f4253bfb86eef15cee517.jpg\"\u003e\n武大樱花已经争奇斗艳了，北方还是萧瑟一片[2]\u003c/p\u003e\n\u003ch1 id=\"后记\"\u003e后记\u003c/h1\u003e\n\u003cp\u003e没来北京之前，我想着以后一定要在北京工作，还要争取在北京安家落户。虽然北京生活压力很大，但是机会和挑战也多，这不就是我要的生活吗；而且北京是全国的政治、经济和文化中心，可以很方便的去感受中国几千年的文化和历史。在北京生活一个月之后，我的想法变了，很重要的原因是北京的雾霾和气候。雾霾不是一个人一天两天能够解决的，你必须尽量减少不必要的出行，跑步健身什么的就更别提了；北京属于北温带季风气候，冬天很干燥，而且放眼望去看不到绿色，ict旁边种的树，光秃秃的只有树干。这种钢筋混凝土构筑起来的城市，我真的不喜欢。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e和南方的一些城市相比，北京真的不适宜居住，尤其对于南方人！\u003c/strong\u003e\u003c/p\u003e\n\u003ch1 id=\"参考\"\u003e参考\u003c/h1\u003e\n\u003cp\u003e[1]. 新京报网：\u003ca href=\"http://www.bjnews.com.cn/news/2015/03/28/358122.html\"\u003ehttp://www.bjnews.com.cn/news/2015/03/28/358122.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[2]. 中新网：\u003ca href=\"http://www.chinanews.com/tp/hd2011/2015/03-21/496005.shtml\"\u003ehttp://www.chinanews.com/tp/hd2011/2015/03-21/496005.shtml\u003c/a\u003e\u003c/p\u003e","title":"One month in Beijing"},{"content":"在上一题POJ Problem 1837: Balance中，我们曾讲到，如果只有两个挂钩，问题会好办得多，就拿题目给的样例数据来说：\nSample Input 2 4 -2 3 3 4 5 8 Sample Output 2 如上图所示，给定重量为3,4,5,8的砝码，放在一个左右臂长分别为2和3的天平上，要使天平平衡，问有多少种方法。\n这个问题可以稍加转换，假设放在左边的重量为x，右边为y，则有如下方程组成立：\n$$ \\begin{cases} x+y=3+4+5+8=20\\\\ 2x=3y \\end{cases} $$马上解出x=12,y=8。这样就相当于把原问题转换为：已知序列3,4,5,8，问从中取若干个数使和为12（或8）的方案数有多少个？ 因为取出数字和为8，则剩余和为12，所以和为8和12的方案数是相等的。\n因为这里只有4个数字，一眼就能看出有(3,4,5)，(4,8)能使和为12，即只有两种方案。如果给的数字较多较大，该怎样写代码求出呢？可以使用动态规划求解。\n设dp[i][j]表示从前i个数中选若干个数使得和为j的方案数，则我们可以得到这样的状态转换方程：\n$$ \\begin{cases} dp[i][j]=1\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\text{if}i=0\\\u0026\\\u0026j=0\\\\ dp[i][j]=dp[i-1][j]\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\text{if}w[i]\u003ej\\\\ dp[i][j]=dp[i-1][j]+dp[i-1][j-w[i]]\\qquad\\quad\\text{if}w[i]\u003c=j \\end{cases} $$ 当i=0\u0026amp;\u0026amp;j=0时，dp[i][j]=1表示从0个数中取若干个数使得和为0，当然只有1种方案，那就是什么都不取 当w[i]\u0026gt;j时，第i个数用不上，因为你单个数字都超过j了，怎么使和为j呢，所以直接dp[i][j]=dp[i-1][j] 当w[i]\u0026lt;=j时，第i个数可以用了，这个时候分两种情况，用或者不用第i个数，如果不用，则和w[i]\u0026gt;j时一样dp[i][j]=dp[i-1][j]，如果用的话，则要从前i-1个数中取若干个数使和为j-w[i]，也就是dp[i-1][j-w[i]]，这样总的方案数就是用和不用第i个数的方案数之和，即dp[i][j]=dp[i-1][j]+dp[i-1][j-w[i]] 下面是针对这个例子我手算的一个图：\n以上面的内容设计一个OJ题如下：\n描述： 给定一个正整数数字序列，从中取出若干个数字，使得这些数字之和为某一个特定的值，求所有取法的方案数。 输入： 输入包含多个测试用例，每个测试用例的第一行有两个数N,S，N表示这个数字序列共有多少个数字；S表示取出的数字之和为S。后面一行包含N个正整数。 N,S为0程序结束 输出： 每个测试用例输出一行，表示从N个数中取若干个数使得和为S的方案总数。 样例输入： 4 8 3 4 5 8 4 12 3 4 5 8 10 10 10 9 8 7 6 5 4 3 2 1 0 0 样例输出： 2 2 10 知道了状态转换方程，我们可以很快的写出以上OJ的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #include \u0026lt;algorithm\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; int main() { int n, s, sum; while (cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; s \u0026amp;\u0026amp; n \u0026amp;\u0026amp; s) { vector\u0026lt;int\u0026gt; w(n + 1); vector\u0026lt;int\u0026gt; dp(s + 1, 0); sum = 0; w[0] = 0; /* 额外添加的第0个数字为0 */ for (int i = 1; i \u0026lt;= n; i++) { cin \u0026gt;\u0026gt; w[i]; sum += w[i]; /* 所有数字之和 */ } if (sum \u0026lt; s) /* 如果所有数字加起来都小于s，则怎么取都不存在和为s的方案 */ { cout \u0026lt;\u0026lt; \u0026#34;0\u0026#34; \u0026lt;\u0026lt; endl; continue; } sort(w.begin(), w.end()); /* 首先对这些数字从小到大排序，因为取大的数字的时候会用到取小的数字的结果 */ dp[0] = 1; /* 相当于dp[0][0]=1; */ for (int i = 1; i \u0026lt;= n; i++) { for (int j = s; j \u0026gt;= 1; j – ) /* 从后往前测试，这样只需要一行空间 */ { if (w[i] \u0026lt;= j) dp[j] += dp[j - w[i]]; } } cout \u0026lt;\u0026lt; dp[s] \u0026lt;\u0026lt; endl; } return (0); } 代码中添加了几个操作，首先如果所有数字之和都小于s，则肯定无解；其次，我们先对数字序列从小到大排序，这样DP填表；最后我们填表的时候是从右往左填的，这样只需要一行空间dp[j]，而不是二维dp[i][j]。\n","permalink":"http://localhost:1313/posts/2014-11-15-subset-sum-problem/","summary":"\u003cp\u003e在上一题POJ Problem 1837: Balance中，我们曾讲到，如果只有两个挂钩，问题会好办得多，就拿题目给的样例数据来说：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eSample Input\n2 4\n-2 3\n3 4 5 8\n\nSample Output\n2\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cimg alt=\"number-balance.png\" loading=\"lazy\" src=\"/posts/2014-11-15-subset-sum-problem/number-balance.png\"\u003e\n如上图所示，给定重量为3,4,5,8的砝码，放在一个左右臂长分别为2和3的天平上，要使天平平衡，问有多少种方法。\u003c/p\u003e\n\u003cp\u003e这个问题可以稍加转换，假设放在左边的重量为x，右边为y，则有如下方程组成立：\u003c/p\u003e\n$$\n\\begin{cases}\nx+y=3+4+5+8=20\\\\\n2x=3y\n\\end{cases}\n$$\u003cp\u003e马上解出x=12,y=8。这样就相当于把原问题转换为：\u003cstrong\u003e已知序列3,4,5,8，问从中取若干个数使和为12（或8）的方案数有多少个？\u003c/strong\u003e 因为取出数字和为8，则剩余和为12，所以和为8和12的方案数是相等的。\u003c/p\u003e\n\u003cp\u003e因为这里只有4个数字，一眼就能看出有(3,4,5)，(4,8)能使和为12，即只有两种方案。如果给的数字较多较大，该怎样写代码求出呢？可以使用动态规划求解。\u003c/p\u003e\n\u003cp\u003e设dp[i][j]表示从前i个数中选若干个数使得和为j的方案数，则我们可以得到这样的状态转换方程：\u003c/p\u003e\n$$\n\\begin{cases}\ndp[i][j]=1\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\text{if}i=0\\\u0026\\\u0026j=0\\\\\ndp[i][j]=dp[i-1][j]\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\text{if}w[i]\u003ej\\\\\ndp[i][j]=dp[i-1][j]+dp[i-1][j-w[i]]\\qquad\\quad\\text{if}w[i]\u003c=j\n\\end{cases}\n$$\u003col\u003e\n\u003cli\u003e当i=0\u0026amp;\u0026amp;j=0时，dp[i][j]=1表示从0个数中取若干个数使得和为0，当然只有1种方案，那就是什么都不取\u003c/li\u003e\n\u003cli\u003e当w[i]\u0026gt;j时，第i个数用不上，因为你单个数字都超过j了，怎么使和为j呢，所以直接dp[i][j]=dp[i-1][j]\u003c/li\u003e\n\u003cli\u003e当w[i]\u0026lt;=j时，第i个数可以用了，这个时候分两种情况，用或者不用第i个数，如果不用，则和w[i]\u0026gt;j时一样dp[i][j]=dp[i-1][j]，如果用的话，则要从前i-1个数中取若干个数使和为j-w[i]，也就是dp[i-1][j-w[i]]，这样总的方案数就是用和不用第i个数的方案数之和，即dp[i][j]=dp[i-1][j]+dp[i-1][j-w[i]]\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e下面是针对这个例子我手算的一个图：\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"20141115_170507.jpg\" loading=\"lazy\" src=\"/posts/2014-11-15-subset-sum-problem/20141115_170507.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e以上面的内容设计一个OJ题如下：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e描述：\n给定一个正整数数字序列，从中取出若干个数字，使得这些数字之和为某一个特定的值，求所有取法的方案数。\n\n输入：\n输入包含多个测试用例，每个测试用例的第一行有两个数N,S，N表示这个数字序列共有多少个数字；S表示取出的数字之和为S。后面一行包含N个正整数。\nN,S为0程序结束\n\n输出：\n每个测试用例输出一行，表示从N个数中取若干个数使得和为S的方案总数。\n\n样例输入：\n4 8\n3 4 5 8\n4 12\n3 4 5 8\n10 10\n10 9 8 7 6 5 4 3 2 1\n0 0\n\n样例输出：\n2\n2\n10\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e知道了状态转换方程，我们可以很快的写出以上OJ的代码：\u003c/p\u003e","title":"从一个数字序列中取若干个数字使得和为某个数，问共有多少种取数方案？"},{"content":"一、第一次使用Github的步骤： 在这个页面中填写Repo名称 不要勾选Initialize this repository with a README 点击Create repository 在本地使用Git命令行工具进入到和第1步填写Repo相同名称的文件夹中（此文件夹中已包含你要push到Github上的内容），执行以下几个命令： 1 2 3 4 5 6 git init touch README.md #optional git add . git commit -m \u0026#39;your comment\u0026#39; git remote add origin https://github.com/UserName/RepoName git push origin master 如果你在第2步中勾选了Initialize this repository with a README，那么在第4步中省略touch README.md并且在git add .之前，执行第5行代码，然后git pull origin master将远端（remote）的内容pull到本地 关于Git命令中的fetch和pull的区别，请看这篇博文 关于Git bash和Github的连接，请看这篇博文 二、Git命令中fetch和pull的区别（转载） Git中从远程的分支获取最新的版本到本地有这样2个命令：\ngit fetch：相当于是从远程获取最新版本到本地，不会自动merge 1 2 3 git fetch origin master git log -p master..origin/master git merge origin/master 以上命令的含义：首先从远程的origin的master主分支下载最新的版本到origin/master分支上，然后比较本地的master分支和origin/master分支的差别，最后进行合并。\n上述过程其实可以用以下更清晰的方式来进行：\n1 2 3 git fetch origin master:tmp git diff tmp git merge tmp 从远程获取最新的版本到本地的test分支上，之后再进行比较合并。\ngit pull：相当于是从远程获取最新版本并merge到本地 1 git pull origin master 上述命令其实相当于git fetch + git merge。\n在实际使用中，git fetch更安全一些，因为在merge前，我们可以查看更新情况，然后再决定是否合并。\n","permalink":"http://localhost:1313/posts/2014-11-11-git-notes/","summary":"\u003ch1 id=\"一第一次使用github的步骤\"\u003e一、第一次使用Github的步骤：\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e在\u003ca href=\"https://github.com/new\"\u003e这个页面\u003c/a\u003e中填写Repo名称\u003c/li\u003e\n\u003cli\u003e不要勾选Initialize this repository with a README\u003c/li\u003e\n\u003cli\u003e点击Create repository\u003c/li\u003e\n\u003cli\u003e在本地使用Git命令行工具进入到和第1步填写Repo相同名称的文件夹中（此文件夹中已包含你要push到Github上的内容），执行以下几个命令：\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\n\u003ctable style=\"border-spacing:0;padding:0;margin:0;border:0;\"\u003e\u003ctr\u003e\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e1\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e2\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e3\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e4\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e5\n\u003c/span\u003e\u003cspan style=\"white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f\"\u003e6\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd style=\"vertical-align:top;padding:0;margin:0;border:0;;width:100%\"\u003e\n\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit init\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etouch README.md \u003cspan style=\"color:#75715e\"\u003e#optional\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit add .\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit commit -m \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;your comment\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit remote add origin https://github.com/UserName/RepoName\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit push origin master\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003col start=\"5\"\u003e\n\u003cli\u003e如果你在第2步中勾选了Initialize this repository with a README，那么在第4步中省略touch README.md并且在git add .之前，执行第5行代码，然后git pull origin master将远端（remote）的内容pull到本地\u003c/li\u003e\n\u003cli\u003e关于Git命令中的fetch和pull的区别，请看\u003ca href=\"https://blog.csdn.net/wfdtxz/article/details/8632811\"\u003e这篇博文\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e关于Git bash和Github的连接，请看\u003ca href=\"https://www.cnblogs.com/fnng/archive/2011/08/25/2153807.html\"\u003e这篇博文\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"二git命令中fetch和pull的区别转载\"\u003e二、Git命令中fetch和pull的区别（\u003ca href=\"https://blog.csdn.net/wfdtxz/article/details/8632811\"\u003e转载\u003c/a\u003e）\u003c/h1\u003e\n\u003cp\u003eGit中从远程的分支获取最新的版本到本地有这样2个命令：\u003c/p\u003e","title":"Git相关笔记"}]