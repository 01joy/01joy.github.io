<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>HMM on bitJoy</title>
    <link>http://localhost:1313/tags/hmm/</link>
    <description>Recent content in HMM on bitJoy</description>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Sun, 21 Aug 2016 18:46:01 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/hmm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>隐马尔可夫模型及其应用（2）学习问题&amp;识别问题</title>
      <link>http://localhost:1313/posts/2016-08-21-introduction-to-hmm-2/</link>
      <pubDate>Sun, 21 Aug 2016 18:46:01 +0800</pubDate>
      <guid>http://localhost:1313/posts/2016-08-21-introduction-to-hmm-2/</guid>
      <description>&lt;p&gt;上一回介绍了HMM的解码问题，今天我们介绍HMM的学习问题和识别问题，先来看学习问题。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;正如上一回结束时所说，&lt;strong&gt;HMM的学习问题&lt;/strong&gt;是：仅已知观测序列\(\vec y\)，要估计出模型参数组\(\vec\lambda=(\mu,A,B)\)，其中\(\mu\)为初始概率分布向量，\(A\)为转移概率矩阵，\(B\)为发射概率矩阵。&lt;/p&gt;
&lt;h1 id=&#34;算法设计&#34;&gt;算法设计&lt;/h1&gt;
&lt;p&gt;求解HMM的参数学习问题，就是求解如下的最优化问题：&lt;/p&gt;
$$\begin{equation} P(\vec Y = \vec y|\hat \lambda)=\max\limits_{\vec \lambda} P(\vec Y = \vec y|\vec \lambda)\end{equation}$$&lt;p&gt;也就是找一个参数\(\vec \lambda\)，使得模型在该参数下最有可能产生当前的观测\(\vec y\)。如果使用极大似然法求解，对于似然函数\(P(\vec Y=\vec y|\vec \lambda)=\sum\limits_{i_1,…,i_T}\mu_{i_1}b_{i_1y_1}a_{i_1i_2}…a_{i_{T-1}i_T}b_{i_Ty_T}\)而言，这个最大值问题的计算量过大，在实际中是不可能被采用的。为此，人们构造了一个递推算法，使其能相当合理地给出模型参数\(\vec \lambda\)的粗略估计。其核心思想是：并不要求备选\(\vec\lambda\)使得\(P(\vec Y=\vec y|\vec \lambda)\)达到最大或局部极大，而只要求使\(P(\vec Y=\vec y|\vec \lambda)\)相当大，从而使计算变为实际可能。&lt;/p&gt;
&lt;h1 id=&#34;em算法&#34;&gt;EM算法&lt;/h1&gt;
&lt;p&gt;为此，我们定义一个描述模型“趋势”的量\(Q(\vec\lambda^*|\vec\lambda)\)代替似然函数\(P(\vec Y=\vec y|\vec\lambda)\)，其定义为：&lt;/p&gt;
$$\begin{equation} Q(\vec\lambda^*|\vec\lambda)=\sum\limits_{\vec x}P(\vec x,\vec y|\vec\lambda)\ln P(\vec x,\vec y|\vec\lambda^*)\end{equation}$$&lt;p&gt;利用在\(0 &lt; x &lt; 1\)时，不等式\(\ln x\leq x-1\)成立，可以证明：&lt;/p&gt;
$$\begin{equation} Q(\vec\lambda^*|\vec\lambda)-Q(\vec\lambda|\vec\lambda)\leq P(\vec Y=\vec y|\vec\lambda^*)-P(\vec Y=\vec y|\vec\lambda)\end{equation}$$&lt;p&gt;由此可见，对于固定的\(\vec\lambda\)，只要\(Q(\vec\lambda^*|\vec\lambda)&gt;Q(\vec\lambda|\vec\lambda)\)，就有\(P(\vec Y=\vec y|\vec\lambda^*)&gt;P(\vec Y=\vec y|\vec\lambda)\)。于是想把模型\(\vec\lambda_m\)修改为更好的模型\(\vec\lambda_{m+1}\)，只需找\(\vec\lambda_{m+1}\)使得：&lt;/p&gt;
$$\begin{equation}Q(\vec\lambda_{m+1}|\vec\lambda_m)=\sup_{\vec\lambda}Q(\vec\lambda|\vec\lambda_m)\end{equation}$$&lt;p&gt;即只要把\(Q(\vec\lambda|\vec\lambda_m)\)关于\(\vec\lambda\)的最大值处取成\(\vec\lambda_{m+1}\)，就有\(P(\vec Y=\vec y|\vec\lambda_{m+1})&gt;P(\vec Y=\vec y|\vec\lambda_m)\)。&lt;/p&gt;
&lt;p&gt;这样得到的模型序列\(\{\vec\lambda_m\}\)能保证\(P(\vec Y=\vec y|\vec\lambda_m)\)关于\(m\)是严格递增的，虽然在这里还不能在理论上证明\(P(\vec Y=\vec y|\vec\lambda_m)\)收敛到\(\max_{\vec\lambda}P(\vec Y=\vec y|\vec\lambda)\)，但是当\(m\)充分大时，\(\vec\lambda_m\)也还能提供在实际中较为满意的粗略近似。&lt;/p&gt;</description>
    </item>
    <item>
      <title>隐马尔可夫模型及其应用（1）简介&amp;解码问题</title>
      <link>http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/</link>
      <pubDate>Sat, 20 Aug 2016 18:12:53 +0800</pubDate>
      <guid>http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://zh.wikipedia.org/wiki/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B&#34;&gt;隐马尔可夫模型&lt;/a&gt;（Hidden Markov Model, HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。&lt;/p&gt;
&lt;p&gt;先举一个简单的例子以直观地理解HMM的实质——韦小宝的骰子。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;hmm-2&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/hmm-2.jpg&#34;&gt;&lt;/p&gt;
&lt;p&gt;假设韦小宝有两个骰子，一个正常的骰子A，A以1/6的概率均等的出现每个点；一个不正常的骰子B，B出现5,6点数的概率为0.3，出现其他点数的概率为0.1。显然投掷B更容易出现大的点数。每次试验&lt;strong&gt;第一次投掷时&lt;/strong&gt;，韦小宝会以0.4的概率出千（即投掷B）。但是在一次试验中，韦小宝不太可能一直出千，所以骰子会在A、B之间转换，比如这次投了B，下次可能会以0.1的概率投A。A、B之间的转移概率如下图。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;hmm-1&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/hmm-1.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;某一次试验，我们观察到韦小宝掷出的骰子序列为\(O=(1,3,4,5,5,6,6,3,2,6)\)，请问韦小宝什么时候出千了。这个问题就可以通过HMM求解。&lt;/p&gt;
&lt;p&gt;HMM有2个状态：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;观测状态。我们观察到的骰子序列称为观测状态\(\mathbf{Y}=\{y_1,y_2,…,y_T\}\)&lt;/li&gt;
&lt;li&gt;隐状态。隐含在每个观测状态里面的是隐状态\(\mathbf{X}=\{x_1,x_2,…,x_T\}\)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;T是时间，也可以认为是观测的次数。HMM有3个参数：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始分布\(\mathbf{\mu}=(\mu_i)\)，\(\mu_i=Pr(x_1=i)\)，即第一次观测时，每个隐状态出现的概率&lt;/li&gt;
&lt;li&gt;转移概率矩阵\(A=(a_{ij})\)，\(a_{ij}=Pr(x_{t+1}=j|x_t=i)\)，即t时刻的隐状态为i，t+1时刻转移到隐状态j的概率&lt;/li&gt;
&lt;li&gt;发射概率矩阵\(B=(b_{il})\)，\(b_{il}=Pr(y_t=l|x_t=i)\)，即t时候隐状态为i的情况下，观测到状态为l的概率&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;参数\(\mathbf{\lambda=\{\mu,A,B\}}\)称为HMM的模型参数。具体到上面的例子，我们有初始分布和转移概率为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;hmm-3&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/hmm-3.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;发射概率为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;hmm-4&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/hmm-4.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;观测状态为\(\mathbf{Y}=(1,3,4,5,5,6,6,3,2,6)\)，问题就是求解出隐状态\(\mathbf{X}\)，此问题被称为HMM的解码问题，可以由著名的&lt;a href=&#34;https://zh.wikipedia.org/wiki/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95&#34;&gt;维特比算法（Viterbi algorithm）&lt;/a&gt;解决。&lt;/p&gt;
&lt;p&gt;解码问题是要求出使得观测状态\(Y\)出现概率最大的隐状态\(X\)，假设有N个隐状态（本例为2），共有T个时刻（本例为10），则每个时刻有N个取值可能，则共有\(N^T\)条可能的隐状态链（本例为\(2^{10}\)）。我们需要求出每一条隐状态链下T个发射概率的乘积，然后取最大值，这是指数时间复杂度的（\(O(N^T)\)）。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;hmm-5&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/hmm-5.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;但是Viterbi算法是一个动态规划算法，只需多项式时间即可解决该问题。该算法的原理很好理解，假设我们求得到\(s_{i2}\)的最大概率路径为下图中的红线\(s_{11}\rightarrow s_{22}\rightarrow … s_{i2}\)，则在求经过\(s_{i2}\)到\(s_{(i+1)1}\)的最大概率路径时，不需要再测试\(s_{13}\rightarrow s_{21}\rightarrow s_{i2}\rightarrow s_{(i+1)1}\)这条路径（下图蓝线），因为显然已经知道红线概率大于蓝线概率了。图中还有很多类似蓝线的路径都可以不用计算了，大大提高了求解速度。&lt;/p&gt;
&lt;p&gt;因为计算第\(i+1\)时刻的累积概率只和第\(i\)时刻的概率有关，每次至多计算\(N*N\)个概率乘积（可以从\(i\)时刻的\(N\)个状态到达\(i+1\)时刻的某个状态，\(i+1\)时刻共有\(N\)个状态），最多计算T次（共T个时刻），所以时间复杂度降到了\(O(N^2T)\)。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;hmm-6&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/hmm-6.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;下面我们形式化的描述Viterbi算法。&lt;/p&gt;
&lt;p&gt;假设\(\delta_t(i)\)为\(t\)时刻取到隐状态\(i\)，且1~t的观测状态都符合观测值\(Y\)的各个路径的最大概率，即&lt;/p&gt;
$$
\begin{equation}\delta_t(i)=\underset{i_1,…,i_{t-1}}{\max}Pr(X_t=i,X_{t-1}=i_{t-1},…,X_1=i_1,Y_t=y_t,…,Y_1=y_1|\mathbf{\lambda})\end{equation}
$$&lt;p&gt;联系上图，可认为\(\delta_t(i)\)为红线。则递推公式为：&lt;/p&gt;
$$
\begin{equation}\delta_{t+1}(i)=b_{iy_{t+1}}\underset{j}{\max}(\delta_t(j)a_{ji})\end{equation}
$$&lt;p&gt;由\(j\)到\(i\)的转移概率，再乘上\(i\)发射\(y_{t+1}\)的概率。&lt;/p&gt;
&lt;p&gt;在初始时刻\(t=1\)，有：&lt;/p&gt;
$$
\begin{equation}\delta_1(i)=\mu_ib_{iy_1}\end{equation}
$$&lt;p&gt;最后的全局最大概率为\(\underset{j}{\max}\delta_T(j)\)。为了得到完整路径，我们保留每一隐状态取得最大概率时的上一隐状态，即：&lt;/p&gt;
$$
\begin{equation}\psi_{t+1}(i)=j^*\end{equation}
$$&lt;p&gt;其中\(j^*\)要满足&lt;/p&gt;
$$
\begin{equation}\delta_{t+1}(i)=b_{iy_{t+1}}\delta_t(j^*)a_{j^*i}\end{equation}
$$&lt;p&gt;最后使用如下回溯法得到所有最佳隐状态：&lt;/p&gt;
$$\begin{equation}X_T=i^*\in\{i:\delta_T(i)=\underset{j}{\max}\delta_T(j)\}\end{equation}$$$$\begin{equation}X_t=\psi_{t+1}(X_{t+1})\end{equation}$$&lt;p&gt;下面我们利用Viterbi算法来求解韦小宝的骰子这个例子。&lt;/p&gt;
&lt;p&gt;\(t=1\)时，\(y_1=1\)，有\(\delta_1(A)=0.6*1/6=0.1\)，\(\delta_1(B)=0.4*0.1=0.04\)。&lt;/p&gt;
&lt;p&gt;\(t=2\)时，\(y_2=3\)，有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;隐状态为A：a）A-&amp;gt;A有\(\delta_2(A)=(1/6)*0.1*0.8=1.33*10^{-2}\)；b）B-&amp;gt;A有\(\delta_2(A)=(1/6)*0.04*0.1=6.6*10^{-4}\)。所以A-&amp;gt;A，\(\psi_2(A)=A\)。&lt;/li&gt;
&lt;li&gt;隐状态为B：a）A-&amp;gt;B有\(\delta_2(B)=0.1*0.1*0.2=2*10^{-3}\)；b）B-&amp;gt;B有\(\delta_2(B)=0.1*0.04*0.9=3.6*10^{-3}\)。所以B-&amp;gt;B，\(\psi_2(B)=B\)。
如此计算下去，可以得到如下表：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt=&#34;hmm-7&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/hmm-7.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;\(t=10\)时最大概率为\(\delta_{10}(B)\)，经过回溯得到最佳隐状态为：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;hmm-8&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/2016-08-20-introduction-to-hmm-1/hmm-8.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;所以HMM很神奇吧，可以抓住韦小宝从第5次开始就一直在出千，而且出千之后，掷出的点数大部分为5和6。&lt;/p&gt;
&lt;p&gt;Viterbi算法还可用于解决语音识别或者拼音输入法。我们知道中文的一个拼音可以对应多个汉字，连续的一段拼音就能组成成千上万种可能的句子，哪一个句子才是最佳候选呢？我们可以把每个拼音当成观测状态，同音的汉字当成可能的隐状态。通过背景语料库统计得到每个汉字出现在词首的概率、汉字之间的转移概率和汉字与拼音之间的发射概率，这样我们就能得到模型参数，然后利用Viterbi算法求解出一个最佳的隐状态序列，这样就能完成一个简易的拼音输入法。&lt;/p&gt;
&lt;p&gt;HMM在实际中主要有3个方面的应用，分别是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从一段观测序列\(\mathbf{Y}\)及已知模型\(\mathbf{\lambda=(\mu,A,B)}\)出发，估计出隐状态\(\mathbf{X}\)的最佳值，称为解码问题，这是状态估计问题。这篇博客讨论的就是这个问题。&lt;/li&gt;
&lt;li&gt;从一段观测序列\(\mathbf{Y}\)出发，估计模型参数组\(\mathbf{\lambda=(\mu,A,B)}\)，称为学习问题，就是参数估计问题。&lt;/li&gt;
&lt;li&gt;对于一个特定的观测链\(\mathbf{Y}\)，已知它可能是由已经学习好的若干模型之一所得的观测，要决定此观测究竟是得自其中哪一个模型，这称为识别问题，就是分类问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;关于HMM的学习问题和识别问题，请听下回分解。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
