<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>搜索引擎 on bitJoy</title>
    <link>http://localhost:1313/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/</link>
    <description>Recent content in 搜索引擎 on bitJoy</description>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Thu, 07 Jan 2016 19:07:17 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>和我一起构建搜索引擎（三）构建索引</title>
      <link>http://localhost:1313/posts/2016-01-07-introduction-to-building-a-search-engine-3/</link>
      <pubDate>Thu, 07 Jan 2016 19:07:17 +0800</pubDate>
      <guid>http://localhost:1313/posts/2016-01-07-introduction-to-building-a-search-engine-3/</guid>
      <description>&lt;p&gt;目前正是所谓的“大数据”时代，数据量多到难以计数，怎样结构化的存储以便于分析计算，是当前的一大难题。上一篇博客我们简单抓取了1000个搜狐新闻数据，搜索的过程就是从这1000个新闻中找出和关键词相关的新闻来，那么怎样快速搜索呢，总不可能依次打开xml文件一个字一个字的找吧，这时就需要借助倒排索引这个强大的数据结构。&lt;/p&gt;
&lt;p&gt;在讲倒排索引之前，我们先介绍一下布尔检索。布尔检索只是简单返回包含某个关键词的文档，比如查询“苹果手机”，则返回所有包含“苹果”和“手机”关键词的文档，布尔检索并不对返回结果排序，所以有可能返回的第一个文档是“某个男孩边吃苹果边玩手机…“。&lt;/p&gt;
&lt;p&gt;实现布尔检索并不难，我们需要构建一个如下图的词项文档矩阵：&lt;/p&gt;
&lt;p&gt;图1. 布尔检索中的词项文档矩阵
图1. 布尔检索中的词项文档矩阵
每行对应一个词项，每列对应一个文档，如果该值为1，表示该行词项出现在该列文档中。比如词项”苹果“出现在doc1和doc3文档中，如果我们要找同时出现”苹果“和”手机“的文档，只需把他们对应的向量取出来进行”与“操作，此为101&amp;amp;011=001，所以doc3同时出现了”苹果“和”手机“两个关键词，我们将其返回。&lt;/p&gt;
&lt;p&gt;布尔检索虽然很快，但是它也有很多缺陷，比如不能对结果排序，词项只有出现和不出现两种状态，但是一篇文档中出现10次“苹果“和只出现1次”苹果“，他们的相关度肯定是不相同的。所以需要对布尔检索进行改进。&lt;/p&gt;
&lt;p&gt;在扫描文档时，不但记录某词项出现与否，还记录该词项出现的次数，即词项频率(tf)；同时我们记录该文档的长度(ld)，以及某词项在不同文档中出现的次数，即文档频率(df)。&lt;/p&gt;
&lt;p&gt;图2. 倒排索引结构图
图2. 倒排索引结构图
这样我们就得到了如上图的倒排索引。左边部分被称为词典，存储的是1000个新闻中所有不同的词项；右边部分被称为倒排记录表，存储的是出现Term_i的那些文档信息。倒排索引中存储的变量都是为了给后续检索模型使用。&lt;/p&gt;
&lt;p&gt;讲到这里，我们需要解决如下几个问题。&lt;/p&gt;
&lt;p&gt;怎样得到一篇文档中的所有词项。给我们一篇新闻稿子，人类很容易分辨出”苹果“和”手机“是两个不同的词项，但是计算机怎么知道是这两个词呢？为什么不是”苹”、”国手“和”机“呢？这就需要进行中文分词，我们可以借助开源的jieba中文分词组件来完成，jieba分词能够将一个中文句子切成一个个词项，这样我们就可以统计tf, df了。
有些词，如”的“、”地“、”得“、”如果“等，几乎每篇文档都会出现，他们起不到很好的区分文档的效果，这类词被称为”停用词“，我们需要把他们去掉。去停词的步骤可以在jieba分词之后完成。
怎样存储倒排记录表。假设1000个文档共有20000个不同的词项，如果用类似图1的矩阵形式存储，需要耗费1000&lt;em&gt;20000=2&lt;/em&gt;10^7个存储单元，但是图1往往是一个稀疏矩阵，因为一个文档中可能只出现了200个不同的词项，剩余的19800个词项都是空的。用矩阵方式存储时空效率都不高。所以我们可以采用图2的方式，词典用B-树或hash存储，倒排记录表用邻接链表存储方式，这样能大大减少存储空间。如果我们要将图2保存到数据库，可以对倒排记录表序列化成一个长的字符串，写入到一个单元格，读取的时候再反序列化。比如每个Doc内部用’\t’连接，Doc之间用’\n’连接，读取的时候split即可。
倒排索引构建算法使用内存式单遍扫描索引构建方法（SPIMI），其实就是依次对每篇新闻进行分词，如果出现新的词项则插入到词典中，否则将该文档的信息追加到词项对应的倒排记录表中。SPIMI的伪代码如下：&lt;/p&gt;
&lt;p&gt;图3. SPIMI算法伪代码
图3. SPIMI算法伪代码
下面是构建索引的所有代码：&lt;/p&gt;
&lt;p&gt;[python]&lt;/p&gt;
&lt;h1 id=&#34;---coding-utf-8---&#34;&gt;-&lt;em&gt;- coding: utf-8 -&lt;/em&gt;-&lt;/h1&gt;
&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;
Created on Sat Dec 5 23:31:22 2015&lt;/p&gt;
&lt;p&gt;@author: bitjoy.net
&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;
&lt;p&gt;from os import listdir
import xml.etree.ElementTree as ET
import jieba
import sqlite3
import configparser&lt;/p&gt;
&lt;p&gt;class Doc:
docid = 0
date_time = ”
tf = 0
ld = 0
def &lt;strong&gt;init&lt;/strong&gt;(self, docid, date_time, tf, ld):
self.docid = docid
self.date_time = date_time
self.tf = tf
self.ld = ld
def &lt;strong&gt;repr&lt;/strong&gt;(self):
return(str(self.docid) + ‘\t’ + self.date_time + ‘\t’ + str(self.tf) + ‘\t’ + str(self.ld))
def &lt;strong&gt;str&lt;/strong&gt;(self):
return(str(self.docid) + ‘\t’ + self.date_time + ‘\t’ + str(self.tf) + ‘\t’ + str(self.ld))&lt;/p&gt;</description>
    </item>
    <item>
      <title>和我一起构建搜索引擎（一）简介</title>
      <link>http://localhost:1313/posts/2016-01-04-introduction-to-building-a-search-engine-1/</link>
      <pubDate>Mon, 04 Jan 2016 13:56:23 +0800</pubDate>
      <guid>http://localhost:1313/posts/2016-01-04-introduction-to-building-a-search-engine-1/</guid>
      <description>&lt;p&gt;我们上网用得最多的一项服务应该是搜索，不管大事小情，都喜欢百度一下或谷歌一下，那么百度和谷歌是怎样从浩瀚的网络世界中快速找到你想要的信息呢，这就是搜索引擎的艺术，属于信息检索的范畴。&lt;/p&gt;
&lt;p&gt;这学期学习了《现代信息检索》课程，使用的是Stanford的教材&lt;a href=&#34;http://nlp.stanford.edu/IR-book/&#34;&gt;Introduction to Information Retrieval&lt;/a&gt;，网上有电子版，大家可以参考。&lt;/p&gt;
&lt;p&gt;本课程的大作业是完成一个新闻搜索引擎，要求是这样的：定向采集3-4个新闻网站，实现这些网站信息的抽取、索引和检索。网页数目不少于10万条。能按相关度、时间和热度（需要自己定义）进行排序，能实现相似新闻的自动聚类。&lt;/p&gt;
&lt;p&gt;截止日期12月31日，我们已经在规定时间完成了该系统，自认为检索效果不错，所以在此把过程记录如下，欢迎讨论。&lt;/p&gt;
&lt;p&gt;网上有很多开源的全文搜索引擎，比如Lucene、Sphinx、Whoosh等，都提供了很好的API，开发者只需要调用相关接口就可以实现一个全功能的搜索引擎。不过既然学习了IR这门课，自然要把相关技术实践一下，所以我们打算自己实现一个搜索引擎。&lt;/p&gt;
&lt;p&gt;这是简介部分，主要介绍整个搜索引擎的思路和框架。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;search engine outline&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/2016-01-04-introduction-to-building-a-search-engine-1/search-engine-architecture.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图为本搜索引擎的框架图。首先爬虫程序从特定的几个新闻网站抓取新闻数据，然后过滤网页中的图片、视频、广告等无关元素，抽取新闻的主体内容，得到结构化的xml数据。然后一方面使用内存式单遍扫描索引构建方法（SPIMI）构建倒排索引，供检索模型使用；另一方面根据向量空间模型计算两两新闻之间的余弦相似度，供推荐模块使用。最后利用概率检索模型中的BM25公式计算给定关键词下的文档相关性评分，BM25打分结合时间因素得到热度评分，根据评分给出排序结果。&lt;/p&gt;
&lt;p&gt;在后续博文中，我会详细介绍每个部分的实现。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;完整可运行的新闻搜索引擎Demo请看我的Github项目&lt;a href=&#34;https://github.com/01joy/news_search_engine&#34;&gt;news_search_engine&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;以下是系列博客：&lt;/p&gt;
&lt;p&gt;和我一起构建搜索引擎（一）简介&lt;/p&gt;
&lt;p&gt;和我一起构建搜索引擎（二）网络爬虫&lt;/p&gt;
&lt;p&gt;和我一起构建搜索引擎（三）构建索引&lt;/p&gt;
&lt;p&gt;和我一起构建搜索引擎（四）检索模型&lt;/p&gt;
&lt;p&gt;和我一起构建搜索引擎（五）推荐阅读&lt;/p&gt;
&lt;p&gt;和我一起构建搜索引擎（六）系统展示&lt;/p&gt;
&lt;p&gt;和我一起构建搜索引擎（七）总结展望&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
