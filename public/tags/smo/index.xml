<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>SMO on bitJoy</title>
    <link>http://localhost:1313/tags/smo/</link>
    <description>Recent content in SMO on bitJoy</description>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Mon, 02 May 2016 15:12:39 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/smo/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SVM之序列最小最优化算法（SMO算法）</title>
      <link>http://localhost:1313/posts/2016-05-02-svm-smo-algorithm/</link>
      <pubDate>Mon, 02 May 2016 15:12:39 +0800</pubDate>
      <guid>http://localhost:1313/posts/2016-05-02-svm-smo-algorithm/</guid>
      <description>&lt;p&gt;SVM回顾&lt;/p&gt;
&lt;p&gt;支持向量机（SVM）的一大特点是最大化间距（max margin）。对于如上图的二分类问题，虽然有很多线可以将左右两部分分开，但是只有中间的红线效果是最好的，因为它的可活动范围（margin）是最大的，从直观上来说很好理解。&lt;/p&gt;
&lt;p&gt;对于线性二分类问题，假设分类面为&lt;/p&gt;
$$\begin{equation} u=\vec w \cdot \vec x-b \end{equation}$$&lt;p&gt;则margin为&lt;/p&gt;
$$\begin{equation} m=\frac{1}{||w||_2} \end{equation}$$&lt;p&gt;根据max margin规则和约束条件，得到如下优化问题，我们要求的就是参数$\vec w$和$b$：&lt;/p&gt;
$$\begin{equation} \min\limits_{\vec w,b}\frac{1}{2}||\vec w||^2 \quad\text{subject to}\quad y_i(\vec w\cdot \vec x_i-b) \geq 1, \forall i,\end{equation}$$&lt;p&gt;对于正样本，类标号$y_i$为+1，反之则为-1。根据拉格朗日对偶，(3)可以转换为如下的二次规划（QP）问题，其中$\alpha_i$为拉格朗日乘子。&lt;/p&gt;
$$\begin{equation} \min\limits_{\vec \alpha}\Psi(\vec\alpha)=\min\limits_{\vec \alpha}\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Ny_iy_j(\vec x_i\cdot\vec x_j)\alpha_i\alpha_j-\sum_{i=1}^N\alpha_i,\end{equation}$$&lt;p&gt;其中N为样本数量。上式还需满足如下两个约束条件：&lt;/p&gt;
$$\begin{equation} \alpha_i\geq 0, \forall i,\end{equation}$$$$\begin{equation} \sum_{i=1}^Ny_i\alpha_i=0.\end{equation}$$&lt;p&gt;一旦求解出所有的拉格朗日乘子，则我们可以通过如下的公式得到分类面参数$\vec w$和$b$。&lt;/p&gt;
$$\begin{equation}\vec w=\sum_{i=1}^Ny_i\alpha_i\vec x_i,\quad b=\vec w\cdot\vec x_k-y_k\quad\text{for some}\quad\alpha_k&gt;0.\end{equation}$$&lt;p&gt;当然并不是所有的数据都可以完美的线性划分，可能有少量数据就是混在对方阵营，这时可以通过引入松弛变量$\xi_i$得到软间隔形式的SVM：&lt;/p&gt;
$$\begin{equation} \min\limits_{\vec w,b,\vec\xi}\frac{1}{2}||\vec w||^2+C\sum_{i=1}^N\xi_i \quad\text{subject to}\quad y_i(\vec w\cdot \vec x_i-b) \geq 1-\xi_i, \forall i,\end{equation}$$&lt;p&gt;其中的$\xi_i$为松弛变量，能假装把错的样本分对，$C$对max margin和margin failures的trades off。对于这个新的优化问题，约束变成了一个box constraint：&lt;/p&gt;
$$\begin{equation}0\leq\alpha_i \leq C,\forall i.\end{equation}$$&lt;p&gt;而松弛变量$\xi_i$不再出现在对偶公式中了。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
